[
  {
    "title": "Untitled",
    "url": "posts/数学笔记/01.ProximalGradientMethod.html",
    "category": "数学笔记",
    "tags": [],
    "excerpt": "&lt;h2 id=\"数学知识\"&gt;数学知识&lt;a class=\"anchor-link\" href=\"#数学知识\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;可导：即设&lt;span class=\"math-inline\"&gt;y=f(x)&lt;/span&gt; 一个单变量函数， 如果&lt;span class=\"math-inline\"&gt;y&lt;/span&gt; &lt;span class=\""
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/20.克拉默法则、逆矩阵、体积.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;h2 id=\"求逆矩阵\"&gt;求逆矩阵&lt;a class=\"anchor-link\" href=\"#求逆矩阵\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;我们从逆矩阵开始，对于二阶矩阵有&lt;span class=\"math-inline\"&gt;\\begin{bmatrix}a&amp;b\\c&amp;d\\end{bmatrix}^{-1}=\\frac{1}{ad-b"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/32.基变换和图像压缩.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;h2 id=\"图像压缩\"&gt;图像压缩&lt;a class=\"anchor-link\" href=\"#图像压缩\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;本讲我们介绍一种图片有损压缩的一种方法：JPEG。&lt;/p&gt;\n&lt;p&gt;假设我们有一张图片，长宽皆为&lt;span class=\"math-inline\"&gt;512&lt;/span&gt;个像素，我们用&lt;span class=\"m"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/12.图和网络.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;h2 id=\"图和网络\"&gt;图和网络&lt;a class=\"anchor-link\" href=\"#图和网络\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;pre&gt;&lt;code class=\"language-python\"&gt;import networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/10.四个基本子空间.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;p&gt;对于&lt;span class=\"math-inline\"&gt;m \\times n&lt;/span&gt;矩阵&lt;span class=\"math-inline\"&gt;A&lt;/span&gt;，&lt;span class=\"math-inline\"&gt;rank(A)=r&lt;/span&gt;有：&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;行空间&lt;span class=\"math-inline\"&gt;C(A^T) \\in \\mathbb{R}^n,"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/19.行列式公式和代数余子式.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;p&gt;上一讲中，我们从三个简单的性质扩展出了一些很好的推论，本讲将继续使用这三条基本性质：&lt;/p&gt;\n&lt;ol&gt;\n&lt;li&gt;&lt;span class=\"math-inline\"&gt;\\det I=1&lt;/span&gt;；&lt;/li&gt;\n&lt;li&gt;交换行行列式变号；&lt;/li&gt;\n&lt;li&gt;对行列式的每一行都可以单独使用线性运算，其值不变；&lt;/li&gt;\n&lt;/ol&gt;\n&lt;p&gt;我们使用这三条性质推导二阶方阵行列式：&lt;/p&gt;\n&lt;p&gt;&lt;di"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/30.奇异值分解.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;p&gt;本讲我们介绍将一个矩阵写为&lt;span class=\"math-inline\"&gt;[Math Processing Error]A=U\\varSigma V^T&lt;/span&gt;，分解的因子分别为正交矩阵、对角矩阵、正交矩阵，与前面几讲的分解不同的是，这两个正交矩阵通常是不同的，而且这个式子可以对任意矩阵使用，不仅限于方阵、可对角化的方阵等。&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;在正定一讲中（第二十八"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/23.微分方程.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;h2 id=\"微分方程fracmathrmdumathrmdtau\"&gt;微分方程&lt;span class=\"math-inline\"&gt;\\frac{\\mathrm{d}u}{\\mathrm{d}t}=Au&lt;/span&gt;&lt;a class=\"anchor-link\" href=\"#微分方程fracmathrmdumathrmdtau\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/"
  },
  {
    "title": "第二讲：矩阵消元",
    "url": "posts/数学笔记/02.线性代数/02.矩阵消元.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;h1 id=\"第二讲矩阵消元\"&gt;第二讲：矩阵消元&lt;a class=\"anchor-link\" href=\"#第二讲矩阵消元\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;这个方法最早由高斯提出，我们以前解方程组的时候都会使用，现在来看如何使用矩阵实现消元法。&lt;/p&gt;\n&lt;h2 id=\"消元法\"&gt;消元法&lt;a class=\"anchor-link\" href=\""
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/22.对角化和A的幂.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;h2 id=\"对角化矩阵\"&gt;对角化矩阵&lt;a class=\"anchor-link\" href=\"#对角化矩阵\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;上一讲我们提到关键方程&lt;span class=\"math-inline\"&gt;Ax=\\lambda x&lt;/span&gt;，通过&lt;span class=\"math-inline\"&gt;\\det(A-\\lambda I"
  },
  {
    "title": "第四讲：$A$ 的 $LU$ 分解",
    "url": "posts/数学笔记/02.线性代数/04.矩阵的LU分解.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;h1 id=\"第四讲a-的-lu-分解\"&gt;第四讲：&lt;span class=\"math-inline\"&gt;A&lt;/span&gt; 的 &lt;span class=\"math-inline\"&gt;LU&lt;/span&gt; 分解&lt;a class=\"anchor-link\" href=\"#第四讲a-的-lu-分解\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;&lt;span class=\""
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/16.投影矩阵和最小二乘.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;p&gt;上一讲中，我们知道了投影矩阵&lt;span class=\"math-inline\"&gt;P=A(A^TA)^{-1}A^T&lt;/span&gt;，&lt;span class=\"math-inline\"&gt;Pb&lt;/span&gt;将会把向量投影在&lt;span class=\"math-inline\"&gt;A&lt;/span&gt;的列空间中。&lt;/p&gt;\n&lt;p&gt;举两个极端的例子： &lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;如果&lt;span class=\"ma"
  },
  {
    "title": "线性代数",
    "url": "posts/数学笔记/02.线性代数/00.线性代数知识点总结.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;h1 id=\"线性代数\"&gt;线性代数&lt;a class=\"anchor-link\" href=\"#线性代数\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;h2 id=\"行列式\"&gt;行列式&lt;a class=\"anchor-link\" href=\"#行列式\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;1.行列式按"
  },
  {
    "title": "plt.plot([48/17], [12/17], 'o')",
    "url": "posts/数学笔记/02.线性代数/15.子空间投影.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;p&gt;从&lt;span class=\"math-inline\"&gt;\\mathbb{R}^2&lt;/span&gt;空间讲起，有向量&lt;span class=\"math-inline\"&gt;a, b&lt;/span&gt;，做&lt;span class=\"math-inline\"&gt;b&lt;/span&gt;在&lt;span class=\"math-inline\"&gt;a&lt;/span&gt;上的投影&lt;span class=\"math-inline\"&gt;p&lt;/sp"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/28.正定矩阵和最小值.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;p&gt;本讲我们会了解如何完整的测试一个矩阵是否正定，测试&lt;span class=\"math-inline\"&gt;x^TAx&lt;/span&gt;是否具有最小值，最后了解正定的几何意义——椭圆（ellipse）和正定性有关，双曲线（hyperbola）与正定无关。另外，本讲涉及的矩阵均为实对称矩阵。&lt;/p&gt;\n&lt;h2 id=\"正定性的判断\"&gt;正定性的判断&lt;a class=\"anchor-link\" href=\"#"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/25.对称矩阵及正定性.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;h2 id=\"对称矩阵\"&gt;对称矩阵&lt;a class=\"anchor-link\" href=\"#对称矩阵\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;前面我们学习了矩阵的特征值与特征向量，也了解了一些特殊的矩阵及其特征值、特征向量，特殊矩阵的特殊性应该会反映在其特征值、特征向量中。如马尔科夫矩阵，有一特征值为&lt;span class=\"math-inline"
  },
  {
    "title": "第六讲：列空间和零空间",
    "url": "posts/数学笔记/02.线性代数/06.列空间和零空间.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;h1 id=\"第六讲列空间和零空间\"&gt;第六讲：列空间和零空间&lt;a class=\"anchor-link\" href=\"#第六讲列空间和零空间\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;对向量子空间&lt;span class=\"math-inline\"&gt;S&lt;/span&gt;和&lt;span class=\"math-inline\"&gt;T&lt;/span&gt;，有&lt;span cl"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/31.线性变换及对应矩阵.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;p&gt;如何判断一个操作是不是线性变换？线性变换需满足以下两个要求：&lt;/p&gt;\n&lt;p&gt;&lt;div class=\"math-display\"&gt;&lt;br /&gt;\nT(v+w)=T(v)+T(w)\\&lt;br /&gt;\nT(cv)=cT(v)&lt;br /&gt;\n&lt;/div&gt;&lt;/p&gt;\n&lt;p&gt;即变换&lt;span class=\"math-inline\"&gt;T&lt;/span&gt;需要同时满足加法和数乘不变的性质。将两个性质合成一个式子为：&lt;sp"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/03.乘法和逆矩阵.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;h2 id=\"乘法和逆矩阵\"&gt;乘法和逆矩阵&lt;a class=\"anchor-link\" href=\"#乘法和逆矩阵\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;上一讲大概介绍了矩阵乘法和逆矩阵，本讲就来做进一步说明。&lt;/p&gt;\n&lt;h3 id=\"矩阵乘法\"&gt;矩阵乘法&lt;a class=\"anchor-link\" href=\"#矩阵乘法\" title=\"Perm"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/33.左右逆和伪逆.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;p&gt;前面我们涉及到的逆（inverse）都是指左、右乘均成立的逆矩阵，即&lt;span class=\"math-inline\"&gt;A^{-1}A=I=AA^{-1}&lt;/span&gt;。在这种情况下，&lt;span class=\"math-inline\"&gt;m\\times n&lt;/span&gt;矩阵&lt;span class=\"math-inline\"&gt;A&lt;/span&gt;满足&lt;span class=\"math-inline\""
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/05.转换、置换、向量空间R.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;h2 id=\"转换置换向量空间r\"&gt;转换、置换、向量空间R&lt;a class=\"anchor-link\" href=\"#转换置换向量空间r\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h2 id=\"置换矩阵permutation-matrix\"&gt;置换矩阵（Permutation Matrix）&lt;a class=\"anchor-link\" href=\"#置换矩阵"
  },
  {
    "title": "第七讲：求解$Ax=0$，主变量，特解",
    "url": "posts/数学笔记/02.线性代数/07.求解Ax=0主变量——特解.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;h1 id=\"第七讲求解ax0主变量特解\"&gt;第七讲：求解&lt;span class=\"math-inline\"&gt;Ax=0&lt;/span&gt;，主变量，特解&lt;a class=\"anchor-link\" href=\"#第七讲求解ax0主变量特解\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;举例：&lt;span class=\"math-inline\"&gt;3 \\times 4&lt;"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/24.马尔科夫矩阵、傅里叶级数.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;h2 id=\"马尔科夫矩阵\"&gt;马尔科夫矩阵&lt;a class=\"anchor-link\" href=\"#马尔科夫矩阵\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;马尔科夫矩阵（Markov matrix）是指具有以下两个特性的矩阵：&lt;/p&gt;\n&lt;ol&gt;\n&lt;li&gt;矩阵中的所有元素&lt;strong&gt;大于等于&lt;/strong&gt;&lt;span class=\"math-in"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/18.行列式及其性质.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;p&gt;行列式（determinant）的性质：&lt;/p&gt;\n&lt;ol&gt;\n&lt;li&gt;\n&lt;p&gt;&lt;span class=\"math-inline\"&gt;\\det{I}=1&lt;/span&gt;，单位矩阵行列式值为一。&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;交换行行列式变号。&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ol&gt;\n&lt;p&gt;在给出第三个性质之前，先由前两个性质可知，对置换矩阵有&lt;span class=\"math-inline\"&gt;\\det"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/21.特征值和特征向量.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;h2 id=\"特征值特征向量的由来\"&gt;特征值、特征向量的由来&lt;a class=\"anchor-link\" href=\"#特征值特征向量的由来\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;给定矩阵&lt;span class=\"math-inline\"&gt;A&lt;/span&gt;，矩阵&lt;span class=\"math-inline\"&gt;A&lt;/span&gt;乘以向量&lt;span "
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/14.正交向量与子空间.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;p&gt;在四个基本子空间中，提到对于秩为&lt;span class=\"math-inline\"&gt;r&lt;/span&gt;的&lt;span class=\"math-inline\"&gt;m \\times n&lt;/span&gt;矩阵，其行空间（&lt;span class=\"math-inline\"&gt;dim C(A^T)=r&lt;/span&gt;）与零空间（&lt;span class=\"math-inline\"&gt;dim N(A)=n-r&lt;/span"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/17.正交矩阵和Gram-Schmidt正交化法.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;h2 id=\"标准正交矩阵\"&gt;标准正交矩阵&lt;a class=\"anchor-link\" href=\"#标准正交矩阵\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;定义标准正交向量（orthonormal）：&lt;span class=\"math-inline\"&gt;q_i^Tq_j=\\begin{cases}0\\quad i\\neq j\\1\\quad i=j\\en"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/09.线性相关性、基、维数.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;p&gt;&lt;span class=\"math-inline\"&gt;v_1,\\ v_2,\\ \\cdots,\\ v_n&lt;/span&gt;是&lt;span class=\"math-inline\"&gt;m\\times n&lt;/span&gt;矩阵&lt;span class=\"math-inline\"&gt;A&lt;/span&gt;的列向量：&lt;/p&gt;\n&lt;p&gt;如果&lt;span class=\"math-inline\"&gt;A&lt;/span&gt;零空间中有且仅有&lt;spa"
  },
  {
    "title": "第一讲：方程组的几何解释",
    "url": "posts/数学笔记/02.线性代数/01.方程组的几何解释.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;h1 id=\"第一讲方程组的几何解释\"&gt;第一讲：方程组的几何解释&lt;a class=\"anchor-link\" href=\"#第一讲方程组的几何解释\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;我们从求解线性方程组来开始这门课，从一个普通的例子讲起：方程组有&lt;span class=\"math-inline\"&gt;2&lt;/span&gt;个未知数，一共有&lt;span cl"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/26.对称矩阵及正定性.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;h2 id=\"对称矩阵\"&gt;对称矩阵&lt;a class=\"anchor-link\" href=\"#对称矩阵\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;前面我们学习了矩阵的特征值与特征向量，也了解了一些特殊的矩阵及其特征值、特征向量，特殊矩阵的特殊性应该会反映在其特征值、特征向量中。如马尔科夫矩阵，有一特征值为&lt;span class=\"math-inline"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/27.复数矩阵和快速傅里叶变换.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;p&gt;本讲主要介绍复数向量、复数矩阵的相关知识（包括如何做复数向量的点积运算、什么是复数对称矩阵等），以及傅里叶矩阵（最重要的复数矩阵）和快速傅里叶变换。&lt;/p&gt;\n&lt;h2 id=\"复数矩阵运算\"&gt;复数矩阵运算&lt;a class=\"anchor-link\" href=\"#复数矩阵运算\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;先介绍复数向量，我们不妨换一个字"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/11.矩阵空间、秩1矩阵和小世界图.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;h2 id=\"矩阵空间秩1矩阵和小世界图\"&gt;矩阵空间、秩1矩阵和小世界图&lt;a class=\"anchor-link\" href=\"#矩阵空间秩1矩阵和小世界图\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"矩阵空间\"&gt;矩阵空间&lt;a class=\"anchor-link\" href=\"#矩阵空间\" title=\"Permanent link\"&gt;&p"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/02.线性代数/29.相似矩阵和若尔当形.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;p&gt;在本讲的开始，先接着上一讲来继续说一说正定矩阵。&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;正定矩阵的逆矩阵有什么性质？我们将正定矩阵分解为&lt;span class=\"math-inline\"&gt;A=S\\Lambda S^{-1}&lt;/span&gt;，引入其逆矩阵&lt;span class=\"math-inline\"&gt;A^{-1}=S\\Lambda^{-1}S^{-1}&lt;/span&gt;，我们知道正定矩阵的特征值均为"
  },
  {
    "title": "第八讲：求解$Ax=b$：可解性和解的结构",
    "url": "posts/数学笔记/02.线性代数/08.求解Ax=b 可解性和解的结构.html",
    "category": "数学笔记/02.线性代数",
    "tags": [],
    "excerpt": "&lt;h1 id=\"第八讲求解axb可解性和解的结构\"&gt;第八讲：求解&lt;span class=\"math-inline\"&gt;Ax=b&lt;/span&gt;：可解性和解的结构&lt;a class=\"anchor-link\" href=\"#第八讲求解axb可解性和解的结构\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;举例，同上一讲：&lt;span class=\"math-inline"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/05.分布/05.分布度量.html",
    "category": "数学笔记/05.分布",
    "tags": [],
    "excerpt": "&lt;h2 id=\"信息量\"&gt;信息量&lt;a class=\"anchor-link\" href=\"#信息量\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;假设 &lt;span class=\"math-inline\"&gt;X&lt;/span&gt;​ 是一个离散型随机变量，其取值集合为&lt;span class=\"math-inline\"&gt;\\chi&lt;/span&gt;​，概率分布函数为&lt;span"
  },
  {
    "title": "先验分布与后验分布",
    "url": "posts/数学笔记/05.分布/04.先验分布与后验分布.html",
    "category": "数学笔记/05.分布",
    "tags": [],
    "excerpt": "&lt;h2 id=\"1回顾贝叶斯定理\"&gt;1.回顾贝叶斯定理&lt;a class=\"anchor-link\" href=\"#1回顾贝叶斯定理\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;首先，我们先来复习一下贝叶斯定理：&lt;br /&gt;\n&lt;div class=\"math-display\"&gt;&lt;br /&gt;\np(\\Theta|X)=\\frac{p(X|\\Theta)p(\\Th"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/05.分布/01.统计量及其分布.html",
    "category": "数学笔记/05.分布",
    "tags": [],
    "excerpt": "&lt;h2 id=\"统计量及其分布\"&gt;统计量及其分布&lt;a class=\"anchor-link\" href=\"#统计量及其分布\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"总体与样本\"&gt;&lt;strong&gt;总体与样本&lt;/strong&gt;&lt;a class=\"anchor-link\" href=\"#总体与样本\" title=\"Permanent link\"&gt;&"
  },
  {
    "title": "模拟 logits 和真实标签",
    "url": "posts/数学笔记/05.分布/06.交叉熵.html",
    "category": "数学笔记/05.分布",
    "tags": [],
    "excerpt": "&lt;h2 id=\"交叉熵\"&gt;交叉熵&lt;a class=\"anchor-link\" href=\"#交叉熵\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;假设有两个分布&lt;span class=\"math-inline\"&gt;p，q&lt;/span&gt;​，则它们在给定样本集上的交叉熵定义如下：&lt;/p&gt;\n&lt;p&gt;&lt;div class=\"math-display\"&gt;&lt;br /&gt;\nCE"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/05.分布/16.最优传输之梯度流.html",
    "category": "数学笔记/05.分布",
    "tags": [],
    "excerpt": "&lt;h2 id=\"梯度流\"&gt;梯度流&lt;a class=\"anchor-link\" href=\"#梯度流\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"欧式空间\"&gt;欧式空间&lt;a class=\"anchor-link\" href=\"#欧式空间\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h3&gt;\n&lt;p&gt;如果我们想要求解 &lt;span"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/05.分布/13.Wasserstein距离.html",
    "category": "数学笔记/05.分布",
    "tags": [],
    "excerpt": "&lt;h2 id=\"推土机距离问题earth-movers-distance\"&gt;推土机距离问题（Earth Mover's Distance）&lt;a class=\"anchor-link\" href=\"#推土机距离问题earth-movers-distance\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;假设地面上有 &lt;span class=\"math-inli"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/05.分布/03.假设检验.html",
    "category": "数学笔记/05.分布",
    "tags": [],
    "excerpt": "&lt;h2 id=\"六假设检验\"&gt;六、假设检验&lt;a class=\"anchor-link\" href=\"#六假设检验\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"61-假设检验的基本思想和概念\"&gt;6.1 假设检验的基本思想和概念&lt;a class=\"anchor-link\" href=\"#61-假设检验的基本思想和概念\" title=\"Permanen"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/05.分布/02.参数估计.html",
    "category": "数学笔记/05.分布",
    "tags": [],
    "excerpt": "&lt;p&gt;统计学与概率论的区别就是归纳和演绎，前者通过样本推测总体的分布，而后者已知总体分布去研究样本。因此参数估计则是归纳的过程，参数估计有两种形式：&lt;strong&gt;点估计&lt;/strong&gt;和&lt;strong&gt;区间估计&lt;/strong&gt;&lt;u&gt;（点估计和区间估计都是对于未知参数的估计，而&lt;strong&gt;点估计给出的是一个参数可能的值&lt;/strong&gt;，&lt;strong&gt;区间估计给出的是参数可能在的范围&lt;/"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/05.分布/21.两个多元正态分布的KL散度巴氏距离和W距离.html",
    "category": "数学笔记/05.分布",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://zhuanlan.zhihu.com/p/387938179\"&gt;两个多元正态分布的KL散度、巴氏距离和W距离&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;p&gt;最佳排版请看原链接：&lt;/p&gt;\n&lt;p&gt;&lt;a href=\"https://kexue.fm/archives/8512\"&gt;两个多元正态分布的KL散度、巴氏距离和W距离 - 科学空"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/05.分布/15.最优传输之生成模型.html",
    "category": "数学笔记/05.分布",
    "tags": [],
    "excerpt": "&lt;h2 id=\"wgan\"&gt;WGAN&lt;a class=\"anchor-link\" href=\"#wgan\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;假设 &lt;span class=\"math-inline\"&gt;F&lt;/span&gt; 的形式为： &lt;div class=\"math-display\"&gt;F = \\left[ \\begin{array}{c} f_1\\ f"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/05.分布/12.最优运输概述.html",
    "category": "数学笔记/05.分布",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;文章来源：&lt;a href=\"https://zhuanlan.zhihu.com/p/639733453\"&gt;Optimal Transport的前世今生 | (一) 从Monge问题到Kantorovich问题&lt;/a&gt;&lt;br /&gt;\n仓库：&lt;a href=\"https://github.com/changwxx/Awesome-Optimal-Transport-in"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/05.分布/14.基于最优传输的分类损失函数.html",
    "category": "数学笔记/05.分布",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://zhuanlan.zhihu.com/p/662721431\"&gt;EMO：基于最优传输思想设计的分类损失函数&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;p&gt;众所周知，分类任务的标准损失是交叉熵（Cross Entropy，等价于最大似然MLE，即Maximum Likelihood Estimation），它有着简单高效的特点，但"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/04.矩阵/14.低秩近似之路四ID.html",
    "category": "数学笔记/04.矩阵",
    "tags": [],
    "excerpt": ""
  },
  {
    "title": "低秩近似之路（三）：CR",
    "url": "posts/数学笔记/04.矩阵/13.低秩近似之路三CR.html",
    "category": "数学笔记/04.矩阵",
    "tags": [],
    "excerpt": "&lt;h1 id=\"低秩近似之路三cr\"&gt;低秩近似之路（三）：CR&lt;a class=\"anchor-link\" href=\"#低秩近似之路三cr\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; [苏剑林]&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;Link:&lt;/strong&gt; [https://zhuanlan.zhihu.c"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/04.矩阵/01.理解矩阵-孟岩.html",
    "category": "数学笔记/04.矩阵",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://so.csdn.net/so/search?q=%E7%90%86%E8%A7%A3%E7%9F%A9%E9%98%B5&amp;t=blog&amp;u=myan\"&gt;理解矩阵（转载自孟岩）&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;p&gt;前不久chensh出于不可告人的目的，要充当老师，教别人线性代数。于是我被揪住就线性代数中一"
  },
  {
    "title": "低秩近似之路（二）：SVD",
    "url": "posts/数学笔记/04.矩阵/12.低秩近似之路二SVD.html",
    "category": "数学笔记/04.矩阵",
    "tags": [],
    "excerpt": "&lt;h1 id=\"低秩近似之路二svd\"&gt;低秩近似之路（二）：SVD&lt;a class=\"anchor-link\" href=\"#低秩近似之路二svd\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; [苏剑林]&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;Link:&lt;/strong&gt; [https://zhuanlan.zhih"
  },
  {
    "title": "低秩近似之路（一）：伪逆",
    "url": "posts/数学笔记/04.矩阵/11.低秩近似之路一伪逆.html",
    "category": "数学笔记/04.矩阵",
    "tags": [],
    "excerpt": "&lt;h1 id=\"低秩近似之路一伪逆\"&gt;低秩近似之路（一）：伪逆&lt;a class=\"anchor-link\" href=\"#低秩近似之路一伪逆\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; [苏剑林]&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;Link:&lt;/strong&gt; [https://zhuanlan.zhihu.c"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/04.矩阵/15.Monarch矩阵.html",
    "category": "数学笔记/04.矩阵",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;文章来源：&lt;a href=\"https://zhuanlan.zhihu.com/p/714259385\"&gt;Monarch矩阵-计算高效的稀疏型矩阵分解&lt;/a&gt; &lt;br&gt;&lt;br /&gt;\n最佳排版请看原博客：&lt;a href=\"https://kexue.fm/archives/10249\"&gt;Monarch矩阵：计算高效的稀疏型矩阵分解 - 科学空间|Scientific"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/04.矩阵/02.新理解矩阵-苏剑林.html",
    "category": "数学笔记/04.矩阵",
    "tags": [],
    "excerpt": "&lt;h2 id=\"新理解矩阵1矩阵是什么\"&gt;&lt;a href=\"https://spaces.ac.cn/archives/1765\"&gt;《新理解矩阵1》：矩阵是什么？&lt;/a&gt;&lt;a class=\"anchor-link\" href=\"#新理解矩阵1矩阵是什么\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;前边我承诺过会写一些关于自己对矩阵的理解。其实孟岩在《理解矩"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/03.概率论与数理统计/02.随机变量及其分布.html",
    "category": "数学笔记/03.概率论与数理统计",
    "tags": [],
    "excerpt": "&lt;h2 id=\"随机变量及其分布\"&gt;随机变量及其分布&lt;a class=\"anchor-link\" href=\"#随机变量及其分布\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"随机变量的概念\"&gt;随机变量的概念&lt;a class=\"anchor-link\" href=\"#随机变量的概念\" title=\"Permanent link\"&gt;&para;&lt;/a"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/03.概率论与数理统计/03.随机变量的数字特征.html",
    "category": "数学笔记/03.概率论与数理统计",
    "tags": [],
    "excerpt": "&lt;h2 id=\"三随机变量的数字特征\"&gt;三、随机变量的数字特征&lt;a class=\"anchor-link\" href=\"#三随机变量的数字特征\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"31-数学期望均值与中位数\"&gt;3.1 数学期望（均值）与中位数&lt;a class=\"anchor-link\" href=\"#31-数学期望均值与中位数\" titl"
  },
  {
    "title": "Untitled",
    "url": "posts/数学笔记/03.概率论与数理统计/01.事件与概率.html",
    "category": "数学笔记/03.概率论与数理统计",
    "tags": [],
    "excerpt": "&lt;h2 id=\"一事件与概率\"&gt;一、事件与概率&lt;a class=\"anchor-link\" href=\"#一事件与概率\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"11-随机试验和随机事件\"&gt;1.1 随机试验和随机事件&lt;a class=\"anchor-link\" href=\"#11-随机试验和随机事件\" title=\"Permanent link"
  },
  {
    "title": "ES6 语法",
    "url": "posts/ProgramNotes/01.ES6语法.html",
    "category": "ProgramNotes",
    "tags": [],
    "excerpt": "&lt;h1 id=\"es6-语法\"&gt;ES6 语法&lt;a class=\"anchor-link\" href=\"#es6-语法\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;ECMAScript 6.0（以下简称 ES6，ECMAScript 是一种由 Ecma 国际(前身为欧洲计算机制造商 协会,英文名称是 European Computer Manufacture"
  },
  {
    "title": "Untitled",
    "url": "posts/ProgramNotes/105.编码.html",
    "category": "ProgramNotes",
    "tags": [],
    "excerpt": "&lt;h2 id=\"编码\"&gt;编码&lt;a class=\"anchor-link\" href=\"#编码\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;ol&gt;\n&lt;li&gt;\n&lt;p&gt;最早只有127个字符被编码到计算机里，也就是大小写英文字母、数字和一些符号，这个编码表被称为&lt;strong&gt;ASCII编码&lt;/strong&gt;，比如大写字母A的编码是65，小写字母z的编码是122。&lt;/"
  },
  {
    "title": "Reference",
    "url": "posts/ProgramNotes/22.Shell.html",
    "category": "ProgramNotes",
    "tags": [],
    "excerpt": "&lt;h1 id=\"reference\"&gt;Reference&lt;a class=\"anchor-link\" href=\"#reference\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;ul&gt;\n&lt;li&gt;https://zhuanlan.zhihu.com/p/463932084&lt;/li&gt;\n&lt;li&gt;https://www.jianshu.com/p/a891af6f8"
  },
  {
    "title": "Untitled",
    "url": "posts/ProgramNotes/02.Maven.html",
    "category": "ProgramNotes",
    "tags": [],
    "excerpt": "&lt;h2 id=\"1maven-简介\"&gt;1.Maven 简介&lt;a class=\"anchor-link\" href=\"#1maven-简介\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;Maven 的本质是一个项目管理工具，将项目开发和管理过程抽象成一个项目对象模型(POM)&lt;br /&gt;\nPOM (Project Object Model)：项目对象模型&lt;br"
  },
  {
    "title": "把本地的 /path/to/local/file 文件传输到远程的 /path/to/remote/file",
    "url": "posts/ProgramNotes/23.SSH.html",
    "category": "ProgramNotes",
    "tags": [],
    "excerpt": "&lt;ul&gt;\n&lt;li&gt;https://zhuanlan.zhihu.com/p/21999778&lt;/li&gt;\n&lt;li&gt;https://blog.csdn.net/li528405176/article/details/82810342&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2 id=\"基础\"&gt;基础&lt;a class=\"anchor-link\" href=\"#基础\" title=\"Permanent link\"&gt;&pa"
  },
  {
    "title": "Untitled",
    "url": "posts/ProgramNotes/101.快捷键.html",
    "category": "ProgramNotes",
    "tags": [],
    "excerpt": "&lt;h2 id=\"快捷键\"&gt;快捷键&lt;a class=\"anchor-link\" href=\"#快捷键\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h2 id=\"mac-键盘符号说明\"&gt;Mac 键盘符号说明&lt;a class=\"anchor-link\" href=\"#mac-键盘符号说明\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;"
  },
  {
    "title": "参考文章",
    "url": "posts/ProgramNotes/10.软件工程.html",
    "category": "ProgramNotes",
    "tags": [],
    "excerpt": "&lt;h2 id=\"软件设计的整体流程\"&gt;软件设计的整体流程&lt;a class=\"anchor-link\" href=\"#软件设计的整体流程\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;软件需求分析阶段：输出了《软件需求规格说明书》，不涉及具体实现方法。用户能看得明白，开发人员也可据此进行下面的工作，&lt;strong&gt;搞清楚“要解决什么问题”。&lt;/st"
  },
  {
    "title": "Untitled",
    "url": "posts/ProgramNotes/104.文件上传.html",
    "category": "ProgramNotes",
    "tags": [],
    "excerpt": "&lt;h2 id=\"分布式文件系统\"&gt;分布式文件系统&lt;a class=\"anchor-link\" href=\"#分布式文件系统\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h2 id=\"什么是分布式文件系统\"&gt;什么是分布式文件系统&lt;a class=\"anchor-link\" href=\"#什么是分布式文件系统\" title=\"Permanent link\"&gt;&pa"
  },
  {
    "title": "Zotero+坚果云搞定多设备文献管理",
    "url": "posts/ProgramNotes/102.Zotero+坚果云搞定多设备文献管理.html",
    "category": "ProgramNotes",
    "tags": [],
    "excerpt": "&lt;h1 id=\"zotero坚果云搞定多设备文献管理\"&gt;Zotero+坚果云搞定多设备文献管理&lt;a class=\"anchor-link\" href=\"#zotero坚果云搞定多设备文献管理\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;blockquote&gt;\n&lt;p&gt;文章引用自 &lt;a href=\"https://sspai.com/post/64283\"&gt;少数派"
  },
  {
    "title": "Untitled",
    "url": "posts/ProgramNotes/104.云平台.html",
    "category": "ProgramNotes",
    "tags": [],
    "excerpt": "&lt;h2 id=\"云平台核心\"&gt;云平台核心&lt;a class=\"anchor-link\" href=\"#云平台核心\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"为什么用云平台\"&gt;为什么用云平台&lt;a class=\"anchor-link\" href=\"#为什么用云平台\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h3&gt;\n&lt;u"
  },
  {
    "title": "Untitled",
    "url": "posts/ProgramNotes/03.汇编语言.html",
    "category": "ProgramNotes",
    "tags": [],
    "excerpt": "&lt;p&gt;学习编程其实就是学高级语言，即那些为人类设计的计算机语言。&lt;/p&gt;\n&lt;p&gt;但是，计算机不理解高级语言，必须通过编译器转成二进制代码，才能运行。学会高级语言，并不等于理解计算机实际的运行步骤。&lt;/p&gt;\n&lt;p&gt;&lt;img alt=\"img\" src=\"https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208152007588.pn"
  },
  {
    "title": "git commit 将暂存区的文件修改提交到本地仓库",
    "url": "posts/ProgramNotes/90.Git/00.Git教程.html",
    "category": "ProgramNotes/90.Git",
    "tags": [],
    "excerpt": "&lt;p&gt;&lt;img src=\"https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202211042053980.png\"/&gt;&lt;/p&gt;\n&lt;hr/&gt;\n&lt;blockquote&gt;\n&lt;p&gt;参考 &lt;a href=\"https://www.bilibili.com/video/BV1MU4y1Y7h5/?spm_id_from=333.999.0.0&a"
  },
  {
    "title": "查看 Linux 命令帮助信息",
    "url": "posts/ProgramNotes/01.Linux/02.帮助信息.html",
    "category": "ProgramNotes/01.Linux",
    "tags": [],
    "excerpt": "&lt;h1 id=\"查看-linux-命令帮助信息\"&gt;查看 Linux 命令帮助信息&lt;a class=\"anchor-link\" href=\"#查看-linux-命令帮助信息\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;blockquote&gt;\n&lt;p&gt;Linux 中有非常多的命令，想全部背下来是很困难的事。所以，我认为学习 Linux 的第一步，就是了解如何快速检索"
  },
  {
    "title": "scp",
    "url": "posts/ProgramNotes/01.Linux/106.scp.html",
    "category": "ProgramNotes/01.Linux",
    "tags": [],
    "excerpt": "&lt;h1 id=\"scp\"&gt;scp&lt;a class=\"anchor-link\" href=\"#scp\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;加密的方式在本地主机和远程主机之间复制文件&lt;/p&gt;\n&lt;h2 id=\"补充说明\"&gt;补充说明&lt;a class=\"anchor-link\" href=\"#补充说明\" title=\"Permanent link\"&gt;&par"
  },
  {
    "title": "Linux 文件内容查看编辑",
    "url": "posts/ProgramNotes/01.Linux/04.文件内容.html",
    "category": "ProgramNotes/01.Linux",
    "tags": [],
    "excerpt": "&lt;h1 id=\"linux-文件内容查看编辑\"&gt;Linux 文件内容查看编辑&lt;a class=\"anchor-link\" href=\"#linux-文件内容查看编辑\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;blockquote&gt;\n&lt;p&gt;关键词：&lt;code&gt;cat&lt;/code&gt;, &lt;code&gt;head&lt;/code&gt;, &lt;code&gt;tail&lt;/code&gt;, &lt;"
  },
  {
    "title": "Untitled",
    "url": "posts/ProgramNotes/01.Linux/00.学习资源.html",
    "category": "ProgramNotes/01.Linux",
    "tags": [],
    "excerpt": "&lt;h2 id=\"学习资源\"&gt;学习资源&lt;a class=\"anchor-link\" href=\"#学习资源\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/dunwu/linux-tutorial?tab=readme-ov-file\"&gt;linux-tutorial&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;["
  },
  {
    "title": "命令行的艺术",
    "url": "posts/ProgramNotes/01.Linux/101.命令行的艺术.html",
    "category": "ProgramNotes/01.Linux",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;转载自 https://github.com/jlevy/the-art-of-command-line&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;p&gt;&lt;em&gt;&lt;a href=\"README-cs.md\"&gt;Čeština&lt;/a&gt; ∙ &lt;a href=\"README-de.md\"&gt;Deutsch&lt;/a&gt; ∙ &lt;a href=\"README-el.md\"&gt;Ελληνικά&lt;"
  },
  {
    "title": "free",
    "url": "posts/ProgramNotes/01.Linux/102.free.html",
    "category": "ProgramNotes/01.Linux",
    "tags": [],
    "excerpt": "&lt;h1 id=\"free\"&gt;free&lt;a class=\"anchor-link\" href=\"#free\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;显示内存的使用情况&lt;/p&gt;\n&lt;h2 id=\"补充说明\"&gt;补充说明&lt;a class=\"anchor-link\" href=\"#补充说明\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h"
  },
  {
    "title": "top",
    "url": "posts/ProgramNotes/01.Linux/107.top.html",
    "category": "ProgramNotes/01.Linux",
    "tags": [],
    "excerpt": "&lt;h1 id=\"top\"&gt;top&lt;a class=\"anchor-link\" href=\"#top\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;显示或管理执行中的程序&lt;/p&gt;\n&lt;h2 id=\"补充说明\"&gt;补充说明&lt;a class=\"anchor-link\" href=\"#补充说明\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2"
  },
  {
    "title": "Linux 系统管理",
    "url": "posts/ProgramNotes/01.Linux/07.系统管理.html",
    "category": "ProgramNotes/01.Linux",
    "tags": [],
    "excerpt": "&lt;h1 id=\"linux-系统管理\"&gt;Linux 系统管理&lt;a class=\"anchor-link\" href=\"#linux-系统管理\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;blockquote&gt;\n&lt;p&gt;关键词：&lt;code&gt;lsb_release&lt;/code&gt;, &lt;code&gt;reboot&lt;/code&gt;, &lt;code&gt;exit&lt;/code&gt;, &lt;co"
  },
  {
    "title": "iotop",
    "url": "posts/ProgramNotes/01.Linux/105.iotop.html",
    "category": "ProgramNotes/01.Linux",
    "tags": [],
    "excerpt": "&lt;h1 id=\"iotop\"&gt;iotop&lt;a class=\"anchor-link\" href=\"#iotop\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;用来监视磁盘 I/O 使用状况的工具&lt;/p&gt;\n&lt;h2 id=\"补充说明\"&gt;补充说明&lt;a class=\"anchor-link\" href=\"#补充说明\" title=\"Permanent link\"&gt;&"
  },
  {
    "title": "Linux 硬件管理",
    "url": "posts/ProgramNotes/01.Linux/09.硬件管理.html",
    "category": "ProgramNotes/01.Linux",
    "tags": [],
    "excerpt": "&lt;h1 id=\"linux-硬件管理\"&gt;Linux 硬件管理&lt;a class=\"anchor-link\" href=\"#linux-硬件管理\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;blockquote&gt;\n&lt;p&gt;关键词：&lt;code&gt;df&lt;/code&gt;, &lt;code&gt;du&lt;/code&gt;, &lt;code&gt;top&lt;/code&gt;, &lt;code&gt;free&lt;/code&gt;"
  },
  {
    "title": "grep",
    "url": "posts/ProgramNotes/01.Linux/103.grep.html",
    "category": "ProgramNotes/01.Linux",
    "tags": [],
    "excerpt": "&lt;h1 id=\"grep\"&gt;grep&lt;a class=\"anchor-link\" href=\"#grep\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;强大的文本搜索工具&lt;/p&gt;\n&lt;h2 id=\"补充说明\"&gt;补充说明&lt;a class=\"anchor-link\" href=\"#补充说明\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h"
  },
  {
    "title": "新建用户加入组",
    "url": "posts/ProgramNotes/01.Linux/06.用户管理.html",
    "category": "ProgramNotes/01.Linux",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;关键词：&lt;code&gt;groupadd&lt;/code&gt;, &lt;code&gt;groupdel&lt;/code&gt;, &lt;code&gt;groupmod&lt;/code&gt;, &lt;code&gt;useradd&lt;/code&gt;, &lt;code&gt;userdel&lt;/code&gt;, &lt;code&gt;usermod&lt;/code&gt;, &lt;code&gt;passwd&lt;/code&gt;, &lt;code&gt;su&lt;/code&gt;,"
  },
  {
    "title": "Untitled",
    "url": "posts/ProgramNotes/01.Linux/01.常用命令.html",
    "category": "ProgramNotes/01.Linux",
    "tags": [],
    "excerpt": "&lt;h2 id=\"linux命令\"&gt;Linux命令&lt;a class=\"anchor-link\" href=\"#linux命令\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;查看 Linux 命令帮助信息：&lt;code&gt;help&lt;/code&gt;, &lt;code&gt;whatis&lt;/code&gt;, &lt;code&gt;info&lt;/code&gt;, &lt;code&gt;which&lt;/c"
  },
  {
    "title": "vmstat",
    "url": "posts/ProgramNotes/01.Linux/108vmstat.html",
    "category": "ProgramNotes/01.Linux",
    "tags": [],
    "excerpt": "&lt;h1 id=\"vmstat\"&gt;vmstat&lt;a class=\"anchor-link\" href=\"#vmstat\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;显示虚拟内存状态&lt;/p&gt;\n&lt;h2 id=\"补充说明\"&gt;补充说明&lt;a class=\"anchor-link\" href=\"#补充说明\" title=\"Permanent link\"&gt;&para;&lt;/"
  },
  {
    "title": "Linux 网络管理",
    "url": "posts/ProgramNotes/01.Linux/08.网络管理.html",
    "category": "ProgramNotes/01.Linux",
    "tags": [],
    "excerpt": "&lt;h1 id=\"linux-网络管理\"&gt;Linux 网络管理&lt;a class=\"anchor-link\" href=\"#linux-网络管理\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;blockquote&gt;\n&lt;p&gt;关键词：&lt;code&gt;curl&lt;/code&gt;, &lt;code&gt;wget&lt;/code&gt;, &lt;code&gt;telnet&lt;/code&gt;, &lt;code&gt;ip&lt;/"
  },
  {
    "title": "Linux 文件目录管理",
    "url": "posts/ProgramNotes/01.Linux/03.文件目录管理.html",
    "category": "ProgramNotes/01.Linux",
    "tags": [],
    "excerpt": "&lt;h1 id=\"linux-文件目录管理\"&gt;Linux 文件目录管理&lt;a class=\"anchor-link\" href=\"#linux-文件目录管理\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;blockquote&gt;\n&lt;p&gt;关键词：&lt;code&gt;cd&lt;/code&gt;, &lt;code&gt;ls&lt;/code&gt;, &lt;code&gt;pwd&lt;/code&gt;, &lt;code&gt;mkdir"
  },
  {
    "title": "iostat",
    "url": "posts/ProgramNotes/01.Linux/104.iostat.html",
    "category": "ProgramNotes/01.Linux",
    "tags": [],
    "excerpt": "&lt;h1 id=\"iostat\"&gt;iostat&lt;a class=\"anchor-link\" href=\"#iostat\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;监视系统输入输出设备和 CPU 的使用情况&lt;/p&gt;\n&lt;h2 id=\"补充说明\"&gt;补充说明&lt;a class=\"anchor-link\" href=\"#补充说明\" title=\"Permanent l"
  },
  {
    "title": "Linux 软件管理",
    "url": "posts/ProgramNotes/01.Linux/10.软件管理.html",
    "category": "ProgramNotes/01.Linux",
    "tags": [],
    "excerpt": "&lt;h1 id=\"linux-软件管理\"&gt;Linux 软件管理&lt;a class=\"anchor-link\" href=\"#linux-软件管理\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;blockquote&gt;\n&lt;p&gt;关键词：&lt;code&gt;rpm&lt;/code&gt;, &lt;code&gt;yum&lt;/code&gt;, &lt;code&gt;apt-get&lt;/code&gt;&lt;/p&gt;\n&lt;/blockq"
  },
  {
    "title": "Linux 文件压缩和解压",
    "url": "posts/ProgramNotes/01.Linux/05.文件压缩和解压.html",
    "category": "ProgramNotes/01.Linux",
    "tags": [],
    "excerpt": "&lt;h1 id=\"linux-文件压缩和解压\"&gt;Linux 文件压缩和解压&lt;a class=\"anchor-link\" href=\"#linux-文件压缩和解压\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;blockquote&gt;\n&lt;p&gt;关键词：&lt;code&gt;tar&lt;/code&gt;, &lt;code&gt;gzip&lt;/code&gt;, &lt;code&gt;zip&lt;/code&gt;, &lt;code"
  },
  {
    "title": "Untitled",
    "url": "posts/Others/01.mac/01.Mac软件安装.html",
    "category": "Others/01.mac",
    "tags": [],
    "excerpt": "&lt;h2 id=\"homebrew\"&gt;Homebrew&lt;a class=\"anchor-link\" href=\"#homebrew\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;.bash_profile和.zshrc都在用户目录下(~)&lt;br /&gt;\n.bash_profile需要使用source执行下，方可生效(可能需要手动创建.bash_profile)&lt;"
  },
  {
    "title": "Untitled",
    "url": "posts/Others/08.GitHub/01.Issues.html",
    "category": "Others/08.GitHub",
    "tags": [],
    "excerpt": "&lt;h2 id=\"accuracy\"&gt;accuracy&lt;a class=\"anchor-link\" href=\"#accuracy\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;title：Question regarding reproduction of results&lt;/p&gt;\n&lt;p&gt;Hello,&lt;br /&gt;\nThanks for sharing the "
  },
  {
    "title": "Untitled",
    "url": "posts/Others/08.GitHub/03.Github Action自动部署.html",
    "category": "Others/08.GitHub",
    "tags": [],
    "excerpt": "&lt;p&gt;TODO&lt;/p&gt;\n&lt;blockquote&gt;\n&lt;p&gt;Github Action 官方文档：https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#name&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;h2 id=\"github-action-概述\"&gt;Github Action 概述&lt;a"
  },
  {
    "title": "创建挂载目录",
    "url": "posts/Others/06.实用工具/02.阿里云服务器博客部署.html",
    "category": "Others/06.实用工具",
    "tags": [],
    "excerpt": "&lt;p&gt;&lt;img alt=\"image-20231103170538800\" src=\"https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202311031705837.png\" /&gt;&lt;/p&gt;\n&lt;h2 id=\"1-服务器\"&gt;1. 服务器&lt;a class=\"anchor-link\" href=\"#1-服务器\" title=\"Permanen"
  },
  {
    "title": "Untitled",
    "url": "posts/Others/06.实用工具/01.网站规范.html",
    "category": "Others/06.实用工具",
    "tags": [],
    "excerpt": "&lt;h2 id=\"图像\"&gt;图像&lt;a class=\"anchor-link\" href=\"#图像\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;div align=center&gt;&lt;img src=\"\"/&gt;&lt;/div&gt;\n\n&lt;hr /&gt;\n&lt;h2 id=\"普通文章标题\"&gt;普通文章标题&lt;a class=\"anchor-link\" href=\"#普通文章标题\" title=\""
  },
  {
    "title": "Untitled",
    "url": "posts/Others/06.实用工具/05.Latex.html",
    "category": "Others/06.实用工具",
    "tags": [],
    "excerpt": "&lt;h2 id=\"表-1-数学模式重音符\"&gt;表 1: 数学模式重音符&lt;a class=\"anchor-link\" href=\"#表-1-数学模式重音符\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;示例&lt;/th&gt;\n&lt;th&gt;代码&lt;/th&gt;\n&lt;th&gt;示例&lt;/th&gt;\n&lt;th&gt;代码&lt;/th&gt;\n&lt;th&gt;示例&lt;/th&gt;\n&l"
  },
  {
    "title": "Untitled",
    "url": "posts/Others/06.实用工具/00.README模板.html",
    "category": "Others/06.实用工具",
    "tags": [],
    "excerpt": "&lt;h2 id=\"readme-模板\"&gt;README 模板&lt;a class=\"anchor-link\" href=\"#readme-模板\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;div align=center&gt;&lt;img src=\"https://hw-universal.oss-cn-beijing.aliyuncs.com/code.gif\" style"
  },
  {
    "title": "查看当前版本",
    "url": "posts/Others/04.ai/02.conda.html",
    "category": "Others/04.ai",
    "tags": [],
    "excerpt": "&lt;h2 id=\"conda\"&gt;conda&lt;a class=\"anchor-link\" href=\"#conda\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;conda是一个包，依赖和环境管理工具，适用于多种语言，如: Python, R, Scala, Java, Javascript, C/ C++, FORTRAN&lt;/p&gt;\n&lt;p&gt;conda默认随min"
  },
  {
    "title": "Untitled",
    "url": "posts/Others/04.ai/03.cuda.html",
    "category": "Others/04.ai",
    "tags": [],
    "excerpt": "&lt;h2 id=\"cuda安装多版本切换\"&gt;cuda安装（多版本切换）&lt;a class=\"anchor-link\" href=\"#cuda安装多版本切换\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;除了软连接，还可以通过配置环境变量（CUDA_PATH ）来实现版本切换。&lt;br /&gt;\n当本机上安装有多个版本cuda时可以通过一下步骤进行管理/版本切换，比如我"
  },
  {
    "title": "Untitled",
    "url": "posts/Others/04.ai/01.PyTorch.html",
    "category": "Others/04.ai",
    "tags": [],
    "excerpt": "&lt;h2 id=\"pytorch\"&gt;PyTorch&lt;a class=\"anchor-link\" href=\"#pytorch\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;在开发过程中可能会有多个项目同时进行，不同项目之间使用的 Python 版本和一些库的版本不一样，这就会导致冲突。因此这里使用 Anaconda 来管理多个 Python 虚拟环境。Anac"
  },
  {
    "title": "Untitled",
    "url": "posts/Others/07.NAS/05.NAS网络.html",
    "category": "Others/07.NAS",
    "tags": [],
    "excerpt": "&lt;h2 id=\"nas网络篇\"&gt;NAS网络篇&lt;a class=\"anchor-link\" href=\"#nas网络篇\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;假设把NAS搭建在家中，家庭成员们在外地如何取用家中NAS里面的资料？&lt;/p&gt;\n&lt;p&gt;外网访问NAS有很多方法：&lt;/p&gt;\n&lt;p&gt;1、有公网IPv4&lt;/p&gt;\n&lt;p&gt;2、无公网IPv4，有IPv6 &lt;"
  },
  {
    "title": "Untitled",
    "url": "posts/Others/07.NAS/04.NAS硬盘.html",
    "category": "Others/07.NAS",
    "tags": [],
    "excerpt": "&lt;h2 id=\"nas硬盘篇\"&gt;NAS硬盘篇&lt;a class=\"anchor-link\" href=\"#nas硬盘篇\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;目前市面上的硬盘主要分为企业盘、NAS盘、监控盘以及普通盘，NAS硬盘可供选择的其实不多，其中只有企业盘与NAS盘比较合适，监控盘算是凑合。我简单列举一下市面上在售的NAS盘与企业盘：  &lt;/p&gt;"
  },
  {
    "title": "Untitled",
    "url": "posts/Others/07.NAS/01.NAS简介.html",
    "category": "Others/07.NAS",
    "tags": [],
    "excerpt": "&lt;h2 id=\"nas介绍篇\"&gt;NAS介绍篇&lt;a class=\"anchor-link\" href=\"#nas介绍篇\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;NAS 可以看作“简化版”“私人”存储服务器，相当于将企业应用的存储设备下放到普通家用环境。  &lt;/p&gt;\n&lt;p&gt;它能做到的事情有很多，许多刚接触到 NAS 的玩家，大多是看上它的超大网盘、多人共享"
  },
  {
    "title": "Untitled",
    "url": "posts/Others/07.NAS/03.NAS玩法.html",
    "category": "Others/07.NAS",
    "tags": [],
    "excerpt": "&lt;h2 id=\"nas玩法篇\"&gt;NAS玩法篇&lt;a class=\"anchor-link\" href=\"#nas玩法篇\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"家庭影院\"&gt;家庭影院&lt;a class=\"anchor-link\" href=\"#家庭影院\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h3&gt;\n&lt;p&gt;一般而言，"
  },
  {
    "title": "Untitled",
    "url": "posts/Others/07.NAS/02.NAS选购.html",
    "category": "Others/07.NAS",
    "tags": [],
    "excerpt": "&lt;h2 id=\"nas选购篇\"&gt;NAS选购篇&lt;a class=\"anchor-link\" href=\"#nas选购篇\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;本部分有每个市面上在售NAS产品型号表格，包括有品牌型号，所采用的处理器，网口与它的一些特色功能，同时还会提供过去出现过的历史最低价格，供大家参考，在下方的产品介绍中会根据个人经验针对每个产品进行"
  },
  {
    "title": "创建挂载目录",
    "url": "posts/Others/03.linux/03.Linux软件安装.html",
    "category": "Others/03.linux",
    "tags": [],
    "excerpt": "&lt;h2 id=\"anaconda\"&gt;Anaconda&lt;a class=\"anchor-link\" href=\"#anaconda\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;ol&gt;\n&lt;li&gt;下载相关环境&lt;/li&gt;\n&lt;/ol&gt;\n&lt;p&gt;&lt;code&gt;sh\n   wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/ar"
  },
  {
    "title": "Untitled",
    "url": "posts/Others/09.Tex/03.table.html",
    "category": "Others/09.Tex",
    "tags": [],
    "excerpt": "&lt;h3 id=\"2-table-环境\"&gt;&lt;strong&gt;2. &lt;code&gt;table&lt;/code&gt; 环境&lt;/strong&gt;&lt;a class=\"anchor-link\" href=\"#2-table-环境\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h3&gt;\n&lt;p&gt;&lt;strong&gt;应用场景&lt;/strong&gt;：&lt;/p&gt;\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;基础表格&lt;/stron"
  },
  {
    "title": "Untitled",
    "url": "posts/Others/09.Tex/02.figure.html",
    "category": "Others/09.Tex",
    "tags": [],
    "excerpt": "&lt;h3 id=\"1-figure-环境\"&gt;&lt;strong&gt;1. &lt;code&gt;figure&lt;/code&gt; 环境&lt;/strong&gt;&lt;a class=\"anchor-link\" href=\"#1-figure-环境\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h3&gt;\n&lt;p&gt;&lt;strong&gt;应用场景&lt;/strong&gt;：&lt;/p&gt;\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;单张图片插入&lt;/"
  },
  {
    "title": "Untitled",
    "url": "posts/Others/09.Tex/01.Tex.html",
    "category": "Others/09.Tex",
    "tags": [],
    "excerpt": "&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;文档结构&lt;/strong&gt;：&lt;/li&gt;\n&lt;/ol&gt;\n&lt;ul&gt;\n&lt;li&gt;使用&lt;code&gt;\\documentclass&lt;/code&gt;命令设置文档类型，如&lt;code&gt;article&lt;/code&gt;、&lt;code&gt;book&lt;/code&gt;、&lt;code&gt;report&lt;/code&gt;等。&lt;/li&gt;\n&lt;li&gt;使用&lt;code&gt;\\title&lt;/code&g"
  },
  {
    "title": "Untitled",
    "url": "posts/Others/05.IDE/01.IDEA.html",
    "category": "Others/05.IDE",
    "tags": [],
    "excerpt": "&lt;h2 id=\"mac-中-idea-快捷键\"&gt;Mac 中 IDEA 快捷键&lt;a class=\"anchor-link\" href=\"#mac-中-idea-快捷键\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"mac-键盘符号说明\"&gt;Mac 键盘符号说明&lt;a class=\"anchor-link\" href=\"#mac-键盘符号说明\" title"
  },
  {
    "title": "Untitled",
    "url": "posts/Others/05.IDE/02.PyCharm.html",
    "category": "Others/05.IDE",
    "tags": [],
    "excerpt": "&lt;h2 id=\"pycharm-远程连接服务器\"&gt;pycharm 远程连接服务器&lt;a class=\"anchor-link\" href=\"#pycharm-远程连接服务器\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;https://blog.csdn.net/qq_36667170/article/details/121716527&lt;/p&gt;\n&lt;h2 id="
  },
  {
    "title": "Untitled",
    "url": "posts/Others/02.win/01.Windows软件安装.html",
    "category": "Others/02.win",
    "tags": [],
    "excerpt": "&lt;h2 id=\"anaconda\"&gt;Anaconda&lt;a class=\"anchor-link\" href=\"#anaconda\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;官网 &lt;a href=\"https://www.anaconda.com/products/individual\"&gt;https://www.anaconda.com/pro"
  },
  {
    "title": "Untitled",
    "url": "posts/Others/02.win/02.C盘瘦身.html",
    "category": "Others/02.win",
    "tags": [],
    "excerpt": "&lt;div align=center&gt;&lt;img src=\"https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20241027212237.png\" style=\"zoom: 60%;\" /&gt;&lt;/div&gt;"
  },
  {
    "title": "Untitled",
    "url": "posts/大模型/104.temp.html",
    "category": "大模型",
    "tags": [],
    "excerpt": "&lt;div align=center&gt;&lt;img src=\"https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20240528122659.png\"/&gt;&lt;/div&gt;\n\n&lt;h3 id=\"为什么现在的-llm-都是-decoder-only-的架构\"&gt;为什么现在的 LLM 都是 Decoder only 的架构？&lt;a class=\"anchor-link"
  },
  {
    "title": "简介",
    "url": "posts/大模型/09.智能体.html",
    "category": "大模型",
    "tags": [],
    "excerpt": "&lt;h2 id=\"基于大模型的智能体agent\"&gt;基于大模型的智能体(Agent)&lt;a class=\"anchor-link\" href=\"#基于大模型的智能体agent\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h1 id=\"简介\"&gt;简介&lt;a class=\"anchor-link\" href=\"#简介\" title=\"Permanent link\"&gt;&par"
  },
  {
    "title": "Untitled",
    "url": "posts/大模型/103.Scaling Law.html",
    "category": "大模型",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://zhuanlan.zhihu.com/p/667489780\"&gt;解析大模型中的Scaling Law&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;p&gt;在大模型的研发中，通常会有下面一些需求：&lt;/p&gt;\n&lt;ol&gt;\n&lt;li&gt;计划训练一个10B的模型，想知道至少需要多大的数据？&lt;/li&gt;\n&lt;li&gt;收集到了1T的数据，想知道能训练一个多大的"
  },
  {
    "title": "1. Llama进化史",
    "url": "posts/大模型/05.常见模型篇/11.LLAMA.html",
    "category": "大模型/05.常见模型篇",
    "tags": [],
    "excerpt": "&lt;p&gt;TODO&lt;/p&gt;\n&lt;blockquote&gt;\n&lt;p&gt;LLAMA(Large Language Model Meta AI)&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;h2 id=\"llama1\"&gt;LLaMA1&lt;a class=\"anchor-link\" href=\"#llama1\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;blockquote&gt;\n&lt;p&gt;文章地"
  },
  {
    "title": "Untitled",
    "url": "posts/大模型/05.常见模型篇/01.常见大模型综述.html",
    "category": "大模型/05.常见模型篇",
    "tags": [],
    "excerpt": "&lt;h2 id=\"chatglm-6b\"&gt;ChatGLM-6B&lt;a class=\"anchor-link\" href=\"#chatglm-6b\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;blockquote&gt;\n&lt;p&gt;清华唐杰老师团队&lt;br /&gt;\n项目: https://github.com/THUDM/ChatGLM-6B&lt;br /&gt;\nBlog: https:"
  },
  {
    "title": "Untitled",
    "url": "posts/大模型/00.前置篇/01.模型仓库介绍.html",
    "category": "大模型/00.前置篇",
    "tags": [],
    "excerpt": "&lt;h2 id=\"实战\"&gt;实战&lt;a class=\"anchor-link\" href=\"#实战\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"self-llm\"&gt;self-llm&lt;a class=\"anchor-link\" href=\"#self-llm\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h3&gt;\n&lt;blockqu"
  },
  {
    "title": "Untitled",
    "url": "posts/大模型/04.微调篇/04.RAG.html",
    "category": "大模型/04.微调篇",
    "tags": [],
    "excerpt": ""
  },
  {
    "title": "Untitled",
    "url": "posts/大模型/04.微调篇/03.Prompt Engineering.html",
    "category": "大模型/04.微调篇",
    "tags": [],
    "excerpt": ""
  },
  {
    "title": "Untitled",
    "url": "posts/大模型/04.微调篇/05.Fine-tuning.html",
    "category": "大模型/04.微调篇",
    "tags": [],
    "excerpt": ""
  },
  {
    "title": "第7章 大模型之Adaptation",
    "url": "posts/大模型/04.微调篇/07.大模型之Adaptation.html",
    "category": "大模型/04.微调篇",
    "tags": [],
    "excerpt": "&lt;h1 id=\"第7章-大模型之adaptation\"&gt;第7章 大模型之Adaptation&lt;a class=\"anchor-link\" href=\"#第7章-大模型之adaptation\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;&lt;img alt=\"adaptation_1\" src=\"https://markdownimg-hw.oss-cn-bei"
  },
  {
    "title": "Untitled",
    "url": "posts/大模型/04.微调篇/01.LLM微调.html",
    "category": "大模型/04.微调篇",
    "tags": [],
    "excerpt": "&lt;p&gt;大模型微调的三个节点：&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;节点 1 ChatGPT&lt;/strong&gt;：由于 ChatGPT 惊人的效果，让大家意识到 AGI 的可能性，并重视起了大模型+开放指令微调+强化学习这种三阶段范式&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;节点 2 LLaMA&lt;/strong&gt;：LLaMA 是 Meta 在今年 2 月份推出的基座模型，宣称 LLaMA-13B 在大"
  },
  {
    "title": "[提示工程、RAG和微调 - 哪个才是大模型应用优化的最佳路径？](https://mp.weixin.qq.com/s/muOs95RFVQlr-WuTJ8tT3w)",
    "url": "posts/大模型/04.微调篇/02.大模型应用优化路径.html",
    "category": "大模型/04.微调篇",
    "tags": [],
    "excerpt": "&lt;h1 id=\"提示工程rag和微调---哪个才是大模型应用优化的最佳路径\"&gt;&lt;a href=\"https://mp.weixin.qq.com/s/muOs95RFVQlr-WuTJ8tT3w\"&gt;提示工程、RAG和微调 - 哪个才是大模型应用优化的最佳路径？&lt;/a&gt;&lt;a class=\"anchor-link\" href=\"#提示工程rag和微调---哪个才是大模型应用优化的最佳路径\" title"
  },
  {
    "title": "【LLM】从零开始训练大模型",
    "url": "posts/大模型/03.训练篇/01.从零训练大模型.html",
    "category": "大模型/03.训练篇",
    "tags": [],
    "excerpt": "&lt;h1 id=\"llm从零开始训练大模型\"&gt;【LLM】从零开始训练大模型&lt;a class=\"anchor-link\" href=\"#llm从零开始训练大模型\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; 何枝&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;Date:&lt;/strong&gt; 2023-11-13&lt;/p&gt;\n&lt;p&gt;&lt;s"
  },
  {
    "title": "-------------------------------------------------------------------------------",
    "url": "posts/大模型/03.训练篇/101.DeepSpeed-Megatron MoE并行训练.html",
    "category": "大模型/03.训练篇",
    "tags": [],
    "excerpt": "&lt;p&gt;&lt;a href=\"https://zhuanlan.zhihu.com/p/681692152\"&gt;图解大模型训练系列之：DeepSpeed-Megatron MoE并行训练（源码解读篇）&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;大家好，赶在节前把MoE的原理篇和源码篇一起出完，这次，没人能再喊我鸽王了吧！！  &lt;/p&gt;\n&lt;p&gt;&lt;img alt=\"\" src=\"(20240206)图解大模型训练系列之DeepS"
  },
  {
    "title": "Untitled",
    "url": "posts/大模型/03.训练篇/11.大模型框架.html",
    "category": "大模型/03.训练篇",
    "tags": [],
    "excerpt": "&lt;h2 id=\"大模型框架分类整理\"&gt;大模型框架分类整理&lt;a class=\"anchor-link\" href=\"#大模型框架分类整理\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"一训练框架\"&gt;&lt;strong&gt;一、训练框架&lt;/strong&gt;&lt;a class=\"anchor-link\" href=\"#一训练框架\" title=\"Permanent "
  },
  {
    "title": "Untitled",
    "url": "posts/大模型/03.训练篇/102.大模型数据精度.html",
    "category": "大模型/03.训练篇",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://zhuanlan.zhihu.com/p/20329244481\"&gt;大模型精度：FP32、TF32、FP16、BF16、FP8、FP4、NF4、INT8&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;p&gt;大模型的训练和推理，经常涉及到精度的概念，种类很多，而且同等精度级别下，还分不同格式。比如：&lt;/p&gt;\n&lt;p&gt;&lt;a href=\"ht"
  },
  {
    "title": "Untitled",
    "url": "posts/大模型/03.训练篇/06.模型训练.html",
    "category": "大模型/03.训练篇",
    "tags": [],
    "excerpt": "&lt;h2 id=\"1-预训练阶段pretraining-stage\"&gt;1. 预训练阶段（Pretraining Stage）&lt;a class=\"anchor-link\" href=\"#1-预训练阶段pretraining-stage\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;当前，不少工作选择在一个较强的基座模型上进行微调，且通常效果不错（如：[&lt;a hr"
  },
  {
    "title": "第8章 分布式训练",
    "url": "posts/大模型/03.训练篇/08.分布式训练.html",
    "category": "大模型/03.训练篇",
    "tags": [],
    "excerpt": "&lt;h1 id=\"第8章-分布式训练\"&gt;第8章 分布式训练&lt;a class=\"anchor-link\" href=\"#第8章-分布式训练\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;h2 id=\"81-为什么分布式训练越来越流行\"&gt;8.1 为什么分布式训练越来越流行&lt;a class=\"anchor-link\" href=\"#81-为什么分布式训练越来越流行\" t"
  },
  {
    "title": "Untitled",
    "url": "posts/大模型/03.训练篇/04.大模型的数据.html",
    "category": "大模型/03.训练篇",
    "tags": [],
    "excerpt": "&lt;h2 id=\"大语言模型背后的数据\"&gt;大语言模型背后的数据&lt;a class=\"anchor-link\" href=\"#大语言模型背后的数据\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;我们要清楚，大型语言模型是在\"原始文本\"上进行训练的。为了实现高度的能力（如语言和世界知识），这些文本应涵盖广泛的领域、类型、语言等。&lt;/p&gt;\n&lt;p&gt;网络是寻找这种文本的自"
  },
  {
    "title": "Untitled",
    "url": "posts/大模型/03.训练篇/05.大模型各阶段数据.html",
    "category": "大模型/03.训练篇",
    "tags": [],
    "excerpt": "&lt;p&gt;以下是目前常见大模型在不同训练阶段的方案总结及对应阶段的样例数据：&lt;/p&gt;\n&lt;hr /&gt;\n&lt;h3 id=\"1-预训练阶段pretraining\"&gt;&lt;strong&gt;1. 预训练阶段（Pretraining）&lt;/strong&gt;&lt;a class=\"anchor-link\" href=\"#1-预训练阶段pretraining\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h"
  },
  {
    "title": "Untitled",
    "url": "posts/大模型/07.实战篇/01.大模型部署.html",
    "category": "大模型/07.实战篇",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://www.zhihu.com/question/648879790/answer/3504152602\"&gt;目前有什么可以本地部署的大模型推荐?&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;h2 id=\"如何找到最新的大模型\"&gt;如何找到最新的大模型？&lt;a class=\"anchor-link\" href=\"#如何找到最新的大模型\" t"
  },
  {
    "title": "升级pip",
    "url": "posts/大模型/07.实战篇/101.Qwen2.5-7B-Instruct/02.Langchain 接入.html",
    "category": "大模型/07.实战篇/101.Qwen2.5-7B-Instruct",
    "tags": [],
    "excerpt": "&lt;h2 id=\"环境准备\"&gt;环境准备&lt;a class=\"anchor-link\" href=\"#环境准备\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;本文基础环境如下：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;----------------\nubuntu 22.04\npython 3.12\ncuda 12.1\npytorch 2.3.0\n-------------"
  },
  {
    "title": "升级pip",
    "url": "posts/大模型/07.实战篇/101.Qwen2.5-7B-Instruct/01.FastApi 部署调用.html",
    "category": "大模型/07.实战篇/101.Qwen2.5-7B-Instruct",
    "tags": [],
    "excerpt": "&lt;h2 id=\"环境准备\"&gt;环境准备&lt;a class=\"anchor-link\" href=\"#环境准备\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;本文基础环境如下：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;----------------\nubuntu 22.04\npython 3.12\ncuda 12.1\npytorch 2.3.0\n-------------"
  },
  {
    "title": "model_download.py",
    "url": "posts/大模型/07.实战篇/101.Qwen2.5-7B-Instruct/06.o1-like 推理链实现.html",
    "category": "大模型/07.实战篇/101.Qwen2.5-7B-Instruct",
    "tags": [],
    "excerpt": "&lt;h2 id=\"openai-o1-model-简介\"&gt;&lt;strong&gt;OpenAI o1 model 简介&lt;/strong&gt;&lt;a class=\"anchor-link\" href=\"#openai-o1-model-简介\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;OpenAI o1&lt;/strong&gt; 系列模型是使用强化学习训练的大语言模"
  },
  {
    "title": "更换 pypi 源加速库的安装",
    "url": "posts/大模型/07.实战篇/101.Qwen2.5-7B-Instruct/05.Lora 微调.html",
    "category": "大模型/07.实战篇/101.Qwen2.5-7B-Instruct",
    "tags": [],
    "excerpt": "&lt;p&gt;本节我们简要介绍如何基于 transformers、peft 等框架，对 Qwen2.5-7B-Instruct 模型进行 Lora 微调。Lora 是一种高效微调方法，深入了解其原理可参见博客：&lt;a href=\"https://zhuanlan.zhihu.com/p/650197598\"&gt;知乎|深入浅出 Lora&lt;/a&gt;。&lt;/p&gt;\n&lt;p&gt;这个教程会在同目录下给大家提供一个 &lt;a href"
  },
  {
    "title": "model_download.py",
    "url": "posts/大模型/07.实战篇/101.Qwen2.5-7B-Instruct/03.vLLM 部署调用.html",
    "category": "大模型/07.实战篇/101.Qwen2.5-7B-Instruct",
    "tags": [],
    "excerpt": "&lt;h2 id=\"vllm-简介\"&gt;vLLM 简介&lt;a class=\"anchor-link\" href=\"#vllm-简介\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;code&gt;vLLM&lt;/code&gt; 框架是一个高效的大语言模型&lt;strong&gt;推理和部署服务系统&lt;/strong&gt;，具备以下特性：&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;高效的内存管理&lt;"
  },
  {
    "title": "升级 pip",
    "url": "posts/大模型/07.实战篇/101.Qwen2.5-7B-Instruct/04.WebDemo部署.html",
    "category": "大模型/07.实战篇/101.Qwen2.5-7B-Instruct",
    "tags": [],
    "excerpt": "&lt;h2 id=\"环境准备\"&gt;环境准备&lt;a class=\"anchor-link\" href=\"#环境准备\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;----------------\nubuntu 22.04\npython 3.12\ncuda 12.1\npytorch 2.3.0\n----------------\n&lt;/code&gt;&lt;/pre&gt;"
  },
  {
    "title": "更换 pypi 源加速库的安装",
    "url": "posts/大模型/07.实战篇/101.Qwen2.5-7B-Instruct/07.Lora 微调 SwanLab可视化记录版.html",
    "category": "大模型/07.实战篇/101.Qwen2.5-7B-Instruct",
    "tags": [],
    "excerpt": "&lt;p&gt;本节我们简要介绍基于 transformers、peft 等框架，使用 Qwen2.5-7B-Instruct 模型在&lt;strong&gt;中文法律问答数据集 DISC-Law-SFT&lt;/strong&gt; 上进行Lora微调训练，同时使用 &lt;a href=\"https://github.com/swanhubx/swanlab\"&gt;SwanLab&lt;/a&gt; 监控训练过程与评估模型效果，并比较0.5B/1"
  },
  {
    "title": "Untitled",
    "url": "posts/大模型/02.架构篇/204.KV Cache.html",
    "category": "大模型/02.架构篇",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://zhuanlan.zhihu.com/p/662498827\"&gt;大模型推理加速：看图学KV Cache&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;p&gt;KV Cache是Transformer标配的推理加速功能，transformer官方use_cache这个参数默认是True，但是它&lt;strong&gt;只能用于Decoder架构的模"
  },
  {
    "title": "Untitled",
    "url": "posts/大模型/02.架构篇/02.模型架构.html",
    "category": "大模型/02.架构篇",
    "tags": [],
    "excerpt": "&lt;h2 id=\"模型架构\"&gt;模型架构&lt;a class=\"anchor-link\" href=\"#模型架构\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;到目前为止，我们已经将语言模型定义为对词元序列的概率分布 &lt;span class=\"math-inline\"&gt;p(x_{1},…,x_{L})&lt;/span&gt;，我们已经看到这种定义非常优雅且强大（通过提示，语言"
  },
  {
    "title": "Untitled",
    "url": "posts/大模型/01.基础篇/999.LLM术语.html",
    "category": "大模型/01.基础篇",
    "tags": [],
    "excerpt": "&lt;h2 id=\"token模型理解和处理的基本单位\"&gt;Token：模型理解和处理的基本单位&lt;a class=\"anchor-link\" href=\"#token模型理解和处理的基本单位\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;在 AI 领域，Token 是指模型处理的基本数据单位。它可以是单词、字符、短语甚至图像片段、声音片段等。例如，一句话会被分割成"
  },
  {
    "title": "Untitled",
    "url": "posts/大模型/01.基础篇/01.LLM概述.html",
    "category": "大模型/01.基础篇",
    "tags": [],
    "excerpt": "&lt;h2 id=\"大模型-ai-应用全栈开发知识体系\"&gt;大模型 AI 应用全栈开发知识体系&lt;a class=\"anchor-link\" href=\"#大模型-ai-应用全栈开发知识体系\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;div align=center&gt;&lt;img src=\"https://markdownimg-hw.oss-cn-beijing.al"
  },
  {
    "title": "Untitled",
    "url": "posts/大模型/01.基础篇/02.分词.html",
    "category": "大模型/01.基础篇",
    "tags": [],
    "excerpt": "&lt;h2 id=\"分词\"&gt;分词&lt;a class=\"anchor-link\" href=\"#分词\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;语言模型 &lt;span class=\"math-inline\"&gt;p&lt;/span&gt; 是建立在词元（token）序列的上的一个概率分布输出，其中每个词元来自某个词汇表&lt;span class=\"math-inline\"&gt;V&lt;/s"
  },
  {
    "title": "Untitled",
    "url": "posts/大模型/01.基础篇/03.词嵌入.html",
    "category": "大模型/01.基础篇",
    "tags": [],
    "excerpt": ""
  },
  {
    "title": "Untitled",
    "url": "posts/大模型/06.大模型持续学习/03.Continual Learning for Large Language Models A Survey.html",
    "category": "大模型/06.大模型持续学习",
    "tags": [],
    "excerpt": ""
  },
  {
    "title": "Untitled",
    "url": "posts/大模型/06.大模型持续学习/01.Towards Lifelong Learning of Large Language Models A Survey.html",
    "category": "大模型/06.大模型持续学习",
    "tags": [],
    "excerpt": ""
  },
  {
    "title": "Untitled",
    "url": "posts/大模型/06.大模型持续学习/02.Recent Advances of Foundation Language Models-based Continual Learning-A Survey.html",
    "category": "大模型/06.大模型持续学习",
    "tags": [],
    "excerpt": ""
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/15.对比学习/02.对比学习在学什么.html",
    "category": "AINotes/15.对比学习",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;文章来源：&lt;a href=\"https://zhuanlan.zhihu.com/p/634466306\"&gt;对比学习在学啥？&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;p&gt;对比学习是大模型的入门算法。它的想法很简单：对于输入&lt;span class=\"math-inline\"&gt;x&lt;/span&gt;, 找一些它的正样本和负样本，希望在学习之后的网络特征空间中，&lt;span"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/09.MoE/07.MoE-Fusion.html",
    "category": "AINotes/09.MoE",
    "tags": [],
    "excerpt": "&lt;h2 id=\"multi-modal-gated-mixture-of-local-to-global-experts-for-dynamic-image-fusion\"&gt;&lt;a href=\"https://arxiv.org/abs/2302.01392\"&gt;Multi-Modal Gated Mixture of Local-to-Global Experts for Dynamic Image"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/09.MoE/01.MoE.html",
    "category": "AINotes/09.MoE",
    "tags": [],
    "excerpt": "&lt;p&gt;随着 Mixtral 8x7B (&lt;a href=\"https://mistral.ai/news/mixtral-of-experts/\"&gt;announcement&lt;/a&gt;, &lt;a href=\"https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\"&gt;model card&lt;/a&gt;) 的推出，一种称为混合专家模型 (Mixed Expert Mo"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/09.MoE/05.ST-MoE.html",
    "category": "AINotes/09.MoE",
    "tags": [],
    "excerpt": "&lt;h2 id=\"st-moe-designing-stableand-transferable-sparse-expert-models\"&gt;ST-MoE: Designing Stableand Transferable Sparse Expert Models&lt;a class=\"anchor-link\" href=\"#st-moe-designing-stableand-transferable"
  },
  {
    "title": "这里我们假设定义n_embed为32， num_experts=4, top_k=2",
    "url": "posts/AINotes/09.MoE/101.零实现一个MOE.html",
    "category": "AINotes/09.MoE",
    "tags": [],
    "excerpt": "&lt;p&gt;&lt;a href=\"https://zhuanlan.zhihu.com/p/701777558\"&gt;从零实现一个MOE（专家混合模型）&lt;/a&gt;&lt;/p&gt;\n&lt;h2 id=\"什么是混合模型moe\"&gt;什么是混合模型（MOE）&lt;a class=\"anchor-link\" href=\"#什么是混合模型moe\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;MOE主要由"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/09.MoE/09.pMoE.html",
    "category": "AINotes/09.MoE",
    "tags": [],
    "excerpt": "&lt;h2 id=\"patch-level-routing-in-mixture-of-experts-is-provably-sample-efficient-for-convolutional-neural-networks\"&gt;Patch-level Routing in Mixture-of-Experts is Provably Sample-efficient for Convolution"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/09.MoE/02.MoE综述.html",
    "category": "AINotes/09.MoE",
    "tags": [],
    "excerpt": "&lt;h2 id=\"moe模型\"&gt;MoE模型&lt;a class=\"anchor-link\" href=\"#moe模型\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;模型&lt;/th&gt;\n&lt;th&gt;发布时间&lt;/th&gt;\n&lt;th&gt;备注&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;GPT4&"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/09.MoE/08.Switch Transformers.html",
    "category": "AINotes/09.MoE",
    "tags": [],
    "excerpt": ""
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/09.MoE/06.MoE 知识蒸馏.html",
    "category": "AINotes/09.MoE",
    "tags": [],
    "excerpt": "&lt;h2 id=\"one-student-knows-all-experts-know-from-sparse-to-dense\"&gt;One Student Knows All Experts Know: From Sparse to Dense&lt;a class=\"anchor-link\" href=\"#one-student-knows-all-experts-know-from-sparse-to"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/09.MoE/04.V-MoE.html",
    "category": "AINotes/09.MoE",
    "tags": [],
    "excerpt": "&lt;h2 id=\"scaling-vision-with-sparse-mixture-of-experts\"&gt;&lt;a href=\"https://arxiv.org/pdf/2106.05974.pdf\"&gt;Scaling Vision with Sparse Mixture of Experts&lt;/a&gt;&lt;a class=\"anchor-link\" href=\"#scaling-vision-with"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/09.MoE/03.Soft MoE.html",
    "category": "AINotes/09.MoE",
    "tags": [],
    "excerpt": "&lt;h2 id=\"from-sparse-to-soft-mixtures-of-experts\"&gt;From Sparse to Soft Mixtures of Experts&lt;a class=\"anchor-link\" href=\"#from-sparse-to-soft-mixtures-of-experts\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;b"
  },
  {
    "title": "use sinusoidal position embedding to encode time step (https://arxiv.org/abs/1706.03762)",
    "url": "posts/AINotes/06.扩散模型/01.DDPM.html",
    "category": "AINotes/06.扩散模型",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://zhuanlan.zhihu.com/p/535042237\"&gt;生成扩散模型漫谈（一）：DDPM = 拆楼 + 建楼&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;p&gt;说到生成模型，&lt;strong&gt;&lt;a href=\"https://kexue.fm/tag/vae/\"&gt;VAE&lt;/a&gt;&lt;/strong&gt;、&lt;strong&gt;&lt;a href"
  },
  {
    "title": "文生图模型之Stable Diffusion",
    "url": "posts/AINotes/06.扩散模型/04.StableDiffusion.html",
    "category": "AINotes/06.扩散模型",
    "tags": [],
    "excerpt": "&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; [Hao Bai]&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;Link:&lt;/strong&gt; [https://www.zhihu.com/question/536012286/answer/2671001607]&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;泻药。实验室最近人人都在做扩散，从连续到离散，从CV到NLP，基本上都被diffusion洗了一遍。但是"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/06.扩散模型/02.VAE.html",
    "category": "AINotes/06.扩散模型",
    "tags": [],
    "excerpt": "&lt;p&gt;说起生成模型，大家最容易想到的就是GAN，GAN是&lt;strong&gt;通过对抗训练实现的一种隐式生成模型&lt;/strong&gt;。虽然GAN很强大，但其实还有很多与GAN不同的生成模型，最常见的就是基于&lt;strong&gt;最大化似然的模型&lt;/strong&gt;，&lt;strong&gt;变分自动编码器&lt;/strong&gt;（Variational Autoencoder，VAE）就属于这种类型。&lt;/p&gt;\n&lt;h3 id=\"自"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/06.扩散模型/101.扩散模型和最优传输.html",
    "category": "AINotes/06.扩散模型",
    "tags": [],
    "excerpt": "&lt;p&gt;&lt;a href=\"https://zhuanlan.zhihu.com/p/11439249557\"&gt;扩散模型 vs. 最优传输&lt;/a&gt;&lt;/p&gt;\n&lt;h2 id=\"references\"&gt;References&lt;a class=\"anchor-link\" href=\"#references\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;《Understan"
  },
  {
    "title": "扩散模型之DDIM",
    "url": "posts/AINotes/06.扩散模型/03.DDIM.html",
    "category": "AINotes/06.扩散模型",
    "tags": [],
    "excerpt": "&lt;h1 id=\"扩散模型之ddim\"&gt;扩散模型之DDIM&lt;a class=\"anchor-link\" href=\"#扩散模型之ddim\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; [小小将]&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;Link:&lt;/strong&gt; [https://zhuanlan.zhihu.com/"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/08.PTM/01.Pre-Trained Models-Past Present and Future.html",
    "category": "AINotes/08.PTM",
    "tags": [],
    "excerpt": "&lt;p&gt;利用深度学习自动学习特征已经逐步取代了人工构建特征和统计方法。但其中一个关键问题是需要大量的数据，否则会因为参数过多过拟合。但是这个成本非常高昂，因此长久以来，我们都在研究一个关键问题：如何在有限数据下训练高效的深度学习模型？&lt;/p&gt;\n&lt;p&gt;一个重要的里程碑是转移学习——受人类启发，不是从大量数据中从头开始学习，而是利用少量样例来解决问题。转移学习有两个阶段：预训练+微调，微调阶段就是转移预"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/08.PTM/02.PFM.html",
    "category": "AINotes/08.PTM",
    "tags": [],
    "excerpt": "&lt;h2 id=\"kimi全文翻译-arrow_down\"&gt;Kimi全文翻译 :arrow_down:&lt;a class=\"anchor-link\" href=\"#kimi全文翻译-arrow_down\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/03.循环神经网络/31.RNN.html",
    "category": "AINotes/03.循环神经网络",
    "tags": [],
    "excerpt": "&lt;h2 id=\"recurrent-neural-networks\"&gt;Recurrent Neural Networks&lt;a class=\"anchor-link\" href=\"#recurrent-neural-networks\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;blockquote&gt;\n&lt;p&gt;RNN，或者说最常用的LSTM，一般用于记住之前的状态，"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/03.循环神经网络/32.LSTM.html",
    "category": "AINotes/03.循环神经网络",
    "tags": [],
    "excerpt": "&lt;h2 id=\"其他rnn\"&gt;其他RNN&lt;a class=\"anchor-link\" href=\"#其他rnn\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;div align=center&gt;&lt;img src=\"https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202402251112181.png\""
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/14.PEFT/06.Propulsion.html",
    "category": "AINotes/14.PEFT",
    "tags": [],
    "excerpt": "&lt;h2 id=\"propulsion-steering-llm-with-tiny-fine-tuning\"&gt;&lt;a href=\"https://arxiv.org/abs/2409.10927\"&gt;Propulsion: Steering LLM with Tiny Fine-Tuning&lt;/a&gt;&lt;a class=\"anchor-link\" href=\"#propulsion-steering-ll"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/14.PEFT/01.LoRA/03.LoRA-GA.html",
    "category": "AINotes/14.PEFT/01.LoRA",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;来源：&lt;a href=\"https://kexue.fm/archives/10226\"&gt;对齐全量微调！这是我看过最精彩的LoRA改进（一）&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;p&gt;众所周知，LoRA是一种常见的参数高效的微调方法。LoRA利用低秩分解来降低微调参数量，节省微调显存，同时训练好的权重可以合并到原始权重上，推理架构不需要作出改变，是一种训练和"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/14.PEFT/01.LoRA/04.LoRA-Pro.html",
    "category": "AINotes/14.PEFT/01.LoRA",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;ul&gt;\n&lt;li&gt;解读来源：&lt;a href=\"https://kexue.fm/archives/10266\"&gt;对齐全量微调！这是我看过最精彩的LoRA改进（二）&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;文章：&lt;a href=\"https://arxiv.org/abs/2407.18242\"&gt;LoRA-Pro: Are Low-Rank Adapters Properly Optim"
  },
  {
    "title": "------------------------------------",
    "url": "posts/AINotes/14.PEFT/01.LoRA/01.LoRA.html",
    "category": "AINotes/14.PEFT/01.LoRA",
    "tags": [],
    "excerpt": "&lt;h2 id=\"一全参数微调\"&gt;一、全参数微调&lt;a class=\"anchor-link\" href=\"#一全参数微调\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;div align=center&gt;&lt;img src=\"https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20251025153210.png\" s"
  },
  {
    "title": "------------------------------------",
    "url": "posts/AINotes/14.PEFT/01.LoRA/02.AdaLoRA.html",
    "category": "AINotes/14.PEFT/01.LoRA",
    "tags": [],
    "excerpt": "&lt;h2 id=\"一adalora在做一件什么事\"&gt;一、AdaLoRA在做一件什么事&lt;a class=\"anchor-link\" href=\"#一adalora在做一件什么事\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"11-lora是怎么做微调的\"&gt;1.1 LoRA是怎么做微调的&lt;a class=\"anchor-link\" href=\"#11-l"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/14.PEFT/01.LoRA/05.LoRAPlus.html",
    "category": "AINotes/14.PEFT/01.LoRA",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://arxiv.org/abs/2402.12354\"&gt;《LoRA+: Efficient Low Rank Adaptation of Large Models》&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;解读来源：&lt;a href=\"https://zhuanlan.zhihu.com/p/687660397\"&gt;配置不同的学习率，LoRA还能再涨一点？&lt;"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/14.PEFT/01.LoRA/08.AM-LoRA.html",
    "category": "AINotes/14.PEFT/01.LoRA",
    "tags": [],
    "excerpt": "&lt;h2 id=\"learning-attentional-mixture-ofloras-for-language-model-continual-learning\"&gt;Learning Attentional Mixture ofLoRAs for Language Model Continual Learning&lt;a class=\"anchor-link\" href=\"#learning-att"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/14.PEFT/01.LoRA/06.MoSLoRA.html",
    "category": "AINotes/14.PEFT/01.LoRA",
    "tags": [],
    "excerpt": "&lt;h2 id=\"kimi全文翻译-arrow_down\"&gt;Kimi全文翻译 :arrow_down:&lt;a class=\"anchor-link\" href=\"#kimi全文翻译-arrow_down\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/14.PEFT/01.LoRA/09.MiLoRA.html",
    "category": "AINotes/14.PEFT/01.LoRA",
    "tags": [],
    "excerpt": "&lt;h2 id=\"milora-effcient-mixture-of-low-rank-adaptation-for-large-language-models-fine-tuning\"&gt;MiLoRA: Effcient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning&lt;a class=\"anchor-link"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/14.PEFT/01.LoRA/10.LoRAMoE/01.LoRAMoE.html",
    "category": "AINotes/14.PEFT/01.LoRA/10.LoRAMoE",
    "tags": [],
    "excerpt": "&lt;h2 id=\"loramoe\"&gt;LoRAMoE&lt;a class=\"anchor-link\" href=\"#loramoe\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h2 id=\"1-背景\"&gt;1. 背景&lt;a class=\"anchor-link\" href=\"#1-背景\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;大"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/14.PEFT/01.LoRA/10.LoRAMoE/03.MOLE.html",
    "category": "AINotes/14.PEFT/01.LoRA/10.LoRAMoE",
    "tags": [],
    "excerpt": "&lt;h2 id=\"mixture-of-lora-experts\"&gt;&lt;a href=\"https://arxiv.org/abs/2404.13628\"&gt;Mixture of LoRA Experts&lt;/a&gt;&lt;a class=\"anchor-link\" href=\"#mixture-of-lora-experts\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h2"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/14.PEFT/04.P-Tuning/01.P-tuning.html",
    "category": "AINotes/14.PEFT/04.P-Tuning",
    "tags": [],
    "excerpt": ""
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/14.PEFT/02.PrefixTuning/01.Prefix tuning.html",
    "category": "AINotes/14.PEFT/02.PrefixTuning",
    "tags": [],
    "excerpt": "&lt;h2 id=\"prefix-tuning-optimizing-continuous-prompts-for-generationacl-2021\"&gt;Prefix-Tuning: Optimizing Continuous Prompts for Generation（ACL 2021）&lt;a class=\"anchor-link\" href=\"#prefix-tuning-optimizing-"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/14.PEFT/00.Survey/00.PEFT综述.html",
    "category": "AINotes/14.PEFT/00.Survey",
    "tags": [],
    "excerpt": "&lt;h2 id=\"lst-ladder-side-tuning-for-parameter-and-memory-efficient-transfer-learning\"&gt;LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning&lt;a class=\"anchor-link\" href=\"#lst-ladde"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/14.PEFT/00.Survey/01.Towards a Unified View of Parameter-Efficient Transfer Learning.html",
    "category": "AINotes/14.PEFT/00.Survey",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://github.com/jxhe/unify-parameter-efficient-tuning\"&gt;Code&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/14.PEFT/00.Survey/02.Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models A Critical Review and Assessment.html",
    "category": "AINotes/14.PEFT/00.Survey",
    "tags": [],
    "excerpt": "&lt;h2 id=\"0-摘要\"&gt;0 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;随着基于 Transformer 的预训练语言模型（PLMs）参数数量的不断增长，特别是具有数十亿参数的大型语言模型（LLMs）的出现，许多自然语言处理（NLP）任务展示出了显著的成功。然而，这些模型的巨大规"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/14.PEFT/03.PromptTuning/01.Prompt-Tuning.html",
    "category": "AINotes/14.PEFT/03.PromptTuning",
    "tags": [],
    "excerpt": "&lt;h2 id=\"面向预训练语言模型的-prompt-tuning-技术发展历程\"&gt;面向预训练语言模型的 Prompt-Tuning 技术发展历程&lt;a class=\"anchor-link\" href=\"#面向预训练语言模型的-prompt-tuning-技术发展历程\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;img alt=\"图片\" src=\"http"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/14.PEFT/05.Adapter/101.R-Adapter.html",
    "category": "AINotes/14.PEFT/05.Adapter",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://zhuanlan.zhihu.com/p/718589490\"&gt;R-Adapter：零样本模型微调新突破，提升鲁棒性与泛化能力 | ECCV 2024&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;大规模图像-文本预训练模型实现了零样本分类，并在不同数据分布下提供了一致的准确性。然而，这些模型在下游任务中通常需要微调优化，这会降低对于超出分布范围的数据的泛"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/14.PEFT/05.Adapter/01.Adapter综述.html",
    "category": "AINotes/14.PEFT/05.Adapter",
    "tags": [],
    "excerpt": "&lt;h2 id=\"parameter-efficient-transfer-learning-for-nlp-adaptericml-2019\"&gt;Parameter-Efficient Transfer Learning for NLP Adapter（ICML 2019）&lt;a class=\"anchor-link\" href=\"#parameter-efficient-transfer-learn"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/02.卷积神经网络/01.卷积神经网络.html",
    "category": "AINotes/02.卷积神经网络",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;Convolutional Neural Networks&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;h2 id=\"11-为什么cnn\"&gt;1.1 为什么CNN&lt;a class=\"anchor-link\" href=\"#11-为什么cnn\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;计算机视觉要面临一个挑战，就是数据的输入可能会非"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/44.LTCIL/01.APART.html",
    "category": "AINotes/44.LTCIL",
    "tags": [],
    "excerpt": "&lt;h2 id=\"adaptive-adapter-routing-for-long-tailed-class-incremental-learning\"&gt;Adaptive Adapter Routing for Long-Tailed Class-Incremental Learning&lt;a class=\"anchor-link\" href=\"#adaptive-adapter-routing-f"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/44.LTCIL/00.Survey/01.不平衡类增量学习综述.html",
    "category": "AINotes/44.LTCIL/00.Survey",
    "tags": [],
    "excerpt": "&lt;h2 id=\"cvpr2024-gr\"&gt;CVPR2024 GR&lt;a class=\"anchor-link\" href=\"#cvpr2024-gr\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"http://export.arxiv.org/abs/2402.18528\"&gt;Gradient Reweighting:"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/22.Mamba/05.VMamba.html",
    "category": "AINotes/22.Mamba",
    "tags": [],
    "excerpt": "&lt;p&gt;论文标题：VMamba: Visual State Space Model&lt;/p&gt;\n&lt;p&gt;论文地址: https://arxiv.org/abs/2401.10166&lt;/p&gt;\n&lt;p&gt;代码地址: https://github.com/MzeroMiko/VMamba&lt;/p&gt;"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/22.Mamba/01.概述.html",
    "category": "AINotes/22.Mamba",
    "tags": [],
    "excerpt": "&lt;h2 id=\"背景\"&gt;背景&lt;a class=\"anchor-link\" href=\"#背景\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;Transformer:以其注意力机制而闻名，其中序列的任何部分都可以动态地与任何其他部分相互作用，特别是具有因果注意力机制的的Transformer，擅长处理序列中的单个元素。但是它们带来了显著的计算和内存成本，与序列"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/22.Mamba/02.符号说明.html",
    "category": "AINotes/22.Mamba",
    "tags": [],
    "excerpt": "&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;符号&lt;/th&gt;\n&lt;th&gt;维度&lt;/th&gt;\n&lt;th&gt;符号说明&lt;/th&gt;\n&lt;th&gt;默认值&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;span class=\"math-inline\"&gt;A&lt;/span&gt;&lt;/td&gt;\n&lt;td&gt;&lt;/td&gt;\n&lt;td&gt;&lt;/td&gt"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/22.Mamba/03.问题.html",
    "category": "AINotes/22.Mamba",
    "tags": [],
    "excerpt": "&lt;h2 id=\"selection-mechanism\"&gt;Selection Mechanism&lt;a class=\"anchor-link\" href=\"#selection-mechanism\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;img alt=\"20240306151603\" src=\"https://markdownimg-hw.oss-c"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/22.Mamba/04.Vision Mamba.html",
    "category": "AINotes/22.Mamba",
    "tags": [],
    "excerpt": "&lt;p&gt;论文地址：https://arxiv.org/pdf/2401.09417.pdf&lt;br /&gt;\n项目地址：https://github.com/hustvl/Vim&lt;br /&gt;\n论文标题：Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model&lt;/p&gt;\n&lt;div al"
  },
  {
    "title": "load pre-trained image processor for efficientnet-b7 and model weight",
    "url": "posts/AINotes/07.计算机视觉/101.基于架构方法的嵌入对比.html",
    "category": "AINotes/07.计算机视觉",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://mp.weixin.qq.com/s/mvozZ_iIRtFgmHUoAne80Q\"&gt;图像相似性搜索比较：EfficientNet vs. ViT vs. DINO-v2 vs. CLIP vs. BLIP-2&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;p&gt;在本文中，我将使用Flickr数据集[6]比较EfficientNet["
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/07.计算机视觉/01DINO系列讲解/01.DINO.html",
    "category": "AINotes/07.计算机视觉/01DINO系列讲解",
    "tags": [],
    "excerpt": ""
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/05.VisionTransformer/102.Sparse-Tuning.html",
    "category": "AINotes/05.VisionTransformer",
    "tags": [],
    "excerpt": "&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;参数效率微调（PEFT）已成为适应预训练ViT模型到下游应用的流行解决方案。虽然当前的 PEFT 方法实现了参数效率，但它们忽视了在微调和推理期间的计算和 GPU 内存效率，未能满足实际需求。在本文中，我们"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/05.VisionTransformer/01.VIT全文翻译.html",
    "category": "AINotes/05.VisionTransformer",
    "tags": [],
    "excerpt": "&lt;h2 id=\"an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale\"&gt;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&lt;a class=\"anchor-link\" href=\"#an-image-is-w"
  },
  {
    "title": "ViT 微调时position embedding如何插值（interpolate）【源码解析】",
    "url": "posts/AINotes/05.VisionTransformer/04.ViT微调时position_embedding插值.html",
    "category": "AINotes/05.VisionTransformer",
    "tags": [],
    "excerpt": "&lt;h1 id=\"vit-微调时position-embedding如何插值interpolate源码解析\"&gt;ViT 微调时position embedding如何插值（interpolate）【源码解析】&lt;a class=\"anchor-link\" href=\"#vit-微调时position-embedding如何插值interpolate源码解析\" title=\"Permanent link\""
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/05.VisionTransformer/00.ViT综述.html",
    "category": "AINotes/05.VisionTransformer",
    "tags": [],
    "excerpt": "&lt;h2 id=\"peeling-back-the-layers-interpreting-the-storytelling-of-vit\"&gt;&lt;a href=\"https://dl.acm.org/doi/10.1145/3664647.3681712\"&gt;Peeling Back the Layers: Interpreting the Storytelling of ViT&lt;/a&gt;&lt;a class"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/05.VisionTransformer/101.DyT.html",
    "category": "AINotes/05.VisionTransformer",
    "tags": [],
    "excerpt": "&lt;h2 id=\"dynamic-tuning-towards-parameter-and-inference-efficiency-for-vit-adaptation\"&gt;&lt;a href=\"http://arxiv.org/abs/2403.11808\"&gt;Dynamic Tuning Towards Parameter and Inference Efficiency for ViT Adapta"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/05.VisionTransformer/02.VIT解读.html",
    "category": "AINotes/05.VisionTransformer",
    "tags": [],
    "excerpt": "&lt;h2 id=\"vision-transformer-详解\"&gt;Vision Transformer 详解&lt;a class=\"anchor-link\" href=\"#vision-transformer-详解\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;div align=center&gt;&lt;img src=\"https://markdownimg-hw.oss-c"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/05.VisionTransformer/51.ViT_four secrets.html",
    "category": "AINotes/05.VisionTransformer",
    "tags": [],
    "excerpt": "&lt;h2 id=\"vision-transformer-to-discover-the-four-secrets-of-image-patches\"&gt;&lt;a href=\"https://linkinghub.elsevier.com/retrieve/pii/S1566253524000265\"&gt;Vision transformer: To discover the “four secrets” of"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/05.VisionTransformer/03.ViT模型列表.html",
    "category": "AINotes/05.VisionTransformer",
    "tags": [],
    "excerpt": "&lt;h2 id=\"符号定义\"&gt;符号定义&lt;a class=\"anchor-link\" href=\"#符号定义\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;在论文的 Table1 中有给出三个模型（Base/ Large/ Huge）的参数&lt;/p&gt;\n&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Model&lt;/th&gt;\n&lt;th&gt;Patch Size&lt;/th&gt;\n&lt;"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/04.Transformer/06.Self-Attention拓展.html",
    "category": "AINotes/04.Transformer",
    "tags": [],
    "excerpt": "&lt;h3 id=\"self-attention-vs-cnn3\"&gt;Self-attention v.s. CNN3&lt;a class=\"anchor-link\" href=\"#self-attention-vs-cnn3\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h3&gt;\n&lt;p&gt;我们可以来比较一下，Self-attention 跟 CNN 之间，有什么样的差异或者是关联性&lt;"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/04.Transformer/07.Transformer模型详解_图解最完整版.html",
    "category": "AINotes/04.Transformer",
    "tags": [],
    "excerpt": "&lt;p&gt;&lt;a href=\"https://zhuanlan.zhihu.com/p/338817680\"&gt;Transformer模型详解（图解最完整版）&lt;/a&gt;&lt;/p&gt;\n&lt;h2 id=\"前言\"&gt;前言&lt;a class=\"anchor-link\" href=\"#前言\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;Transformer由论文《Attention i"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/04.Transformer/01.Self-Attention概述.html",
    "category": "AINotes/04.Transformer",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://hw-universal.oss-cn-beijing.aliyuncs.com/self_v7.pptx\"&gt;self_v7.pptx&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;h2 id=\"sophisticated-input\"&gt;Sophisticated Input&lt;a class=\"anchor-link\" href=\""
  },
  {
    "title": "Bert_李宏毅",
    "url": "posts/AINotes/04.Transformer/30.Bert.html",
    "category": "AINotes/04.Transformer",
    "tags": [],
    "excerpt": "&lt;h2 id=\"seq2seq\"&gt;Seq2Seq&lt;a class=\"anchor-link\" href=\"#seq2seq\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;在开始讲解Attention之前，我们先简单回顾一下Seq2Seq模型，传统的机器翻译基本都是基于Seq2Seq模型来做的，该模型分为encoder层与decoder层，并均为RNN或RNN"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/04.Transformer/11.Transformer中FFNs的作用.html",
    "category": "AINotes/04.Transformer",
    "tags": [],
    "excerpt": "&lt;h2 id=\"transformer-与-ffn\"&gt;Transformer 与 FFN&lt;a class=\"anchor-link\" href=\"#transformer-与-ffn\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://zhuanlan.zhihu.com/p/685943779\"&gt;聊一聊"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/04.Transformer/101.Temp.html",
    "category": "AINotes/04.Transformer",
    "tags": [],
    "excerpt": "&lt;h2 id=\"a-high-level-look\"&gt;A High-Level Look&lt;a class=\"anchor-link\" href=\"#a-high-level-look\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;首先将这个模型看成是一个黑箱操作。在机器翻译中，就是输入一种语言，输出另一种语言。&lt;/p&gt;\n&lt;p&gt;&lt;img style=\"\" sr"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/04.Transformer/03.Multi-head-SelfAttention.html",
    "category": "AINotes/04.Transformer",
    "tags": [],
    "excerpt": "&lt;h2 id=\"multi-head-self-attention\"&gt;Multi-head Self-attention&lt;a class=\"anchor-link\" href=\"#multi-head-self-attention\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;Self-attention 有一个进阶的版本，叫做 &lt;strong&gt;Multi-"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/04.Transformer/08.Transformer代码实现.html",
    "category": "AINotes/04.Transformer",
    "tags": [],
    "excerpt": "&lt;h2 id=\"多层transformer\"&gt;多层Transformer&lt;a class=\"anchor-link\" href=\"#多层transformer\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;在多层Transformer中，多层编码器先对输入序列进行编码，然后得到最后一个Encoder的输出Memory；解码器先通过Masked Multi-H"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/04.Transformer/10.Self-Attention Q K V的原理.html",
    "category": "AINotes/04.Transformer",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;文章来源：&lt;a href=\"https://www.zhihu.com/question/592626839/answer/3304714001\"&gt;为什么Self-Attention要通过线性变换计算Q K V，背后的原理或直观解释是什么？&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;p&gt;“线性变换”是机器学习中针对数据常用的变换方式，通过线性变换可以将数据进行降"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/04.Transformer/07.Transformer.html",
    "category": "AINotes/04.Transformer",
    "tags": [],
    "excerpt": "&lt;h2 id=\"transformer\"&gt;Transformer&lt;a class=\"anchor-link\" href=\"#transformer\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;blockquote&gt;\n&lt;p&gt;paper: &lt;a href=\"https://arxiv.org/abs/1706.03762\"&gt;Attention Is All You"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/04.Transformer/02.Self-Attention理论推导.html",
    "category": "AINotes/04.Transformer",
    "tags": [],
    "excerpt": "&lt;h2 id=\"理论推导\"&gt;理论推导&lt;a class=\"anchor-link\" href=\"#理论推导\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;Self-Attention的Input，是一串的Vector，那&lt;strong&gt;这个Vector可能是你整个Network的Input，它也可能是某个Hidden Layer的Output&lt;/strong&gt;"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/04.Transformer/09.Transformer模型的参数量与计算量.html",
    "category": "AINotes/04.Transformer",
    "tags": [],
    "excerpt": "&lt;h2 id=\"1-前言\"&gt;1. 前言&lt;a class=\"anchor-link\" href=\"#1-前言\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;最近，OpenAI推出的ChatGPT展现出了卓越的性能，引发了大规模语言模型(Large Language Model, LLM)的研究热潮。大规模语言模型的“大”体现在两个方面：模型参数规模大，训练数据"
  },
  {
    "title": "变分自编码器VAE：原来是这么一回事 | 附开源代码",
    "url": "posts/AINotes/04.Transformer/40.VAE.html",
    "category": "AINotes/04.Transformer",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;文章来源：&lt;a href=\"https://zhuanlan.zhihu.com/p/348498294\"&gt;机器学习方法—优雅的模型（一）：变分自编码器（VAE）&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;h2 id=\"1-introduction\"&gt;1. Introduction&lt;a class=\"anchor-link\" href=\"#1-introduct"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/04.Transformer/05.Self-Attention-APP.html",
    "category": "AINotes/04.Transformer",
    "tags": [],
    "excerpt": "&lt;h2 id=\"applications-\"&gt;Applications …&lt;a class=\"anchor-link\" href=\"#applications-\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;Self-attention 当然是用得很广，我们已经提过很多次 transformer 这个东西&lt;/p&gt;\n&lt;div align=center&gt;&lt;img"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/999.深度学习调参指南/06.优化器.html",
    "category": "AINotes/999.深度学习调参指南",
    "tags": [],
    "excerpt": "&lt;h3 id=\"选择优化器\"&gt;选择优化器&lt;a class=\"anchor-link\" href=\"#选择优化器\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h3&gt;\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;总结&lt;/em&gt;&lt;/strong&gt;： &lt;em&gt;从针对手头问题类型的最常用的优化器开始。&lt;/em&gt;&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;没有一个优化器是适用于所有类型的机器学习问题和模"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/999.深度学习调参指南/01.模型.html",
    "category": "AINotes/999.深度学习调参指南",
    "tags": [],
    "excerpt": "&lt;h3 id=\"选择模型架构\"&gt;选择模型架构&lt;a class=\"anchor-link\" href=\"#选择模型架构\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h3&gt;\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;总结&lt;/em&gt;&lt;/strong&gt;： &lt;em&gt;在开始一个新项目时，尽量重用有效的模型。&lt;/em&gt;&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;首先选择一个完善且常用的模型架构来开始工作。这样可"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/999.深度学习调参指南/07.初始化.html",
    "category": "AINotes/999.深度学习调参指南",
    "tags": [],
    "excerpt": "&lt;ul&gt;\n&lt;li&gt;预训练参数是最好的参数初始化方式，其次是Xavir。&lt;/li&gt;\n&lt;/ul&gt;"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/999.深度学习调参指南/09.激活函数.html",
    "category": "AINotes/999.深度学习调参指南",
    "tags": [],
    "excerpt": "&lt;ul&gt;\n&lt;li&gt;ReLu、Sigmoid、Softmax、Tanh是最常用的4个激活函数。&lt;/li&gt;\n&lt;li&gt;对于输出层，常用sigmoid和softMax激活函数，中间层常用ReLu激活函数，RNN常用Tanh激活函数。&lt;/li&gt;\n&lt;/ul&gt;"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/999.深度学习调参指南/04.学习率.html",
    "category": "AINotes/999.深度学习调参指南",
    "tags": [],
    "excerpt": "&lt;ul&gt;\n&lt;li&gt;学习率最好是从高到低2倍速度递减一般从0.01开始。&lt;/li&gt;\n&lt;li&gt;如果使用微调，则learning rate设置为0.0001较好。learning rate设置上有很多trick，包括cosing learning rate等。&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h3 id=\"最好的学习率衰减方案是什么\"&gt;最好的学习率衰减方案是什么&lt;a class=\"anchor-link\" hr"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/999.深度学习调参指南/08.Epoch.html",
    "category": "AINotes/999.深度学习调参指南",
    "tags": [],
    "excerpt": "&lt;ul&gt;\n&lt;li&gt;Epoch number和Early stopping是息息相关的，需要输出loss看一下，到底是什么epoch时效果最好，及时early stopping。&lt;/li&gt;\n&lt;li&gt;Epoch越大，会浪费计算资源；epoch太小，则训练模型提取特征没到极致。&lt;/li&gt;\n&lt;/ul&gt;"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/999.深度学习调参指南/00.深度学习调参指南.html",
    "category": "AINotes/999.深度学习调参指南",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/schrodingercatss/tuning_playbook_zh_cn\"&gt;tuning_playbook_zh_cn&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.zhihu.com/question/25097993/answer/2718208647\"&gt;深度学习调参有"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/999.深度学习调参指南/03.损失函数.html",
    "category": "AINotes/999.深度学习调参指南",
    "tags": [],
    "excerpt": "&lt;ul&gt;\n&lt;li&gt;Focal loss对于极大不平衡的数据集确实有奇效，其中gamma因子可以成10倍数衰减&lt;/li&gt;\n&lt;li&gt;Loss function是Model和数据之外，第三重要的参数。具体使用MSE、Cross entropy、Focal还是其他自定义，需要具体问题具体分析。&lt;/li&gt;\n&lt;/ul&gt;"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/999.深度学习调参指南/05.Batch Size.html",
    "category": "AINotes/999.深度学习调参指南",
    "tags": [],
    "excerpt": "&lt;ul&gt;\n&lt;li&gt;batch size不能太大，也不能太小；太小会浪费计算资源，太大则会浪费内存；一般设置为16的倍数。对于推荐来说32-64-128-512测试效果再高一般也不会正向了，再低训练太慢了。&lt;/li&gt;\n&lt;li&gt;Learning rate和batch size是两个重要的参数，而且二者也是相互影响的，在反向传播时直接影响梯度。一般情况下，先调batch size，再调learning "
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/999.深度学习调参指南/02.数据.html",
    "category": "AINotes/999.深度学习调参指南",
    "tags": [],
    "excerpt": "&lt;ul&gt;\n&lt;li&gt;数据量太大的情况下，可以先用1/10，1/100的数据先去估算一下训练或者推理时间，心里有个底。&lt;/li&gt;\n&lt;li&gt;视觉问题一定要使用数据增强。&lt;/li&gt;\n&lt;li&gt;一定要进行数据预处理，把数据分布分散到均值为0，方差为1的区间，利于训练模型。&lt;/li&gt;\n&lt;/ul&gt;"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/08.模型优化/02.批量和动量.html",
    "category": "AINotes/01.MLTutorials/08.模型优化",
    "tags": [],
    "excerpt": "&lt;h2 id=\"批量大小对梯度下降法的影响\"&gt;批量大小对梯度下降法的影响&lt;a class=\"anchor-link\" href=\"#批量大小对梯度下降法的影响\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;实际上在计算梯度的时候，并不是对所有数据的损失 &lt;span class=\"math-inline\"&gt;L&lt;/span&gt; 计算梯度，而是把所有的数据分成一个一"
  },
  {
    "title": "AI | 算法工程师必备的深度学习--最优化（上）",
    "url": "posts/AINotes/01.MLTutorials/08.模型优化/103.AI算法工程师必备的深度学习最优化上.html",
    "category": "AINotes/01.MLTutorials/08.模型优化",
    "tags": [],
    "excerpt": "&lt;p&gt;&lt;img alt=\"cover_image\" src=\"https://mmbiz.qlogo.cn/mmbiz_jpg/x3YYuUrJv099lPmzMAIv0VO5yKmukdjxibNYMM0Oiaa7rVlbf0KsQ3WtWn7LkMia5NfSZSnnNdG6Hk9w3CUTT69IA/0?wx_fmt=jpeg\" /&gt;&lt;/p&gt;\n&lt;h1 id=\"ai--算法工程师必备的深度学习"
  },
  {
    "title": "机器学习调参自动优化方法",
    "url": "posts/AINotes/01.MLTutorials/08.模型优化/106.机器学习调参自动优化方法.html",
    "category": "AINotes/01.MLTutorials/08.模型优化",
    "tags": [],
    "excerpt": "&lt;p&gt;&lt;img alt=\"cover_image\" src=\"https://mmbiz.qlogo.cn/mmbiz_jpg/vI9nYe94fsEdWl1RjERNWqia63EmoBmWJFgw9TUA0ibJm5hvHWMcHXm4YmAkBibr3yZX8b4RZic2VR0yuLRGL9BibOg/0?wx_fmt=jpeg\" /&gt;&lt;/p&gt;\n&lt;h1 id=\"机器学习调参自动优化方法\"&gt;"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/08.模型优化/102.各种Optimizer梯度下降优化算法回顾和总结.html",
    "category": "AINotes/01.MLTutorials/08.模型优化",
    "tags": [],
    "excerpt": "&lt;p&gt;&lt;a href=\"https://zhuanlan.zhihu.com/p/343564175\"&gt;论文阅读笔记：各种Optimizer梯度下降优化算法回顾和总结&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;不管是使用PyTorch还是TensorFlow，用多了Optimizer优化器封装好的函数，对其内部使用的优化算法却没有仔细研究过，也很难对其优点和缺点进行实用的解释。所以打算以这一篇论文为主线并结合多篇优秀"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/08.模型优化/04.weight_decay.html",
    "category": "AINotes/01.MLTutorials/08.模型优化",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://blog.csdn.net/zhaohongfei_358/article/details/129625803\"&gt;权重衰减weight_decay参数从入门到精通&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;h2 id=\"1-什么是权重衰减weight-decay\"&gt;1 什么是权重衰减(Weight Decay)&lt;a class="
  },
  {
    "title": "PyTorch | 优化神经网络训练的17种方法",
    "url": "posts/AINotes/01.MLTutorials/08.模型优化/104.PyTorch优化神经网络训练的17种方法.html",
    "category": "AINotes/01.MLTutorials/08.模型优化",
    "tags": [],
    "excerpt": "&lt;h1 id=\"pytorch--优化神经网络训练的17种方法\"&gt;PyTorch | 优化神经网络训练的17种方法&lt;a class=\"anchor-link\" href=\"#pytorch--优化神经网络训练的17种方法\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;blockquote&gt;\n&lt;p&gt;https://mp.weixin.qq.com/s/WUN015"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/08.模型优化/101.优化算法.html",
    "category": "AINotes/01.MLTutorials/08.模型优化",
    "tags": [],
    "excerpt": "&lt;h2 id=\"todo\"&gt;TODO&lt;a class=\"anchor-link\" href=\"#todo\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h2 id=\"gradient-descent--momentum\"&gt;Gradient Descent + Momentum&lt;a class=\"anchor-link\" href=\"#gradient-desce"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/08.模型优化/03.学习率.html",
    "category": "AINotes/01.MLTutorials/08.模型优化",
    "tags": [],
    "excerpt": "&lt;p&gt;临界点其实不一定是在训练一个网络的时候会遇到的最大的障碍。图 3.18 中的横坐标代表参数更新的次数，竖坐标表示损失。一般在训练一个网络的时候，损失原来很大，随着参数不断的更新，损失会越来越小，最后就卡住了，损失不再下降。当我们走到临界点的时候，意味着梯度非常小，但损失不再下降的时候，梯度并没有真的变得很小，图 3.19 给出了示例。图 3.19 中横轴是迭代次数，竖轴是梯度的范数（norm"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/08.模型优化/01.局部极小值与鞍点.html",
    "category": "AINotes/01.MLTutorials/08.模型优化",
    "tags": [],
    "excerpt": "&lt;p&gt;&lt;strong&gt;临界点&lt;/strong&gt;：&lt;br /&gt;\n&lt;strong&gt;局部极小值&lt;/strong&gt; ：&lt;br /&gt;\n&lt;strong&gt;鞍点&lt;/strong&gt;：&lt;/p&gt;\n&lt;h2 id=\"局部极小值与鞍点\"&gt;局部极小值与鞍点&lt;a class=\"anchor-link\" href=\"#局部极小值与鞍点\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;我们在做优"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/08.模型优化/109.梯度下降.html",
    "category": "AINotes/01.MLTutorials/08.模型优化",
    "tags": [],
    "excerpt": "&lt;h2 id=\"梯度下降\"&gt;梯度下降&lt;a class=\"anchor-link\" href=\"#梯度下降\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;解决下面的最优化问题：&lt;br /&gt;\n&lt;div class=\"math-display\"&gt;&lt;br /&gt;\n\\theta^∗= \\underset{ \\theta }{\\operatorname{arg\\ min}"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/08.模型优化/05.优化器.html",
    "category": "AINotes/01.MLTutorials/08.模型优化",
    "tags": [],
    "excerpt": "&lt;h2 id=\"一个框架回顾优化算法\"&gt;一个框架回顾优化算法&lt;a class=\"anchor-link\" href=\"#一个框架回顾优化算法\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;深度学习优化算法经历了 SGD -&gt; SGDM -&gt; NAG -&gt;AdaGrad -&gt; AdaDelta -&gt; Adam -&gt; Nada"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/08.模型优化/110.反向传播.html",
    "category": "AINotes/01.MLTutorials/08.模型优化",
    "tags": [],
    "excerpt": "&lt;h2 id=\"背景\"&gt;背景&lt;a class=\"anchor-link\" href=\"#背景\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"梯度下降\"&gt;梯度下降&lt;a class=\"anchor-link\" href=\"#梯度下降\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h3&gt;\n&lt;p&gt;&lt;img alt=\"\" src=\""
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/08.模型优化/107.Dropout.html",
    "category": "AINotes/01.MLTutorials/08.模型优化",
    "tags": [],
    "excerpt": "&lt;h2 id=\"dropout\"&gt;Dropout&lt;a class=\"anchor-link\" href=\"#dropout\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"how-to-train\"&gt;How to train?&lt;a class=\"anchor-link\" href=\"#how-to-train\" title=\"Permanent li"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/01.机器学习基础/104.经典网络.html",
    "category": "AINotes/01.MLTutorials/01.机器学习基础",
    "tags": [],
    "excerpt": "&lt;h2 id=\"lenet-5\"&gt;LeNet-5&lt;a class=\"anchor-link\" href=\"#lenet-5\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;LeNet-5&lt;/strong&gt;的网络结构，假设你有一张32×32×1的图片，&lt;strong&gt;LeNet-5&lt;/strong&gt;可以识别图中的手写数字，比如像这样手写数字7。&lt;"
  },
  {
    "title": "GAT（Graph Attention Network）",
    "url": "posts/AINotes/01.MLTutorials/01.机器学习基础/11.GAT.html",
    "category": "AINotes/01.MLTutorials/01.机器学习基础",
    "tags": [],
    "excerpt": "&lt;h1 id=\"gatgraph-attention-network\"&gt;GAT（Graph Attention Network）&lt;a class=\"anchor-link\" href=\"#gatgraph-attention-network\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;h2 id=\"1-基础概念\"&gt;&lt;strong&gt;1 基础概念&lt;/strong&gt;"
  },
  {
    "title": "Graph Convolutional Network",
    "url": "posts/AINotes/01.MLTutorials/01.机器学习基础/10.GCN.html",
    "category": "AINotes/01.MLTutorials/01.机器学习基础",
    "tags": [],
    "excerpt": "&lt;h1 id=\"graph-convolutional-network\"&gt;Graph Convolutional Network&lt;a class=\"anchor-link\" href=\"#graph-convolutional-network\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;h2 id=\"overview\"&gt;Overview&lt;a class=\"an"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/01.机器学习基础/101.各种机器学习算法的应用场景.html",
    "category": "AINotes/01.MLTutorials/01.机器学习基础",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://www.zhihu.com/question/26726794/answer/151282052\"&gt;各种机器学习算法的应用场景分别是什么（比如朴素贝叶斯、决策树、K 近邻、SVM、逻辑回归最大熵模型）？&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;p&gt;关于这个问题我今天正好看到了这个文章。讲的正是各个算法的优劣分析，很中肯。&lt;/p"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/01.机器学习基础/40.网络压缩.html",
    "category": "AINotes/01.MLTutorials/01.机器学习基础",
    "tags": [],
    "excerpt": "&lt;h2 id=\"network-pruning修剪\"&gt;Network Pruning（修剪）&lt;a class=\"anchor-link\" href=\"#network-pruning修剪\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"introduction\"&gt;INTRODUCTION&lt;a class=\"anchor-link\" href=\"#in"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/01.机器学习基础/09.Embedding.html",
    "category": "AINotes/01.MLTutorials/01.机器学习基础",
    "tags": [],
    "excerpt": "&lt;div align=center&gt;&lt;img src=\"https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20240224165837.png\" style=\"zoom: 50%;\" /&gt;&lt;/div&gt;\n\n&lt;p&gt;“Embedding”在字面上的翻译是“嵌入”，但在机器学习和自然语言处理的上下文中，我们更倾向于将其理解为一种“向量化”或“向量表示”的"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/01.机器学习基础/07.训练调试策略.html",
    "category": "AINotes/01.MLTutorials/01.机器学习基础",
    "tags": [],
    "excerpt": "&lt;h2 id=\"训练策略\"&gt;训练策略&lt;a class=\"anchor-link\" href=\"#训练策略\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;img alt=\"image-20220926172200984\" src=\"https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image20220926"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/01.机器学习基础/03.归一化和标准化.html",
    "category": "AINotes/01.MLTutorials/01.机器学习基础",
    "tags": [],
    "excerpt": "&lt;h2 id=\"归一化-normalization\"&gt;归一化 Normalization&lt;a class=\"anchor-link\" href=\"#归一化-normalization\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;归一化一般是将数据映射到指定的范围，用于去除不同维度数据的量纲以及量纲单位。&lt;/p&gt;\n&lt;p&gt;常见的映射范围有 [0, 1] 和 ["
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/01.机器学习基础/400.感知机.html",
    "category": "AINotes/01.MLTutorials/01.机器学习基础",
    "tags": [],
    "excerpt": "&lt;h3 id=\"导读\"&gt;导读&lt;a class=\"anchor-link\" href=\"#导读\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h3&gt;\n&lt;p&gt;感知机是二类分类的&lt;strong&gt;线性分类模型&lt;/strong&gt;，输入为实例的特征向量，输出为实例的类别。感知机对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面，属于判别模型。&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;损失"
  },
  {
    "title": "Logistic Regression",
    "url": "posts/AINotes/01.MLTutorials/01.机器学习基础/06.逻辑回归.html",
    "category": "AINotes/01.MLTutorials/01.机器学习基础",
    "tags": [],
    "excerpt": "&lt;h1 id=\"logistic-regression\"&gt;Logistic Regression&lt;a class=\"anchor-link\" href=\"#logistic-regression\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;h2 id=\"classification\"&gt;Classification&lt;a class=\"anchor-link\" h"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/01.机器学习基础/35.线性判别分析LDA.html",
    "category": "AINotes/01.MLTutorials/01.机器学习基础",
    "tags": [],
    "excerpt": "&lt;h2 id=\"线性判别分析lda\"&gt;线性判别分析（LDA）&lt;a class=\"anchor-link\" href=\"#线性判别分析lda\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h4 id=\"二分类\"&gt;二分类&lt;a class=\"anchor-link\" href=\"#二分类\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h4&gt;\n"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/01.机器学习基础/02.单变量线性回归.html",
    "category": "AINotes/01.MLTutorials/01.机器学习基础",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;单变量线性回归(Linear Regression with One Variable)&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;h2 id=\"模型表示model-representation\"&gt;模型表示(Model Representation)&lt;a class=\"anchor-link\" href=\"#模型表示model-representation\" title"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/01.机器学习基础/01.机器学习术语.html",
    "category": "AINotes/01.MLTutorials/01.机器学习基础",
    "tags": [],
    "excerpt": "&lt;h2 id=\"batch\"&gt;Batch&lt;a class=\"anchor-link\" href=\"#batch\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;我们实际上在算微分的时候,并不是真的对所有 Data 算出来的 &lt;span class=\"math-inline\"&gt;L&lt;/span&gt; 作微分,你是把所有的 Data 分成一个一个的 Batch(Mini"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/01.机器学习基础/103.哈希学习.html",
    "category": "AINotes/01.MLTutorials/01.机器学习基础",
    "tags": [],
    "excerpt": "&lt;h2 id=\"1-背景介绍\"&gt;1. 背景介绍&lt;a class=\"anchor-link\" href=\"#1-背景介绍\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;首先介绍一下最近邻搜索：最近邻搜索问题，也叫相似性搜索，近似搜索，是从给定数据库中找到里查询点最近的点集的问题。&lt;/p&gt;\n&lt;p&gt;&lt;img src=\"https://markdownimg-hw."
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/01.机器学习基础/16.损失函数.html",
    "category": "AINotes/01.MLTutorials/01.机器学习基础",
    "tags": [],
    "excerpt": "&lt;h2 id=\"loss-functioncost-function\"&gt;Loss Function/Cost Function&lt;a class=\"anchor-link\" href=\"#loss-functioncost-function\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;blockquote&gt;\n&lt;p&gt;&lt;strong&gt;损失函数&lt;/strong&gt;度量模"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/01.机器学习基础/15.距离函数.html",
    "category": "AINotes/01.MLTutorials/01.机器学习基础",
    "tags": [],
    "excerpt": "&lt;div align=center&gt;&lt;img src=\"https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208160952736.jpg\"/&gt;&lt;/div&gt;\n\n&lt;h2 id=\"欧氏距离euclidean-distance\"&gt;欧氏距离（Euclidean Distance）&lt;a class=\"anchor-link\" href=\"#"
  },
  {
    "title": "Reinforcement Learning",
    "url": "posts/AINotes/01.MLTutorials/01.机器学习基础/21.强化学习.html",
    "category": "AINotes/01.MLTutorials/01.机器学习基础",
    "tags": [],
    "excerpt": "&lt;h1 id=\"reinforcement-learning\"&gt;Reinforcement Learning&lt;a class=\"anchor-link\" href=\"#reinforcement-learning\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;blockquote&gt;\n&lt;p&gt;基于李宏毅老师2021年课程的精简版&lt;/p&gt;\n&lt;/blockquote&gt;\n"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/01.机器学习基础/100.机器学习算法集锦从贝叶斯到深度学习及各自优缺点.html",
    "category": "AINotes/01.MLTutorials/01.机器学习基础",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://zhuanlan.zhihu.com/p/25327755\"&gt;机器学习算法集锦：从贝叶斯到深度学习及各自优缺点&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;&lt;em&gt;在我们日常生活中所用到的推荐系统、智能图片美化应用和聊天机器人等应用中，各种各样的机器学习和数据处理算法正尽职尽责地发挥着自己的功效。本文筛选并简单介绍了一些最常见算法类别，还为每一个类别列出"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/01.机器学习基础/08.激活函数.html",
    "category": "AINotes/01.MLTutorials/01.机器学习基础",
    "tags": [],
    "excerpt": "&lt;p&gt;激活函数（Activation Function）是一种添加到人工神经网络中的函数，旨在帮助网络学习数据中的复杂模式。类似于人类大脑中基于神经元的模型，激活函数最终决定了要发射给下一个神经元的内容。&lt;/p&gt;\n&lt;h2 id=\"sigmoid-函数\"&gt;Sigmoid 函数&lt;a class=\"anchor-link\" href=\"#sigmoid-函数\" title=\"Permanent link"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/01.机器学习基础/36.Sequence2sequence.html",
    "category": "AINotes/01.MLTutorials/01.机器学习基础",
    "tags": [],
    "excerpt": "&lt;h2 id=\"sequence-to-sequence-seq2seq\"&gt;Sequence-to-sequence (Seq2seq)&lt;a class=\"anchor-link\" href=\"#sequence-to-sequence-seq2seq\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;Transformer就是一个,==Sequence-to-"
  },
  {
    "title": "多变量线性回归(Linear Regression with Multiple Variables)",
    "url": "posts/AINotes/01.MLTutorials/01.机器学习基础/05.多变量线性回归.html",
    "category": "AINotes/01.MLTutorials/01.机器学习基础",
    "tags": [],
    "excerpt": "&lt;h1 id=\"多变量线性回归linear-regression-with-multiple-variables\"&gt;多变量线性回归(Linear Regression with Multiple Variables)&lt;a class=\"anchor-link\" href=\"#多变量线性回归linear-regression-with-multiple-variables\" title=\"Perma"
  },
  {
    "title": "正则化",
    "url": "posts/AINotes/01.MLTutorials/01.机器学习基础/04.正则化.html",
    "category": "AINotes/01.MLTutorials/01.机器学习基础",
    "tags": [],
    "excerpt": "&lt;h1 id=\"正则化\"&gt;正则化&lt;a class=\"anchor-link\" href=\"#正则化\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;&lt;strong&gt;正则化主要用于避免过拟合的产生和减少网络误差。&lt;/strong&gt;&lt;/p&gt;\n&lt;p&gt;正则化一般具有如下形式：&lt;/p&gt;\n&lt;p&gt;&lt;div class=\"math-display\"&gt;&lt;br /&gt;\nJ(w,b)"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/01.机器学习基础/102.数据集.html",
    "category": "AINotes/01.MLTutorials/01.机器学习基础",
    "tags": [],
    "excerpt": "&lt;h2 id=\"train--dev--test-sets\"&gt;Train / Dev / Test sets&lt;a class=\"anchor-link\" href=\"#train--dev--test-sets\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;假设这是训练数据，我用一个长方形表示，我们通常会将这些数据划分成几部分&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;训练"
  },
  {
    "title": "贝叶斯分类器",
    "url": "posts/AINotes/01.MLTutorials/01.机器学习基础/13.BayesClassifier.html",
    "category": "AINotes/01.MLTutorials/01.机器学习基础",
    "tags": [],
    "excerpt": "&lt;h1 id=\"贝叶斯分类器\"&gt;贝叶斯分类器&lt;a class=\"anchor-link\" href=\"#贝叶斯分类器\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;&lt;strong&gt;贝叶斯分类器&lt;/strong&gt;（Bayes Classifier）是一种通过最大化后验概率进行单点估计的分类器。&lt;/p&gt;\n&lt;p&gt;这一章的内容大致如下：&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;\n"
  },
  {
    "title": "模型评估与选择",
    "url": "posts/AINotes/01.MLTutorials/02.模型与算法/06.模型评估与选择.html",
    "category": "AINotes/01.MLTutorials/02.模型与算法",
    "tags": [],
    "excerpt": "&lt;h1 id=\"模型评估与选择\"&gt;模型评估与选择&lt;a class=\"anchor-link\" href=\"#模型评估与选择\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;h2 id=\"误差\"&gt;误差&lt;a class=\"anchor-link\" href=\"#误差\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;在分类任务中，通"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/02.模型与算法/02.PCA.html",
    "category": "AINotes/01.MLTutorials/02.模型与算法",
    "tags": [],
    "excerpt": "&lt;h2 id=\"1相关背景\"&gt;1.相关背景&lt;a class=\"anchor-link\" href=\"#1相关背景\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;在许多领域的研究与应用中，通常需要对含有多个变量的数据进行观测，收集大量数据后进行分析寻找规律。多变量大数据集无疑会为研究和应用提供丰富的信息，但是也在一定程度上增加了数据采集的工作量。更重要的是在很"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/02.模型与算法/101.正交普鲁克问题.html",
    "category": "AINotes/01.MLTutorials/02.模型与算法",
    "tags": [],
    "excerpt": "&lt;h2 id=\"正交普鲁克问题\"&gt;正交普鲁克问题&lt;a class=\"anchor-link\" href=\"#正交普鲁克问题\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;blockquote&gt;\n&lt;p&gt;OrthogonalProcrustesProblem&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;p&gt;&lt;div class=\"math-display\"&gt;&lt;br /&gt;\n"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/02.模型与算法/04.K-means.html",
    "category": "AINotes/01.MLTutorials/02.模型与算法",
    "tags": [],
    "excerpt": "&lt;p&gt;K-means 是我们最常用的基于欧式距离的聚类算法，其认为两个目标的距离越近，相似度越大。&lt;/p&gt;\n&lt;p&gt;本文大致思路为：先介绍经典的牧师-村名模型来引入 K-means 算法，然后介绍算法步骤和时间复杂度，通过介绍其优缺点来引入算法的调优与改进，最后我们利用之前学的 EM 算法，对其进行收敛证明。&lt;/p&gt;\n&lt;h2 id=\"1-算法\"&gt;1. 算法&lt;a class=\"anchor-link\""
  },
  {
    "title": "初始化",
    "url": "posts/AINotes/01.MLTutorials/02.模型与算法/102.指数移动平均EDA.html",
    "category": "AINotes/01.MLTutorials/02.模型与算法",
    "tags": [],
    "excerpt": "&lt;p&gt;在深度学习中，经常会使用EMA（指数移动平均）这个方法对模型的参数做平均，以求提高测试指标并增加模型鲁棒。&lt;/p&gt;\n&lt;h2 id=\"ema的定义\"&gt;EMA的定义&lt;a class=\"anchor-link\" href=\"#ema的定义\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;指数移动平均（Exponential Moving Average）也叫权"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/02.模型与算法/103.梯度提升GB.html",
    "category": "AINotes/01.MLTutorials/02.模型与算法",
    "tags": [],
    "excerpt": "&lt;p&gt;&lt;strong&gt;Gradient Boosting&lt;/strong&gt;是Boosting中的一大类算法，它的思想借鉴于梯度下降法，其基本原理是根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器以累加的形式结合到现有模型中。&lt;/p&gt;\n&lt;div align=center&gt;&lt;img src=\"https://markdownimg-hw.oss-cn-beijing.al"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/02.模型与算法/01.KNN.html",
    "category": "AINotes/01.MLTutorials/02.模型与算法",
    "tags": [],
    "excerpt": "&lt;h2 id=\"knn-简介\"&gt;KNN 简介&lt;a class=\"anchor-link\" href=\"#knn-简介\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;假定我们现在有一个训练集，和一个测试集，对于其中一个测试样本，在训练集中找到与该样本最邻近的 &lt;span class=\"math-inline\"&gt;k&lt;/span&gt; 个样本，在这 &lt;"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/01.MLTutorials/02.模型与算法/03.SVD.html",
    "category": "AINotes/01.MLTutorials/02.模型与算法",
    "tags": [],
    "excerpt": "&lt;h2 id=\"svd性质\"&gt;SVD性质&lt;a class=\"anchor-link\" href=\"#svd性质\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;矩阵的奇异值分解一定存在，但不唯一；&lt;/li&gt;\n&lt;li&gt;奇异值唯一，但矩阵&lt;span class=\"math-inline\"&gt;U&lt;/span&gt; &lt;span class=\"math-inlin"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/39.CL/101.Theory on Mixture-of-Experts in Continual Learning.html",
    "category": "AINotes/39.CL",
    "tags": [],
    "excerpt": "&lt;h2 id=\"theory-on-mixture-of-experts-in-continual-learning\"&gt;&lt;a href=\"http://arxiv.org/abs/2406.16437\"&gt;Theory on Mixture-of-Experts in Continual Learning&lt;/a&gt;&lt;a class=\"anchor-link\" href=\"#theory-on-mixt"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/41.小样本类增量学习FSCIL/04.CPE-CLIP.html",
    "category": "AINotes/41.小样本类增量学习FSCIL",
    "tags": [],
    "excerpt": "&lt;h2 id=\"multimodal-parameter-efficient-few-shot-class-incremental-learning\"&gt;&lt;a href=\"https://arxiv.org/abs/2303.04751\"&gt;Multimodal Parameter-Efficient Few-Shot Class Incremental Learning&lt;/a&gt;&lt;a class=\"a"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/41.小样本类增量学习FSCIL/05.CoACT.html",
    "category": "AINotes/41.小样本类增量学习FSCIL",
    "tags": [],
    "excerpt": "&lt;h2 id=\"few-shot-tuning-of-foundation-models-for-class-incremental-learning\"&gt;&lt;a href=\"https://arxiv.org/abs/2405.16625\"&gt;Few-shot Tuning of Foundation Models for Class-incremental Learning&lt;/a&gt;&lt;a class="
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/41.小样本类增量学习FSCIL/06.ASP.html",
    "category": "AINotes/41.小样本类增量学习FSCIL",
    "tags": [],
    "excerpt": "&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;少样本类增量学习（Few-Shot Class-Incremental Learning，FSCIL）模型旨在在保留旧类知识的同时，利用稀缺样本逐步学习新类别。现有的 FSCIL 方法通常对整个骨干网络进行"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/41.小样本类增量学习FSCIL/03.PriViLege.html",
    "category": "AINotes/41.小样本类增量学习FSCIL",
    "tags": [],
    "excerpt": "&lt;h2 id=\"pre-trained-vision-and-language-transformers-are-few-shot-incremental-learners\"&gt;&lt;a href=\"https://arxiv.org/abs/2404.02117\"&gt;Pre-trained Vision and Language Transformers Are Few-Shot Incremental"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/41.小样本类增量学习FSCIL/00.综述Survey/02.A survey on few-shot class-incremental learning.html",
    "category": "AINotes/41.小样本类增量学习FSCIL/00.综述Survey",
    "tags": [],
    "excerpt": "&lt;h2 id=\"a-survey-on-few-shot-class-incremental-learning\"&gt;&lt;a href=\"https://arxiv.org/abs/2304.08130\"&gt;A survey on few-shot class-incremental learning&lt;/a&gt;&lt;a class=\"anchor-link\" href=\"#a-survey-on-few-sho"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/41.小样本类增量学习FSCIL/00.综述Survey/01.小样本类增量学习论文阅读.html",
    "category": "AINotes/41.小样本类增量学习FSCIL/00.综述Survey",
    "tags": [],
    "excerpt": "&lt;h2 id=\"graph-based-methods\"&gt;Graph-Based Methods&lt;a class=\"anchor-link\" href=\"#graph-based-methods\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"topic\"&gt;TOPIC&lt;a class=\"anchor-link\" href=\"#topic\" title"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/41.小样本类增量学习FSCIL/00.综述Survey/03.Few-shot Class-incremental Learning A Survey.html",
    "category": "AINotes/41.小样本类增量学习FSCIL/00.综述Survey",
    "tags": [],
    "excerpt": "&lt;h2 id=\"few-shot-class-incremental-learning-a-survey\"&gt;&lt;a href=\"https://arxiv.org/abs/2308.06764\"&gt;Few-shot Class-incremental Learning: A Survey&lt;/a&gt;&lt;a class=\"anchor-link\" href=\"#few-shot-class-increment"
  },
  {
    "title": "Generate some random data",
    "url": "posts/AINotes/00.Python/04.NumPy/02.ndarray.html",
    "category": "AINotes/00.Python/04.NumPy",
    "tags": [],
    "excerpt": "&lt;h2 id=\"1numpy-的ndarray一种多维数组对象\"&gt;1、NumPy 的&lt;code&gt;ndarray&lt;/code&gt;：一种多维数组对象&lt;a class=\"anchor-link\" href=\"#1numpy-的ndarray一种多维数组对象\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;NumPy 最重要的一个特点就是其 N 维数组对象（即&lt;code"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/00.Python/04.NumPy/08.其他.html",
    "category": "AINotes/00.Python/04.NumPy",
    "tags": [],
    "excerpt": "&lt;h2 id=\"示例随机漫步\"&gt;示例：随机漫步&lt;a class=\"anchor-link\" href=\"#示例随机漫步\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;我们通过模拟随机漫步来说明如何运用数组运算。先来看一个简单的随机漫步的例子：从 0 开始，步长 1 和 -1 出现的概率相等。&lt;/p&gt;\n&lt;p&gt;下面是一个通过内置的&lt;code&gt;random&lt;/co"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/00.Python/04.NumPy/07.伪随机数生成.html",
    "category": "AINotes/00.Python/04.NumPy",
    "tags": [],
    "excerpt": "&lt;h2 id=\"伪随机数生成\"&gt;伪随机数生成&lt;a class=\"anchor-link\" href=\"#伪随机数生成\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;code&gt;numpy.random&lt;/code&gt;模块对 Python 内置的&lt;code&gt;random&lt;/code&gt;进行了补充，增加了一些用于高效生成多种概率分布的样本值的函数。例如，你可以用&lt;c"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/00.Python/04.NumPy/01.Numpy基础.html",
    "category": "AINotes/00.Python/04.NumPy",
    "tags": [],
    "excerpt": "&lt;p&gt;NumPy（Numerical Python的简称）是Python数值计算最重要的基础包。大多数提供科学计算的包都是用NumPy的数组作为构建基础。&lt;/p&gt;\n&lt;p&gt;NumPy的部分功能如下：&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;ndarray，一个具有矢量算术运算和复杂广播能力的快速且节省空间的多维数组。&lt;/li&gt;\n&lt;li&gt;用于对整组数据进行快速运算的标准数学函数（无需编写循环）。&lt;/li&gt;\n&lt;li&gt;"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/00.Python/04.NumPy/05.文件输入输出.html",
    "category": "AINotes/00.Python/04.NumPy",
    "tags": [],
    "excerpt": "&lt;h2 id=\"4用于数组的文件输入输出\"&gt;4、用于数组的文件输入输出&lt;a class=\"anchor-link\" href=\"#4用于数组的文件输入输出\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;NumPy 能够读写磁盘上的文本数据或二进制数据。这一小节只讨论 NumPy 的内置二进制格式，因为更多的用户会使用 pandas 或其它工具加载文本或表格数"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/00.Python/04.NumPy/03.通用函数.html",
    "category": "AINotes/00.Python/04.NumPy",
    "tags": [],
    "excerpt": "&lt;h2 id=\"2通用函数快速的元素级数组函数\"&gt;2、通用函数：快速的元素级数组函数&lt;a class=\"anchor-link\" href=\"#2通用函数快速的元素级数组函数\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;通用函数（即&lt;code&gt;ufunc&lt;/code&gt;）是一种对&lt;code&gt;ndarray&lt;/code&gt;中的数据执行元素级运算的函数。你可以将其"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/00.Python/04.NumPy/04.数据处理.html",
    "category": "AINotes/00.Python/04.NumPy",
    "tags": [],
    "excerpt": "&lt;h2 id=\"利用数组进行数据处理\"&gt;利用数组进行数据处理&lt;a class=\"anchor-link\" href=\"#利用数组进行数据处理\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;NumPy 数组使你可以将许多种数据处理任务表述为简洁的数组表达式（否则需要编写循环）。用数组表达式代替循环的做法，通常被称为向量化。一般来说，向量化数组运算要比等价的纯 "
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/00.Python/04.NumPy/06.线性代数.html",
    "category": "AINotes/00.Python/04.NumPy",
    "tags": [],
    "excerpt": "&lt;h2 id=\"线性代数\"&gt;线性代数&lt;a class=\"anchor-link\" href=\"#线性代数\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;线性代数（如矩阵乘法、矩阵分解、行列式以及其他方阵数学等）是任何数组库的重要组成部分。不像某些语言（如 MATLAB），通过*对两个二维数组相乘得到的是一个元素级的积，而不是一个矩阵点积。因此，NumPy 提"
  },
  {
    "title": "以三个双引号或单引号开头的字符串可以折行",
    "url": "posts/AINotes/00.Python/02.数据结构 函数 和文件/01.字符串.html",
    "category": "AINotes/00.Python/02.数据结构 函数 和文件",
    "tags": [],
    "excerpt": "&lt;h2 id=\"字符串\"&gt;字符串&lt;a class=\"anchor-link\" href=\"#字符串\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h2 id=\"表示\"&gt;表示&lt;a class=\"anchor-link\" href=\"#表示\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;由一对&lt;strong"
  },
  {
    "title": "乘号表示列表元素的重复",
    "url": "posts/AINotes/00.Python/02.数据结构 函数 和文件/03.列表.html",
    "category": "AINotes/00.Python/02.数据结构 函数 和文件",
    "tags": [],
    "excerpt": "&lt;p&gt;列表（&lt;code&gt;list&lt;/code&gt;），是一种结构化的、非标量类型，它的值是有序序列，每个值都可以通过索引进行标识。&lt;/p&gt;\n&lt;h2 id=\"列表定义\"&gt;列表定义&lt;a class=\"anchor-link\" href=\"#列表定义\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;列表用 &lt;strong&gt;[ ]&lt;/strong&gt; 表示，是有"
  },
  {
    "title": "定义一个ArgumentParser实例",
    "url": "posts/AINotes/00.Python/02.数据结构 函数 和文件/14.常用函数.html",
    "category": "AINotes/00.Python/02.数据结构 函数 和文件",
    "tags": [],
    "excerpt": "&lt;h2 id=\"argparse\"&gt;argparse&lt;a class=\"anchor-link\" href=\"#argparse\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;code&gt;argparse&lt;/code&gt; 是 Python 的一个标准库，用于编写用户友好的命令行接口。它允许开发者定义命令行参数，并解析这些参数以供程序使用。下面是对 &lt;code&gt;"
  },
  {
    "title": "创建集合的字面量语法",
    "url": "posts/AINotes/00.Python/02.数据结构 函数 和文件/05.集合(set).html",
    "category": "AINotes/00.Python/02.数据结构 函数 和文件",
    "tags": [],
    "excerpt": "&lt;h2 id=\"集合set\"&gt;集合(set)&lt;a class=\"anchor-link\" href=\"#集合set\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"定义\"&gt;定义&lt;a class=\"anchor-link\" href=\"#定义\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h3&gt;\n&lt;p&gt;Python中的集合跟数"
  },
  {
    "title": "当需要计算阶乘的时候不用再写循环求阶乘而是直接调用已经定义好的函数",
    "url": "posts/AINotes/00.Python/02.数据结构 函数 和文件/11.函数和模块的使用.html",
    "category": "AINotes/00.Python/02.数据结构 函数 和文件",
    "tags": [],
    "excerpt": "&lt;h2 id=\"定义函数\"&gt;定义函数&lt;a class=\"anchor-link\" href=\"#定义函数\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;def 函数名 (参数(0个或多个))&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;函数体&lt;/p&gt;\n&lt;p&gt;return 返回值&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;函数"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/00.Python/02.数据结构 函数 和文件/02.序列类型.html",
    "category": "AINotes/00.Python/02.数据结构 函数 和文件",
    "tags": [],
    "excerpt": "&lt;h2 id=\"序列类型定义\"&gt;序列类型定义&lt;a class=\"anchor-link\" href=\"#序列类型定义\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;序列是具有&lt;strong&gt;先后关系&lt;/strong&gt;的一组元素;是一个基类类型&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;序列是一维元素向量，元素类型可以不同&lt;/p&gt;\n&lt;/li&gt;\n&lt;"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/00.Python/02.数据结构 函数 和文件/04.元组.html",
    "category": "AINotes/00.Python/02.数据结构 函数 和文件",
    "tags": [],
    "excerpt": "&lt;h2 id=\"定义\"&gt;定义&lt;a class=\"anchor-link\" href=\"#定义\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;元组是一种序列类型，一旦创建就&lt;strong&gt;不能被修改&lt;/strong&gt;;&lt;/li&gt;\n&lt;li&gt;使用小括号 &lt;strong&gt;() 或 tuple() 创建&lt;/strong&gt;，元素间用逗号 , 分隔可以使用或不"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/00.Python/02.数据结构 函数 和文件/06.字典(dict).html",
    "category": "AINotes/00.Python/02.数据结构 函数 和文件",
    "tags": [],
    "excerpt": "&lt;h2 id=\"字典dict\"&gt;字典(dict)&lt;a class=\"anchor-link\" href=\"#字典dict\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"定义\"&gt;定义&lt;a class=\"anchor-link\" href=\"#定义\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h3&gt;\n&lt;ol&gt;\n&lt;li&gt;采用大"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/00.Python/03.进阶/101.下划线的几种含义.html",
    "category": "AINotes/00.Python/03.进阶",
    "tags": [],
    "excerpt": "&lt;p&gt;在 Python 中，下划线（&lt;code&gt;_&lt;/code&gt;）有多种用途和约定，它们通常与变量命名、模块导入、特殊方法（魔术方法）等相关。以下是一些常见的用途和含义：&lt;/p&gt;\n&lt;div align=center&gt;&lt;img src=\"https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20240616100415.png\"/&gt;&lt;/div&gt;\n\n&lt;o"
  },
  {
    "title": "python中的模块、库、包有什么区别？",
    "url": "posts/AINotes/00.Python/03.进阶/102.python中的模块库包的区别.html",
    "category": "AINotes/00.Python/03.进阶",
    "tags": [],
    "excerpt": "&lt;h1 id=\"python中的模块库包有什么区别\"&gt;python中的模块、库、包有什么区别？&lt;a class=\"anchor-link\" href=\"#python中的模块库包有什么区别\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; [风影忍着]&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;Link:&lt;/strong&gt; "
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/00.Python/03.进阶/12.类和对象.html",
    "category": "AINotes/00.Python/03.进阶",
    "tags": [],
    "excerpt": "&lt;h2 id=\"类和对象\"&gt;类和对象&lt;a class=\"anchor-link\" href=\"#类和对象\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;&quot;把一组数据结构和处理它们的方法组成对象（object），把相同行为的对象归纳为类（class），通过类的封装（encapsulation）隐藏内部细节，通过继承（inheritance）实现类的特"
  },
  {
    "title": "调用__call__()方法 C语言中文网 http://c.biancheng.net",
    "url": "posts/AINotes/00.Python/03.进阶/13.面向对象进阶.html",
    "category": "AINotes/00.Python/03.进阶",
    "tags": [],
    "excerpt": "&lt;h2 id=\"面向对象进阶\"&gt;面向对象进阶&lt;a class=\"anchor-link\" href=\"#面向对象进阶\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"property-装饰器\"&gt;@property 装饰器&lt;a class=\"anchor-link\" href=\"#property-装饰器\" title=\"Permanent link\""
  },
  {
    "title": "用户名是admin且密码是123456则身份验证成功否则身份验证失败",
    "url": "posts/AINotes/00.Python/01.语法基础/02.语言元素.html",
    "category": "AINotes/00.Python/01.语法基础",
    "tags": [],
    "excerpt": "&lt;h2 id=\"变量和类型\"&gt;变量和类型&lt;a class=\"anchor-link\" href=\"#变量和类型\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;在程序设计中，变量是一种存储数据的载体。计算机中的变量是实际存在的数据或者说是存储器中存储数据的一块内存空间，变量的值可以被读取和修改，这是所有计算和控制的基础。计算机能处理的数据有很多种类型，除了数值"
  },
  {
    "title": "... 此处省略上面的代码 ...",
    "url": "posts/AINotes/00.Python/01.语法基础/01.初识Python.html",
    "category": "AINotes/00.Python/01.语法基础",
    "tags": [],
    "excerpt": "&lt;h2 id=\"python的历史\"&gt;Python的历史&lt;a class=\"anchor-link\" href=\"#python的历史\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;ol&gt;\n&lt;li&gt;1989年圣诞节：Guido von Rossum开始写Python语言的编译器。&lt;/li&gt;\n&lt;li&gt;1991年2月：第一个Python编译器（同时也是解释器）诞生，"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/11.LongTailed/01.Deep Long-Tailed Learning-A Survey.html",
    "category": "AINotes/11.LongTailed",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;https://arxiv.org/abs/2110.04596&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;h2 id=\"kimi-arrow_down\"&gt;Kimi :arrow_down:&lt;a class=\"anchor-link\" href=\"#kimi-arrow_down\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 i"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/11.LongTailed/02.A Systematic Review on Long-Tailed Learning.html",
    "category": "AINotes/11.LongTailed",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;https://arxiv.org/abs/2408.00483&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;h2 id=\"kimi-arrow_down\"&gt;Kimi :arrow_down:&lt;a class=\"anchor-link\" href=\"#kimi-arrow_down\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 i"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/16.小样本学习/01.小样本学习论文阅读.html",
    "category": "AINotes/16.小样本学习",
    "tags": [],
    "excerpt": "&lt;h2 id=\"a-closer-look-at-few-shot-classification\"&gt;&lt;a href=\"https://arxiv.org/abs/1904.04232\"&gt;A Closer Look at Few-shot Classification&lt;/a&gt;&lt;a class=\"anchor-link\" href=\"#a-closer-look-at-few-shot-classif"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/16.小样本学习/00.小样本学习.html",
    "category": "AINotes/16.小样本学习",
    "tags": [],
    "excerpt": "&lt;p&gt;&lt;strong&gt;Link:&lt;/strong&gt; &lt;a href=\"https://zhuanlan.zhihu.com/p/258562899\"&gt;小样本学习——概念、原理与方法简介（Few-shot learning）&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;&lt;a href=\"https://zhida.zhihu.com/search?content_id=145589507&amp;cont"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/16.小样本学习/02.UCL.html",
    "category": "AINotes/16.小样本学习",
    "tags": [],
    "excerpt": "&lt;h2 id=\"universal-representation-learning-from-multiple-domains-for-few-shot-classification\"&gt;&lt;a href=\"https://arxiv.org/abs/2103.13841\"&gt;Universal representation learning from multiple domains for few-"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/51.分布外检测/03.PViT.html",
    "category": "AINotes/51.分布外检测",
    "tags": [],
    "excerpt": "&lt;h2 id=\"pvit-prior-augmented-vision-transformer-for-out-of-distribution-detection\"&gt;&lt;a href=\"http://arxiv.org/abs/2410.20631\"&gt;PViT: Prior-augmented Vision Transformer for Out-of-distribution Detection&lt;"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/51.分布外检测/02.Mahalanobis.html",
    "category": "AINotes/51.分布外检测",
    "tags": [],
    "excerpt": "&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;在许多现实世界的机器学习应用中，检测从训练分布中统计或对抗性偏离的测试样本是部署一个良好分类器的基本要求。然而，已知带有 softmax 分类器的深度神经网络即使对于此类异常样本也会产生高度自信的后验分布。"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/51.分布外检测/01.ViM.html",
    "category": "AINotes/51.分布外检测",
    "tags": [],
    "excerpt": ""
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/42.Logit Rectify/01.BiC.html",
    "category": "AINotes/40.CIL/42.Logit Rectify",
    "tags": [],
    "excerpt": "&lt;h2 id=\"large-scale-incremental-learning\"&gt;Large scale incremental learning&lt;a class=\"anchor-link\" href=\"#large-scale-incremental-learning\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/45.Energy/02.EA.html",
    "category": "AINotes/40.CIL/45.Energy",
    "tags": [],
    "excerpt": "&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;在类别增量学习（Class Incremental Learning, CIL）中，模型需要能够持续学习新的类别。然而，标准的深度神经网络（DNNs）会遭受灾难性遗忘的问题。最近的研究表明，类别不平衡是导致"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/45.Energy/03.ESN.html",
    "category": "AINotes/40.CIL/45.Energy",
    "tags": [],
    "excerpt": "&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;本文关注增量学习阶段中普遍存在的性能不平衡问题。为了避免明显的阶段学习瓶颈，我们提出了一种全新的基于阶段隔离的增量学习框架，该框架利用一系列阶段隔离的分类器来执行每个阶段的学习任务，而不受其他阶段的干扰。具"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/45.Energy/01.ELI.html",
    "category": "AINotes/40.CIL/45.Energy",
    "tags": [],
    "excerpt": "&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;深度学习模型在增量学习新任务时往往会遗忘先前学到的知识。这种行为的发生是因为为新任务优化的参数更新可能与适合旧任务的更新不一致。由此导致的潜在表示不匹配会导致遗忘。在本工作中，我们提出了 &lt;strong&gt;E"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/21.Neuron Expansion/01.PackNet.html",
    "category": "AINotes/40.CIL/21.Neuron Expansion",
    "tags": [],
    "excerpt": "&lt;h2 id=\"packnet-adding-multiple-tasks-to-a-single-network-by-iterative-pruning\"&gt;PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning&lt;a class=\"anchor-link\" href=\"#packnet-adding-mult"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/22.Backbone Expansion/01.DER.html",
    "category": "AINotes/40.CIL/22.Backbone Expansion",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;DER: Dynamically Expandable Representation for Class Incremental Learning](https://arxiv.org/abs/2103.16788) | CVPR 2021 | &lt;a href=\"https://github.com/Rhyssiyan/DER-ClassIL.pytorch\"&gt;Co"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/22.Backbone Expansion/02.FOSTER.html",
    "category": "AINotes/40.CIL/22.Backbone Expansion",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;FOSTER: &lt;a href=\"https://arxiv.org/abs/2204.04662\"&gt;Feature Boosting and Compression for Class-Incremental Learning&lt;/a&gt; | &lt;a href=\"https://github.com/G-U-NECCV22-FOSTER\"&gt;Code&lt;/a&gt; &lt;/p&gt;\n&lt;"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/43.Weight Rectify/01.WA.html",
    "category": "AINotes/40.CIL/43.Weight Rectify",
    "tags": [],
    "excerpt": "&lt;h2 id=\"maintaining-discrimination-and-fairness-in-class-incremental-learning\"&gt;Maintaining discrimination and fairness in class incremental learning&lt;a class=\"anchor-link\" href=\"#maintaining-discrimina"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/30.Parameter Regularization/01.EWC.html",
    "category": "AINotes/40.CIL/30.Parameter Regularization",
    "tags": [],
    "excerpt": "&lt;h2 id=\"overcoming-catastrophic-forgetting-in-neural-networks\"&gt;Overcoming catastrophic forgetting in neural networks&lt;a class=\"anchor-link\" href=\"#overcoming-catastrophic-forgetting-in-neural-networks\""
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/30.Parameter Regularization/02.CoFiMA.html",
    "category": "AINotes/40.CIL/30.Parameter Regularization",
    "tags": [],
    "excerpt": "&lt;h2 id=\"weighted-ensemble-models-are-strong-continual-learners\"&gt;Weighted Ensemble Models Are Strong Continual Learners&lt;a class=\"anchor-link\" href=\"#weighted-ensemble-models-are-strong-continual-learne"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/20.Data Regularization/01.GBSS.html",
    "category": "AINotes/40.CIL/20.Data Regularization",
    "tags": [],
    "excerpt": "&lt;h2 id=\"gradient-based-sample-selection-for-online-continual-learning\"&gt;Gradient based sample selection for online continual learning&lt;a class=\"anchor-link\" href=\"#gradient-based-sample-selection-for-on"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/23.PEFT Expansion/10.InfLoRA.html",
    "category": "AINotes/40.CIL/23.PEFT Expansion",
    "tags": [],
    "excerpt": "&lt;h2 id=\"inflora-interference-free-low-rank-adaptation-for-continual-learning\"&gt;&lt;a href=\"https://arxiv.org/abs/2404.00228\"&gt;InfLoRA-Interference-Free Low-Rank Adaptation for Continual Learning&lt;/a&gt;&lt;a clas"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/23.PEFT Expansion/08.Ease.html",
    "category": "AINotes/40.CIL/23.PEFT Expansion",
    "tags": [],
    "excerpt": "&lt;h2 id=\"expandable-subspace-ensemble-for-pre-trained-model-based-class-incremental-learning\"&gt;&lt;a href=\"http://arxiv.org/abs/2403.12030\"&gt;Expandable Subspace Ensemble for Pre-Trained Model-Based Class-In"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/23.PEFT Expansion/09.APER.html",
    "category": "AINotes/40.CIL/23.PEFT Expansion",
    "tags": [],
    "excerpt": "&lt;h2 id=\"revisiting-class-incremental-learning-with-pre-trained-models-generalizability-and-adaptivity-are-all-you-need\"&gt;Revisiting Class-Incremental Learning with Pre-Trained Models Generalizability a"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/23.PEFT Expansion/15.LAE.html",
    "category": "AINotes/40.CIL/23.PEFT Expansion",
    "tags": [],
    "excerpt": "&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;“预训练 → 下游适应”为持续学习（Continual Learning, CL）带来了新的机遇和挑战。尽管最近的最先进 CL 方法通过参数高效调优（Parameter-Efficient-Tuning, "
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/23.PEFT Expansion/19.PILoRA.html",
    "category": "AINotes/40.CIL/23.PEFT Expansion",
    "tags": [],
    "excerpt": "&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;现有的联邦学习方法在涉及数据隐私和非独立同分布（Non-IID）数据的场景中，已经有效地处理了去中心化学习问题。然而，在实际情况下，每个客户端动态学习新类，要求全局模型能够对所有已见类别进行分类。为了在低通"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/23.PEFT Expansion/16.HiDe-Prompt.html",
    "category": "AINotes/40.CIL/23.PEFT Expansion",
    "tags": [],
    "excerpt": "&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;基于提示的持续学习利用预训练知识进行下游持续学习，在监督预训练下几乎达到了性能的顶峰。然而，我们的实证研究表明，当前策略在更现实的自监督预训练下未能充分发挥其潜力，而自监督预训练对于处理实践中大量未标记数据"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/23.PEFT Expansion/14.MOS.html",
    "category": "AINotes/40.CIL/23.PEFT Expansion",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"http://arxiv.org/abs/2412.09441\"&gt;MOS: Model Surgery for Pre-Trained Model-Based Class-Incremental Learning&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/23.PEFT Expansion/01.l2p.html",
    "category": "AINotes/40.CIL/23.PEFT Expansion",
    "tags": [],
    "excerpt": "&lt;h2 id=\"learning-to-prompt-for-continual-learning\"&gt;&lt;a href=\"https://arxiv.org/abs/2112.08654\"&gt;Learning to Prompt for Continual Learning&lt;/a&gt;&lt;a class=\"anchor-link\" href=\"#learning-to-prompt-for-continua"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/23.PEFT Expansion/18.CPrompt.html",
    "category": "AINotes/40.CIL/23.PEFT Expansion",
    "tags": [],
    "excerpt": "&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;持续学习使模型能够自主适应不断变化的环境或数据流，而不会忘记旧知识。基于提示的方法建立在冻结的预训练模型上，以高效地学习任务特定的提示和分类器。现有的基于提示的方法在训练和测试之间存在不一致，限制了其有效性"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/23.PEFT Expansion/04.CODA-Prompt.html",
    "category": "AINotes/40.CIL/23.PEFT Expansion",
    "tags": [],
    "excerpt": "&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;计算机视觉模型在学习不断变化的训练数据中的新概念时，会遭受一种称为灾难性遗忘的现象。这种持续学习问题的典型解决方案需要广泛复习以前见过的数据，这增加了内存成本，并且可能违反数据隐私。最近，大规模预训练视觉变"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/23.PEFT Expansion/07.CPP.html",
    "category": "AINotes/40.CIL/23.PEFT Expansion",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"http://arxiv.org/abs/2303.09447\"&gt;Steering Prototypes with Prompt-tuning for Rehearsal-free Continual Learning&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" h"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/23.PEFT Expansion/06.PromptFusion.html",
    "category": "AINotes/40.CIL/23.PEFT Expansion",
    "tags": [],
    "excerpt": "&lt;h2 id=\"promptfusion-decoupling-stability-and-plasticity-for-continual-learning\"&gt;&lt;a href=\"http://arxiv.org/abs/2303.07223\"&gt;PromptFusion: Decoupling Stability and Plasticity for Continual Learning&lt;/a&gt;&lt;"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/23.PEFT Expansion/02.DualPrompt.html",
    "category": "AINotes/40.CIL/23.PEFT Expansion",
    "tags": [],
    "excerpt": "&lt;h2 id=\"dualprompt-complementary-prompting-for-rehearsal-free-continual-learning\"&gt;&lt;a href=\"https://arxiv.org/abs/2204.04799\"&gt;DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning&lt;/"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/12.Generative Replay/01.DCMI.html",
    "category": "AINotes/40.CIL/12.Generative Replay",
    "tags": [],
    "excerpt": "&lt;h2 id=\"dual-consistency-model-inversion-for-non-exemplar-class-incremental-learning\"&gt;Dual-consistency Model Inversion for Non-exemplar Class Incremental Learning&lt;a class=\"anchor-link\" href=\"#dual-con"
  },
  {
    "title": "Rainbow Memory: Continual Learning with a Memory of Diverse Samples",
    "url": "posts/AINotes/40.CIL/11.Direct Replay/02.RM.html",
    "category": "AINotes/40.CIL/11.Direct Replay",
    "tags": [],
    "excerpt": "&lt;h1 id=\"rainbow-memory-continual-learning-with-a-memory-of-diverse-samples\"&gt;Rainbow Memory: Continual Learning with a Memory of Diverse Samples&lt;a class=\"anchor-link\" href=\"#rainbow-memory-continual-le"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/11.Direct Replay/01.iCaRL.html",
    "category": "AINotes/40.CIL/11.Direct Replay",
    "tags": [],
    "excerpt": "&lt;h2 id=\"icarl-incremental-classifier-and-representation-learning\"&gt;iCaRL: Incremental Classifier and Representation Learning&lt;a class=\"anchor-link\" href=\"#icarl-incremental-classifier-and-representation"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/31.Logit Distillation/01.LwF.html",
    "category": "AINotes/40.CIL/31.Logit Distillation",
    "tags": [],
    "excerpt": "&lt;h2 id=\"learning-without-forgetting\"&gt;Learning without Forgetting&lt;a class=\"anchor-link\" href=\"#learning-without-forgetting\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;a href=\"https://arxiv.org/abs/1606"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/31.Logit Distillation/02.COIL.html",
    "category": "AINotes/40.CIL/31.Logit Distillation",
    "tags": [],
    "excerpt": "&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;传统的学习系统在封闭世界中针对固定数量的类别进行训练，并且需要预先收集数据集。然而，在现实世界的应用中，新的类别经常出现并且需要增量学习。例如，在电子商务中，每天都会出现新的产品类型；在社交媒体社区中，新的"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/32.Feature Distillation/01.LUCIR.html",
    "category": "AINotes/40.CIL/32.Feature Distillation",
    "tags": [],
    "excerpt": "&lt;h2 id=\"learning-a-unified-classifier-incrementally-via-rebalancing\"&gt;Learning a unified classifier incrementally via rebalancing&lt;a class=\"anchor-link\" href=\"#learning-a-unified-classifier-incrementall"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/44.Template-Based Classification/01.RanPAC.html",
    "category": "AINotes/40.CIL/44.Template-Based Classification",
    "tags": [],
    "excerpt": "&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;持续学习（Continual Learning, CL）的目标是在非平稳的数据流中逐步学习不同的任务（如分类），而不遗忘旧任务。大多数 CL 工作专注于在从头学习的框架下应对灾难性遗忘。然而，随着基础模型的"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/00.Survey/11.Architecture Matters in Continual Learning.html",
    "category": "AINotes/40.CIL/00.Survey",
    "tags": [],
    "excerpt": "&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;在持续学习（Continual Learning）领域，大量研究致力于通过设计能够应对分布变化的新算法，来克服神经网络的灾难性遗忘问题。然而，这些研究大多仅专注于为“固定的神经网络架构”开发新的算法，并在很"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/00.Survey/03.Continual Learning for Large Language Models A Survey.html",
    "category": "AINotes/40.CIL/00.Survey",
    "tags": [],
    "excerpt": "&lt;h2 id=\"continual-learning-for-large-language-models-a-survey\"&gt;Continual Learning for Large Language Models: A Survey&lt;a class=\"anchor-link\" href=\"#continual-learning-for-large-language-models-a-survey"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/00.Survey/04.Recent Advances of Foundation Language Models-based Continual Learning-A Survey.html",
    "category": "AINotes/40.CIL/00.Survey",
    "tags": [],
    "excerpt": "&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;近期，在自然语言处理（NLP）和计算机视觉（CV）领域，基础语言模型（LMs）取得了显著成就。与传統神经网络模型不同，基础LMs通过在大量无监督数据集上进行预训练，获得了丰富的常识知识，并通过大量参数获得了"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/00.Survey/02.Continual Learning with Pre-Trained Models A Survey.html",
    "category": "AINotes/40.CIL/00.Survey",
    "tags": [],
    "excerpt": "&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;在当今世界的应用中，经常面临流数据，这要求学习系统随着数据的演变而吸收新知识。持续学习（Continual Learning, CL）旨在实现这一目标，并同时克服在学习新知识时对以前知识的巨大遗忘。典型的C"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/00.Survey/01.类增量学习综述.html",
    "category": "AINotes/40.CIL/00.Survey",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;基于 &lt;a href=\"https://arxiv.org/abs/2302.03648\"&gt;Class-Incremental Learning: A Survey&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;div align=center&gt;&lt;img src=\"https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/00.Survey/05.Continual Learning With Knowledge Distillation A Survey.html",
    "category": "AINotes/40.CIL/00.Survey",
    "tags": [],
    "excerpt": "&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;持续学习中的首要挑战是缓解灾难性遗忘，使模型在学习新任务的同时保留对先前任务的知识。知识蒸馏（KD）作为一种正则化方法，因其在学习新任务时通过模仿早期模型的输出来保持模型在先前任务上的性能而受到广泛关注，从"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/41.Feature Rectify/03.RKR.html",
    "category": "AINotes/40.CIL/41.Feature Rectify",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;Rectification-Based Knowledge Retention for Task Incremental Learning&lt;br /&gt;\n&lt;a href=\"https://mp.weixin.qq.com/s/cakOgXWGyMW7AUMc9i-0Gw\"&gt;TPAMI 2024 | 无需重训练即可掌握新技能，深度学习模型实现终身学习革命！&lt;/a&gt;&lt;/p"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/41.Feature Rectify/04.FA.html",
    "category": "AINotes/40.CIL/41.Feature Rectify",
    "tags": [],
    "excerpt": "&lt;h2 id=\"memory-efficient-incremental-learning-through-feature-adaptation\"&gt;&lt;a href=\"http://arxiv.org/abs/2004.00713\"&gt;Memory-Efficient Incremental Learning Through Feature Adaptation&lt;/a&gt;&lt;a class=\"anchor"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/41.Feature Rectify/06.RKR.html",
    "category": "AINotes/40.CIL/41.Feature Rectify",
    "tags": [],
    "excerpt": ""
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/41.Feature Rectify/02.SDC.html",
    "category": "AINotes/40.CIL/41.Feature Rectify",
    "tags": [],
    "excerpt": "&lt;h2 id=\"semantic-drift-compensation-for-class-incremental-learning\"&gt;Semantic Drift Compensation for Class-Incremental Learning&lt;a class=\"anchor-link\" href=\"#semantic-drift-compensation-for-class-increm"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/41.Feature Rectify/08.DYSON.html",
    "category": "AINotes/40.CIL/41.Feature Rectify",
    "tags": [],
    "excerpt": "&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;本文聚焦于一个具有挑战性的问题——在线任务无关的类增量学习（OTFCIL）。与现有方法从数据流中持续学习特征空间不同，我们提出了一种新的计算 - 对齐范式。该范式首先为现有类计算一个最优几何结构（即类原型分"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/41.Feature Rectify/05.CCLL.html",
    "category": "AINotes/40.CIL/41.Feature Rectify",
    "tags": [],
    "excerpt": "&lt;h2 id=\"calibrating-cnns-for-lifelong-learning\"&gt;Calibrating CNNs for Lifelong Learning&lt;a class=\"anchor-link\" href=\"#calibrating-cnns-for-lifelong-learning\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/41.Feature Rectify/07.ADC.html",
    "category": "AINotes/40.CIL/41.Feature Rectify",
    "tags": [],
    "excerpt": "&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;持续学习方法通常面临灾难性遗忘的问题，尤其是在不存储先前任务样本的方法中，这一问题尤为严重。为了减少特征提取器的潜在漂移，现有的无样本方法通常在第一个任务显著大于后续任务的情况下进行评估。然而，在更具挑战性"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/41.Feature Rectify/01.FCS.html",
    "category": "AINotes/40.CIL/41.Feature Rectify",
    "tags": [],
    "excerpt": "&lt;h2 id=\"fcs-feature-calibration-and-separation-for-non-exemplar-class-incremental-learning\"&gt;&lt;a href=\"https://ieeexplore.ieee.org/document/10657158/?arnumber=10657158\"&gt;FCS: Feature Calibration and Sepa"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/40.CIL/33.Relational Distillation/01.PRD.html",
    "category": "AINotes/40.CIL/33.Relational Distillation",
    "tags": [],
    "excerpt": "&lt;h2 id=\"prototype-sample-relation-distillation-towards-replay-free-continual-learning\"&gt;Prototype-sample relation distillation: towards replay-free continual learning&lt;a class=\"anchor-link\" href=\"#proto"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/12.多模态/00.Multimodal_Survey.html",
    "category": "AINotes/12.多模态",
    "tags": [],
    "excerpt": "&lt;p&gt;不同模态进行对齐:CLIP、VLMo&lt;br /&gt;\n大语言模型能力会更重要:Frozen、FLamingo、BLIP-2&lt;br /&gt;\nMLP即可完成对齐:LLaVA、MiniGPT-4&lt;br /&gt;\n视觉编码器很重要:Deepseek-VL、Qwen2.5-VL&lt;/p&gt;"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/12.多模态/03.BLIP.html",
    "category": "AINotes/12.多模态",
    "tags": [],
    "excerpt": "&lt;h2 id=\"摘要\"&gt;摘要&lt;a class=\"anchor-link\" href=\"#摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;BLIP 是一种多模态 Transformer 模型，主要针对以往的视觉语言训练 (Vision-Language Pre-training, VLP) 框架的两个常见问题：&lt;/p&gt;\n&lt;ol&gt;\n&lt;li&gt;大多数现有的预训练"
  },
  {
    "title": "多模态超详细解读 (七)：BLIP-2：节约多模态训练成本：冻结预训练好的视觉语言模型参数",
    "url": "posts/AINotes/12.多模态/04.BLIP-2.html",
    "category": "AINotes/12.多模态",
    "tags": [],
    "excerpt": "&lt;h1 id=\"多模态超详细解读-七blip-2节约多模态训练成本冻结预训练好的视觉语言模型参数\"&gt;多模态超详细解读 (七)：BLIP-2：节约多模态训练成本：冻结预训练好的视觉语言模型参数&lt;a class=\"anchor-link\" href=\"#多模态超详细解读-七blip-2节约多模态训练成本冻结预训练好的视觉语言模型参数\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;"
  },
  {
    "title": "image_encoder - ResNet or Vision Transformer",
    "url": "posts/AINotes/12.多模态/02.CLIP解读.html",
    "category": "AINotes/12.多模态",
    "tags": [],
    "excerpt": "&lt;p&gt;&lt;strong&gt;论文地址&lt;/strong&gt;：&lt;a href=\"https://arxiv.org/pdf/2103.00020.pdf\"&gt;https://arxiv.org/pdf/2103.00020.pdf&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;代码地址&lt;/strong&gt;：&lt;a href=\"https://github.com/OpenAI/CLIP\"&gt;https://github.co"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/43.多模态增量学习MMCL/04.RAIL.html",
    "category": "AINotes/43.多模态增量学习MMCL",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"http://arxiv.org/abs/2406.18868\"&gt;Advancing Cross-domain Discriminability in Continual Learning of Vision-Language Models&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anc"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/43.多模态增量学习MMCL/01.PROOF.html",
    "category": "AINotes/43.多模态增量学习MMCL",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://arxiv.org/abs/2305.19270\"&gt;Paper&lt;/a&gt; &lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;类别增量学习（Class-In"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/43.多模态增量学习MMCL/12.MoE-Adapters4CL.html",
    "category": "AINotes/43.多模态增量学习MMCL",
    "tags": [],
    "excerpt": "&lt;h2 id=\"boosting-continual-learning-of-vision-language-models-via-mixture-of-experts-adapters\"&gt;&lt;a href=\"http://arxiv.org/abs/2403.11549\"&gt;Boosting Continual Learning of Vision-Language Models via Mixtu"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/43.多模态增量学习MMCL/06.CLAP4CLIP.html",
    "category": "AINotes/43.多模态增量学习MMCL",
    "tags": [],
    "excerpt": "&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;持续学习（Continual Learning, CL）旨在帮助深度神经网络在学习新知识的同时保留已学知识。由于其强大的泛化能力，预训练的视觉 - 语言模型（如对比语言 - 图像预训练模型，CLIP）最近作"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/43.多模态增量学习MMCL/05.RAPF.html",
    "category": "AINotes/43.多模态增量学习MMCL",
    "tags": [],
    "excerpt": "&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;类增量学习是一个具有挑战性的问题，其目标是训练一个能够随着时间的推移分类越来越多类数据的模型。随着视觉语言预训练模型（如 CLIP）的发展，这些模型展示了良好的泛化能力，使其在完全冻结参数的情况下在类增量学"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/43.多模态增量学习MMCL/02.GMM.html",
    "category": "AINotes/43.多模态增量学习MMCL",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://arxiv.org/abs/2403.18383\"&gt;Paper&lt;/a&gt; | &lt;a href=\"https://github.com/DoubleClass/GMM\"&gt;Code&lt;/a&gt; | CVPR 2024&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;h2 id=\"chatgpt全文翻译-arrow_down\"&gt;ChatGPT全文翻译 :"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/43.多模态增量学习MMCL/03.CLIP-CIL.html",
    "category": "AINotes/43.多模态增量学习MMCL",
    "tags": [],
    "excerpt": "&lt;h2 id=\"chatgpt全文翻译-arrow_down\"&gt;ChatGPT全文翻译 :arrow_down:&lt;a class=\"anchor-link\" href=\"#chatgpt全文翻译-arrow_down\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/43.多模态增量学习MMCL/13.S-Prompts.html",
    "category": "AINotes/43.多模态增量学习MMCL",
    "tags": [],
    "excerpt": "&lt;h2 id=\"s-prompts-learning-with-pre-trained-transformers-an-occams-razor-for-domain-incremental-learning\"&gt;&lt;a href=\"http://arxiv.org/abs/2207.12819\"&gt;S-Prompts Learning with Pre-trained Transformers: An"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/43.多模态增量学习MMCL/00.Survey/00.多模态类增量学习.html",
    "category": "AINotes/43.多模态增量学习MMCL/00.Survey",
    "tags": [],
    "excerpt": ""
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/43.多模态增量学习MMCL/00.Survey/02.Recent Advances of Multimodal Continual Learning A Comprehensive Survey.html",
    "category": "AINotes/43.多模态增量学习MMCL/00.Survey",
    "tags": [],
    "excerpt": ""
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/43.多模态增量学习MMCL/00.Survey/01.When Continue Learning Meets Multimodal Large Language Model A Survey.html",
    "category": "AINotes/43.多模态增量学习MMCL/00.Survey",
    "tags": [],
    "excerpt": ""
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/17.迁移学习/04.基于特征的迁移学习.html",
    "category": "AINotes/17.迁移学习",
    "tags": [],
    "excerpt": "&lt;h3 id=\"特征变换迁移法\"&gt;特征变换迁移法&lt;a class=\"anchor-link\" href=\"#特征变换迁移法\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h3&gt;\n&lt;ul&gt;\n&lt;li&gt;目标是：如何求解特征变换 &lt;span class=\"math-inline\"&gt;T&lt;/span&gt;，使得特征变化后的源域和目标域的概率分布差异达到最小。&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/17.迁移学习/06.基于相关性的迁移学习.html",
    "category": "AINotes/17.迁移学习",
    "tags": [],
    "excerpt": ""
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/17.迁移学习/05.基于参数的迁移学习.html",
    "category": "AINotes/17.迁移学习",
    "tags": [],
    "excerpt": ""
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/17.迁移学习/07.模型预训练迁移法.html",
    "category": "AINotes/17.迁移学习",
    "tags": [],
    "excerpt": "&lt;h3 id=\"模型预训练迁移法\"&gt;模型预训练迁移法&lt;a class=\"anchor-link\" href=\"#模型预训练迁移法\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h3&gt;\n&lt;p&gt;第三种比较常用的方法则是模型预训练迁移法。也就是说，如果已经有一个在源域上训练好的模型 &lt;span class=\"math-inline\"&gt;f_{s}&lt;/span&gt;，并且目标域本身有一"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/17.迁移学习/01.迁移学习.html",
    "category": "AINotes/17.迁移学习",
    "tags": [],
    "excerpt": "&lt;p&gt;迁移学习（Transfer Learning）根据 领域 和 任务的相似性，可以这样划分：&lt;/p&gt;\n&lt;div align=center&gt;&lt;img src=\"https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20250106154958.png\" style=\"zoom: 80%;\" /&gt;&lt;/div&gt;\n\n&lt;p&gt;根据 源Domain和目前Dom"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/17.迁移学习/11.领域自适应和领域泛化.html",
    "category": "AINotes/17.迁移学习",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://zhuanlan.zhihu.com/p/481537410\"&gt;Domain Generalization | 域适应、域泛化、OOD、开放集问题定义&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;p&gt;在实际场景中，训练集和测试集往往存在分布差异，导致模型不work。领域自适应是解决这类问题的一种方法，但是它需要测试数据的一些先验知"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/17.迁移学习/03.基于样本的迁移学习.html",
    "category": "AINotes/17.迁移学习",
    "tags": [],
    "excerpt": "&lt;h3 id=\"样本权重迁移法\"&gt;样本权重迁移法&lt;a class=\"anchor-link\" href=\"#样本权重迁移法\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h3&gt;\n&lt;ol&gt;\n&lt;li&gt;\n&lt;p&gt;样本权重迁移法。此类方法学习目标是学习源域样本的权重 &lt;span class=\"math-inline\"&gt;v_i&lt;/span&gt;。&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;特征"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/999.Reference.html",
    "category": "AINotes/30.PyTorch",
    "tags": [],
    "excerpt": "&lt;h2 id=\"reference\"&gt;Reference&lt;a class=\"anchor-link\" href=\"#reference\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;ol&gt;\n&lt;li&gt;https://pytorch.org/docs/stable/index.html&lt;/li&gt;\n&lt;li&gt;https://www.w3cschool.cn/pytorc"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/10.Pytorch Lightning/01.概述.html",
    "category": "AINotes/30.PyTorch/10.Pytorch Lightning",
    "tags": [],
    "excerpt": "&lt;p&gt;使用 PyTorch Lightning 进行模型训练可以简化深度学习项目的开发流程，提高代码的可读性和可维护性。以下是使用 PyTorch Lightning 完成模型训练的主要步骤：&lt;/p&gt;\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;安装 PyTorch Lightning&lt;/strong&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;p&gt;首先，确保已安装 PyTorch Lightning。可以使用以下命令通过 "
  },
  {
    "title": "Pytorch Lightning 完全攻略",
    "url": "posts/AINotes/30.PyTorch/10.Pytorch Lightning/99.Pytorch_Lightning.html",
    "category": "AINotes/30.PyTorch/10.Pytorch Lightning",
    "tags": [],
    "excerpt": "&lt;h1 id=\"pytorch-lightning-完全攻略\"&gt;Pytorch Lightning 完全攻略&lt;a class=\"anchor-link\" href=\"#pytorch-lightning-完全攻略\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; [Takanashi]&lt;/p&gt;\n&lt;p&gt;&lt;stron"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/10.Pytorch Lightning/02.记录训练loss.html",
    "category": "AINotes/30.PyTorch/10.Pytorch Lightning",
    "tags": [],
    "excerpt": "&lt;h2 id=\"1-导入必要的模块\"&gt;1. 导入必要的模块&lt;a class=\"anchor-link\" href=\"#1-导入必要的模块\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;pre&gt;&lt;code class=\"language-python\"&gt;from pytorch_lightning import LightningModule, Trainer\nf"
  },
  {
    "title": "批次的大小",
    "url": "posts/AINotes/30.PyTorch/05.训练/01.基本配置.html",
    "category": "AINotes/30.PyTorch/05.训练",
    "tags": [],
    "excerpt": "&lt;h2 id=\"常见的包\"&gt;常见的包&lt;a class=\"anchor-link\" href=\"#常见的包\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;pre&gt;&lt;code class=\"language-python\"&gt;import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom tor"
  },
  {
    "title": "定义模型和优化器",
    "url": "posts/AINotes/30.PyTorch/05.训练/03.优化器.html",
    "category": "AINotes/30.PyTorch/05.训练",
    "tags": [],
    "excerpt": "&lt;h2 id=\"优化器\"&gt;优化器&lt;a class=\"anchor-link\" href=\"#优化器\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;PyTorch 中的优化器是用于管理并更新模型中可学习参数的值，使得模型输出更加接近真实标签。&lt;/p&gt;\n&lt;h2 id=\"optimizer\"&gt;Optimizer&lt;a class=\"anchor-link\" href"
  },
  {
    "title": "定义一个简单的线性模型",
    "url": "posts/AINotes/30.PyTorch/05.训练/02.损失函数.html",
    "category": "AINotes/30.PyTorch/05.训练",
    "tags": [],
    "excerpt": "&lt;h2 id=\"损失函数\"&gt;损失函数&lt;a class=\"anchor-link\" href=\"#损失函数\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;损失函数是衡量模型输出与真实标签之间的差异。我们还经常听到代价函数和目标函数，它们之间差异如下：&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;损失函数(Loss Function)是计算&lt;strong&gt;一个&lt;/strong&gt;样"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/05.训练/04.PyTorch计算图.html",
    "category": "AINotes/30.PyTorch/05.训练",
    "tags": [],
    "excerpt": ""
  },
  {
    "title": "参数如下：",
    "url": "posts/AINotes/30.PyTorch/07.可视化/03.TensorBoard.html",
    "category": "AINotes/30.PyTorch/07.可视化",
    "tags": [],
    "excerpt": "&lt;h2 id=\"tensorboard-安装\"&gt;TensorBoard 安装&lt;a class=\"anchor-link\" href=\"#tensorboard-安装\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;在已安装 PyTorch 的环境下使用 pip 安装即可：&lt;/p&gt;\n&lt;pre&gt;&lt;code class=\"language-bash\"&gt;pip ins"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/07.可视化/05.wandb相关参数解释.html",
    "category": "AINotes/30.PyTorch/07.可视化",
    "tags": [],
    "excerpt": "&lt;h2 id=\"epoch与global-step的关系\"&gt;Epoch与Global Step的关系&lt;a class=\"anchor-link\" href=\"#epoch与global-step的关系\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Epoch&lt;/strong&gt;：表示完整遍历一次训练数据集的次数。&lt;/li&gt;\n&lt;li&gt;"
  },
  {
    "title": "resize操作是为了和传入神经网络训练图片大小一致",
    "url": "posts/AINotes/30.PyTorch/07.可视化/02.CNN卷积层可视化.html",
    "category": "AINotes/30.PyTorch/07.可视化",
    "tags": [],
    "excerpt": "&lt;h2 id=\"cnn-可视化\"&gt;CNN 可视化&lt;a class=\"anchor-link\" href=\"#cnn-可视化\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;卷积神经网络（CNN）是深度学习中非常重要的模型结构，它广泛地用于图像处理，极大地提升了模型表现，推动了计算机视觉的发展和进步。但 CNN 是一个“黑盒模型”，人们并不知道 CNN 是如何获"
  },
  {
    "title": "安装方法一",
    "url": "posts/AINotes/30.PyTorch/07.可视化/01.可视化网络结构.html",
    "category": "AINotes/30.PyTorch/07.可视化",
    "tags": [],
    "excerpt": "&lt;h2 id=\"可视化网络结构\"&gt;可视化网络结构&lt;a class=\"anchor-link\" href=\"#可视化网络结构\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;随着深度神经网络做的的发展，网络的结构越来越复杂，我们也很难确定每一层的输入结构，输出结构以及参数等信息，这样导致我们很难在短时间内完成 debug。因此掌握一个可以用来可视化网络结构的工具"
  },
  {
    "title": "初始化wandb",
    "url": "posts/AINotes/30.PyTorch/07.可视化/04.使用wandb可视化训练过程.html",
    "category": "AINotes/30.PyTorch/07.可视化",
    "tags": [],
    "excerpt": "&lt;h2 id=\"使用-wandb-可视化训练过程\"&gt;使用 wandb 可视化训练过程&lt;a class=\"anchor-link\" href=\"#使用-wandb-可视化训练过程\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;Tensorboard 对数据的保存仅限于本地，也很难分析超参数不同对实验的影响。wandb 的出现很好的解决了这些问题。wandb 是"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/09.timm/03.timm代码解读.html",
    "category": "AINotes/30.PyTorch/09.timm",
    "tags": [],
    "excerpt": "&lt;h2 id=\"1-创建dataset\"&gt;1 创建dataset&lt;a class=\"anchor-link\" href=\"#1-创建dataset\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;code&gt;timm&lt;/code&gt; 库通过 &lt;code&gt;create_dataset&lt;/code&gt; 函数来得到 &lt;code&gt;dataset_train&lt;/code&gt; "
  },
  {
    "title": "使用 create_model",
    "url": "posts/AINotes/30.PyTorch/09.timm/05.create_model解读.html",
    "category": "AINotes/30.PyTorch/09.timm",
    "tags": [],
    "excerpt": "&lt;h3 id=\"使用\"&gt;使用&lt;a class=\"anchor-link\" href=\"#使用\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h3&gt;\n&lt;pre&gt;&lt;code class=\"language-python\"&gt;import timm\nmodel = timm.create_model(&quot;vit_deit_base_patch16_384&quot;, p"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/09.timm/01.timm概述.html",
    "category": "AINotes/30.PyTorch/09.timm",
    "tags": [],
    "excerpt": "&lt;h2 id=\"概述\"&gt;概述&lt;a class=\"anchor-link\" href=\"#概述\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;PyTorchImageModels，简称 timm，是一个巨大的 PyTorch 代码集合，包括了一系列：&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;image models&lt;/li&gt;\n&lt;li&gt;layers&lt;/li&gt;\n&lt;li&gt;util"
  },
  {
    "title": "Representation layer",
    "url": "posts/AINotes/30.PyTorch/09.timm/04.timm-vit代码解读.html",
    "category": "AINotes/30.PyTorch/09.timm",
    "tags": [],
    "excerpt": "&lt;p&gt;&lt;a href=\"https://zhuanlan.zhihu.com/p/350837279\"&gt;视觉 Transformer 优秀开源工作：timm 库 vision transformer 代码解读&lt;/a&gt;&lt;/p&gt;\n&lt;h2 id=\"timm库-vision_transformerpy代码解读\"&gt;timm库 vision_transformer.py代码解读&lt;a class=\"anchor"
  },
  {
    "title": "prints: torch.Size([1000])",
    "url": "posts/AINotes/30.PyTorch/09.timm/02.timm使用教程.html",
    "category": "AINotes/30.PyTorch/09.timm",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://zhuanlan.zhihu.com/p/404107277\"&gt;视觉神经网络模型优秀开源工作：timm 库使用方法和代码解读&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;h2 id=\"使用教程\"&gt;使用教程&lt;a class=\"anchor-link\" href=\"#使用教程\" title=\"Permanent link\"&gt;&para"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/103.常用代码片段/101.CosineLinear.html",
    "category": "AINotes/30.PyTorch/103.常用代码片段",
    "tags": [],
    "excerpt": "&lt;pre&gt;&lt;code class=\"language-python\"&gt;class CosineLinear(nn.Module):\n    def __init__(self, in_features, out_features, nb_proxy=1, to_reduce=False, sigma=True):\n        super(CosineLinear, self).__init__"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/103.常用代码片段/99.注意事项.html",
    "category": "AINotes/30.PyTorch/103.常用代码片段",
    "tags": [],
    "excerpt": "&lt;ul&gt;\n&lt;li&gt;不要使用太大的线性层。因为 nn.Linear(m,n)使用的是的内存，线性层太大很容易超出现有显存。&lt;/li&gt;\n&lt;li&gt;不要在太长的序列上使用 RNN。因为 RNN 反向传播使用的是 BPTT 算法，其需要的内存和输入序列的长度呈线性关系。&lt;/li&gt;\n&lt;li&gt;model(x) 前用 model.train() 和 model.eval() 切换网络状态。&lt;/li&gt;\n&lt;li&gt;不需"
  },
  {
    "title": "convolutional neural network (2 convolutional layers)",
    "url": "posts/AINotes/30.PyTorch/103.常用代码片段/04.模型相关.html",
    "category": "AINotes/30.PyTorch/103.常用代码片段",
    "tags": [],
    "excerpt": "&lt;h2 id=\"模型定义和操作\"&gt;模型定义和操作&lt;a class=\"anchor-link\" href=\"#模型定义和操作\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;一个简单两层卷积网络的示例&lt;/p&gt;\n&lt;pre&gt;&lt;code class=\"language-python\"&gt;# convolutional neural network (2 convolut"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/103.常用代码片段/03.数据相关.html",
    "category": "AINotes/30.PyTorch/103.常用代码片段",
    "tags": [],
    "excerpt": "&lt;h2 id=\"计算数据集的均值和标准差\"&gt;计算数据集的均值和标准差&lt;a class=\"anchor-link\" href=\"#计算数据集的均值和标准差\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;pre&gt;&lt;code class=\"language-python\"&gt;import os\nimport cv2\nimport numpy as np\nfrom tor"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/103.常用代码片段/02.参数相关.html",
    "category": "AINotes/30.PyTorch/103.常用代码片段",
    "tags": [],
    "excerpt": "&lt;h2 id=\"参数相关\"&gt;参数相关&lt;a class=\"anchor-link\" href=\"#参数相关\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"打印梯度更新的参数\"&gt;打印梯度更新的参数&lt;a class=\"anchor-link\" href=\"#打印梯度更新的参数\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h3&gt;"
  },
  {
    "title": "假设输入张量",
    "url": "posts/AINotes/30.PyTorch/103.常用代码片段/05.分类相关.html",
    "category": "AINotes/30.PyTorch/103.常用代码片段",
    "tags": [],
    "excerpt": "&lt;h2 id=\"计算分类准确率\"&gt;计算分类准确率&lt;a class=\"anchor-link\" href=\"#计算分类准确率\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;pre&gt;&lt;code class=\"language-python\"&gt;_, preds = torch.max(logits, dim=1) # 获取预测结果\n\ncorrect += preds."
  },
  {
    "title": "install imgaug either via pypi",
    "url": "posts/AINotes/30.PyTorch/101.数据增强/05.imgaug.html",
    "category": "AINotes/30.PyTorch/101.数据增强",
    "tags": [],
    "excerpt": "&lt;h2 id=\"imgaug-简介和安装\"&gt;imgaug 简介和安装&lt;a class=\"anchor-link\" href=\"#imgaug-简介和安装\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"imgaug-简介\"&gt;imgaug 简介&lt;a class=\"anchor-link\" href=\"#imgaug-简介\" title=\"Permane"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/101.数据增强/01.数据增强概述.html",
    "category": "AINotes/30.PyTorch/101.数据增强",
    "tags": [],
    "excerpt": "&lt;h2 id=\"数据增强data-augmentation\"&gt;数据增强（Data augmentation）&lt;a class=\"anchor-link\" href=\"#数据增强data-augmentation\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;大部分的计算机视觉任务使用很多的数据，所以数据扩充是经常使用的一种技巧来提高计算机视觉系统的表现。我认"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/101.数据增强/02.基础数据增强.html",
    "category": "AINotes/30.PyTorch/101.数据增强",
    "tags": [],
    "excerpt": "&lt;div align=center&gt;&lt;img src=\"https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20240605173714.png\"/&gt;&lt;/div&gt;\n\n&lt;p&gt;-&lt;a href=\"https://blog.csdn.net/qq_40507857/article/details/114098499\"&gt;使用 PyTorch 和 Album"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/101.数据增强/04.自监督学习增强.html",
    "category": "AINotes/30.PyTorch/101.数据增强",
    "tags": [],
    "excerpt": "&lt;h3 id=\"3-自监督学习增强方法\"&gt;3. 自监督学习增强方法&lt;a class=\"anchor-link\" href=\"#3-自监督学习增强方法\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h3&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;SimCLR 风格的增强&lt;/strong&gt;：包括一系列的随机裁剪、翻转、颜色抖动和模糊等操作，用于自监督学习。&lt;/li&gt;\n&lt;li&gt;&lt;st"
  },
  {
    "title": "定义TransMix增强方法",
    "url": "posts/AINotes/30.PyTorch/101.数据增强/03.高级数据增强.html",
    "category": "AINotes/30.PyTorch/101.数据增强",
    "tags": [],
    "excerpt": "&lt;h2 id=\"高级数据增强方法\"&gt;高级数据增强方法&lt;a class=\"anchor-link\" href=\"#高级数据增强方法\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Cutout&lt;/strong&gt;：在图像上随机遮挡一个矩形区域。&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Mixup&lt;/strong&gt;：将两张图像按照一定比例进行线"
  },
  {
    "title": "原始代码",
    "url": "posts/AINotes/30.PyTorch/06.并行计算/09.分布式evaluation.html",
    "category": "AINotes/30.PyTorch/06.并行计算",
    "tags": [],
    "excerpt": "&lt;h2 id=\"分布式-evaluation\"&gt;分布式 evaluation&lt;a class=\"anchor-link\" href=\"#分布式-evaluation\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;blockquote&gt;\n&lt;p&gt;all_reduce, barrier 等 API 是 distributed 中更为基础和底层的 API。这些 API "
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/06.并行计算/08.GPU集群上的分布式.html",
    "category": "AINotes/30.PyTorch/06.并行计算",
    "tags": [],
    "excerpt": "&lt;h2 id=\"gpu-集群上的分布式\"&gt;GPU 集群上的分布式&lt;a class=\"anchor-link\" href=\"#gpu-集群上的分布式\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;blockquote&gt;\n&lt;p&gt;Slurm，是一个用于 Linux 系统的免费、开源的任务调度工具。它提供了三个关键功能。第一，为用户分配资源(计算机节点)，以供用户执行工"
  },
  {
    "title": "main.py",
    "url": "posts/AINotes/30.PyTorch/06.并行计算/07.Horovod.html",
    "category": "AINotes/30.PyTorch/06.并行计算",
    "tags": [],
    "excerpt": "&lt;h2 id=\"horovod-的优雅实现\"&gt;Horovod 的优雅实现&lt;a class=\"anchor-link\" href=\"#horovod-的优雅实现\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;blockquote&gt;\n&lt;p&gt;Horovod 是 Uber 开源的深度学习工具，它的发展吸取了 Facebook \"Training ImageNet In "
  },
  {
    "title": "# torch.distributed",
    "url": "posts/AINotes/30.PyTorch/06.并行计算/06.Apex.html",
    "category": "AINotes/30.PyTorch/06.并行计算",
    "tags": [],
    "excerpt": "&lt;h2 id=\"使用-apex-再加速\"&gt;使用 Apex 再加速&lt;a class=\"anchor-link\" href=\"#使用-apex-再加速\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;blockquote&gt;\n&lt;p&gt;Apex 是 NVIDIA 开源的用于混合精度训练和分布式训练库。Apex 对混合精度训练的过程进行了封装，改两三行配置就可以进行混合精度的"
  },
  {
    "title": "main.py",
    "url": "posts/AINotes/30.PyTorch/06.并行计算/05.multiprocessing.html",
    "category": "AINotes/30.PyTorch/06.并行计算",
    "tags": [],
    "excerpt": "&lt;h2 id=\"使用-torchmultiprocessing-取代启动器\"&gt;使用 torch.multiprocessing 取代启动器&lt;a class=\"anchor-link\" href=\"#使用-torchmultiprocessing-取代启动器\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;blockquote&gt;\n&lt;p&gt;有的同学可能比较熟悉 torc"
  },
  {
    "title": "nproc_per_node: 这个参数是指你使用这台服务器上面的几张显卡",
    "url": "posts/AINotes/30.PyTorch/06.并行计算/101.DDP启动.html",
    "category": "AINotes/30.PyTorch/06.并行计算",
    "tags": [],
    "excerpt": "&lt;p&gt;下面将对 &lt;strong&gt;&lt;code&gt;torchrun&lt;/code&gt;&lt;/strong&gt; 的使用做一个较为详细的介绍，包括命令行参数如何指定，以及在脚本里如何获取分布式相关信息并进行初始化。&lt;/p&gt;\n&lt;hr /&gt;\n&lt;h2 id=\"torchdistributedlaunch-启动器\"&gt;torch.distributed.launch 启动器&lt;a class=\"anchor-link\" href"
  },
  {
    "title": "以下二选一, 第一个是使用gloo后端需要设置的, 第二个是使用nccl需要设置的",
    "url": "posts/AINotes/30.PyTorch/06.并行计算/03.DDP.html",
    "category": "AINotes/30.PyTorch/06.并行计算",
    "tags": [],
    "excerpt": "&lt;h2 id=\"distributed-data-parallelddp\"&gt;Distributed Data Parallel，DDP&lt;a class=\"anchor-link\" href=\"#distributed-data-parallelddp\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h2 id=\"使用\"&gt;使用&lt;a class=\"anchor-lin"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/06.并行计算/999.References.html",
    "category": "AINotes/30.PyTorch/06.并行计算",
    "tags": [],
    "excerpt": "&lt;h2 id=\"参考资料\"&gt;参考资料：&lt;a class=\"anchor-link\" href=\"#参考资料\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://blog.csdn.net/kuweicai/article/details/120516410\"&gt;Pytorch 并行训练（DP， DDP）的原理和应用&lt;/a"
  },
  {
    "title": "这里要 model.cuda()",
    "url": "posts/AINotes/30.PyTorch/06.并行计算/02.DP.html",
    "category": "AINotes/30.PyTorch/06.并行计算",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;单机多卡（ Data Parallel，DP）&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;h2 id=\"简介\"&gt;简介&lt;a class=\"anchor-link\" href=\"#简介\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;DataParallel 可以帮助我们（使用单进程控）将模型和数据加载到多个 GPU 中，控制数据在 G"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/06.并行计算/01.并行计算简介.html",
    "category": "AINotes/30.PyTorch/06.并行计算",
    "tags": [],
    "excerpt": "&lt;h2 id=\"network-partitioning\"&gt;Network partitioning&lt;a class=\"anchor-link\" href=\"#network-partitioning\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;网络结构分布到不同的设备中(Network partitioning)，在刚开始做模型并行的时候，这个方案使用的"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/06.并行计算/04.DP 与 DDP 的优缺点.html",
    "category": "AINotes/30.PyTorch/06.并行计算",
    "tags": [],
    "excerpt": "&lt;h3 id=\"dp-与-ddp-的优缺点\"&gt;DP 与 DDP 的优缺点&lt;a class=\"anchor-link\" href=\"#dp-与-ddp-的优缺点\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h3&gt;\n&lt;h4 id=\"dp-的优势\"&gt;DP 的优势&lt;a class=\"anchor-link\" href=\"#dp-的优势\" title=\"Permanent lin"
  },
  {
    "title": "创建存放在 GPU 的数据",
    "url": "posts/AINotes/30.PyTorch/02.Tensors/1.Tensors.html",
    "category": "AINotes/30.PyTorch/02.Tensors",
    "tags": [],
    "excerpt": "&lt;h2 id=\"tensors\"&gt;&lt;a href=\"https://pytorch.org/docs/stable/torch.html#tensors\"&gt;Tensors&lt;/a&gt;&lt;a class=\"anchor-link\" href=\"#tensors\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;Tensor 中文为张量。张量的意思是一个多维数组，它是标量"
  },
  {
    "title": "再来反向传播⼀一次，注意grad是累加的",
    "url": "posts/AINotes/30.PyTorch/02.Tensors/2.自动求导.html",
    "category": "AINotes/30.PyTorch/02.Tensors",
    "tags": [],
    "excerpt": "&lt;h2 id=\"自动求导\"&gt;自动求导&lt;a class=\"anchor-link\" href=\"#自动求导\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;PyTorch 中，所有神经网络的核心是&lt;code&gt;autograd&lt;/code&gt;包。autograd 包为张量上的所有操作提供了自动求导机制。它是一个在运行时定义 ( define-by-run ）的框架，"
  },
  {
    "title": "假设tensor_gpu是你的GPU上的Tensor",
    "url": "posts/AINotes/30.PyTorch/02.Tensors/08.tensor可视化为图片.html",
    "category": "AINotes/30.PyTorch/02.Tensors",
    "tags": [],
    "excerpt": "&lt;h2 id=\"matplotlib\"&gt;matplotlib&lt;a class=\"anchor-link\" href=\"#matplotlib\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;pre&gt;&lt;code class=\"language-python\"&gt;import numpy as np\nimport matplotlib.pyplot as plt\n  #"
  },
  {
    "title": "创建一个形状为 (4, 4) 的张量",
    "url": "posts/AINotes/30.PyTorch/02.Tensors/06.tensor维度转换.html",
    "category": "AINotes/30.PyTorch/02.Tensors",
    "tags": [],
    "excerpt": "&lt;h2 id=\"view-转换维度\"&gt;view() 转换维度&lt;a class=\"anchor-link\" href=\"#view-转换维度\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;blockquote&gt;\n&lt;p&gt;&lt;code&gt;torch.view&lt;/code&gt; 是 PyTorch 中用于重新塑形张量（tensor）的函数，它返回一个新的张量，这个张量与原始张量"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/02.Tensors/4.AI硬件加速设备.html",
    "category": "AINotes/30.PyTorch/02.Tensors",
    "tags": [],
    "excerpt": "&lt;h2 id=\"ai硬件加速设备\"&gt;AI硬件加速设备&lt;a class=\"anchor-link\" href=\"#ai硬件加速设备\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;在进行模型部署和训练时，我们有时会受限于CPU和GPU的性能。这时，专用的AI芯片就显得尤为重要。在正式开始本节内容之前，我们先了解一下什么是CPU和GPU。&lt;/p&gt;\n&lt;p&gt;CPU即C"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/02.Tensors/05.tensor类型转换.html",
    "category": "AINotes/30.PyTorch/02.Tensors",
    "tags": [],
    "excerpt": "&lt;h3 id=\"int---float\"&gt;int -&gt; float&lt;a class=\"anchor-link\" href=\"#int---float\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h3&gt;\n&lt;blockquote&gt;\n&lt;p&gt;torch.tensor(x, dtype=数据格式)&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;pre&gt;&lt;code class=\"la"
  },
  {
    "title": "tensor([[ 1, 20],",
    "url": "posts/AINotes/30.PyTorch/02.Tensors/07.常见函数.html",
    "category": "AINotes/30.PyTorch/02.Tensors",
    "tags": [],
    "excerpt": "&lt;h2 id=\"torchtopk\"&gt;torch.topk()&lt;a class=\"anchor-link\" href=\"#torchtopk\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;ol&gt;\n&lt;li&gt;作用&lt;br /&gt;\n   取一个 tensor 的 topk 元素，返回值为降序后的前 k 个大小的元素值及索引&lt;/li&gt;\n&lt;li&gt;使用方法&lt;/li&gt;\n&lt;/ol&gt;"
  },
  {
    "title": "文章结构",
    "url": "posts/AINotes/30.PyTorch/08.实战/02.RNN详解及其实现.html",
    "category": "AINotes/30.PyTorch/08.实战",
    "tags": [],
    "excerpt": "&lt;h1 id=\"文章结构\"&gt;文章结构&lt;a class=\"anchor-link\" href=\"#文章结构\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;提及 RNN，绝大部分人都知道他是一个用于序列任务的神经网络，会提及他保存了时序信息，但是，为什么需要考虑时序的信息？为什么说 RNN 保存了时序的信息？RNN又存在哪些问题？ 本篇内容将按照以下顺序逐步带你"
  },
  {
    "title": "Swin Transformer 解读",
    "url": "posts/AINotes/30.PyTorch/08.实战/05.Swin-Transformer解读.html",
    "category": "AINotes/30.PyTorch/08.实战",
    "tags": [],
    "excerpt": "&lt;h1 id=\"swin-transformer-解读\"&gt;Swin Transformer 解读&lt;a class=\"anchor-link\" href=\"#swin-transformer-解读\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;\n&lt;font size=3&gt;&lt;b&gt;[Swin-T] Swin Transformer: Hierarchical Vi"
  },
  {
    "title": "Transformer 解读",
    "url": "posts/AINotes/30.PyTorch/08.实战/04.Transformer解读.html",
    "category": "AINotes/30.PyTorch/08.实战",
    "tags": [],
    "excerpt": "&lt;h1 id=\"transformer-解读\"&gt;Transformer 解读&lt;a class=\"anchor-link\" href=\"#transformer-解读\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;\n&lt;font size=3&gt;&lt;b&gt;[Transformer] Attention Is All You Need&lt;/b&gt;&lt;/font&gt;\n&lt;br&gt;\n&lt;"
  },
  {
    "title": "ViT解读",
    "url": "posts/AINotes/30.PyTorch/08.实战/06.ViT解读.html",
    "category": "AINotes/30.PyTorch/08.实战",
    "tags": [],
    "excerpt": "&lt;h1 id=\"vit解读\"&gt;ViT解读&lt;a class=\"anchor-link\" href=\"#vit解读\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;\n&lt;font size=3&gt;&lt;b&gt;[ViT] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.&lt;/b"
  },
  {
    "title": "文章结构",
    "url": "posts/AINotes/30.PyTorch/08.实战/03.LSTM解读及实战.html",
    "category": "AINotes/30.PyTorch/08.实战",
    "tags": [],
    "excerpt": "&lt;h1 id=\"文章结构\"&gt;文章结构&lt;a class=\"anchor-link\" href=\"#文章结构\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;在&lt;a href=\"./RNN%E8%AF%A6%E8%A7%A3%E5%8F%8A%E5%85%B6%E5%AE%9E%E7%8E%B0.md\"&gt;RNN详解及其实战&lt;/a&gt;中，我们简单讨论了为什么需要RNN"
  },
  {
    "title": "ResNet源码解读",
    "url": "posts/AINotes/30.PyTorch/08.实战/01.ResNet源码解读.html",
    "category": "AINotes/30.PyTorch/08.实战",
    "tags": [],
    "excerpt": "&lt;h1 id=\"resnet源码解读\"&gt;ResNet源码解读&lt;a class=\"anchor-link\" href=\"#resnet源码解读\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;p&gt;本文对残差神经网络（ResNet）的源码进行解读。残差神经网络是由微软研究院的何恺明、张祥雨、任少卿、孙剑等人提出的。它的主要贡献是发现了在增加网络层数的过程中，‎随着训练"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/01.PyTorch概述/01.Pytorch的概述.html",
    "category": "AINotes/30.PyTorch/01.PyTorch概述",
    "tags": [],
    "excerpt": "&lt;h2 id=\"pytorch-实现模型训练的-5-大要素\"&gt;PyTorch 实现模型训练的 5 大要素&lt;a class=\"anchor-link\" href=\"#pytorch-实现模型训练的-5-大要素\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;div align=center&gt;&lt;img src=\"https://markdownimg-hw.oss-c"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/03.数据处理/101.torch.utils.data.html",
    "category": "AINotes/30.PyTorch/03.数据处理",
    "tags": [],
    "excerpt": "&lt;h3 id=\"tensordataset\"&gt;TensorDataset&lt;a class=\"anchor-link\" href=\"#tensordataset\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h3&gt;\n&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://blog.csdn.net/raelum/article/details/125647614\""
  },
  {
    "title": "设置训练集的数据增强和转化",
    "url": "posts/AINotes/30.PyTorch/03.数据处理/02.torchvision.transforms.html",
    "category": "AINotes/30.PyTorch/03.数据处理",
    "tags": [],
    "excerpt": "&lt;h2 id=\"transforms\"&gt;transforms&lt;a class=\"anchor-link\" href=\"#transforms\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;我们在安装&lt;code&gt;PyTorch&lt;/code&gt;时，还安装了&lt;code&gt;torchvision&lt;/code&gt;，这是一个计算机视觉工具包。有 3 个主要的模块：&lt;/p&gt;\n&lt;"
  },
  {
    "title": "使用示例",
    "url": "posts/AINotes/30.PyTorch/03.数据处理/01.DataLoader 与 DataSet.html",
    "category": "AINotes/30.PyTorch/03.数据处理",
    "tags": [],
    "excerpt": "&lt;h2 id=\"dataloader-与-dataset\"&gt;DataLoader 与 DataSet&lt;a class=\"anchor-link\" href=\"#dataloader-与-dataset\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;blockquote&gt;\n&lt;p&gt;PyTorch 数据读入是通过 Dataset+DataLoader 的方式完成的，D"
  },
  {
    "title": "手动设置分布式环境变量（仅用于 Debug）",
    "url": "posts/AINotes/30.PyTorch/102.面经与bug解决/102.DDP.html",
    "category": "AINotes/30.PyTorch/102.面经与bug解决",
    "tags": [],
    "excerpt": "&lt;p&gt;本地pycharm debug报错，该如何修改&lt;/p&gt;\n&lt;p&gt;这个错误是因为在 &lt;strong&gt;本地 PyCharm 调试&lt;/strong&gt; 分布式训练代码时，&lt;strong&gt;没有正确设置环境变量&lt;/strong&gt;，导致 &lt;code&gt;torch.distributed&lt;/code&gt; 无法找到 &lt;code&gt;RANK&lt;/code&gt; 变量。&lt;/p&gt;\n&lt;hr /&gt;\n&lt;h2 id=\"为什么会发生这个错"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/102.面经与bug解决/01.model.html",
    "category": "AINotes/30.PyTorch/102.面经与bug解决",
    "tags": [],
    "excerpt": "&lt;h2 id=\"modeleval\"&gt;model.eval()&lt;a class=\"anchor-link\" href=\"#modeleval\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"pytorch中modeleval会对哪些函数有影响\"&gt;pytorch中model.eval()会对哪些函数有影响？&lt;a class=\"anchor-link\" "
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/102.面经与bug解决/101.Pytorch中的多个loss和的梯度回传.html",
    "category": "AINotes/30.PyTorch/102.面经与bug解决",
    "tags": [],
    "excerpt": "&lt;p&gt;TODO&lt;br /&gt;\n总 loss 由多个 loss 组成。如果只有一个 loss，那么直接 loss.backward()即可，不止一个 loss 时， backward()放在哪里？&lt;/p&gt;\n&lt;p&gt;目前的写法：&lt;/p&gt;\n&lt;pre&gt;&lt;code class=\"language-python\"&gt;loss1= Loss(output[0], target)\nloss2= Loss(output["
  },
  {
    "title": "output = net(fake_img, 'conv', 'prelu')",
    "url": "posts/AINotes/30.PyTorch/04.模型/02.模型容器.html",
    "category": "AINotes/30.PyTorch/04.模型",
    "tags": [],
    "excerpt": "&lt;h2 id=\"模型容器\"&gt;模型容器&lt;a class=\"anchor-link\" href=\"#模型容器\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;Torch.nn中一个重要的概念是模型容器 (&lt;a href=\"https://pytorch.org/docs/stable/nn.html#containers\"&gt;Containers&lt;/a&gt;)，常用的容"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/04.模型/101.模型示例.html",
    "category": "AINotes/30.PyTorch/04.模型",
    "tags": [],
    "excerpt": "&lt;p&gt;一个神经网络的典型训练过程如下：&lt;/p&gt;\n&lt;ol&gt;\n&lt;li&gt;定义包含一些可学习参数(或者叫权重）的神经网络&lt;/li&gt;\n&lt;li&gt;在输入数据集上迭代&lt;/li&gt;\n&lt;li&gt;通过网络处理输入&lt;/li&gt;\n&lt;li&gt;计算 loss (输出和正确答案的距离）&lt;/li&gt;\n&lt;li&gt;将梯度反向传播给网络的参数&lt;/li&gt;\n&lt;li&gt;更新网络的权重，一般使用一个简单的规则：&lt;code&gt;weight = weight - "
  },
  {
    "title": "导入必要的package",
    "url": "posts/AINotes/30.PyTorch/04.模型/06.模型修改.html",
    "category": "AINotes/30.PyTorch/04.模型",
    "tags": [],
    "excerpt": "&lt;h2 id=\"模型修改\"&gt;模型修改&lt;a class=\"anchor-link\" href=\"#模型修改\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"修改模型层\"&gt;修改模型层&lt;a class=\"anchor-link\" href=\"#修改模型层\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h3&gt;\n&lt;blockquote"
  },
  {
    "title": "查看随机初始化的conv参数",
    "url": "posts/AINotes/30.PyTorch/04.模型/04.权值初始化.html",
    "category": "AINotes/30.PyTorch/04.模型",
    "tags": [],
    "excerpt": "&lt;h2 id=\"梯度消失与梯度爆炸\"&gt;梯度消失与梯度爆炸&lt;a class=\"anchor-link\" href=\"#梯度消失与梯度爆炸\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;考虑一个 3 层的全连接网络。&lt;/p&gt;\n&lt;p&gt;&lt;span class=\"math-inline\"&gt;H{1}=X \\times W{1}，H{2}=H{1} \\times W{2}"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/04.模型/01.模型构建.html",
    "category": "AINotes/30.PyTorch/04.模型",
    "tags": [],
    "excerpt": "&lt;h2 id=\"模型构建\"&gt;模型构建&lt;a class=\"anchor-link\" href=\"#模型构建\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;网络模型的内容如下，包括&lt;strong&gt;模型创建&lt;/strong&gt;和&lt;strong&gt;权值初始化&lt;/strong&gt;，这些内容都在&lt;code&gt;nn.Module&lt;/code&gt;中有实现。&lt;/p&gt;\n&lt;div alig"
  },
  {
    "title": "实例化模型",
    "url": "posts/AINotes/30.PyTorch/04.模型/03.模型参数.html",
    "category": "AINotes/30.PyTorch/04.模型",
    "tags": [],
    "excerpt": "&lt;h2 id=\"nnparameter\"&gt;nn.Parameter&lt;a class=\"anchor-link\" href=\"#nnparameter\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;在 PyTorch 中，&lt;code&gt;nn.Parameter&lt;/code&gt;是&lt;code&gt;torch.nn.module.Parameter&lt;/code&gt;类的一个实例"
  },
  {
    "title": "保存整个模型",
    "url": "posts/AINotes/30.PyTorch/04.模型/05.模型保存与加载.html",
    "category": "AINotes/30.PyTorch/04.模型",
    "tags": [],
    "excerpt": "&lt;h2 id=\"模型存储内容\"&gt;模型存储内容&lt;a class=\"anchor-link\" href=\"#模型存储内容\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;一个PyTorch模型主要包含两个部分：模型结构和权重。其中模型是继承nn.Module的类，权重的数据结构是一个字典（key是层名，value是权重向量）。存储也由此分为两种形式：存储整个模型（"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/30.PyTorch/04.模型/11.nn.Module.html",
    "category": "AINotes/30.PyTorch/04.模型",
    "tags": [],
    "excerpt": "&lt;h2 id=\"state_dict\"&gt;state_dict&lt;a class=\"anchor-link\" href=\"#state_dict\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;blockquote&gt;\n&lt;p&gt;在PyTorch中，&lt;code&gt;state_dict&lt;/code&gt;是一个字典对象，用于存储模型或优化器的参数。这个字典将每一层或优化器的参数映射到"
  },
  {
    "title": "定义模型",
    "url": "posts/AINotes/30.PyTorch/04.模型/07.模型优化.html",
    "category": "AINotes/30.PyTorch/04.模型",
    "tags": [],
    "excerpt": "&lt;h2 id=\"torchoptimsgd\"&gt;torch.optim.SGD&lt;a class=\"anchor-link\" href=\"#torchoptimsgd\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;code&gt;torch.optim.SGD&lt;/code&gt; 是 PyTorch 中用于实现随机梯度下降（Stochastic Gradient Desc"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/19.集成学习/01.集成学习.html",
    "category": "AINotes/19.集成学习",
    "tags": [],
    "excerpt": "&lt;h2 id=\"集成学习概述\"&gt;集成学习概述&lt;a class=\"anchor-link\" href=\"#集成学习概述\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;集成学习(ensemble learning)本身不是一个单独的机器学习算法，而是通过构建并结合多个机器学习器来完成学习任务。也就是我们常说的“博采众长”。集成学习可以用于分类问题集成，回归问题集"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/19.集成学习/03.SGBoost.html",
    "category": "AINotes/19.集成学习",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://zhuanlan.zhihu.com/p/87885678\"&gt;【机器学习】决策树（下）——XGBoost、LightGBM（非常详细）&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;p&gt;本文是决策树的第三篇，主要介绍基于 Boosting 框架的主流集成算法，包括 XGBoost 和 LightGBM。&lt;/p&gt;\n&lt;p&gt;不知道为什么知"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/19.集成学习/04.LightGBM.html",
    "category": "AINotes/19.集成学习",
    "tags": [],
    "excerpt": "&lt;h2 id=\"2-lightgbm\"&gt;2. LightGBM&lt;a class=\"anchor-link\" href=\"#2-lightgbm\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;LightGBM 由微软提出，主要用于解决 GDBT 在海量数据中遇到的问题，以便其可以更好更快地用于工业实践中。&lt;/p&gt;\n&lt;p&gt;从 LightGBM 名字我们可以看出其"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/19.集成学习/02.决策树.html",
    "category": "AINotes/19.集成学习",
    "tags": [],
    "excerpt": "&lt;p&gt;决策树是一个非常常见并且优秀的机器学习算法，它易于理解、可解释性强，其可作为分类算法，也可用于回归模型。&lt;/p&gt;\n&lt;h2 id=\"1-id3\"&gt;1. ID3&lt;a class=\"anchor-link\" href=\"#1-id3\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;ID3 算法是建立在奥卡姆剃刀（用较少的东西，同样可以做好事情）的基础上：越是小"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/42.UCIL/04.Exploiting Fine-Grained Prototype Distribution for Boosting Unsupervised Class Incremental Learning.html",
    "category": "AINotes/42.UCIL",
    "tags": [],
    "excerpt": "&lt;h2 id=\"kimi全文翻译-arrow_down\"&gt;Kimi全文翻译 :arrow_down:&lt;a class=\"anchor-link\" href=\"#kimi全文翻译-arrow_down\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/42.UCIL/02.ProCA.html",
    "category": "AINotes/42.UCIL",
    "tags": [],
    "excerpt": "&lt;h2 id=\"chatgpt全文翻译-arrow_down\"&gt;ChatGPT全文翻译 :arrow_down:&lt;a class=\"anchor-link\" href=\"#chatgpt全文翻译-arrow_down\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-"
  },
  {
    "title": "ChatGPT总结 :arrow_down:",
    "url": "posts/AINotes/42.UCIL/03.POCON.html",
    "category": "AINotes/42.UCIL",
    "tags": [],
    "excerpt": "&lt;h1 id=\"chatgpt总结-arrow_down\"&gt;ChatGPT总结 :arrow_down:&lt;a class=\"anchor-link\" href=\"#chatgpt总结-arrow_down\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;h3 id=\"1-问题阐述\"&gt;1. 问题阐述&lt;a class=\"anchor-link\" href=\"#1-问题"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/42.UCIL/01.iLAP.html",
    "category": "AINotes/42.UCIL",
    "tags": [],
    "excerpt": "&lt;h2 id=\"chatgpt全文翻译-arrow_down\"&gt;ChatGPT全文翻译 :arrow_down:&lt;a class=\"anchor-link\" href=\"#chatgpt全文翻译-arrow_down\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/42.UCIL/00.Survey/00.无监督类增量学习.html",
    "category": "AINotes/42.UCIL/00.Survey",
    "tags": [],
    "excerpt": "&lt;h2 id=\"半监督增量学习\"&gt;半监督增量学习&lt;a class=\"anchor-link\" href=\"#半监督增量学习\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;首先，让我们先学习下半监督式的增量学习，其旨在通过利用未标记的数据来减少对标签的依赖。常规的做法是先利用一小部分带有标签的数据集训练出一个深度学习模型，然后利用该模型对未标记的数据打上标签。"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/45.DIL/01.DUCT.html",
    "category": "AINotes/45.DIL",
    "tags": [],
    "excerpt": "&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;域增量学习（Domain-Incremental Learning, DIL）涉及模型在不同域中逐步适应新概念。尽管预训练模型的最新进展为 DIL 提供了坚实的基础，但学习新概念通常会导致预训练知识的灾难性"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/13.知识蒸馏/11.AED.html",
    "category": "AINotes/13.知识蒸馏",
    "tags": [],
    "excerpt": "&lt;h2 id=\"ensemble-knowledge-distillation-for-ctr-prediction\"&gt;&lt;a href=\"http://arxiv.org/abs/2011.04106\"&gt;Ensemble Knowledge Distillation for CTR Prediction&lt;/a&gt;&lt;a class=\"anchor-link\" href=\"#ensemble-knowl"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/13.知识蒸馏/01.知识蒸馏概述.html",
    "category": "AINotes/13.知识蒸馏",
    "tags": [],
    "excerpt": "&lt;p&gt;知识蒸馏是一种模型压缩方法，是一种基于“教师-学生网络思想”的训练方法，由于其简单，有效，在工业界被广泛应用。这一技术的理论来自于2015年Hinton发表的一篇神作 &lt;a href=\"http://arxiv.org/abs/1503.02531\"&gt;Distilling the Knowledge in a Neural Network&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;Knowledge Disti"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/13.知识蒸馏/02.知识蒸馏分类.html",
    "category": "AINotes/13.知识蒸馏",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://zhuanlan.zhihu.com/p/664343770\"&gt;Springer知识蒸馏专著解读 | 面向图像识别的知识蒸馏综述&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;p&gt;本次文章介绍我们发表于由Springer出版的专著《&lt;a href=\"https://link.zhihu.com/?target=https%3A//l"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/13.知识蒸馏/12.WKD.html",
    "category": "AINotes/13.知识蒸馏",
    "tags": [],
    "excerpt": "&lt;h2 id=\"全文翻译\"&gt;全文翻译&lt;a class=\"anchor-link\" href=\"#全文翻译\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h2 id=\"0-摘要\"&gt;0. 摘要&lt;a class=\"anchor-link\" href=\"#0-摘要\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;自Hinton等人的"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/101.GPU/06.GPU和显存分析.html",
    "category": "AINotes/101.GPU",
    "tags": [],
    "excerpt": "&lt;p&gt;&lt;a href=\"https://zhuanlan.zhihu.com/p/31558973\"&gt;科普帖：深度学习中GPU和显存分析&lt;/a&gt;&lt;/p&gt;\n&lt;h2 id=\"0-预备知识\"&gt;0 预备知识&lt;a class=\"anchor-link\" href=\"#0-预备知识\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;code&gt;nvidia-smi&lt;/cod"
  },
  {
    "title": "l2p-cifar",
    "url": "posts/AINotes/101.GPU/11.执行脚本.html",
    "category": "AINotes/101.GPU",
    "tags": [],
    "excerpt": "&lt;h2 id=\"nohup\"&gt;nohup&lt;a class=\"anchor-link\" href=\"#nohup\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;执行脚本：&lt;code&gt;nohup ./train.sh &gt; ./res/.out 2&gt;&amp;1 &amp;&lt;/code&gt;&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;脚本配置"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/101.GPU/02.GPU服务器概念.html",
    "category": "AINotes/101.GPU",
    "tags": [],
    "excerpt": "&lt;h2 id=\"概念\"&gt;概念&lt;a class=\"anchor-link\" href=\"#概念\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;&lt;code&gt;显卡&lt;/code&gt; ：显卡，是图形处理单元（GPU）的简称，是独立的处理单元，可以进行图像处理和计算，硬件设备。&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;&lt;code&gt;CUDA&lt;/code&gt;：英"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/101.GPU/07.内存与显存_CPU与GPU_GPU与CUDA.html",
    "category": "AINotes/101.GPU",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://blog.csdn.net/Hanx09/article/details/107322958\"&gt;内存与显存、CPU与GPU、GPU与CUDA&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;h2 id=\"内存与显存\"&gt;内存与显存&lt;a class=\"anchor-link\" href=\"#内存与显存\" title=\"Permanent "
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/101.GPU/05.GPU通信.html",
    "category": "AINotes/101.GPU",
    "tags": [],
    "excerpt": "&lt;p&gt;TODO&lt;br /&gt;\n- https://mp.weixin.qq.com/s/KbYKAnZYQfLB2VkKQPhCVQ&lt;/p&gt;\n&lt;hr /&gt;\n&lt;p&gt;在计算机的世界里，总线就是这样一条高速公路，它连接着CPU（中央处理器，相当于城市的中心）、内存、硬盘和其他外围设备。&lt;br /&gt;\n&lt;strong&gt;总线是计算机硬件之间的高速公路&lt;/strong&gt;&lt;/p&gt;\n&lt;p&gt;总线由三部分组成：数据总线、"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/101.GPU/04.GPU显存.html",
    "category": "AINotes/101.GPU",
    "tags": [],
    "excerpt": "&lt;h2 id=\"显存大小和带宽\"&gt;显存大小和带宽&lt;a class=\"anchor-link\" href=\"#显存大小和带宽\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;NVIDIA GPU 显存有两种类型，GDDR和HBM，每种也有不同的型号。针对显存我们通常会关注两个指标：显存大小和显存带宽。HBM显存通常可以提供更高的显存带宽，但是价格也更贵，通常在训"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/101.GPU/03.GPU算力.html",
    "category": "AINotes/101.GPU",
    "tags": [],
    "excerpt": "&lt;p&gt;TODO&lt;br /&gt;\nhttps://mp.weixin.qq.com/s/9MqBP6OVlS_uq-VVuKc2cg&lt;/p&gt;\n&lt;hr /&gt;\n&lt;h2 id=\"tensor-core-算力计算\"&gt;Tensor Core 算力计算&lt;a class=\"anchor-link\" href=\"#tensor-core-算力计算\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/101.GPU/01.常用命令.html",
    "category": "AINotes/101.GPU",
    "tags": [],
    "excerpt": "&lt;h2 id=\"常用命令\"&gt;常用命令&lt;a class=\"anchor-link\" href=\"#常用命令\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;nohop 命令提交的作业在 xshell 断开后仍然可以运行。命令格式：nohop ./train.sh &gt; out 2&gt;&amp;1 &amp;。其中脚本文件需要有可执行权限，'o"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/101.GPU/999.英伟达AI芯片科普.html",
    "category": "AINotes/101.GPU",
    "tags": [],
    "excerpt": "&lt;h2 id=\"1英伟达-gpu-架构演进史\"&gt;1.英伟达 GPU 架构演进史&lt;a class=\"anchor-link\" href=\"#1英伟达-gpu-架构演进史\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;第一代 AI 加速卡叫 Volta&lt;/strong&gt; ，是英伟达第一次为 AI 运算专门设计的张量运算（Tensor Core）架"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/50.论文阅读与写作/00.AI会议期刊.html",
    "category": "AINotes/50.论文阅读与写作",
    "tags": [],
    "excerpt": "&lt;h2 id=\"ccf-会议截稿日期汇总网站\"&gt;CCF 会议截稿日期汇总网站&lt;a class=\"anchor-link\" href=\"#ccf-会议截稿日期汇总网站\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://ccfddl.github.io/\"&gt;https://ccfddl.github.io/&lt;/a&gt;&lt;/l"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/50.论文阅读与写作/11.arXiv.html",
    "category": "AINotes/50.论文阅读与写作",
    "tags": [],
    "excerpt": "&lt;ol&gt;\n&lt;li&gt;arXiv(发音同archive)是一个提供学术文章在线发表的服务器，领域涵盖物理学、数学、非线性科学、计算机科学、定量生命科学、计量金融学和统计学。&lt;/li&gt;\n&lt;li&gt;arXiv名中的“X”对应于希腊字母“χ”（大写为“Χ”，发音chi)。故arXiv的本意即archive（文献库）。&lt;/li&gt;\n&lt;li&gt;发表arXiv的论文不需要通过审核(peer review)，因此被用作发"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/50.论文阅读与写作/04.个人总结.html",
    "category": "AINotes/50.论文阅读与写作",
    "tags": [],
    "excerpt": "&lt;h2 id=\"实验结果\"&gt;实验结果&lt;a class=\"anchor-link\" href=\"#实验结果\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;h3 id=\"baseline\"&gt;Baseline&lt;a class=\"anchor-link\" href=\"#baseline\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h3&gt;\n&lt;p"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/50.论文阅读与写作/06.论文配色.html",
    "category": "AINotes/50.论文阅读与写作",
    "tags": [],
    "excerpt": "&lt;pre&gt;&lt;code class=\"language-python\"&gt;colors = ['#43978F', &quot;#9467bd&quot;,'#ED9F9B','#84C2AE', '#E56F5E', '#F6C957','#589CD6', '#FFB77F',&quot;#17becf&quot;,'#BD514A','#AED185', '#EF7E33', '#B395BD'"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/50.论文阅读与写作/03.ChatGPT润色.html",
    "category": "AINotes/50.论文阅读与写作",
    "tags": [],
    "excerpt": "&lt;h2 id=\"chatgpt学术论文指令\"&gt;ChatGPT学术论文指令&lt;a class=\"anchor-link\" href=\"#chatgpt学术论文指令\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;以下是50个学术论文指令的目录，当然，用完了复制的时候.千万记得删除指令 。&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;一、学术角色预设指令（2个)&lt;/li&gt;\n&lt;li&gt;二"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/50.论文阅读与写作/02.论文写作十规则.html",
    "category": "AINotes/50.论文阅读与写作",
    "tags": [],
    "excerpt": "&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"http://arxiv.org/abs/2211.15969\"&gt;Isolation and Impartial Aggregation: A Paradigm of Incremental Learning without Interference&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;h2 id=\"how-to-write-a-firs"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/50.论文阅读与写作/01.大模型译论文Prompt.html",
    "category": "AINotes/50.论文阅读与写作",
    "tags": [],
    "excerpt": "&lt;h2 id=\"全文翻译\"&gt;全文翻译&lt;a class=\"anchor-link\" href=\"#全文翻译\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;blockquote&gt;\n&lt;ol&gt;\n&lt;li&gt;prompt未能实现全文无遗漏翻译，翻译内容经常被简化&lt;/li&gt;\n&lt;li&gt;优先选择ChatGPT&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/blockquote&gt;\n&lt;p&gt;&lt;strong&gt;"
  },
  {
    "title": "规范与注意事项",
    "url": "posts/AINotes/50.论文阅读与写作/LaTeX.html",
    "category": "AINotes/50.论文阅读与写作",
    "tags": [],
    "excerpt": "&lt;h1 id=\"规范与注意事项\"&gt;规范与注意事项&lt;a class=\"anchor-link\" href=\"#规范与注意事项\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h1&gt;\n&lt;h2 id=\"规范\"&gt;规范&lt;a class=\"anchor-link\" href=\"#规范\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;table&gt;\n&lt;th"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/50.论文阅读与写作/13.论文写作常见问题.html",
    "category": "AINotes/50.论文阅读与写作",
    "tags": [],
    "excerpt": "&lt;ol&gt;\n&lt;li&gt;Overclaim&lt;/li&gt;\n&lt;/ol&gt;\n&lt;p&gt;Overclaim是我一开始投稿CVPR时审稿人给我的反馈，当时还一直在揣摩，为啥审稿人会这么说？是我的实验漏掉了部分，还是我的效果不够有说服力？这点其实是个很主观的评价，作为作者，唯一能做的就是根据你的方法和实验，来总结你的贡献，确保你所claim的每个点都能在方法或者实验里得到印证，有相应的evidence可以support，这"
  },
  {
    "title": "Untitled",
    "url": "posts/AINotes/50.论文阅读与写作/12.论文审稿大致流程.html",
    "category": "AINotes/50.论文阅读与写作",
    "tags": [],
    "excerpt": "&lt;h2 id=\"先看题目摘要和引言\"&gt;先看题目，摘要和引言&lt;a class=\"anchor-link\" href=\"#先看题目摘要和引言\" title=\"Permanent link\"&gt;&para;&lt;/a&gt;&lt;/h2&gt;\n&lt;p&gt;引言部分的teaser图会重点看，看是否把文章的main idea或者卖点说清楚了，引言部分的故事逻辑和动机是重点，动机强不强，直接决定我买不买账&lt;/p&gt;\n&lt;h2 id=\"相关"
  },
  {
    "title": "自我学习 & 自我聚类",
    "url": "posts/AINotes/18.零样本学习/01.零样本学习.html",
    "category": "AINotes/18.零样本学习",
    "tags": [],
    "excerpt": "&lt;p&gt;在zero-shot-learning里面呢？跟刚才讲的task是一样的，source data有label，target data每天label。在刚才task里面可以把source data当做training data，把target data当做testing data，但是实际上在zero-shot learning里面，它的difine又更加严格一点。它的difine是：今天在s"
  }
]