<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Untitled</title>
    <meta name="description" content="Untitled - Hongwei Zhao's Blog">
    <meta name="author" content="Hongwei Zhao">
    
    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <!-- Code Highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css" id="hljs-theme-dark">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css" id="hljs-theme-light" disabled>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    
    <style>
        :root {
            /* Light Theme - 明亮清新配色 */
            --primary-color: #4A90D9;
            --primary-hover: #3678C2;
            --link-color: #E86B5F;
            --text-color: #2D2D2D;
            --text-light: #5A5A5A;
            --text-muted: #8A8A8A;
            --bg-color: #FFFFFF;
            --bg-secondary: #F5F7FA;
            --bg-code: #F8F9FC;
            --border-color: #E8ECF0;
            --shadow: 0 2px 8px rgba(0,0,0,0.06);
            --shadow-lg: 0 8px 24px rgba(0,0,0,0.08);
        }

        [data-theme="dark"] {
            --primary-color: #5dade2;
            --primary-hover: #85c1e9;
            --link-color: #e74c3c;
            --text-color: #e5e7eb;
            --text-light: #9ca3af;
            --text-muted: #6b7280;
            --bg-color: #1a1a2e;
            --bg-secondary: #16213e;
            --bg-code: #0f0f23;
            --border-color: #374151;
            --shadow: 0 1px 3px rgba(0,0,0,0.3);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.4);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 16px;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            transition: background-color 0.3s, color 0.3s;
        }

        a {
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--primary-hover);
            text-decoration: underline;
        }

        /* Layout */
        .page-wrapper {
            display: flex;
            max-width: 1400px;
            margin: 0 auto;
            padding: 2rem;
            gap: 3rem;
        }

        /* Sidebar TOC */
        .toc-sidebar {
            width: 260px;
            flex-shrink: 0;
            position: sticky;
            top: 2rem;
            height: fit-content;
            max-height: calc(100vh - 4rem);
            overflow-y: auto;
        }

        .toc-container {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 1.5rem;
            border: 1px solid var(--border-color);
        }

        .toc-container h3 {
            font-size: 0.85rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-muted);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .toc-container ul {
            list-style: none;
        }

        .toc-container li {
            margin-bottom: 0.5rem;
        }

        .toc-container a {
            font-size: 0.9rem;
            color: var(--text-light);
            display: block;
            padding: 0.25rem 0;
            border-left: 2px solid transparent;
            padding-left: 0.75rem;
            transition: all 0.2s;
        }

        .toc-container a:hover,
        .toc-container a.active {
            color: var(--primary-color);
            border-left-color: var(--primary-color);
            text-decoration: none;
        }

        .toc-container ul ul {
            margin-left: 1rem;
        }

        /* Main Content */
        .main-content {
            flex: 1;
            min-width: 0;
            max-width: 800px;
        }

        /* Header */
        .post-header {
            margin-bottom: 2rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
            margin-bottom: 1.5rem;
        }

        .back-link:hover {
            color: var(--primary-color);
            text-decoration: none;
        }

        .post-header h1 {
            font-size: 2.25rem;
            font-weight: 700;
            line-height: 1.3;
            margin-bottom: 1rem;
            color: var(--text-color);
        }

        .post-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
        }

        .post-meta span {
            display: flex;
            align-items: center;
            gap: 0.4rem;
        }

        .post-meta i {
            color: var(--text-muted);
        }

        .tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 1rem;
        }

        .tag {
            display: inline-block;
            font-size: 0.8rem;
            background: var(--bg-secondary);
            color: var(--text-light);
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            border: 1px solid var(--border-color);
            transition: all 0.2s;
        }

        .tag:hover {
            background: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }

        /* Article Content */
        .post-content {
            font-size: 1rem;
            line-height: 1.9;
        }

        .post-content h1,
        .post-content h2,
        .post-content h3,
        .post-content h4,
        .post-content h5,
        .post-content h6 {
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-weight: 600;
            line-height: 1.4;
            color: var(--text-color);
        }

        .post-content h1 { font-size: 1.875rem; }
        .post-content h2 { 
            font-size: 1.5rem; 
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }
        .post-content h3 { font-size: 1.25rem; }
        .post-content h4 { font-size: 1.125rem; }

        .post-content p {
            margin-bottom: 1.25rem;
        }

        .post-content ul,
        .post-content ol {
            margin: 1.25rem 0;
            padding-left: 1.5rem;
        }

        .post-content li {
            margin-bottom: 0.5rem;
        }

        .post-content blockquote {
            margin: 1.5rem 0;
            padding: 1rem 1.5rem;
            background: var(--bg-secondary);
            border-left: 4px solid var(--primary-color);
            border-radius: 0 8px 8px 0;
            color: var(--text-light);
            font-style: italic;
        }

        .post-content blockquote p:last-child {
            margin-bottom: 0;
        }

        /* Code */
        .post-content code {
            font-family: 'JetBrains Mono', 'Fira Code', Consolas, monospace;
            font-size: 0.9em;
            background: var(--bg-code);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            color: var(--link-color);
        }

        .post-content pre {
            margin: 1.5rem 0;
            padding: 1.25rem;
            background: var(--bg-code);
            border-radius: 10px;
            overflow-x: auto;
            border: 1px solid var(--border-color);
        }

        .post-content pre code {
            background: none;
            padding: 0;
            color: inherit;
            font-size: 0.875rem;
            line-height: 1.6;
        }

        /* Images */
        .post-content img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            margin: 1.5rem 0;
            box-shadow: var(--shadow-lg);
        }

        /* Tables */
        .post-content table {
            width: 100%;
            margin: 1.5rem 0;
            border-collapse: collapse;
            font-size: 0.95rem;
        }

        .post-content th,
        .post-content td {
            padding: 0.75rem 1rem;
            border: 1px solid var(--border-color);
            text-align: left;
        }

        .post-content th {
            background: var(--bg-secondary);
            font-weight: 600;
        }

        .post-content tr:nth-child(even) {
            background: var(--bg-secondary);
        }

        /* Math */
        .math-display {
            margin: 1.5rem 0;
            overflow-x: auto;
            padding: 1rem;
            background: var(--bg-secondary);
            border-radius: 8px;
        }

        /* Anchor links */
        .anchor-link {
            opacity: 0;
            margin-left: 0.5rem;
            color: var(--text-muted);
            font-weight: 400;
            transition: opacity 0.2s;
        }

        .post-content h2:hover .anchor-link,
        .post-content h3:hover .anchor-link,
        .post-content h4:hover .anchor-link {
            opacity: 1;
        }

        /* Theme Toggle */
        .theme-toggle {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            background: var(--primary-color);
            color: white;
            border: none;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.25rem;
            box-shadow: var(--shadow-lg);
            transition: transform 0.2s, background 0.2s;
            z-index: 1000;
        }

        .theme-toggle:hover {
            transform: scale(1.1);
            background: var(--primary-hover);
        }

        /* Comments Section */
        .comments-section {
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-color);
        }

        .comments-section h3 {
            font-size: 1.25rem;
            margin-bottom: 1.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        /* Footer */
        .post-footer {
            margin-top: 3rem;
            padding-top: 1.5rem;
            border-top: 1px solid var(--border-color);
            text-align: center;
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        /* Responsive */
        @media (max-width: 1024px) {
            .toc-sidebar {
                display: none;
            }
            
            .page-wrapper {
                padding: 1.5rem;
            }
        }

        @media (max-width: 768px) {
            .post-header h1 {
                font-size: 1.75rem;
            }
            
            .post-meta {
                flex-direction: column;
                gap: 0.5rem;
            }
            
            .theme-toggle {
                bottom: 1rem;
                right: 1rem;
                width: 44px;
                height: 44px;
            }
        }

        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-secondary);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--border-color);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--text-muted);
        }
    </style>
</head>
<body>
    <div class="page-wrapper">
        <!-- TOC Sidebar -->
        <aside class="toc-sidebar">
            <div class="toc-container">
                <h3><i class="fas fa-list"></i> 目录</h3>
                <div class="toc">
<ul>
<li><a href="#pre-trained-vision-and-language-transformers-are-few-shot-incremental-learners">Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners</a></li>
<li><a href="#0-摘要">0. 摘要</a></li>
<li><a href="#1-引言">1. 引言</a></li>
<li><a href="#2-相关工作">2. 相关工作</a><ul>
<li><a href="#21-少样本类增量学习">2.1 少样本类增量学习</a></li>
<li><a href="#22-视觉-transformer-中的提示工程">2.2 视觉 Transformer 中的提示工程</a></li>
<li><a href="#23-来自语言模型的语义引导">2.3 来自语言模型的语义引导</a></li>
</ul>
</li>
<li><a href="#3-方法">3. 方法</a><ul>
<li><a href="#31-少样本类增量学习的定义">3.1 少样本类增量学习的定义</a></li>
<li><a href="#32-方法概览">3.2 方法概览</a></li>
<li><a href="#33-预训练知识调优pkt">3.3 预训练知识调优（PKT）</a></li>
<li><a href="#32-基于熵的发散损失entropy-based-divergence-loss">3.2 基于熵的发散损失（Entropy-based Divergence Loss）</a></li>
<li><a href="#33-语义知识蒸馏损失semantic-knowledge-distillation-loss">3.3 语义知识蒸馏损失（Semantic Knowledge Distillation Loss）</a><ul>
<li><a href="#小结基础阶段与增量阶段的总损失">小结：基础阶段与增量阶段的总损失</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#4-实验">4. 实验</a><ul>
<li><a href="#41-实验设置">4.1 实验设置</a></li>
<li><a href="#42-主要实验结果">4.2 主要实验结果</a></li>
<li><a href="#43-消融实验ablation-study">4.3 消融实验（Ablation Study）</a></li>
<li><a href="#44-分析analysis">4.4 分析（Analysis）</a><ul>
<li><a href="#441-在预训练-clip-网络上的-privilege">4.4.1 在预训练 CLIP 网络上的 PriViLege</a></li>
<li><a href="#442-pkt-的可微调层数探究">4.4.2 PKT 的可微调层数探究</a></li>
<li><a href="#443-基于熵的发散损失led的有效性验证">4.4.3 基于熵的发散损失（LED）的有效性验证</a></li>
<li><a href="#444-语义知识蒸馏lskd的效果分析">4.4.4 语义知识蒸馏（LSKD）的效果分析</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#5-结论">5. 结论</a></li>
</ul>
</div>

            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <article>
                <header class="post-header">
                    <a href="../../../index.html" class="back-link">
                        <i class="fas fa-arrow-left"></i> 返回博客列表
                    </a>
                    <h1>Untitled</h1>
                    <div class="post-meta">
                        <span><i class="fas fa-calendar-alt"></i> 2026-02-04</span>
                        <span><i class="fas fa-folder"></i> AINotes/41.小样本类增量学习FSCIL</span>
                        <span><i class="fas fa-user"></i> Hongwei Zhao</span>
                    </div>
                    <div class="tags">
                        
                    </div>
                </header>

                <div class="post-content">
                    <h2 id="pre-trained-vision-and-language-transformers-are-few-shot-incremental-learners"><a href="https://arxiv.org/abs/2404.02117">Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners</a><a class="anchor-link" href="#pre-trained-vision-and-language-transformers-are-few-shot-incremental-learners" title="Permanent link">&para;</a></h2>
<blockquote>
<p>CVPR 2024</p>
</blockquote>
<h2 id="0-摘要">0. 摘要<a class="anchor-link" href="#0-摘要" title="Permanent link">&para;</a></h2>
<p>少样本类增量学习（Few-Shot Class Incremental Learning，简称 FSCIL）是一项任务，要求模型在每个类别只有少量样本的情况下，逐步学习新类别而不遗忘旧类别。  </p>
<p>FSCIL 面临两大挑战：灾难性遗忘和过拟合。这些挑战促使以往的研究主要依赖于浅层模型，如 ResNet-18。虽然浅层模型因容量有限，可以缓解遗忘和过拟合问题，但也导致了在少样本增量阶段中知识迁移的不足。  </p>
<p>在本文中，我们主张：在大规模数据集上预训练的大型模型（如视觉与语言 Transformer）可以成为优秀的少样本增量学习者。为此，我们提出了一种新颖的 FSCIL 框架，称为 <strong>PriViLege</strong>（即 Pre-trained Vision and Language transformers with prompting functions and knowledge distillation）。  </p>
<p>我们的框架通过新提出的预训练知识调优（Pre-trained Knowledge Tuning，PKT）及两种新型损失函数——基于熵的发散损失（entropy-based divergence loss）和语义知识蒸馏损失（semantic knowledge distillation loss），有效应对了大型模型中的灾难性遗忘和过拟合问题。  </p>
<p>实验结果表明，所提出的 PriViLege 方法在多个数据集上大幅度超越了当前的最新方法。例如，在 CUB200 上提高了 +9.38%，在 CIFAR-100 上提高了 +20.58%，在 miniImageNet 上提高了 +13.36%。  </p>
<p>我们的实现代码公开在：<br />
https://github.com/KHU-AGI/PriViLege</p>
<hr />
<h2 id="1-引言">1. 引言<a class="anchor-link" href="#1-引言" title="Permanent link">&para;</a></h2>
<p>人类具有卓越的能力，能够仅凭少量数据快速理解新的概念。为了使深度神经网络具备这种能力，最早在文献【33】中引入了少样本类增量学习（Few-Shot Class Incremental Learning，简称 FSCIL）。  </p>
<p>FSCIL 模仿人类的学习方式，通常包括一个基础阶段（base session）和多个增量阶段（incremental sessions）。在基础阶段，网络利用充足的训练数据学习大量类别；而在增量阶段，网络需针对每个新类别，使用极少的样本进行训练。  </p>
<p>由于增量阶段数据受限，如何有效地传递基础阶段所学到的多样知识，成为 FSCIL 成功的关键。  </p>
<p>在 FSCIL 中，主要存在两大挑战：灾难性遗忘【20】和过拟合【33】。灾难性遗忘是指，当网络依次学习新类别时，严重遗忘了之前学到的知识。而过拟合则指，网络过度专注于有限的训练样本，导致整体性能下降。  </p>
<p>为了解决这些问题，已有研究主要采用浅层模型，如 ResNet-18【12, 39, 40】。<br />
浅层模型参数数量有限，便于通过部分冻结减少遗忘，并抑制过拟合。但浅层模型的容量受限，无法充分捕捉和迁移基础阶段的领域知识到增量阶段。  </p>
<p>最近，像 Vision Transformer (ViT)【3】和 Contrastive Language-Image Pretraining (CLIP)【26】等大型预训练模型在计算机视觉领域广泛应用，因其出色的适应性和性能表现。  </p>
<p>因此，大型预训练模型能够有效学习和迁移基础阶段的领域知识，克服浅层模型在知识迁移能力上的局限。  </p>
<p>然而，微调大型预训练模型容易导致预训练知识的遗忘；而完全冻结模型，又阻碍了基础阶段的领域知识学习。如何在保留预训练知识和获取新领域知识之间权衡，成为在 FSCIL 中应用大型模型的一大挑战。  </p>
<p>为了深入探讨大型模型在 FSCIL 中的挑战和应用潜力，我们在 CIFAR-100【14】数据集上进行了 5-way 5-shot 的实验。  </p>
<h2 id="2-相关工作">2. 相关工作<a class="anchor-link" href="#2-相关工作" title="Permanent link">&para;</a></h2>
<h3 id="21-少样本类增量学习">2.1 少样本类增量学习<a class="anchor-link" href="#21-少样本类增量学习" title="Permanent link">&para;</a></h3>
<p>少样本类增量学习（Few-Shot Class Incremental Learning，FSCIL）是一种类别增量学习（Class Incremental Learning, CIL）的变体，但更具挑战性，因为模型需要仅凭少量训练样本学习新类别。  </p>
<p>在以往众多的 FSCIL 方法【34】中，动态网络结构方法【29, 33, 38, 40】通过在训练过程中调整网络结构，来保留先前知识，减少遗忘。  </p>
<p>基于特征和特征空间的方法【2, 12, 23, 30, 41, 42】使模型能够更好地适应新类别，同时提升特征提取器对新类别的泛化能力。  </p>
<p>基于原型（prototype-based）的方法【6, 19, 39】旨在通过对齐原型与分类器权重，提升分类性能。  </p>
<p>然而，之前的方法主要关注于在浅层网络中缓解遗忘和过拟合问题，因此在受限的模型容量下，性能提升是有限的。  </p>
<p>本文采用了如预训练 ViT 和 CLIP 这样的大型模型用于 FSCIL，并介绍了如何有效利用它们来克服 FSCIL 的主要挑战。</p>
<h3 id="22-视觉-transformer-中的提示工程">2.2 视觉 Transformer 中的提示工程<a class="anchor-link" href="#22-视觉-transformer-中的提示工程" title="Permanent link">&para;</a></h3>
<p>提示微调（prompt tuning）【16】和前缀微调（prefix tuning）【17】被广泛应用于视觉 Transformer 的提示工程中。  </p>
<p>提示微调是在输入序列中添加可学习的提示；而前缀微调则通过将提示附加到查询（key）和值（value）上，直接影响注意力模式，以便进行任务特定的知识获取。  </p>
<p>使用提示工程的视觉 Transformer 在类增量学习中展现了显著的性能。  </p>
<p>L2P【37】和 DualPrompt【36】分别利用提示微调和前缀微调来学习新类别，同时冻结预训练的 ViT。L2P 和 DualPrompt 都使用随机初始化的提示进行提示工程。  </p>
<p>近期一些方法【9, 31, 32】提出了通过生成提示以适配领域空间的方法，以实现有效的持续学习。  </p>
<p>例如，CODA-Prompt【31】需要收集多个提示组件，并用与输入相关的权重组合，生成输入特定的提示。<br />
APG【32】和 DAP【9】则采用了包括交叉注意力层、可学习参数组、线性层等多组件构成的提示生成器。  </p>
<p>这些提示生成方法通常需要额外的组件和提示生成器的训练开销。  </p>
<p>与需要额外可学习组件的提示生成方法不同，本文提出了一种轻量化的提示调制（modulation）方法，大幅度减少了额外训练组件的需求，同时提升了提示的表示学习能力。</p>
<h3 id="23-来自语言模型的语义引导">2.3 来自语言模型的语义引导<a class="anchor-link" href="#23-来自语言模型的语义引导" title="Permanent link">&para;</a></h3>
<p>在广义零样本学习（Generalized Zero-Shot Learning）【1, 25】领域中，利用语言嵌入来有效学习新类别已经被广泛研究。  </p>
<p>最近，研究【11】探索了在持续学习（Continual Learning）中引入语言引导以增强表示学习的方法。  </p>
<p>在 FSCIL 领域，已有多种方法引入了额外的语言信息（尤其是类别名称）来提升基础阶段的表示学习。  </p>
<p>例如，文献【42】通过校准已有类别与新类别之间的编码语言信息，解决了分类器权重漂移问题。  </p>
<p>此外，文献【2】提出了一种正则化方法，利用从 GloVe 网络【24】中提取的类别词嵌入所推导出的关系信息。  </p>
<hr />
<h2 id="3-方法">3. 方法<a class="anchor-link" href="#3-方法" title="Permanent link">&para;</a></h2>
<h3 id="31-少样本类增量学习的定义">3.1 少样本类增量学习的定义<a class="anchor-link" href="#31-少样本类增量学习的定义" title="Permanent link">&para;</a></h3>
<p>在 FSCIL 中，训练数据集表示为：<br />
<div class="math-display"><br />
    \mathcal{D} = {D_0, D_1, ..., D_T}<br />
</div><br />
其中，<span class="math-inline">D_0</span> 是基础阶段（base session）的数据集，<span class="math-inline">D_t = {(x_i, y_i)}_{i=1}^{|D_t|}</span> 是第 <span class="math-inline">t</span> 个增量阶段（incremental session）的数据集，<span class="math-inline">1 \leq t \leq T</span>，<span class="math-inline">T</span> 表示总的增量阶段数量。  </p>
<p>在这里，基础阶段的数据集 <span class="math-inline">D_0</span> 通常拥有一个较大的标签空间 <span class="math-inline">C_0</span>，并且每个类别 <span class="math-inline">c \in C_0</span> 都有充足的训练样本。  </p>
<p>相比之下，增量阶段的数据集 <span class="math-inline">D_t</span> 每个类别仅包含少量训练样本，即：<br />
<div class="math-display"><br />
    |D_t| = k \times |C_t|<br />
</div><br />
其中，<span class="math-inline">|C_t|</span> 是第 <span class="math-inline">t</span> 个任务中新类别的数量，<span class="math-inline">k</span> 是每个新类别的样本数。  </p>
<p>不同阶段之间的类别是互不重叠的，且在每个阶段，模型仅能访问当前阶段的数据。  </p>
<p>在这种设定下，FSCIL 的目标是使模型能够从极少量样本中逐步学习新类别，同时保持对所有已遇到类别的分类能力。</p>
<hr />
<h3 id="32-方法概览">3.2 方法概览<a class="anchor-link" href="#32-方法概览" title="Permanent link">&para;</a></h3>
<p>图 2 展示了我们所提出方法的整体框架，其中预训练的 Vision Transformer (ViT) 被用作主干网络（backbone network）。  </p>
<p>为了优化 ViT 中的预训练知识，我们提出了一个新方法，称为 <strong>预训练知识调优（Pre-trained Knowledge Tuning，PKT）</strong> （详见第 3.1 节）。  </p>
<p>PKT 通过训练基础提示（B-Prompt）、视觉 - 语言提示（VL-Prompt）以及 ViT 的部分选定层，来增强基础阶段知识向增量阶段的迁移能力。  </p>
<p>此外，为了在基础阶段强化判别能力，我们引入了一个基于熵的发散损失（详见第 3.2 节），它作用于 VL-Prompt 中的视觉标记（vision token）。  </p>
<p>最后，我们提出了一个语义知识蒸馏损失（详见第 3.3 节），以将语义知识注入到 VL-Prompt 中的语言标记（language token），从而提升模型的表示学习能力。  </p>
<p>在少样本环境下，为了稳定学习，我们采用了各类别的原型（prototype）作为分类器。</p>
<hr />
<h3 id="33-预训练知识调优pkt">3.3 预训练知识调优（PKT）<a class="anchor-link" href="#33-预训练知识调优pkt" title="Permanent link">&para;</a></h3>
<p>近期，基于提示（prompting）方法的预训练大型 ViT 在类增量学习（CIL）中取得了显著性能【21, 36, 37】。  </p>
<p>然而，将大型预训练 ViT 有效地应用于 FSCIL 仍未被深入探索。现有方法在应用时，仍然面临灾难性遗忘和过拟合的问题。  </p>
<p>同时，已有基于提示的方法由于提示量有限，难以充分将基础阶段的知识传递到增量阶段。  </p>
<p>因此，本文提出了一个新方法，称为 <strong>预训练知识调优（PKT）</strong>，通过额外添加提示，并选择性地微调特定层，来在基础阶段有效地学习领域特定知识。  </p>
<p>在 PKT 中，我们特别更新 ViT 主干网络 <span class="math-inline">f_{\theta}</span> 的前 <span class="math-inline">L</span> 层，其中 <span class="math-inline">L</span> 是可调节的超参数，表示需要更新的层数。  </p>
<p>经过实证分析，我们确定了最优的微调层数，例如，在 ViT-B 中设置 <span class="math-inline">L=2</span>。  </p>
<p>由于大部分层被冻结，预训练知识得以在增量学习中保留下来。  </p>
<p>经过基础阶段微调后的 ViT 将被冻结，以保存所学习的领域特定知识，并将其迁移到增量阶段。  </p>
<p>由于冻结了大部分层，因此有效学习领域知识成为一大挑战。为此，我们引入了两个关键提示：</p>
<ul>
<li>基础提示（B-Prompt），记为 <span class="math-inline">P_B \in \mathbb{R}^{L \times 2 \times D}</span>，其中 <span class="math-inline">D</span> 是嵌入维度；</li>
<li>视觉 - 语言提示（VL-Prompt），记为 <span class="math-inline">P_{VL} \in \mathbb{R}^{2 \times D}</span>。  </li>
</ul>
<p>其中，B-Prompt 主要在基础阶段捕捉领域知识，并帮助知识迁移到增量阶段；而 VL-Prompt 则由视觉标记和语言标记组成，旨在将所有先前阶段的正向知识传递到下一个阶段。  </p>
<p>我们使用前缀微调（prefix tuning）训练 B-Prompt，使用提示微调（prompt tuning）训练 VL-Prompt。</p>
<hr />
<p>不过，前缀微调存在一个已知的问题【4,5】，即学习速度较慢，导致 B-Prompt 难以有效学习领域知识，且可能被微调的层忽略。  </p>
<p>为此，我们提出了新的调制提示（modulation prompts）<span class="math-inline">P_M</span>，以辅助 B-Prompt。  </p>
<p>调制提示由两个部分组成：</p>
<ul>
<li>头部特定提示（Head-specific Prompt）<span class="math-inline">P^S_M</span></li>
<li>通用提示（Generic Prompt）<span class="math-inline">P^G_M</span></li>
</ul>
<p>这两部分分别来源于 ViT 的多头自注意力（Multi-Head Self-Attention，MSA）层和后续的 MLP 层。  </p>
<p>它们的公式如下：</p>
<p>首先，多头注意力的输出为：<br />
<div class="math-display"><br />
    h_{MSA} = \text{MSA}(h_Q, h_K, h_V) \tag{1}<br />
</div><br />
然后，头部特定调制提示为：<br />
<div class="math-display"><br />
    P^S_M = [g^S_1(h^1_{MSA}); \ldots ; g^S_H(h^H_{MSA})] \tag{2}<br />
</div><br />
接下来，MLP 层的输出为：<br />
<div class="math-display"><br />
    h_{MLP} = \text{MLP}(h_{MSA}) \tag{3}<br />
</div><br />
再通过卷积得到通用提示：<br />
<div class="math-display"><br />
    P^G_M = g^G(h_{MLP}) \tag{4}<br />
</div><br />
其中，<span class="math-inline">H</span> 表示多头注意力的头数，<span class="math-inline">g^S</span> 和 <span class="math-inline">g^G</span> 分别是 1×1 点卷积操作（point-wise convolution）用于生成头部特定提示和通用提示。  </p>
<hr />
<p>通过预训练层和输入数据，这些调制提示可以扩大 B-Prompt 的特征向量，从而提升 B-Prompt 对领域知识的捕捉能力。  </p>
<p>具体的 PKT 前缀微调过程如下：  </p>
<p>扩展后的键提示：<br />
<div class="math-display"><br />
    \bar{P}'<em>K = P^S_M \odot P^B_K \tag{5}<br />
</div><br />
扩展后的值提示：<br />
<div class="math-display"><br />
    \bar{P}'_V = P^G_M \odot P^B_V \tag{6}<br />
</div><br />
最终的 MSA 计算为：<br />
<div class="math-display"><br />
    \bar{h}</em>{out} = \text{MSA}([\mathcal{P}^{Q}<em>{VL}; h_Q], [\bar{P}'_K; h_K], [\bar{P}'_V; h_V]) \tag{7}<br />
</div><br />
其中，<span class="math-inline">h_Q, h_K, h_V</span> 是输入的查询、键和值，<span class="math-inline">\bar{h}</em>{out}</span> 是输出结果。</p>
<hr />
<p>总结来说，PKT 带来了两大优势：</p>
<ol>
<li>通过在前 <span class="math-inline">L</span> 层引入额外可塑性，并结合调制提示，有效地学习了基础阶段的领域知识。</li>
<li>通过放大 B-Prompt，促进了其更新，从而学习到了有用的领域特定知识。  </li>
</ol>
<p>实验证明，PKT 能显著提升性能，并促进增量阶段的正向知识迁移。</p>
<hr />
<h3 id="32-基于熵的发散损失entropy-based-divergence-loss">3.2 基于熵的发散损失（Entropy-based Divergence Loss）<a class="anchor-link" href="#32-基于熵的发散损失entropy-based-divergence-loss" title="Permanent link">&para;</a></h3>
<p>在使用 PKT 训练提示（prompts）和选择的层的过程中，我们的模型能够有效获取领域知识，以便在增量阶段实现正向迁移。  </p>
<p>为了嵌入多视角（multi-perspective）的知识，我们不仅利用 [CLS] 标记，还引入了视觉标记（vision token）进行分类，并通过平均池化（average pooling）融合它们。  </p>
<p>然而，由于这两个标记具有相同的目标，随着训练的进行，它们的输出特征会趋于相似，这会妨碍视觉标记自身的有效训练。  </p>
<p>为了增强视觉标记本身的判别能力，我们提出了一个新的正则化项，称为 <strong>基于熵的发散损失（LED）</strong>。</p>
<hr />
<p>为了计算 LED，我们首先构建了一个原型分类器 <span class="math-inline">\psi \in \mathbb{R}^{|C_0| \times D}</span>，它由基础阶段各类别的原型（prototype）组成。  </p>
<p>对于每一个基础类别 <span class="math-inline">c_j</span>，其中 <span class="math-inline">c_j \in C_0</span>，<span class="math-inline">j \in {1, 2, ..., |C_0|}</span>，其原型向量 <span class="math-inline">\text{proto}<em>{c_j} \in \mathbb{R}^D</span> 是通过预训练 ViT 网络提取的所有 [CLS] 特征向量的平均值：<br />
<div class="math-display"><br />
    \text{proto}</em>{c_j} = \frac{1}{N_{c_j}} \sum_{k=1}^{N_{c_j}} f^{cls}<em>k \tag{8}<br />
</div><br />
其中，<span class="math-inline">N</em>{c_j}</span> 是类别 <span class="math-inline">c_j</span> 的样本数量，<span class="math-inline">f^{cls}_k</span> 是第 <span class="math-inline">k</span> 个样本的 [CLS] 特征。  </p>
<p>因此，原型分类器可以表示为：<br />
<div class="math-display"><br />
    \psi = [\text{proto}<em>{c_1}; \text{proto}</em>{c_2}; \dots; \text{proto}<em>{c</em>{|C_0|}}] \tag{9}<br />
</div></p>
<hr />
<p>接着，我们用原型分类器 <span class="math-inline">\psi</span> 计算对应于 [CLS] 特征和视觉特征的预测 logits：<br />
<div class="math-display"><br />
    \hat{y}_i^{cls} = \psi(f_i^{cls})<br />
</div></p>
<p><div class="math-display"><br />
    \hat{y}_i^{vis} = \psi(f_i^{vis})<br />
</div><br />
其中，<span class="math-inline">f_i^{cls}</span> 和 <span class="math-inline">f_i^{vis}</span> 分别是输入样本 <span class="math-inline">x_i</span> 的 [CLS] 特征和视觉标记特征。  </p>
<p>需要注意的是，原型分类器 <span class="math-inline">\psi</span> 在计算过程中是固定的，不参与梯度更新，以提供稳定的基础。</p>
<hr />
<p>最后，定义基于熵的发散损失 <span class="math-inline">L_{ED}</span> 为：<br />
<div class="math-display"><br />
    L_{ED} = \log\left( \frac{L_{CE}(\hat{y}<em>i^{vis}, y_i) + L</em>{CE}(\hat{y}<em>i^{cls}, y_i)}{L</em>{KL}(\delta(\hat{y}_i^{vis}), \delta(\hat{y}_i^{cls})) + 1} \right) \tag{10}<br />
</div><br />
其中：</p>
<ul>
<li><span class="math-inline">L_{CE}</span> 是交叉熵损失（Cross Entropy Loss）；</li>
<li><span class="math-inline">L_{KL}</span> 是 Kullback-Leibler 散度（KL Divergence Loss）【15】；</li>
<li><span class="math-inline">\delta(\cdot)</span> 表示 softmax 函数。</li>
</ul>
<hr />
<p>为了最小化 <span class="math-inline">L_{ED}</span>，我们的模型需要：</p>
<ul>
<li>同时最小化 <span class="math-inline">L_{CE}(\hat{y}<em>i^{vis}, y_i)</span> 和 <span class="math-inline">L</em>{CE}(\hat{y}_i^{cls}, y_i)</span>，即提升视觉和 [CLS] 特征各自的分类能力；</li>
<li>同时最大化 <span class="math-inline">\text{KL}(\delta(\hat{y}_i^{vis}), \delta(\hat{y}_i^{cls}))</span>，即让视觉特征和 [CLS] 特征输出尽可能不同，从而避免特征退化成相似表示。  </li>
</ul>
<p>通过这个基于熵的发散损失，我们的模型可以有效利用视觉标记学习领域特定知识，并在基础阶段建立可迁移的特征表示，为后续增量阶段提供支持。</p>
<hr />
<h3 id="33-语义知识蒸馏损失semantic-knowledge-distillation-loss">3.3 语义知识蒸馏损失（Semantic Knowledge Distillation Loss）<a class="anchor-link" href="#33-语义知识蒸馏损失semantic-knowledge-distillation-loss" title="Permanent link">&para;</a></h3>
<p>即使从基础阶段迁移过来的知识丰富且有用，<br />
但在增量阶段，依然很难仅凭少量训练样本准确地学习新类别的表示。  </p>
<p>为了缓解这一问题，<br />
我们需要向模型提供与新类别相关的<strong>外部知识</strong>，以促进更好的适应。  </p>
<p>为此，本文引入了一个新的损失项，称为 <strong>语义知识蒸馏损失（Semantic Knowledge Distillation Loss，LSKD）</strong>，<br />
利用预训练语言模型（如 BERT【10】）生成的语言嵌入，向视觉模型提供附加的语义知识。</p>
<hr />
<p>具体实现如下：  </p>
<p>首先，利用预训练语言模型（PLM）<span class="math-inline">f_\phi</span>，根据每个类别名称生成语言嵌入特征：<br />
<div class="math-display"><br />
    w_{cni} = f_\phi(\text{word}<em>{cni})<br />
</div><br />
其中，<span class="math-inline">\text{word}</em>{cni}</span> 是类别名称 <span class="math-inline">cni</span> 的文本表示，通常被格式化为 "a photo of [cni]"。  </p>
<p>与此同时，通过 ViT 主干网络 <span class="math-inline">f_\theta</span>，我们可以得到 VL-Prompt 中语言标记（language token）的输出特征：<br />
<div class="math-display"><br />
    f_i^{lang} = f_\theta(x_i)[2]<br />
</div><br />
这里的 <span class="math-inline">[2]</span> 表示提取 VL-Prompt 中的第二个标记，即语言标记的特征。</p>
<hr />
<p>为了将语言嵌入特征 <span class="math-inline">w_{cni}</span> 蒸馏到语言标记特征 <span class="math-inline">f_i^{lang}</span>，<br />
我们采用了文献【7】提出的<strong>知识蒸馏损失（Knowledge Distillation Loss，<span class="math-inline">L_{KD}</span>）</strong>。  </p>
<p>然而，考虑到 <span class="math-inline">f_i^{lang}</span> 和 <span class="math-inline">w_{cni}</span> 分别来源于视觉特征空间和语言嵌入空间，<br />
直接应用蒸馏损失以匹配这两种不同分布可能效果不佳。  </p>
<hr />
<p>为了解决这一问题，我们再次使用前文定义的原型分类器 <span class="math-inline">\psi</span> 来稳定对齐输出特征。  </p>
<p>具体地，将语言标记特征 <span class="math-inline">f_i^{lang}</span> 输入到原型分类器 <span class="math-inline">\psi</span> 中，得到预测结果：<br />
<div class="math-display"><br />
    \hat{y}<em>i^{lang} = \psi(f_i^{lang})<br />
</div><br />
随后，我们计算交叉熵损失（<span class="math-inline">L</em>{CE}</span>），以最小化语言特征和真实标签之间的分布差异。  </p>
<p>最终定义<strong>语义知识蒸馏损失（<span class="math-inline">L_{SKD}</span>）</strong>如下：<br />
<div class="math-display"><br />
    L_{SKD} = L_{KD}(f_i^{lang}, w_{cni}) + \gamma \cdot L_{CE}(\hat{y}_i^{lang}, y_i) \tag{11}<br />
</div><br />
其中：</p>
<ul>
<li><span class="math-inline">L_{KD}(\cdot, \cdot)</span> 是特征间的知识蒸馏损失；</li>
<li><span class="math-inline">L_{CE}(\cdot, \cdot)</span> 是交叉熵损失；</li>
<li><span class="math-inline">\gamma</span> 是权衡超参数，我们在所有实验中将其设置为 <span class="math-inline">0.1</span>。</li>
</ul>
<hr />
<p>总结来说，提出的 <span class="math-inline">L_{SKD}</span> 能够有效地将语言空间中的有用语义知识蒸馏到视觉空间中，<br />
为增量阶段的少样本表示学习提供额外支持。  </p>
<p>这不仅有助于缓解少样本带来的学习困难，还能通过基础阶段充分学习大量类别的知识，<br />
从而在增量阶段实现正向知识迁移。</p>
<hr />
<h4 id="小结基础阶段与增量阶段的总损失">小结：基础阶段与增量阶段的总损失<a class="anchor-link" href="#小结基础阶段与增量阶段的总损失" title="Permanent link">&para;</a></h4>
<p>在基础阶段（base session），总的损失函数为：<br />
<div class="math-display"><br />
    L_{base} = L_{CE}(\hat{y}<em>i, y_i) + \alpha \cdot L</em>{ED} + \beta \cdot L_{SKD} \tag{12}<br />
</div><br />
其中，<span class="math-inline">\alpha</span> 和 <span class="math-inline">\beta</span> 分别是用于控制基于熵的发散损失和语义知识蒸馏损失的重要性比例因子。</p>
<hr />
<p>而在增量阶段（incremental sessions），<br />
由于少样本数据不足以支撑判别性特征学习，因此我们不再使用 <span class="math-inline">L_{ED}</span>。<br />
此时，总的损失函数为：<br />
<div class="math-display"><br />
    L_{inc} = L_{CE}(\hat{y}<em>i, y_i) + \beta \cdot L</em>{SKD} \tag{13}<br />
</div></p>
<hr />
<h2 id="4-实验">4. 实验<a class="anchor-link" href="#4-实验" title="Permanent link">&para;</a></h2>
<h3 id="41-实验设置">4.1 实验设置<a class="anchor-link" href="#41-实验设置" title="Permanent link">&para;</a></h3>
<p><strong>数据集与评估指标</strong><br />
我们在三个数据集上与当前最新的 FSCIL 方法进行了对比评估：CIFAR-100【14】、miniImageNet【27】和 CUB200【35】。  </p>
<p>如表 2 所示，我们在所有数据集上均遵循文献【33】提出的划分设置。  </p>
<p>评估指标包括：</p>
<ul>
<li>基础阶段准确率 <span class="math-inline">A_{Base}</span>；</li>
<li>最后阶段准确率 <span class="math-inline">A_{Last}</span>；</li>
<li>所有阶段的平均准确率 <span class="math-inline">A_{Avg}</span>。</li>
</ul>
<p>我们在不同随机种子下进行了 5 次独立实验，并报告其平均结果。</p>
<hr />
<p><strong>基线方法与实现细节</strong><br />
我们将以下最新的 FSCIL 方法作为对比基线：</p>
<ul>
<li>CEC【40】、</li>
<li>WaRP【12】、</li>
<li>NC-FSCIL【39】。</li>
</ul>
<p>此外，还将以下基于 ViT 的方法作为比较：</p>
<ul>
<li>L2P【37】、</li>
<li>DualPrompt【36】。</li>
</ul>
<p>在所有方法中（包括我们的方法），统一使用 <strong>ViT-B/16</strong>【3】作为主干网络，并且该模型预训练于 <strong>ImageNet-21K</strong>【28】。  </p>
<p>我们使用 <strong>BERT-base</strong>【10】提取类别名称的词嵌入。  </p>
<p>对于预训练知识调优（PKT），我们设置微调前两层。<br />
对于损失项比例因子，我们统一设置：</p>
<ul>
<li><span class="math-inline">\alpha = 0.5</span>，</li>
<li><span class="math-inline">\beta = 0.5</span>。  </li>
</ul>
<p>优化器使用 <strong>Adam</strong>【13】，学习率设为 <span class="math-inline">2 \times 10^{-4}</span>，并采用 <strong>余弦退火调度器</strong>（cosine annealing scheduler）【18】。  </p>
<p>训练硬件为 RTX 3090 GPU。<br />
批次大小（batch size）设为 128。  </p>
<p>训练轮次设置：</p>
<ul>
<li>基础阶段：训练 5 个 epoch；</li>
<li>增量阶段：每阶段训练 3 个 epoch。</li>
</ul>
<hr />
<p><strong>表 2. FSCIL 基准测试的配置设置</strong></p>
<table>
<thead>
<tr>
<th style="text-align: left;">数据集</th>
<th style="text-align: left;">基础阶段类别数</th>
<th style="text-align: left;">增量阶段设定</th>
<th style="text-align: left;">总阶段数</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CUB200</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">10-way 5-shot</td>
<td style="text-align: left;">1+10</td>
</tr>
<tr>
<td style="text-align: left;">CIFAR-100</td>
<td style="text-align: left;">60</td>
<td style="text-align: left;">5-way 5-shot</td>
<td style="text-align: left;">1+8</td>
</tr>
<tr>
<td style="text-align: left;">miniImageNet</td>
<td style="text-align: left;">60</td>
<td style="text-align: left;">5-way 5-shot</td>
<td style="text-align: left;">1+8</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="42-主要实验结果">4.2 主要实验结果<a class="anchor-link" href="#42-主要实验结果" title="Permanent link">&para;</a></h3>
<p>我们分别在 CUB200、CIFAR-100 和 miniImageNet 上报告了基础阶段、最后阶段以及平均准确率，详见表 1。  </p>
<p><strong>表 1. 在 CUB200、CIFAR-100 和 miniImageNet 上的性能比较</strong></p>
<table>
<thead>
<tr>
<th style="text-align: left;">方法</th>
<th style="text-align: left;">CUB200 <span class="math-inline">A_{Base}</span></th>
<th style="text-align: left;">CUB200 <span class="math-inline">A_{Last}</span></th>
<th style="text-align: left;">CUB200 <span class="math-inline">A_{Avg}</span></th>
<th style="text-align: left;">CIFAR-100 <span class="math-inline">A_{Base}</span></th>
<th style="text-align: left;">CIFAR-100 <span class="math-inline">A_{Last}</span></th>
<th style="text-align: left;">CIFAR-100 <span class="math-inline">A_{Avg}</span></th>
<th style="text-align: left;">miniImageNet <span class="math-inline">A_{Base}</span></th>
<th style="text-align: left;">miniImageNet <span class="math-inline">A_{Last}</span></th>
<th style="text-align: left;">miniImageNet <span class="math-inline">A_{Avg}</span></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Fine-Tuning + Proto <span class="math-inline">\psi</span></td>
<td style="text-align: left;">84.21±0.13</td>
<td style="text-align: left;">3.79±1.47</td>
<td style="text-align: left;">21.60±1.32</td>
<td style="text-align: left;">91.36±0.15</td>
<td style="text-align: left;">5.19±0.13</td>
<td style="text-align: left;">37.04±1.06</td>
<td style="text-align: left;">93.67±0.02</td>
<td style="text-align: left;">9.87±5.42</td>
<td style="text-align: left;">44.60±0.92</td>
</tr>
<tr>
<td style="text-align: left;">CEC</td>
<td style="text-align: left;">75.40±8.01</td>
<td style="text-align: left;">65.70±8.03</td>
<td style="text-align: left;">72.41±1.18</td>
<td style="text-align: left;">74.20±2.03</td>
<td style="text-align: left;">61.48±3.33</td>
<td style="text-align: left;">67.10±2.92</td>
<td style="text-align: left;">87.43±5.90</td>
<td style="text-align: left;">80.74±7.51</td>
<td style="text-align: left;">83.06±7.14</td>
</tr>
<tr>
<td style="text-align: left;">L2P</td>
<td style="text-align: left;">44.97±2.32</td>
<td style="text-align: left;">15.41±3.45</td>
<td style="text-align: left;">24.99±4.30</td>
<td style="text-align: left;">83.29±0.50</td>
<td style="text-align: left;">49.87±0.31</td>
<td style="text-align: left;">64.08±0.39</td>
<td style="text-align: left;">94.59±0.21</td>
<td style="text-align: left;">56.84±0.32</td>
<td style="text-align: left;">72.97±0.36</td>
</tr>
<tr>
<td style="text-align: left;">DualPrompt</td>
<td style="text-align: left;">53.37±1.83</td>
<td style="text-align: left;">23.25±2.02</td>
<td style="text-align: left;">36.30±2.39</td>
<td style="text-align: left;">85.11±0.29</td>
<td style="text-align: left;">50.93±0.21</td>
<td style="text-align: left;">65.45±0.27</td>
<td style="text-align: left;">95.05±0.20</td>
<td style="text-align: left;">57.14±0.11</td>
<td style="text-align: left;">73.31±0.15</td>
</tr>
<tr>
<td style="text-align: left;">NC-FSCIL</td>
<td style="text-align: left;">78.49±2.32</td>
<td style="text-align: left;">38.80±1.14</td>
<td style="text-align: left;">57.92±1.71</td>
<td style="text-align: left;">89.51±0.23</td>
<td style="text-align: left;">53.70±0.14</td>
<td style="text-align: left;">68.96±0.17</td>
<td style="text-align: left;">77.25±0.42</td>
<td style="text-align: left;">46.35±0.25</td>
<td style="text-align: left;">59.52±0.33</td>
</tr>
<tr>
<td style="text-align: left;">WaRP</td>
<td style="text-align: left;">67.74±5.57</td>
<td style="text-align: left;">49.36±6.56</td>
<td style="text-align: left;">55.85±6.06</td>
<td style="text-align: left;">86.20±1.46</td>
<td style="text-align: left;">65.48±1.87</td>
<td style="text-align: left;">74.55±1.67</td>
<td style="text-align: left;">83.30±1.06</td>
<td style="text-align: left;">67.97±1.28</td>
<td style="text-align: left;">74.13±1.08</td>
</tr>
<tr>
<td style="text-align: left;"><strong>PriViLege（本文方法）</strong></td>
<td style="text-align: left;"><strong>82.21±0.35</strong></td>
<td style="text-align: left;"><strong>75.08±0.52</strong></td>
<td style="text-align: left;"><strong>77.50±0.33</strong></td>
<td style="text-align: left;"><strong>90.88±0.20</strong></td>
<td style="text-align: left;"><strong>86.06±0.32</strong></td>
<td style="text-align: left;"><strong>88.08±0.20</strong></td>
<td style="text-align: left;"><strong>96.68±0.06</strong></td>
<td style="text-align: left;"><strong>94.10±0.13</strong></td>
<td style="text-align: left;"><strong>95.27±0.11</strong></td>
</tr>
</tbody>
</table>
<hr />
<p>从表 1 可以看出，<br />
我们的 PriViLege 方法在所有数据集上<strong>大幅超越了现有最先进方法</strong>。  </p>
<p>具体来说，在：</p>
<ul>
<li><strong>CUB200</strong>：相较于 CEC 方法，<span class="math-inline">A_{Last}</span> 提高了约 <strong>+9.38%</strong>，<span class="math-inline">A_{Avg}</span> 提高了约 <strong>+5.09%</strong>。</li>
<li><strong>CIFAR-100</strong>：相较于 WaRP 方法，<span class="math-inline">A_{Last}</span> 提高了约 <strong>+20.58%</strong>，<span class="math-inline">A_{Avg}</span> 提高了约 <strong>+13.53%</strong>。</li>
<li><strong>miniImageNet</strong>：相较于所有其他方法也有显著提升。</li>
</ul>
<hr />
<p>此外，与 L2P 和 DualPrompt 这些基于提示的方法相比，<br />
我们的 PriViLege 也表现出更强的性能，充分验证了：</p>
<ul>
<li>有效的领域知识学习，</li>
<li>以及增强迁移能力的重要性。  </li>
</ul>
<p>综合来看，PriViLege 所结合的 PKT、LED 和 LSKD 三大机制，<br />
使预训练大模型在 FSCIL 中取得了卓越的效果。</p>
<h3 id="43-消融实验ablation-study">4.3 消融实验（Ablation Study）<a class="anchor-link" href="#43-消融实验ablation-study" title="Permanent link">&para;</a></h3>
<p>为了验证我们方法中各个组件的有效性，<br />
我们在 CUB200 数据集上进行了消融实验。  </p>
<p>作为基线（Baseline），<br />
我们使用了<strong>直接微调预训练 ViT 并使用原型分类器 <span class="math-inline">\psi</span> 进行分类</strong>的方法。  </p>
<hr />
<p><strong>表 3. CUB200 上各组件的消融实验结果</strong></p>
<table>
<thead>
<tr>
<th style="text-align: left;">配置</th>
<th style="text-align: left;"><span class="math-inline">A_{Base}</span></th>
<th style="text-align: left;"><span class="math-inline">A_{Last}</span></th>
<th style="text-align: left;"><span class="math-inline">A_{Avg}</span></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Baseline（微调 ViT+ 原型分类器）</td>
<td style="text-align: left;">84.21±0.13</td>
<td style="text-align: left;">3.79±1.47</td>
<td style="text-align: left;">21.60±1.32</td>
</tr>
<tr>
<td style="text-align: left;">PKT</td>
<td style="text-align: left;">79.06±0.77</td>
<td style="text-align: left;">70.81±0.76</td>
<td style="text-align: left;">73.36±0.77</td>
</tr>
<tr>
<td style="text-align: left;">PKT + LED</td>
<td style="text-align: left;">80.31±0.54</td>
<td style="text-align: left;">72.70±0.45</td>
<td style="text-align: left;">75.04±0.40</td>
</tr>
<tr>
<td style="text-align: left;">PKT + LSKD</td>
<td style="text-align: left;">82.10±0.57</td>
<td style="text-align: left;">73.44±0.40</td>
<td style="text-align: left;">76.27±0.30</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Ours（完整方法）</strong></td>
<td style="text-align: left;"><strong>82.21±0.35</strong></td>
<td style="text-align: left;"><strong>75.08±0.52</strong></td>
<td style="text-align: left;"><strong>77.50±0.33</strong></td>
</tr>
</tbody>
</table>
<hr />
<p>从表 3 可以观察到：  </p>
<ul>
<li><strong>仅应用 PKT</strong> 时，相比基线，<span class="math-inline">A_{Last}</span> 和 <span class="math-inline">A_{Avg}</span> 都有了大幅提升，<br />
  说明我们提出的预训练知识调优（PKT）在基础阶段有效学习了领域知识，并成功迁移到了增量阶段。</li>
<li><strong>引入基于熵的发散损失（LED）</strong> 后，<span class="math-inline">A_{Last}</span> 提升了约 <strong>+1.89%</strong>，<span class="math-inline">A_{Avg}</span> 提升了约 <strong>+1.68%</strong>。</li>
<li><strong>引入语义知识蒸馏损失（LSKD）</strong> 后，<span class="math-inline">A_{Last}</span> 提升了约 <strong>+2.63%</strong>，<span class="math-inline">A_{Avg}</span> 提升了约 <strong>+2.91%</strong>。</li>
</ul>
<p>最终，<br />
结合了 PKT、LED 和 LSKD 的完整方法（PriViLege），<br />
在所有指标上取得了最佳性能，且提升幅度显著。</p>
<hr />
<h3 id="44-分析analysis">4.4 分析（Analysis）<a class="anchor-link" href="#44-分析analysis" title="Permanent link">&para;</a></h3>
<h4 id="441-在预训练-clip-网络上的-privilege">4.4.1 在预训练 CLIP 网络上的 PriViLege<a class="anchor-link" href="#441-在预训练-clip-网络上的-privilege" title="Permanent link">&para;</a></h4>
<p>为了验证我们方法 PriViLege 的适应性，<br />
我们将其应用于 <strong>CLIP</strong>【26】预训练模型，并进行了性能比较。  </p>
<p>在集成 PriViLege 到 CLIP 中时：</p>
<ul>
<li>仅训练了 CLIP 的视觉编码器；</li>
<li>使用 CLIP 的文本编码器提取语言嵌入，用于语义知识蒸馏损失（LSKD）。</li>
</ul>
<p>此外，我们还将现有方法 <strong>CEC</strong> 与 CLIP 的零样本（zero-shot）性能进行了比较。</p>
<hr />
<p><strong>实验结果：</strong></p>
<p>如图 3 所示，<br />
我们发现：</p>
<ul>
<li>现有方法 CEC 的性能<strong>低于</strong> CLIP 的零样本基准；</li>
<li>而我们提出的 PriViLege 方法则在零样本性能基础上<strong>显著提升</strong>。  </li>
</ul>
<p>这表明：</p>
<ul>
<li>PriViLege 所提出的基于熵的发散损失（LED）和语义知识蒸馏损失（LSKD）在 CLIP 框架中同样能带来显著的性能提升。  </li>
</ul>
<p>特别地，<br />
由于 CLIP 本身是以视觉和语言的对比学习方式进行预训练，<br />
因此：</p>
<ul>
<li>LED 有助于提升表示的判别能力，</li>
<li>LSKD 有效地注入了额外的语义知识，<br />
从而促进了少样本增量学习中的适应能力。</li>
</ul>
<hr />
<h4 id="442-pkt-的可微调层数探究">4.4.2 PKT 的可微调层数探究<a class="anchor-link" href="#442-pkt-的可微调层数探究" title="Permanent link">&para;</a></h4>
<p>我们进一步在 CUB200 上研究了 PKT 中微调层数对性能的影响。  </p>
<p><strong>表 4. 不同微调层数下的性能比较（CUB200）</strong></p>
<table>
<thead>
<tr>
<th style="text-align: left;">微调层数（L）</th>
<th style="text-align: left;"><span class="math-inline">A_{Base}</span></th>
<th style="text-align: left;"><span class="math-inline">A_{Last}</span></th>
<th style="text-align: left;"><span class="math-inline">A_{Avg}</span></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">0 层</td>
<td style="text-align: left;">76.07±0.56</td>
<td style="text-align: left;">60.19±1.11</td>
<td style="text-align: left;">67.08±0.71</td>
</tr>
<tr>
<td style="text-align: left;">2 层</td>
<td style="text-align: left;"><strong>79.06±0.77</strong></td>
<td style="text-align: left;"><strong>70.81±0.76</strong></td>
<td style="text-align: left;"><strong>73.36±0.77</strong></td>
</tr>
<tr>
<td style="text-align: left;">5 层</td>
<td style="text-align: left;">78.42±0.84</td>
<td style="text-align: left;">68.52±0.84</td>
<td style="text-align: left;">71.99±0.84</td>
</tr>
<tr>
<td style="text-align: left;">7 层</td>
<td style="text-align: left;">76.96±0.74</td>
<td style="text-align: left;">63.15±2.73</td>
<td style="text-align: left;">68.06±1.54</td>
</tr>
<tr>
<td style="text-align: left;">10 层</td>
<td style="text-align: left;">74.95±0.78</td>
<td style="text-align: left;">57.78±2.30</td>
<td style="text-align: left;">64.19±1.59</td>
</tr>
<tr>
<td style="text-align: left;">12 层</td>
<td style="text-align: left;">73.62±2.72</td>
<td style="text-align: left;">56.02±1.47</td>
<td style="text-align: left;">62.71±2.14</td>
</tr>
</tbody>
</table>
<hr />
<p><strong>分析结果：</strong></p>
<ul>
<li>微调前 2 层（L=2）时，在所有指标上性能最佳。</li>
<li>如果<strong>不微调任何层（L=0）</strong>，由于缺乏学习能力，基础阶段和增量阶段的性能都较低。</li>
<li>如果<strong>微调层数太多</strong>（如 10 层或 12 层），则由于过度破坏预训练知识，导致 <span class="math-inline">A_{Base}</span> 和 <span class="math-inline">A_{Last}</span> 都明显下降。</li>
</ul>
<p>这说明：</p>
<blockquote>
<p>当可微调的层数适中时（如 2 层），可以在保留预训练知识的同时，有效地捕捉领域特定知识。<br />
若调节不当，随机初始化提示反而会妨碍表示学习。</p>
</blockquote>
<hr />
<h4 id="443-基于熵的发散损失led的有效性验证">4.4.3 基于熵的发散损失（LED）的有效性验证<a class="anchor-link" href="#443-基于熵的发散损失led的有效性验证" title="Permanent link">&para;</a></h4>
<p>为了进一步验证 LED 的有效性，<br />
我们进行了特征空间可视化对比实验。  </p>
<p><strong>实验设定：</strong></p>
<ul>
<li>对比不使用 LED（w/o LED）和使用 LED（w/ LED）两种情况下，特征分布的变化。</li>
</ul>
<p><strong>可视化结果：</strong></p>
<ul>
<li>图 4(a)：不使用 LED，部分类别（如 “Herring Gull” 和 “Ivory Gull”）的决策边界模糊。</li>
<li>图 4(b)：使用 LED 后，类别边界变得更清晰，分类效果明显改善。</li>
</ul>
<hr />
<p><strong>结论：</strong><br />
应用 LED 能够提升视觉标记的判别性，<br />
从而在基础阶段有效捕获领域知识，<br />
并为增量阶段学习打下坚实基础。</p>
<hr />
<h4 id="444-语义知识蒸馏lskd的效果分析">4.4.4 语义知识蒸馏（LSKD）的效果分析<a class="anchor-link" href="#444-语义知识蒸馏lskd的效果分析" title="Permanent link">&para;</a></h4>
<p>我们进一步分析了 LSKD 对各类别性能的具体影响，<br />
使用指标 <span class="math-inline">\Delta_{diff}</span> 表示<strong>使用和不使用 LSKD 时每类准确率的差异</strong>。  </p>
<p><strong>实验结果：</strong></p>
<ul>
<li>图 5 展示了前 10 个提升最大的类别和后 10 个下降最大的类别。  </li>
</ul>
<p>在前 10 个提升最多的类别中，性能提升约 <strong>+19.90%</strong>；<br />
而后 10 个下降最多的类别中，下降约 <strong>-6.76%</strong>。</p>
<p>特别值得注意的是：</p>
<ul>
<li>提升最多的类别名称中包含了<strong>颜色（red、green、yellow、white）</strong>或<strong>形状特征（headed、tailed、palm、billed、pelagic、fish）</strong>等描述词。  </li>
</ul>
<p>这表明：</p>
<blockquote>
<p>当类别名称中包含丰富的语义特征信息时，LSKD 能更有效地帮助模型学习，尤其在细粒度数据集（如 CUB200）中效果显著。</p>
</blockquote>
<h2 id="5-结论">5. 结论<a class="anchor-link" href="#5-结论" title="Permanent link">&para;</a></h2>
<p>在本研究中，<br />
我们提出了一种基于大型预训练视觉与语言 Transformer 的新型少样本类增量学习（Few-Shot Class Incremental Learning，FSCIL）方法，命名为 <strong>PriViLege</strong>。  </p>
<p>我们针对 Vision Transformer（ViT）中普遍存在的<strong>灾难性遗忘</strong>和<strong>过拟合</strong>问题，<br />
提出了以下创新技术：</p>
<ul>
<li><strong>预训练知识调优（Pre-trained Knowledge Tuning，PKT）</strong>，  </li>
<li><strong>基于熵的发散损失（entropy-based divergence loss，LED）</strong>，  </li>
<li><strong>语义知识蒸馏损失（semantic knowledge distillation loss，LSKD）</strong>。  </li>
</ul>
<p>通过这些方法，<br />
我们在多个 FSCIL 基准数据集上实现了<strong>显著的性能提升</strong>。  </p>
<p>此外，<br />
我们还验证了 PriViLege 框架不仅适用于 ViT，<br />
同样也可以有效应用到如 CLIP 这样的不同预训练模型上，进一步证明了其广泛适用性和鲁棒性。</p>
<p>我们相信，<br />
PriViLege 为未来在 FSCIL 领域利用大型预训练模型的研究方向开辟了新的可能性。</p>
<p>未来的工作方向包括：  </p>
<blockquote>
<p>探索如何在<strong>没有基础阶段</strong>的更加困难的 FSCIL 场景下，<br />
有效应用大型预训练模型进行增量学习。</p>
</blockquote>
<hr />
                </div>

                <!-- Comments Section (Giscus) -->
                <section class="comments-section">
                    <h3><i class="fas fa-comments"></i> 评论</h3>
                    <script src="https://giscus.app/client.js"
                        data-repo="Geeks-Z/Geeks-Z.github.io"
                        data-repo-id=""
                        data-category="Announcements"
                        data-category-id=""
                        data-mapping="pathname"
                        data-strict="0"
                        data-reactions-enabled="1"
                        data-emit-metadata="0"
                        data-input-position="bottom"
                        data-theme="preferred_color_scheme"
                        data-lang="zh-CN"
                        crossorigin="anonymous"
                        async>
                    </script>
                </section>
            </article>

            <footer class="post-footer">
                <p>© 2025 Hongwei Zhao. Built with ❤️</p>
            </footer>
        </main>
    </div>

    <!-- Theme Toggle Button -->
    <button class="theme-toggle" id="themeToggle" aria-label="切换主题">
        <i class="fas fa-moon"></i>
    </button>

    <script>
        // Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        const html = document.documentElement;
        const icon = themeToggle.querySelector('i');
        const hljsDark = document.getElementById('hljs-theme-dark');
        const hljsLight = document.getElementById('hljs-theme-light');
        
        // Check saved theme or system preference
        const savedTheme = localStorage.getItem('theme');
        const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
        
        if (savedTheme === 'dark' || (!savedTheme && prefersDark)) {
            html.setAttribute('data-theme', 'dark');
            icon.className = 'fas fa-sun';
            hljsDark.disabled = false;
            hljsLight.disabled = true;
        }
        
        themeToggle.addEventListener('click', () => {
            const isDark = html.getAttribute('data-theme') === 'dark';
            if (isDark) {
                html.removeAttribute('data-theme');
                icon.className = 'fas fa-moon';
                localStorage.setItem('theme', 'light');
                hljsDark.disabled = true;
                hljsLight.disabled = false;
            } else {
                html.setAttribute('data-theme', 'dark');
                icon.className = 'fas fa-sun';
                localStorage.setItem('theme', 'dark');
                hljsDark.disabled = false;
                hljsLight.disabled = true;
            }
            
            // Update Giscus theme
            const giscusFrame = document.querySelector('iframe.giscus-frame');
            if (giscusFrame) {
                giscusFrame.contentWindow.postMessage({
                    giscus: {
                        setConfig: {
                            theme: isDark ? 'light' : 'dark'
                        }
                    }
                }, 'https://giscus.app');
            }
        });
        
        // Highlight.js
        document.addEventListener('DOMContentLoaded', () => {
            hljs.highlightAll();
        });
        
        // KaTeX auto-render
        document.addEventListener('DOMContentLoaded', () => {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
        
        // TOC active state
        const tocLinks = document.querySelectorAll('.toc-container a');
        const headings = document.querySelectorAll('.post-content h2, .post-content h3, .post-content h4');
        
        function updateTocActive() {
            let current = '';
            headings.forEach(heading => {
                const rect = heading.getBoundingClientRect();
                if (rect.top <= 100) {
                    current = heading.getAttribute('id');
                }
            });
            
            tocLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === '#' + current) {
                    link.classList.add('active');
                }
            });
        }
        
        window.addEventListener('scroll', updateTocActive);
        updateTocActive();
    </script>
</body>
</html>
