<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Untitled</title>
    <meta name="description" content="Untitled - Hongwei Zhao's Blog">
    <meta name="author" content="Hongwei Zhao">
    
    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <!-- Code Highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css" id="hljs-theme-dark">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css" id="hljs-theme-light" disabled>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    
    <style>
        :root {
            /* Light Theme */
            --primary-color: #2980b9;
            --primary-hover: #1a5276;
            --link-color: #c0392b;
            --text-color: #333;
            --text-light: #666;
            --text-muted: #999;
            --bg-color: #fff;
            --bg-secondary: #f8f9fa;
            --bg-code: #f5f5f5;
            --border-color: #e5e7eb;
            --shadow: 0 1px 3px rgba(0,0,0,0.1);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.1);
        }

        [data-theme="dark"] {
            --primary-color: #5dade2;
            --primary-hover: #85c1e9;
            --link-color: #e74c3c;
            --text-color: #e5e7eb;
            --text-light: #9ca3af;
            --text-muted: #6b7280;
            --bg-color: #1a1a2e;
            --bg-secondary: #16213e;
            --bg-code: #0f0f23;
            --border-color: #374151;
            --shadow: 0 1px 3px rgba(0,0,0,0.3);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.4);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 16px;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            transition: background-color 0.3s, color 0.3s;
        }

        a {
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--primary-hover);
            text-decoration: underline;
        }

        /* Layout */
        .page-wrapper {
            display: flex;
            max-width: 1400px;
            margin: 0 auto;
            padding: 2rem;
            gap: 3rem;
        }

        /* Sidebar TOC */
        .toc-sidebar {
            width: 260px;
            flex-shrink: 0;
            position: sticky;
            top: 2rem;
            height: fit-content;
            max-height: calc(100vh - 4rem);
            overflow-y: auto;
        }

        .toc-container {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 1.5rem;
            border: 1px solid var(--border-color);
        }

        .toc-container h3 {
            font-size: 0.85rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-muted);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .toc-container ul {
            list-style: none;
        }

        .toc-container li {
            margin-bottom: 0.5rem;
        }

        .toc-container a {
            font-size: 0.9rem;
            color: var(--text-light);
            display: block;
            padding: 0.25rem 0;
            border-left: 2px solid transparent;
            padding-left: 0.75rem;
            transition: all 0.2s;
        }

        .toc-container a:hover,
        .toc-container a.active {
            color: var(--primary-color);
            border-left-color: var(--primary-color);
            text-decoration: none;
        }

        .toc-container ul ul {
            margin-left: 1rem;
        }

        /* Main Content */
        .main-content {
            flex: 1;
            min-width: 0;
            max-width: 800px;
        }

        /* Header */
        .post-header {
            margin-bottom: 2rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
            margin-bottom: 1.5rem;
        }

        .back-link:hover {
            color: var(--primary-color);
            text-decoration: none;
        }

        .post-header h1 {
            font-size: 2.25rem;
            font-weight: 700;
            line-height: 1.3;
            margin-bottom: 1rem;
            color: var(--text-color);
        }

        .post-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
        }

        .post-meta span {
            display: flex;
            align-items: center;
            gap: 0.4rem;
        }

        .post-meta i {
            color: var(--text-muted);
        }

        .tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 1rem;
        }

        .tag {
            display: inline-block;
            font-size: 0.8rem;
            background: var(--bg-secondary);
            color: var(--text-light);
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            border: 1px solid var(--border-color);
            transition: all 0.2s;
        }

        .tag:hover {
            background: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }

        /* Article Content */
        .post-content {
            font-size: 1rem;
            line-height: 1.9;
        }

        .post-content h1,
        .post-content h2,
        .post-content h3,
        .post-content h4,
        .post-content h5,
        .post-content h6 {
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-weight: 600;
            line-height: 1.4;
            color: var(--text-color);
        }

        .post-content h1 { font-size: 1.875rem; }
        .post-content h2 { 
            font-size: 1.5rem; 
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }
        .post-content h3 { font-size: 1.25rem; }
        .post-content h4 { font-size: 1.125rem; }

        .post-content p {
            margin-bottom: 1.25rem;
        }

        .post-content ul,
        .post-content ol {
            margin: 1.25rem 0;
            padding-left: 1.5rem;
        }

        .post-content li {
            margin-bottom: 0.5rem;
        }

        .post-content blockquote {
            margin: 1.5rem 0;
            padding: 1rem 1.5rem;
            background: var(--bg-secondary);
            border-left: 4px solid var(--primary-color);
            border-radius: 0 8px 8px 0;
            color: var(--text-light);
            font-style: italic;
        }

        .post-content blockquote p:last-child {
            margin-bottom: 0;
        }

        /* Code */
        .post-content code {
            font-family: 'JetBrains Mono', 'Fira Code', Consolas, monospace;
            font-size: 0.9em;
            background: var(--bg-code);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            color: var(--link-color);
        }

        .post-content pre {
            margin: 1.5rem 0;
            padding: 1.25rem;
            background: var(--bg-code);
            border-radius: 10px;
            overflow-x: auto;
            border: 1px solid var(--border-color);
        }

        .post-content pre code {
            background: none;
            padding: 0;
            color: inherit;
            font-size: 0.875rem;
            line-height: 1.6;
        }

        /* Images */
        .post-content img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            margin: 1.5rem 0;
            box-shadow: var(--shadow-lg);
        }

        /* Tables */
        .post-content table {
            width: 100%;
            margin: 1.5rem 0;
            border-collapse: collapse;
            font-size: 0.95rem;
        }

        .post-content th,
        .post-content td {
            padding: 0.75rem 1rem;
            border: 1px solid var(--border-color);
            text-align: left;
        }

        .post-content th {
            background: var(--bg-secondary);
            font-weight: 600;
        }

        .post-content tr:nth-child(even) {
            background: var(--bg-secondary);
        }

        /* Math */
        .math-display {
            margin: 1.5rem 0;
            overflow-x: auto;
            padding: 1rem;
            background: var(--bg-secondary);
            border-radius: 8px;
        }

        /* Anchor links */
        .anchor-link {
            opacity: 0;
            margin-left: 0.5rem;
            color: var(--text-muted);
            font-weight: 400;
            transition: opacity 0.2s;
        }

        .post-content h2:hover .anchor-link,
        .post-content h3:hover .anchor-link,
        .post-content h4:hover .anchor-link {
            opacity: 1;
        }

        /* Theme Toggle */
        .theme-toggle {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            background: var(--primary-color);
            color: white;
            border: none;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.25rem;
            box-shadow: var(--shadow-lg);
            transition: transform 0.2s, background 0.2s;
            z-index: 1000;
        }

        .theme-toggle:hover {
            transform: scale(1.1);
            background: var(--primary-hover);
        }

        /* Comments Section */
        .comments-section {
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-color);
        }

        .comments-section h3 {
            font-size: 1.25rem;
            margin-bottom: 1.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        /* Footer */
        .post-footer {
            margin-top: 3rem;
            padding-top: 1.5rem;
            border-top: 1px solid var(--border-color);
            text-align: center;
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        /* Responsive */
        @media (max-width: 1024px) {
            .toc-sidebar {
                display: none;
            }
            
            .page-wrapper {
                padding: 1.5rem;
            }
        }

        @media (max-width: 768px) {
            .post-header h1 {
                font-size: 1.75rem;
            }
            
            .post-meta {
                flex-direction: column;
                gap: 0.5rem;
            }
            
            .theme-toggle {
                bottom: 1rem;
                right: 1rem;
                width: 44px;
                height: 44px;
            }
        }

        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-secondary);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--border-color);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--text-muted);
        }
    </style>
</head>
<body>
    <div class="page-wrapper">
        <!-- TOC Sidebar -->
        <aside class="toc-sidebar">
            <div class="toc-container">
                <h3><i class="fas fa-list"></i> 目录</h3>
                <div class="toc">
<ul>
<li><a href="#chatgpt全文翻译-arrow_down">ChatGPT全文翻译 :arrow_down:</a></li>
<li><a href="#0-摘要">0. 摘要</a></li>
<li><a href="#1-引言">1. 引言</a></li>
<li><a href="#2-相关工作">2. 相关工作</a><ul>
<li><a href="#21-类别增量学习">2.1 类别增量学习</a></li>
<li><a href="#22-预训练模型在-cil-中的应用">2.2 预训练模型在 CIL 中的应用</a></li>
<li><a href="#23-视觉语言模型">2.3 视觉语言模型</a></li>
</ul>
</li>
<li><a href="#3-方法">3. 方法</a><ul>
<li><a href="#31-预备知识">3.1 预备知识</a></li>
<li><a href="#32-用于类别增量学习的生成多模态模型">3.2 用于类别增量学习的生成多模态模型</a></li>
<li><a href="#33-优化和推理">3.3 优化和推理</a></li>
</ul>
</li>
<li><a href="#4-实验">4. 实验</a><ul>
<li><a href="#41-实验设置">4.1 实验设置</a></li>
<li><a href="#42-传统-cil-的实验">4.2 传统 CIL 的实验</a></li>
<li><a href="#43-少样本-cil-的实验">4.3 少样本 CIL 的实验</a></li>
<li><a href="#44-可视化">4.4 可视化</a></li>
</ul>
</li>
<li><a href="#5-结论">5. 结论</a><ul>
<li><a href="#6-局限性">6. 局限性</a></li>
<li><a href="#7-更广泛的影响">7. 更广泛的影响</a></li>
</ul>
</li>
</ul>
</div>

            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <article>
                <header class="post-header">
                    <a href="../../../index.html" class="back-link">
                        <i class="fas fa-arrow-left"></i> 返回博客列表
                    </a>
                    <h1>Untitled</h1>
                    <div class="post-meta">
                        <span><i class="fas fa-calendar-alt"></i> 2026-01-28</span>
                        <span><i class="fas fa-folder"></i> AINotes/43.多模态增量学习MMCL</span>
                        <span><i class="fas fa-user"></i> Hongwei Zhao</span>
                    </div>
                    <div class="tags">
                        
                    </div>
                </header>

                <div class="post-content">
                    <blockquote>
<p><a href="https://arxiv.org/abs/2403.18383">Paper</a> | <a href="https://github.com/DoubleClass/GMM">Code</a> | CVPR 2024</p>
</blockquote>
<h2 id="chatgpt全文翻译-arrow_down">ChatGPT全文翻译 :arrow_down:<a class="anchor-link" href="#chatgpt全文翻译-arrow_down" title="Permanent link">&para;</a></h2>
<h2 id="0-摘要">0. 摘要<a class="anchor-link" href="#0-摘要" title="Permanent link">&para;</a></h2>
<p>在类别增量学习（CIL）场景中，由于分类器对当前任务的偏倚而导致的灾难性遗忘现象一直是一个重大挑战。这主要是由判别模型的特性引起的。随着生成多模态模型的流行，我们探索用生成模型替代判别模型来应对 CIL。然而，从判别模型过渡到生成模型需要解决两个关键挑战。主要挑战在于将生成的文本信息转化为不同类别的分类任务。此外，还需要在生成框架内制定 CIL 任务。为此，我们提出了一种新颖的用于类别增量学习的生成多模态模型（GMM）框架。我们的方法直接使用经过调整的生成模型为图像生成标签。在获得详细文本后，我们使用文本编码器提取文本特征，并使用特征匹配来确定与分类预测最相似的标签。在传统的 CIL 设置中，我们在长序列任务场景中取得了显著的更好结果。在少样本 CIL 设置中，我们的准确性比当前所有最先进的方法至少提高了 14%，并且显著减少了遗忘。我们的代码可以在 <a href="https://github.com/DoubleClass/GMM">https://github.com/DoubleClass/GMM</a> 获得。</p>
<h2 id="1-引言">1. 引言<a class="anchor-link" href="#1-引言" title="Permanent link">&para;</a></h2>
<p>深度神经网络 [19, 33, 56] 在许多应用中取得了显著进展，这主要归功于其背后的大量数据和计算资源。然而，这些成就主要依赖于能够同时访问所有所需数据，以便在各种任务上进行训练。在增量获取数据的情况下，这些网络往往会遇到灾难性遗忘的挑战 [43]。因此，能够无缝整合新知识并保留先前获得的知识，成为未来人工智能系统中高度期望的特性。持续学习 [45, 67, 77, 84] 旨在推动神经网络朝着这一目标演进。</p>
<p>许多研究已经深入探讨了持续学习，将其方法归纳为三大类 [14]：基于复习的方法、基于架构的方法、以及基于正则化的方法。此外，混合方法越来越受欢迎，因为它们结合了来自不同视角的见解。在这一研究领域中，三种主要场景 [66] 受到了广泛关注，其中类别增量学习（CIL）[41] 是最具挑战性的设置之一。在我们的工作中，我们专注于类别增量学习，其中每个任务包含一组独特的类别，主要挑战在于使网络能够识别新类别，同时不遗忘以前遇到的类别。</p>
<p>现有的大多数类别增量学习研究都专注于从头训练模型，仅依赖于当前任务的数据 [7, 15, 16, 24, 26, 29, 48, 74, 76, 89]。相比之下，人类则是在较长时间内积累知识，并依赖于大量的先验世界知识。因此，预训练模型在 CIL 中的应用越来越受到关注 [55, 71, 72, 81, 86]，这些模型利用从广泛存在的数据集中获取的知识来处理当前的任务。例如，基于提示的方法 [55, 71, 72] 使用提示调整来总结特定任务的知识，从而利用先前的预训练知识。SLCA [81] 和 ADAM [85] 则仅微调预训练模型，以将现有知识应用于当前任务的目标。</p>
<p>为了应对图像分类的下游任务，预训练模型通常源自判别任务，如在 ImageNet-21K [49] 上进行的有监督学习，或者源自自监督学习的努力 [6, 9, 10, 22]。然而，在我们的研究中，我们探索了使用生成多模态模型来处理图像分类任务。生成模型如 GPT4 [44] 和 LLaVa [32] 在最近几年得到了广泛关注，因为它们能够生成高度信息化的输入图像描述。一方面，它可以利用文本与图像之间丰富的语义对应关系，另一方面，与判别模型不同，它不需要随着每个新任务扩展分类器。</p>
<p>尽管如此，将预训练生成模型的知识应用于下游类别增量学习（CIL）任务并非易事。主要挑战在于将生成的文本信息转化为不同类别的分类任务。此外，在生成框架内制定 CIL 任务也是另一大挑战。Shao 等 [52] 提出了 VAG 系统，它将 CIL 表述为一个持续标签生成问题，保留了语言模型学习新类别的能力。然而，它仅适用于自然语言处理（NLP）领域，这本质上是适合大型语言模型（LLM）的。就我们所知，我们是首个将这种生成方法应用于图像分类领域的增量学习研究。</p>
<p>在这项工作中，我们提出了用于类别增量学习的生成多模态模型（GMM）。如图 1(a)所示，传统的判别方法使用网络骨干提取图像特征，然后将其传递给分类器，以获取图像属于每个标签的概率，具有最高概率的标签就是判别模型的输出。而在图 1(b)中，我们采用了生成方法，直接为给定的图像生成描述性句子，然后与实际的标签文本进行比较。最相似的标签成为我们生成模型的预测结果。这种方法使我们能够利用生成多模态模型中的丰富预训练知识，同时避免使用扩展分类头，从而减轻了模型对当前任务的偏倚，并减少了灾难性遗忘。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20240810230218.png" style="zoom: 60%;" /></div>

<h2 id="2-相关工作">2. 相关工作<a class="anchor-link" href="#2-相关工作" title="Permanent link">&para;</a></h2>
<h3 id="21-类别增量学习">2.1 类别增量学习<a class="anchor-link" href="#21-类别增量学习" title="Permanent link">&para;</a></h3>
<p>在类别增量学习（Class-Incremental Learning, CIL）中，任务是按顺序到达的，每个类别是特定于某一任务且没有重叠的。目标是从新类别中获取知识，同时保留先前遇到的类别的信息。在 CIL [14] 中有三大主要分支，包括基于复习、基于架构、以及基于正则化的方法。基于复习的方法 [1, 7, 48, 75] 存储从旧类别中派生出来的一小部分数据，以表示来自以前任务的知识。这些示例数据可以是原始数据 [48]，生成数据 [18, 54] 或隐藏特征 [20]。基于架构的方法则侧重于修改网络架构以减轻遗忘问题。这些方法包括学习冗余网络架构 [17, 47]，学习不同的专家网络 [3, 50] 或参数 [38, 40, 51] 来适应每个任务，动态扩展网络参数以积累增量知识 [76]。基于正则化的方法则通过引入额外的正则化项来限制网络在适应新任务时的更新。在这种情况下，EWC [26]，SDC [78] 和 Rotated-EWC [35] 希望旧任务重要的参数不要过度更新。此外，从网络输出一致性的角度来看，许多研究 [25, 30, 36, 61, 80] 结合蒸馏技术以防止遗忘。</p>
<h3 id="22-预训练模型在-cil-中的应用">2.2 预训练模型在 CIL 中的应用<a class="anchor-link" href="#22-预训练模型在-cil-中的应用" title="Permanent link">&para;</a></h3>
<p>许多方法 [63, 72, 73] 已经表明，预训练模型对于持续学习是有效的。一个主要分支是训练一组提示（prompts）来保留之前的知识 [55, 68, 71, 72]。在前向过程中将一部分选择的提示输入模型，以提示模型过去的知识。此外，像 SLCA [81] 和 ADAM [85] 等方法则通过微调预训练模型来适应当前任务，取得了令人印象深刻的成果，并减少了遗忘。Continual-CLIP [63] 证明了 CLIP [46] 模型能够在无需额外训练的情况下执行持续学习。这突显了多模态预训练模型在持续学习领域的巨大潜力。受到这一启发，许多方法 [37, 86] 使用 CLIP 作为骨干来利用多模态信息。然而，如果不直接使用分类器，这些方法需要利用扩展的文本特征来计算与图像特征之间的距离以进行分类。这会加剧模型对当前数据的偏倚，进而导致遗忘先前获得的知识。为了避免这种偏倚，我们使用生成模型直接生成预测文本。固定的文本解码器将充当分类器，大大减轻了偏倚问题。</p>
<h3 id="23-视觉语言模型">2.3 视觉语言模型<a class="anchor-link" href="#23-视觉语言模型" title="Permanent link">&para;</a></h3>
<p>近年来，视觉语言多模态模型在各种下游任务中取得了显著进展，并取得了令人瞩目的成果 [5, 28, 34, 70]。传统的视觉语言模型使用不同类型的编码器来提取视觉和语言模型的信息，包括单流（single-stream）[57]，双流（dual-stream）[39] 和融合编码器（fusion encoders）[60]。视觉语言模型的一个关键方面是多模态特征的对齐。比如，CLIP [46] 使用各自的编码器分别提取图像和文本特征，并通过对比损失确保特征空间中正向图像文本对的对齐。VisualGPT [8] 和 Frozen [65] 则利用预训练模型作为视觉语言任务的编码器。从那时起，预训练模型在视觉语言任务中的使用变得越来越流行。例如，Flamingo [2] 和 BLIP-2 [27] 使用门控交叉注意力（gated cross-attention）和 Q-Former 分别对齐预训练的图像和文本编码器。此外，LLaVA [32] 和 MiniGPT-4 [88] 利用更强大的大型语言模型（LLM）[13, 64] 作为文本编码器，而只训练一个投影层来进行对齐。随着 LLM 的日益普及，越来越多的研究 [4, 69, 87] 探索了多模态 LLM 在视觉语言任务中的潜力。</p>
<h2 id="3-方法">3. 方法<a class="anchor-link" href="#3-方法" title="Permanent link">&para;</a></h2>
<p>在本节中，我们介绍类别增量学习和生成多模态模型的基础知识。然后，我们展示了如何利用生成模型进行类别增量学习（CIL）以及相应的学习过程。</p>
<h3 id="31-预备知识">3.1 预备知识<a class="anchor-link" href="#31-预备知识" title="Permanent link">&para;</a></h3>
<p><strong>类别增量学习</strong>。给定 <span class="math-inline">N</span> 个任务 <span class="math-inline">T = {T_1, T_2, ..., T_N}</span>，类别增量学习的目标是以顺序的方式学习每个任务 <span class="math-inline">T_t</span> 及其相关数据 <span class="math-inline">{X_t, Y_t}</span>。对于每个任务，它包含样本 <span class="math-inline">{x_i, y_i}, i = 1, ..., n_t</span>，其中 <span class="math-inline">x_i</span> 是图像，<span class="math-inline">y_i</span> 是对应的 one-hot 标签。通常，<span class="math-inline">X_i \cap X_j = \varnothing</span>，当 <span class="math-inline">i \neq j</span> 时。在推理时，模型将在所有已见任务上进行测试，且不提供任务 ID。在某些情况下，设置了固定的内存存储来保留一些先前任务的样本以防止遗忘。</p>
<p>通常，CIL 模型由特征提取器和分类器头部 <span class="math-inline">F = {f_\theta, H_\varphi}</span> 组成，其参数化为 <span class="math-inline">{\theta, \varphi}</span>。在传统的类别增量学习中，<span class="math-inline">\theta</span> 通常是一个经过修改的 ResNet [21]，其所有参数均可调。在预训练或基于提示的方法中，<span class="math-inline">\theta</span> 代表更少的可训练参数，如线性适配器或几个提示。<span class="math-inline">\varphi</span> 是一个线性分类头部，将图像特征投射到概率预测中，并且为了为新类别做出预测，分类头部必须为每个新任务进行扩展。传统的交叉熵损失通常用于更新 <span class="math-inline">\theta</span> 和 <span class="math-inline">\varphi</span>，其对于任务 <span class="math-inline">t</span> 的表达式如下：</p>
<p><div class="math-display"><br />
    L_{CE}(X_t, Y_t; \theta, \varphi) = - \frac{1}{n_t} \sum_{i=1}^{n_t} y_i \cdot \log H(f(x_i; \theta); \varphi)<br />
</div></p>
<p>在持续学习过程中，由于以前任务的旧样本不足或稀缺，参数 <span class="math-inline">\varphi</span> 很容易偏向当前任务的数据，导致先前获得的知识遗忘，并降低整体性能。</p>
<p><strong>生成多模态模型（GMM）</strong>。多模态模型通过结合视觉和文本信息，在生成详细图像描述方面表现出了卓越的性能。特别是，GPT-4 [44] 被认为是一个先进的模型，能够生成全面的图像描述，并提供对所描绘内容的解释。此外，MiniGPT-4 [88] 提出了一个两阶段的微调过程，使 LLaMa [64] 能够识别图像，并基于图像内容进行进一步的对话。</p>
<p>如图 2 所示，这些模型由一个编码器 <span class="math-inline">f_{\text{enc}}</span> 组成，用于生成内容嵌入，包括图像嵌入和文本嵌入，随后这些嵌入将作为自回归解码器 <span class="math-inline">f_{\text{dec}}</span> 的输入，生成图像描述。输入图像 <span class="math-inline">x_i</span> 被编码为图像嵌入 <span class="math-inline">e_i</span>，问题嵌入 <span class="math-inline">q</span> 与 <span class="math-inline">q_1, ..., q_l</span> 可以与图像嵌入一起连接，生成答案嵌入 <span class="math-inline">s</span> 与 <span class="math-inline">s_1, ..., s_m</span>。输出的标记是根据先前生成的标记一个接一个生成的。例如，<span class="math-inline">s_m</span> 是在所有先前的 <span class="math-inline">m-1</span> 个标记的条件下生成的（见图 2 中的解码器）。</p>
<h3 id="32-用于类别增量学习的生成多模态模型">3.2 用于类别增量学习的生成多模态模型<a class="anchor-link" href="#32-用于类别增量学习的生成多模态模型" title="Permanent link">&para;</a></h3>
<p>我们依照 MiniGPT-4 的基础设置，结合一个冻结的图像编码器 <span class="math-inline">f_{\text{enc}}</span>，后跟一个可训练的投影层来适应下游任务，如图 2 所示。我们的主要创新在于直接利用生成模型生成文本，这些文本随后可作为判别分类的基础。然而，需要解决两个主要挑战。首先，生成多模态模型用于分类时，生成的文本可能与类别名称有显著不同，这需要处理。其次，我们必须为我们的分类基准设计一种机制，使其与生成多模态模型的学习过程保持一致。我们将在下文中介绍这两个方面。</p>
<p><strong>将 GMM 用于分类</strong>。我们使用距离度量来弥合生成模型和判别模型之间的差距。在训练过程中，我们使用真实标签的文本来鼓励模型用简洁准确的句子预测图像的标签，格式为 “This is a photo of [CLS]”，避免对图像中所有内容的详细描述。在测试过程中，模型遵循此格式为给定图像输出类别文本。我们提取 “[CLS]” 中的内容，然后使用 CLIP [46] 文本编码器 <span class="math-inline">f_{\text{text}}</span> 获得其文本特征，并计算与所有已见类别文本特征的距离。然后，最相似的类别被视为生成模型的最终预测结果。</p>
<p><strong>转换 CIL 基准以适应 GMM</strong>。CIL 通常在 ImageNet、CIFAR-100 和 ImageNet-R 数据集上进行评估。这些数据集通常由图像及其对应的 one-hot 标签 <span class="math-inline">{X_t, Y_t}</span> 组成。以 CIFAR100 数据集为例，我们将每张图像与一个句子配对，形成图像 - 文本对的格式 <span class="math-inline">{X_t, S_t}</span>，模板为：“This is a photo of [CLS]”，其中 “[CLS]” 是该类别的标签名称，如 apple、dog 等。接下来，我们根据不同的设置将 100 个类别划分为各种任务，并依次输入模型。完成任务 <span class="math-inline">T</span> 的训练后，模型应该能够分类从任务 0 到任务 <span class="math-inline">T</span> 所涵盖的所有类别。请注意，仅线性投影层会被进一步调整以适应。</p>
<h3 id="33-优化和推理">3.3 优化和推理<a class="anchor-link" href="#33-优化和推理" title="Permanent link">&para;</a></h3>
<p><strong>优化</strong>。对于每个任务 <span class="math-inline">t</span>，我们获取当前任务的图像 - 文本对 <span class="math-inline">{X_t, S_t}</span>，其中 <span class="math-inline">S_t</span> 包含每张图像的对应句子。在训练过程中，我们首先利用分词器对问题和答案进行分词，并获取嵌入。我们利用预训练的编码器 <span class="math-inline">f_{\text{enc}}</span> 和投影层获取输入图像的对应特征：</p>
<p><div class="math-display"><br />
    e_i = f_{\text{enc}}(x_i; \theta_{\text{enc}})<br />
</div></p>
<p>然后，将问题嵌入和问题的真实嵌入（如 “This is a photo of [CLS]”）与图像嵌入一起连接。LLM 解码器 <span class="math-inline">f_{\text{dec}}</span> 的最终输入为：</p>
<p><div class="math-display"><br />
    \hat{e}_i = \text{CONCATE}(\text{bos}, e_i, q, s, \text{eos})<br />
</div></p>
<p>其中，\text{bos} 是句子的起始符号，\text{eos} 是句子的结束符号。这鼓励在位置 <span class="math-inline">m-1</span> 的标记去预测标记 <span class="math-inline">m</span>：</p>
<p><div class="math-display"><br />
    P(\hat{s}<em>1, \hat{s}_2, ..., \hat{s}_m | x_i, q, s) = \prod</em>{j=1}^{m-1} P(s_j | e_i, q, s_1, s_2, ..., s_{j-1})<br />
</div></p>
<p>其中，<span class="math-inline">s_j</span> 表示真实答案标记，而 <span class="math-inline">\hat{s}_m</span> 是生成的预测。然后，我们可以计算交叉熵损失如下：</p>
<p><div class="math-display"><br />
    L_{CE} = - \frac{1}{m} \sum_{j=1}^{m} s_j \cdot \log \hat{s}_j<br />
</div></p>
<p><strong>推理</strong>。在推理过程中，我们使用更新后的投影层与预训练的编码器结合获取图像特征。这些图像特征与问题嵌入结合后传递给 LLM 解码器，以获取文本输出。</p>
<p><div class="math-display"><br />
    \text{pred} = \arg\max \langle f_{\text{text}}(s), f_{\text{text}}(\hat{s}) \rangle<br />
</div></p>
<p>其中 <span class="math-inline">f_{\text{text}}</span> 是文本编码器，<span class="math-inline">\langle , \rangle</span> 是用于计算最终预测结果的余弦相似度。</p>
<h2 id="4-实验">4. 实验<a class="anchor-link" href="#4-实验" title="Permanent link">&para;</a></h2>
<h3 id="41-实验设置">4.1 实验设置<a class="anchor-link" href="#41-实验设置" title="Permanent link">&para;</a></h3>
<p><strong>数据集和基准</strong>。我们在传统的类别增量学习（CIL）和少样本类别增量学习（Few-shot CIL）场景中进行实验。在传统 CIL 中，我们在三个数据集上进行了评估：CIFAR100、Tiny-ImageNet 和 ImageNet-R。CIFAR100 包含 60,000 张 32x32 像素的图像，共有 100 个类别。每个类别有 600 张图像，其中 500 张用于训练集，100 张用于测试集。我们实验了两种设置，B0-n 和 B50-n。前者将 100 个类别分成 n 个任务，后者先在 50 个类别上训练，然后将另外 50 个类别分配到 5/10 个任务中。</p>
<p>Tiny-ImageNet 包含 200 个类别，这些类别来自原始 ImageNet 的 1000 个类别，每个类别有 550 张图像，其中 500 张在训练集中，50 张在测试集中。图像被下采样到 64 × 64 像素，使其更易于处理和分析。我们在第一个任务中训练了一半的 100 个类别，并按照 [89] 的方法将其他 100 个类别分成 5/10/20 个任务。</p>
<p>ImageNet-R [23] 包含 200 个类别的图像，这些图像包含在原始 ImageNet 的 1000 个类别中。然而，许多图像是新添加的，并具有多种风格，如素描、绘画、杂项等。该数据集为持续学习带来了巨大的挑战，因为它具有广泛的图像类别和风格多样性，且样本分布不均匀，每个类别的样本数量从 45 到 500 不等。我们按照 [71] 的方法将数据集分成 10 个任务，每个任务包含 20 个类别。</p>
<p>在少样本 CIL 中，我们使用 CIFAR100 和 mini-ImageNet [49]，并遵循 [62] 提出的划分方法。对于这两个数据集，我们将数据划分为两个部分：基本会话和增量会话。基本会话包括 60 个类别，所有数据可用，而增量会话遵循 5-way 5-shot 设置，这意味着每个会话仅包含 5 个类别，每个类别有 5 个样本。</p>
<p>在传统和少样本场景中，我们将我们的方法与一些当前最先进的方法进行了比较，包括传统方法 [7, 15, 16, 24, 29, 48, 53, 74, 76, 82, 89]，预训练和基于提示的方法 [55, 71, 72]，以及一些专为少样本场景设计的方法 [12, 53, 62, 79]。此外，我们还比较了线性探针基准，其中从图像编码器获得的特征被连接到一个分类器进行分类。我们还考虑了零样本（Zero-shot）方法，即直接使用生成的文本进行分类，而无需进一步微调。</p>
<p><strong>实施细节</strong>。我们遵循 BLIP2 [27] 的方法，使用 EVA-CLIP [59] 预训练的 ViT-g/14 和 BLIP2 预训练的 Qfomer。我们还使用了 MiniGPT-4 预训练的投影层检查点作为我们的初始参数。在多样本 "B0" 设置下，我们采用 3e-7 的学习率，并使用余弦衰减的调度器。整个训练过程只包含 2 个 epoch。在 B50 或 B100 设置中，我们首先以 3e-6 的学习率训练线性层在基础类上，然后在后续任务中，我们采用更低的 3e-7 学习率，均采用余弦衰减调度器。在少样本设置中，我们在基础任务和增量任务中均采用 3e-6 的学习率。我们为基础任务训练一个 epoch，为增量任务训练两个 epoch。</p>
<h3 id="42-传统-cil-的实验">4.2 传统 CIL 的实验<a class="anchor-link" href="#42-传统-cil-的实验" title="Permanent link">&para;</a></h3>
<p>在表 1 中可以看到，我们的方法大幅度超越了所有传统方法，包括基于 ResNet 的 DER 和基于 ViT 的 DyTox。请注意，在没有示例的情况下，我们的性能在 B100-5 设置下略低于 DualPrompt 和 CODA-Prompt。我们认为他们的性能主要是由于骨干网络在 ImageNet-21K 上预训练，这与 CIFAR100 和 Tiny-ImageNet 有很大重叠。另一个有趣的观察是，我们的方法在较长序列设置（B100-10, B100-20）下表现更好。我们认为这是因为生成模型不依赖于分类头，使得它们不易受到当前任务的偏倚，从而减少了对过去任务的遗忘。线性探针设置的表现比我们的方法差，这表明我们主要的贡献并非来自大型预训练的 ViT，而是生成管道。此外，零样本性能优于许多传统基准，这意味着生成多模态模型确实是有效的类别增量学习者，但其输出在没有微调的情况下不够简洁（见图 4）。</p>
<p>在图 3 中，我们在 CIFAR100 和 Tiny-ImageNet 上的最后任务准确率与一些预训练模型进行了比较（所有基准都基于 PILOT [58] 并使用 2000 个示例）。可以观察到，我们的方法在初始任务（0-2）和短序列设置（B0-5, B100-5）中并未超过其他方法。这是因为我们不依赖于有监督的 ImageNet-21K 预训练骨干。此外，我们为了确保效率而训练每个任务仅 1-2 个 epoch，而不牺牲泛化能力。然而，我们的方法在长序列和后续任务中表现出了显著优势。例如，在 CIFAR100 B0-20 设置下，我们比 CODA-Prompt 高出 10 个百分点，比 DualPrompt 高出 7 个百分点。</p>
<h3 id="43-少样本-cil-的实验">4.3 少样本 CIL 的实验<a class="anchor-link" href="#43-少样本-cil-的实验" title="Permanent link">&para;</a></h3>
<p>在表 2 中，我们在 mini-ImageNet 的少样本设置下与几个基准方法进行了比较。评估指标是模型在所有遇到的类别上的准确性。我们的方法在最终任务中大幅度超越了传统方法，准确率提高了超过 26%。此外，我们比最好的判别预训练方法 CODA-Prompt 高出超过 14 个百分点。需要注意的是，我们在第一个任务中的准确性（89.35）可能不如 CODA-Prompt（95.37）。然而，在随后的会话中，我们一直表现优于 CODA-Prompt，这得益于我们同时学习新任务和保留旧任务知识的能力。</p>
<p>在表 3 中，我们的方法在 CIFAR100 数据集的少样本设置下超越了所有其他基准，取得了显著较低的性能下降（PD），仅为 10.06。此外，由于缺乏遗忘，零样本基准可以实现非常低的性能下降。然而，其整体性能并不令人满意，因为其输出的长度和内容在没有微调的情况下不一致且不可预测。</p>
<h3 id="44-可视化">4.4 可视化<a class="anchor-link" href="#44-可视化" title="Permanent link">&para;</a></h3>
<p>在图 4 中，我们展示了一些我们的方法与未微调的 GMM [88] 的比较示例。我们可以看到，未微调的 GMM 提供了对图像内容整体的直观描述，并且输出文本的长度不一。然而，它往往只能识别广泛的类别（如鸟类，汽车），并且在细粒度分类上存在困难（如鹈鹕，消防车）。描述有时会略显重复（如第一个消防车）。相比之下，我们的微调方法能够准确识别图像的实际类别，即使有时与真实标签略有出入（如 “fire engine” 与 “fire truck”）。此外，在测试阶段借助文本编码器的帮助，我们的模型即使在预测相似但不完全相同的文本时，也能实现正确的分类结果。</p>
<h2 id="5-结论">5. 结论<a class="anchor-link" href="#5-结论" title="Permanent link">&para;</a></h2>
<p>在本文中，我们提出了使用生成模型进行类别增量学习的生成多模态模型（GMM）。通过微调生成多模态模型（GMM），我们直接生成要分类图像的标签文本。然后，我们根据生成文本与真实标签的特征相似性，选择最相似的标签作为最终的分类结果。我们的实验表明，这种不需要分类头部的方法在解决持续学习中的分类偏倚问题方面表现非常出色。</p>
<h3 id="6-局限性">6. 局限性<a class="anchor-link" href="#6-局限性" title="Permanent link">&para;</a></h3>
<p>由于我们是首次将生成模型引入类别增量学习领域，我们的方法设计相对简单。我们相信，随着在这个方向上进行更多的专注研究，该领域的持续学习将会取得显著进展。</p>
<h3 id="7-更广泛的影响">7. 更广泛的影响<a class="anchor-link" href="#7-更广泛的影响" title="Permanent link">&para;</a></h3>
<p>我们认为将生成多模态模型（GMM）引入持续学习是既必要又紧迫的。随着 GMM 的快速发展，我们可以利用它们的能力来提高持续学习的性能。此外，将持续学习方法集成到 GMM 的训练过程中，还可以显著降低训练成本。</p>
                </div>

                <!-- Comments Section (Giscus) -->
                <section class="comments-section">
                    <h3><i class="fas fa-comments"></i> 评论</h3>
                    <script src="https://giscus.app/client.js"
                        data-repo="Geeks-Z/Geeks-Z.github.io"
                        data-repo-id=""
                        data-category="Announcements"
                        data-category-id=""
                        data-mapping="pathname"
                        data-strict="0"
                        data-reactions-enabled="1"
                        data-emit-metadata="0"
                        data-input-position="bottom"
                        data-theme="preferred_color_scheme"
                        data-lang="zh-CN"
                        crossorigin="anonymous"
                        async>
                    </script>
                </section>
            </article>

            <footer class="post-footer">
                <p>© 2025 Hongwei Zhao. Built with ❤️</p>
            </footer>
        </main>
    </div>

    <!-- Theme Toggle Button -->
    <button class="theme-toggle" id="themeToggle" aria-label="切换主题">
        <i class="fas fa-moon"></i>
    </button>

    <script>
        // Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        const html = document.documentElement;
        const icon = themeToggle.querySelector('i');
        const hljsDark = document.getElementById('hljs-theme-dark');
        const hljsLight = document.getElementById('hljs-theme-light');
        
        // Check saved theme or system preference
        const savedTheme = localStorage.getItem('theme');
        const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
        
        if (savedTheme === 'dark' || (!savedTheme && prefersDark)) {
            html.setAttribute('data-theme', 'dark');
            icon.className = 'fas fa-sun';
            hljsDark.disabled = false;
            hljsLight.disabled = true;
        }
        
        themeToggle.addEventListener('click', () => {
            const isDark = html.getAttribute('data-theme') === 'dark';
            if (isDark) {
                html.removeAttribute('data-theme');
                icon.className = 'fas fa-moon';
                localStorage.setItem('theme', 'light');
                hljsDark.disabled = true;
                hljsLight.disabled = false;
            } else {
                html.setAttribute('data-theme', 'dark');
                icon.className = 'fas fa-sun';
                localStorage.setItem('theme', 'dark');
                hljsDark.disabled = false;
                hljsLight.disabled = true;
            }
            
            // Update Giscus theme
            const giscusFrame = document.querySelector('iframe.giscus-frame');
            if (giscusFrame) {
                giscusFrame.contentWindow.postMessage({
                    giscus: {
                        setConfig: {
                            theme: isDark ? 'light' : 'dark'
                        }
                    }
                }, 'https://giscus.app');
            }
        });
        
        // Highlight.js
        document.addEventListener('DOMContentLoaded', () => {
            hljs.highlightAll();
        });
        
        // KaTeX auto-render
        document.addEventListener('DOMContentLoaded', () => {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
        
        // TOC active state
        const tocLinks = document.querySelectorAll('.toc-container a');
        const headings = document.querySelectorAll('.post-content h2, .post-content h3, .post-content h4');
        
        function updateTocActive() {
            let current = '';
            headings.forEach(heading => {
                const rect = heading.getBoundingClientRect();
                if (rect.top <= 100) {
                    current = heading.getAttribute('id');
                }
            });
            
            tocLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === '#' + current) {
                    link.classList.add('active');
                }
            });
        }
        
        window.addEventListener('scroll', updateTocActive);
        updateTocActive();
    </script>
</body>
</html>
