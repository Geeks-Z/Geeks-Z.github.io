<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Untitled</title>
    <meta name="description" content="Untitled - Hongwei Zhao's Blog">
    <meta name="author" content="Hongwei Zhao">
    
    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <!-- Code Highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css" id="hljs-theme-dark">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css" id="hljs-theme-light" disabled>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    
    <style>
        :root {
            /* Light Theme - 明亮清新配色 */
            --primary-color: #4A90D9;
            --primary-hover: #3678C2;
            --link-color: #E86B5F;
            --text-color: #2D2D2D;
            --text-light: #5A5A5A;
            --text-muted: #8A8A8A;
            --bg-color: #FFFFFF;
            --bg-secondary: #F5F7FA;
            --bg-code: #F8F9FC;
            --border-color: #E8ECF0;
            --shadow: 0 2px 8px rgba(0,0,0,0.06);
            --shadow-lg: 0 8px 24px rgba(0,0,0,0.08);
        }

        [data-theme="dark"] {
            --primary-color: #5dade2;
            --primary-hover: #85c1e9;
            --link-color: #e74c3c;
            --text-color: #e5e7eb;
            --text-light: #9ca3af;
            --text-muted: #6b7280;
            --bg-color: #1a1a2e;
            --bg-secondary: #16213e;
            --bg-code: #0f0f23;
            --border-color: #374151;
            --shadow: 0 1px 3px rgba(0,0,0,0.3);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.4);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 16px;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            transition: background-color 0.3s, color 0.3s;
        }

        a {
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--primary-hover);
            text-decoration: underline;
        }

        /* Layout */
        .page-wrapper {
            display: flex;
            max-width: 1400px;
            margin: 0 auto;
            padding: 2rem;
            gap: 3rem;
        }

        /* Sidebar TOC */
        .toc-sidebar {
            width: 260px;
            flex-shrink: 0;
            position: sticky;
            top: 2rem;
            height: fit-content;
            max-height: calc(100vh - 4rem);
            overflow-y: auto;
        }

        .toc-container {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 1.5rem;
            border: 1px solid var(--border-color);
        }

        .toc-container h3 {
            font-size: 0.85rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-muted);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .toc-container ul {
            list-style: none;
        }

        .toc-container li {
            margin-bottom: 0.5rem;
        }

        .toc-container a {
            font-size: 0.9rem;
            color: var(--text-light);
            display: block;
            padding: 0.25rem 0;
            border-left: 2px solid transparent;
            padding-left: 0.75rem;
            transition: all 0.2s;
        }

        .toc-container a:hover,
        .toc-container a.active {
            color: var(--primary-color);
            border-left-color: var(--primary-color);
            text-decoration: none;
        }

        .toc-container ul ul {
            margin-left: 1rem;
        }

        /* Main Content */
        .main-content {
            flex: 1;
            min-width: 0;
            max-width: 800px;
        }

        /* Header */
        .post-header {
            margin-bottom: 2rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
            margin-bottom: 1.5rem;
        }

        .back-link:hover {
            color: var(--primary-color);
            text-decoration: none;
        }

        .post-header h1 {
            font-size: 2.25rem;
            font-weight: 700;
            line-height: 1.3;
            margin-bottom: 1rem;
            color: var(--text-color);
        }

        .post-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
        }

        .post-meta span {
            display: flex;
            align-items: center;
            gap: 0.4rem;
        }

        .post-meta i {
            color: var(--text-muted);
        }

        .tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 1rem;
        }

        .tag {
            display: inline-block;
            font-size: 0.8rem;
            background: var(--bg-secondary);
            color: var(--text-light);
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            border: 1px solid var(--border-color);
            transition: all 0.2s;
        }

        .tag:hover {
            background: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }

        /* Article Content */
        .post-content {
            font-size: 1rem;
            line-height: 1.9;
        }

        .post-content h1,
        .post-content h2,
        .post-content h3,
        .post-content h4,
        .post-content h5,
        .post-content h6 {
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-weight: 600;
            line-height: 1.4;
            color: var(--text-color);
        }

        .post-content h1 { font-size: 1.875rem; }
        .post-content h2 { 
            font-size: 1.5rem; 
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }
        .post-content h3 { font-size: 1.25rem; }
        .post-content h4 { font-size: 1.125rem; }

        .post-content p {
            margin-bottom: 1.25rem;
        }

        .post-content ul,
        .post-content ol {
            margin: 1.25rem 0;
            padding-left: 1.5rem;
        }

        .post-content li {
            margin-bottom: 0.5rem;
        }

        .post-content blockquote {
            margin: 1.5rem 0;
            padding: 1rem 1.5rem;
            background: var(--bg-secondary);
            border-left: 4px solid var(--primary-color);
            border-radius: 0 8px 8px 0;
            color: var(--text-light);
            font-style: italic;
        }

        .post-content blockquote p:last-child {
            margin-bottom: 0;
        }

        /* Code */
        .post-content code {
            font-family: 'JetBrains Mono', 'Fira Code', Consolas, monospace;
            font-size: 0.9em;
            background: var(--bg-code);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            color: var(--link-color);
        }

        .post-content pre {
            margin: 1.5rem 0;
            padding: 1.25rem;
            background: var(--bg-code);
            border-radius: 10px;
            overflow-x: auto;
            border: 1px solid var(--border-color);
        }

        .post-content pre code {
            background: none;
            padding: 0;
            color: inherit;
            font-size: 0.875rem;
            line-height: 1.6;
        }

        /* Images */
        .post-content img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            margin: 1.5rem 0;
            box-shadow: var(--shadow-lg);
        }

        /* Tables */
        .post-content table {
            width: 100%;
            margin: 1.5rem 0;
            border-collapse: collapse;
            font-size: 0.95rem;
        }

        .post-content th,
        .post-content td {
            padding: 0.75rem 1rem;
            border: 1px solid var(--border-color);
            text-align: left;
        }

        .post-content th {
            background: var(--bg-secondary);
            font-weight: 600;
        }

        .post-content tr:nth-child(even) {
            background: var(--bg-secondary);
        }

        /* Math */
        .math-display {
            margin: 1.5rem 0;
            overflow-x: auto;
            padding: 1rem;
            background: var(--bg-secondary);
            border-radius: 8px;
        }

        /* Anchor links */
        .anchor-link {
            opacity: 0;
            margin-left: 0.5rem;
            color: var(--text-muted);
            font-weight: 400;
            transition: opacity 0.2s;
        }

        .post-content h2:hover .anchor-link,
        .post-content h3:hover .anchor-link,
        .post-content h4:hover .anchor-link {
            opacity: 1;
        }

        /* Theme Toggle */
        .theme-toggle {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            background: var(--primary-color);
            color: white;
            border: none;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.25rem;
            box-shadow: var(--shadow-lg);
            transition: transform 0.2s, background 0.2s;
            z-index: 1000;
        }

        .theme-toggle:hover {
            transform: scale(1.1);
            background: var(--primary-hover);
        }

        /* Comments Section */
        .comments-section {
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-color);
        }

        .comments-section h3 {
            font-size: 1.25rem;
            margin-bottom: 1.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        /* Footer */
        .post-footer {
            margin-top: 3rem;
            padding-top: 1.5rem;
            border-top: 1px solid var(--border-color);
            text-align: center;
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        /* Responsive */
        @media (max-width: 1024px) {
            .toc-sidebar {
                display: none;
            }
            
            .page-wrapper {
                padding: 1.5rem;
            }
        }

        @media (max-width: 768px) {
            .post-header h1 {
                font-size: 1.75rem;
            }
            
            .post-meta {
                flex-direction: column;
                gap: 0.5rem;
            }
            
            .theme-toggle {
                bottom: 1rem;
                right: 1rem;
                width: 44px;
                height: 44px;
            }
        }

        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-secondary);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--border-color);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--text-muted);
        }
    </style>
</head>
<body>
    <div class="page-wrapper">
        <!-- TOC Sidebar -->
        <aside class="toc-sidebar">
            <div class="toc-container">
                <h3><i class="fas fa-list"></i> 目录</h3>
                <div class="toc">
<ul>
<li><a href="#moe模型">MoE模型</a></li>
<li><a href="#时间线">时间线</a><ul>
<li><a href="#上古时代">上古时代</a></li>
<li><a href="#rnn时代">RNN时代</a></li>
<li><a href="#transformer时代">Transformer时代</a></li>
<li><a href="#gpt时代">GPT时代</a></li>
</ul>
</li>
<li><a href="#奠基工作">奠基工作</a></li>
<li><a href="#patch-level-routing-in-mixture-of-experts-is-provably-sample-efficient-for-convolutional-neural-networks">Patch-level Routing in Mixture-of-Experts is Provably Sample-efficient for Convolutional Neural Networks</a></li>
<li><a href="#robust-mixture-of-expert-training-for-convolutional-neural-networks">Robust Mixture-of-Expert Training for Convolutional Neural Networks</a></li>
<li><a href="#brainformers-trading-simplicity-for-efficiency">Brainformers: Trading Simplicity for Efficiency</a></li>
<li><a href="#sparse-fusion-mixture-of-experts-are-domain-generalizable-learners">Sparse Fusion Mixture-of-Experts are Domain Generalizable Learners</a></li>
<li><a href="#lstm-moe">LSTM MoE</a><ul>
<li><a href="#背景">背景</a></li>
<li><a href="#模型设计">模型设计</a></li>
<li><a href="#负载均衡">负载均衡</a></li>
<li><a href="#实验">实验</a></li>
</ul>
</li>
<li><a href="#gshard">GShard</a></li>
<li><a href="#switch-transformer">Switch Transformer</a><ul>
<li><a href="#模型设计_1">模型设计</a></li>
<li><a href="#负载均衡_1">负载均衡</a></li>
<li><a href="#实验_1">实验</a></li>
</ul>
</li>
<li><a href="#glam">GLaM</a></li>
<li><a href="#st-moe">ST-MoE</a><ul>
<li><a href="#稳定性与效果分析">稳定性与效果分析</a></li>
<li><a href="#模型设计_2">模型设计</a></li>
<li><a href="#实验_2">实验</a></li>
</ul>
</li>
<li><a href="#deepseekmoe">DeepseekMoE</a><ul>
<li><a href="#模型设计_3">模型设计</a></li>
<li><a href="#负载均衡_2">负载均衡</a></li>
<li><a href="#实验_3">实验</a></li>
</ul>
</li>
<li><a href="#dbrx">DBRX</a></li>
<li><a href="#qwen15-moe">Qwen1.5-MoE</a></li>
<li><a href="#mistral">Mistral</a><ul>
<li><a href="#mistral-8x7b">Mistral 8x7B</a></li>
<li><a href="#mistral-8x22b">Mistral 8x22B</a></li>
</ul>
</li>
<li><a href="#moe">MoE++</a></li>
<li><a href="#cfm">CFM</a></li>
<li><a href="#a-closer-look-into-mixture-of-experts-in-large-language-models">A Closer Look into Mixture-of-Experts in Large Language Models</a></li>
<li><a href="#deepseekmoe-towards-ultimate-expert-specialization-inmixture-of-experts-language-models">DeepSeekMoE: Towards Ultimate Expert Specialization inMixture-of-Experts Language Models</a></li>
<li><a href="#harder-tasks-need-more-experts-dynamic-routing-in-moe-models">Harder Tasks Need More Experts: Dynamic Routing in MoE Models</a></li>
<li><a href="#xmoe-sparse-models-with-fine-grained-and-adaptive-expert-selection">XMoE: Sparse Models with Fine-grained and Adaptive Expert Selection</a></li>
<li><a href="#hypermoe-towards-better-mixture-of-experts-via-transferring-amongexperts">HyperMoE: Towards Better Mixture of Experts via Transferring AmongExperts</a></li>
<li><a href="#小结">小结</a></li>
<li><a href="#reference">Reference</a></li>
</ul>
</div>

            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <article>
                <header class="post-header">
                    <a href="../../../index.html" class="back-link">
                        <i class="fas fa-arrow-left"></i> 返回博客列表
                    </a>
                    <h1>Untitled</h1>
                    <div class="post-meta">
                        <span><i class="fas fa-calendar-alt"></i> 2026-02-04</span>
                        <span><i class="fas fa-folder"></i> AINotes/09.MoE</span>
                        <span><i class="fas fa-user"></i> Hongwei Zhao</span>
                    </div>
                    <div class="tags">
                        
                    </div>
                </header>

                <div class="post-content">
                    <h2 id="moe模型">MoE模型<a class="anchor-link" href="#moe模型" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>模型</th>
<th>发布时间</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT4</td>
<td>2023年3月</td>
<td>23年6月George Hotz爆料GPT4是8×220B模型</td>
</tr>
<tr>
<td>Mistral-8×7B</td>
<td>2023年12月</td>
<td>Mistral AI，开源</td>
</tr>
<tr>
<td>LLAMA-MoE</td>
<td>2023年12月</td>
<td>github开源项目</td>
</tr>
<tr>
<td>DeepSeek-MoE</td>
<td>2024年1月</td>
<td>幻方量化(深度求索)，国内首个开源MoE模型，有技术报告</td>
</tr>
<tr>
<td>abab6</td>
<td>2024年1月</td>
<td>MiniMax，号称千亿MoE，无开源，无细节发布</td>
</tr>
<tr>
<td>天工2.0</td>
<td>2024年2月</td>
<td>昆仑万维，无开源，无细节发布</td>
</tr>
<tr>
<td>Step-2</td>
<td>2024年3月</td>
<td>阶跃星辰，无开源，无细节发布</td>
</tr>
<tr>
<td>MM1</td>
<td>2024年3月</td>
<td>苹果，多模态MoE，无开源，有技术报告</td>
</tr>
<tr>
<td>Grok-1</td>
<td>2024年3月</td>
<td>X，开源</td>
</tr>
<tr>
<td>Qwen1.5-MoE-A2.7B</td>
<td>2024年3月</td>
<td>阿里巴巴，开源</td>
</tr>
<tr>
<td>DBRX</td>
<td>2024年3月</td>
<td>Databricks，开源</td>
</tr>
<tr>
<td>Jamba</td>
<td>2024年3月</td>
<td>AI21，开源</td>
</tr>
<tr>
<td>Mistral-8×22B</td>
<td>2024年4月</td>
<td>Mistral AI，开源</td>
</tr>
<tr>
<td>WizardLM-2-8×22B</td>
<td>2024年4月</td>
<td>微软，开源</td>
</tr>
<tr>
<td>天工3.0</td>
<td>2024年4月</td>
<td>昆仑万维，400BMoE</td>
</tr>
<tr>
<td>Arctic</td>
<td>2024年4月</td>
<td>Snowflake，480B，Dense-MoE Hybrid，开源</td>
</tr>
</tbody>
</table>
<h2 id="时间线">时间线<a class="anchor-link" href="#时间线" title="Permanent link">&para;</a></h2>
<h3 id="上古时代">上古时代<a class="anchor-link" href="#上古时代" title="Permanent link">&para;</a></h3>
<p>首先是很多MoE相关论文都会引用的，发表在1991年的论文<a href="https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf">https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf</a>，这篇文章出自Geoffrey Hinton和Michael I. Jordan两位大神之手。虽然在更早的时候就有MoE相关概念的工作，如原文所提到的，1988年这个概念就有了</p>
<blockquote>
<p>This idea was first presented by Jacobs and Hinton at the Connectionist Summer School in Pittsburg in 1988.</p>
</blockquote>
<p>但是大部分MoE文章还是认为是这个工作奠定了MoE的基础。</p>
<h3 id="rnn时代">RNN时代<a class="anchor-link" href="#rnn时代" title="Permanent link">&para;</a></h3>
<p>时隔二十多年，Google在2017年1月发布了<a href="https://arxiv.org/abs/1701.06538">https://arxiv.org/abs/1701.06538</a>，把MoE带进了LSTM，训出了最大137B参数，专家数达到128k的LSTM模型。</p>
<h3 id="transformer时代">Transformer时代<a class="anchor-link" href="#transformer时代" title="Permanent link">&para;</a></h3>
<ol>
<li>2020年6月，Google发布<a href="https://arxiv.org/abs/2006.16668">https://arxiv.org/abs/2006.16668</a>，把MoE应用在encoder-decoder结构的transformer模型上，每两层将一个FFN层替换成一个MoE层，训出了模型参数量从12.5B到600B的一系列MoE模型，每层最大专家数也达到2048个。</li>
<li>2021年1月，Google发布<a href="https://arxiv.org/abs/2101.03961">https://arxiv.org/abs/2101.03961</a> ，在T5（encoder-decoder结构）的基础上，把FFN层替换成MoE层，并简化了routing策略，训出了最大1.6T参数量的switch transformer。Switch Transformers对scaling、蒸馏等做了很多详细的探索，影响深远，是很重要的一个工作。</li>
<li>2022年2月，Google发布<a href="https://arxiv.org/abs/2202.08906">https://arxiv.org/abs/2202.08906</a>，也是一个基于encoder-decoder结构的MoE模型，最大模型有269B的总参数，32B的激活参数。ST-MoE可以说不仅仅是一个MoE工作，对于模型结构、工程实现、训练策略等都做了很多分析，个人认为其重要程度相比Switch Transformer都有过之而无不及。</li>
</ol>
<h3 id="gpt时代">GPT时代<a class="anchor-link" href="#gpt时代" title="Permanent link">&para;</a></h3>
<ol>
<li>2021年12月，Google发布了GLaM，<a href="https://arxiv.org/abs/2112.06905">https://arxiv.org/abs/2112.06905</a>，训出了最大为1.2T参数量的decoder-only模型。（从encoder-decoder到decoder-only，可以看到Google内部在模型结构方向上也有很多不同的尝试）</li>
<li>2024年1月，幻方量化发布<a href="https://arxiv.org/abs/2401.06066">https://arxiv.org/abs/2401.06066</a>，对在23年12月开源的DeepSeekMoE，给出了一些细节。</li>
<li>2024年，Databricks的DBRX、阿里的Qwen1.5-MoE-A2.7B、Mistral AI的Mistral-8x22B等陆续发布。</li>
</ol>
<h2 id="奠基工作">奠基工作<a class="anchor-link" href="#奠基工作" title="Permanent link">&para;</a></h2>
<p>Geoffrey Hinton和Michael I. Jordan的<a href="https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf">https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf</a>是大多数MoE论文都会引用的最早工作。</p>
<ol>
<li>思路</li>
</ol>
<p>这篇文章大致的思路是这样的：对于比较复杂的任务，一般可以拆分为多个子任务。比如要求计算输入文本中有多少个动词和名词，那就可以拆分为“数动词”和“数名词”这两个子任务。</p>
<p>而一个模型如果要同时学习多个子任务，多个子任务相互之间就会互相影响，模型的学习就会比较缓慢、困难，最终的学习效果也不好。</p>
<p>因此这篇文章提出了一种由多个分开的子网络组成的监督学习方法。这些分开的网络，在训练过程中，分别学习处理整个训练数据集中的一个子集，也就是一个子任务。这个思路就是现代MoE的思路，每个子网络（也就是一个expert）学习处理一部分内容。</p>
<p>文章里把这个MoE的方法应用于vowel discrimination task，即元音辨别任务，验证了MoE设计的有效性。元音辨别指的是语音学中区分不同元音的能力，在语音学中，模型需要学习辨别不同的元音因素，以便准确地理解和识别语音输入。通过让多个子模型分别学习分别学习不同元音（a、e、i、o、u）辨别的子任务，最终效果得到了提升。</p>
<ol>
<li>模型设计</li>
</ol>
<p>下图展示的就是这个MoE的思路：各个expert network和gating network接收同样的输入，每个expert给出各自的处理结果；而gating network输出每个expert的权重，就像一个开关一样，控制着每个expert对当前输入的打开程度，只是这个开关不是离散的，而是stochastic的，给出的不是true和false，而是权重。</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-820e61fd82a41774731b10d625eb5d2c_1440w.jpg" />  </p>
<ol>
<li>损失函数优化</li>
</ol>
<p>实际上，MoE这个idea在这篇文章之前就有了。如论文中所提，Jacobs和Hinton在1988就讨论过。但是之前的工作在loss的设计上，和ensemble更相近，多个expert之间更倾向于合作，每个expert会学习其他expert的residual部分。</p>
<p>具体来说，对于case <span class="math-inline">c</span>，假设第 <span class="math-inline">d^c</span> 是对应的ground truth，第 <span class="math-inline">i</span> 个expert的输出是 <span class="math-inline">o_{i}^c</span>，<span class="math-inline">p_{i}^c</span> 是gating network给第 <span class="math-inline">i</span> 个expert分配的权重，那么以前的工作所使用的损失函数 <span class="math-inline">E^{c}</span> 计算如下</p>
<p><span class="math-inline">E^{c}=\left|\left|d^{c}-\sum_{i}p_{i}^{c}o_{i}^{c}\right|\right|^{2} \</span></p>
<p>这样的损失计算方式，是把期望输出和所有expert输出的混合结果进行比较。</p>
<p>这样做的结果是，在训练过程中，每个expert学习的其实是其他expert的组合结果所剩下的残差。这样的学习目标并不能很好迫使每个expert单独输出好的结果，因此不能得到稀疏的模型。</p>
<p>从另一个角度来看，这个损失计算把所有专家耦合在了一起。即当一个expert的输出发生了变化，所有expert的组合结果也会变化，其他所有的expert也需要做相应的改动来适应这个变化。因此各个expert之间更加倾向于合作，而不是相互竞争并单独给出好的结果，让gating network输出稀疏的结果。</p>
<p>虽然可以使用如增加辅助损失函数的做法，迫使模型给出稀疏激活的结果，但是这样相当于增加了很强的先验正则化，对模型最终效果也是有损害的。</p>
<p>而Hinton和Jordan在这个工作里，提出更简单的做法是对loss计算进行修改，使得各个expert之间的关系从合作变成竞争。</p>
<p>假设gating network每次随机选择一个expert，损失计算如下</p>
<p><span class="math-inline">E^{c}=\langle|\mathbf{d}^c-\mathbf{o}<em>i^c|^2\rangle=\sum</em>{i}p_{i}^{c}\left|\left|d^{c}-o_{i}^{c}\right|\right|^{2} \</span></p>
<p>在这个损失函数中，每个expert的输出结果会单独和期望结果进行对比，这就要求每个expert单独给出完整的结果，而不是仅学习其他expert的残差。</p>
<p>这样的loss计算具有localization的特性，即如果一个训练case错了，那么会被修改的主要是被gating network选中且出错的expert，以及负责分配权重的gating network，而不会很大地影响其他expert。</p>
<p>此外，localization还体现在，每个expert只会负责处理输入空间中某个特定子空间的向量，而不是完整的输入空间。</p>
<p>这样一来，不同的expert之间不会直接相互影响，虽然还是有间接的影响，比如某个expert的输出变了，gating network可能会分配新的权重，但是至少不会改变其他expert error的符号（+，-），即优化的方向。</p>
<p>最终的结果是，对于给定的输入，这样的系统会倾向于以高权重分配单一一个expert来预测结果（但其他权重还不是真正的0，不是真正的稀疏）。</p>
<ol>
<li>实操技巧</li>
</ol>
<p>上面提出的这个loss计算，理论上没有问题，实际上也能训练，但是为了得到更好的效果，作者把原loss计算作了如下变化：先指数化再求和，最后再取对数，得到了优化loss。看下变化前后的对比</p>
<p><span class="math-inline">\text{原loss：}E^{c}=\sum_{i}p_{i}^{c}\left|\left|d^{c}-o_{i}^{c}\right|\right|^{2} \<div class="math-display">\text{优化loss：}E^c=-log\sum_ip_i^ce^{-\frac12|\mathbf{d}^c-\mathbf{o}_i^c|^2} \</span></p>
<p>这样做有什么好处呢？来对比一下原loss函数和优化后的loss函数的求导结果</p>
<p><span class="math-inline">\text{原loss导数：}\frac{\partial E^c}{\partial\mathbf{o}_i^c}=-2p_i^c(\mathbf{d}^c-\mathbf{o}_i^c) \</div>\text{优化loss导数：}\frac{\partial E^c}{\partial\mathbf{o}_i^c}=-\left<a href="\mathbf{d}^c-\mathbf{o}_i^c">\frac{p_i^ce^{-\frac{1}{2}|\mathbf{d}^c-\mathbf{o}_i^c|^2}}{\sum_jp_j^ce^{-\frac{1}{2}|\mathbf{d}^c-\mathbf{o}_j^c|^2}}\right</a> \</span></p>
<p>相比原loss函数的导数，优化后的loss函数的导数，把当前第 <span class="math-inline">i</span> 个expert的表现，和其他expert联系起来了。这样能够更好地衡量expert <span class="math-inline">i</span> 对当前case的处理结果好坏。特别是在训练初期，gating network的权重是近似平均分配的，那么使用原loss函数的结果是，对当前case效果最好的expert，学习速度是最慢的（因为loss最小）；而优化的loss函数则可以让当前最好的expert的学习速度最快。相当于让“有天赋”的专家在对应的子任务上尽快提高水平。这样就强化了localization的特征，使得各个expert更快拟合到自己擅长的部分，加速训练。</p>
<p>（BTW，优化后的这个loss导数，和现在的对比学习形式上看起来也很相似）</p>
<p>这个工作在今天看来不很复杂，但是思路还是很踏实有效的，给MoE奠定了基础。</p>
<h2 id="patch-level-routing-in-mixture-of-experts-is-provably-sample-efficient-for-convolutional-neural-networks">Patch-level Routing in Mixture-of-Experts is Provably Sample-efficient for Convolutional Neural Networks<a class="anchor-link" href="#patch-level-routing-in-mixture-of-experts-is-provably-sample-efficient-for-convolutional-neural-networks" title="Permanent link">&para;</a></h2>
<p>混合专家系统中的补丁级路由对于卷积神经网络是样本高效的</p>
<p><strong>简述</strong>：论文主要讨论了混合专家模型（MoE）在深度学习中的应用，特别是patch级路由在MoE（pMoE）中的效果。pMoE能够将输入分成多个补丁（或标记），并只将一部分补丁发送给每个专家，从而显著减少计算量。通过使用混合两层卷积神经网络（CNN）进行监督分类任务，作者证明了pMoE能够减少实现所需推广所需的训练样本数量，并优于其单个专家对应项。这是因为pMoE路由器可以过滤与标签无关的补丁，并将类似的类判别补丁路由到相同的专家，这有助于提高模型的泛化能力。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202411261105396.jpg" />  </p>
<h2 id="robust-mixture-of-expert-training-for-convolutional-neural-networks">Robust Mixture-of-Expert Training for Convolutional Neural Networks<a class="anchor-link" href="#robust-mixture-of-expert-training-for-convolutional-neural-networks" title="Permanent link">&para;</a></h2>
<p>用于卷积神经网络的鲁棒混合专家训练</p>
<p><strong>简述</strong>：论文提出了一种新方法AdvMoE，用于提高混合专家模型的对抗鲁棒性。作者发现传统的对抗训练对于混合专家模型不再有效，因为路由器和专家之间难以相互适应。因此，他们提出了一种新的交替对抗训练框架，将路由器和专家分开进行训练。实验结果表明，AdvMoE可以提高混合专家模型的对抗鲁棒性，并具有更高的效率。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202411261105397.jpg" />  </p>
<h2 id="brainformers-trading-simplicity-for-efficiency">Brainformers: Trading Simplicity for Efficiency<a class="anchor-link" href="#brainformers-trading-simplicity-for-efficiency" title="Permanent link">&para;</a></h2>
<p>以简单换效率</p>
<p><strong>简述</strong>：论文研究了Transformer设计的选择，并发现更复杂的块可以更有效。文章开发了一个名为Brainformer的复杂块，它由多种类型的层组成，并优于最先进的密集和稀疏Transformer。Brainformer具有更高的质量和效率，特别是在训练速度和步骤时间方面。在下游任务评估中，Brainformer也表现出更高的性能。最后，在fewshot评估中，Brainformer也优于通过NAS派生的模型。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202411261105398.jpg" />    </p>
<h2 id="sparse-fusion-mixture-of-experts-are-domain-generalizable-learners">Sparse Fusion Mixture-of-Experts are Domain Generalizable Learners<a class="anchor-link" href="#sparse-fusion-mixture-of-experts-are-domain-generalizable-learners" title="Permanent link">&para;</a></h2>
<p>稀疏融合专家混合是域可泛化学习器</p>
<p><strong>简述</strong>：论文提出了一种名为SF-MoE的新领域泛化方法，该方法基于混合专家（MoE）模型构建。作者发现，混合专家模型可以通过处理多个领域的预测特征来处理分布偏移，从而提高其泛化能力。为此，作者将稀疏性和融合机制引入到MoE框架中，以保持模型的稀疏性和预测性。大量实验表明，SF-MoE是一个可泛化的域学习者，并在五个大规模领域泛化数据集上优于最先进的方法，同时计算成本相同甚至更低。作者还从分布式表示的角度揭示了SF-MoE的内部机制（例如视觉属性）。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202411261105400.jpg" /></p>
<h2 id="lstm-moe">LSTM MoE<a class="anchor-link" href="#lstm-moe" title="Permanent link">&para;</a></h2>
<p>Google在2017年1月发布了 <a href="https://arxiv.org/abs/1701.06538">https://arxiv.org/abs/1701.06538</a>，把MoE应用到了LSTM上，训出了最大137B的LSTM模型。这样规模的模型哪怕放在7年后的今天，也是巨无霸的存在，需要解决很多工程问题。</p>
<p>相比1991年的工作，这里做到了真正的稀疏激活，从而可以在实际计算量较少的情况下，训练巨大的模型。</p>
<h3 id="背景">背景<a class="anchor-link" href="#背景" title="Permanent link">&para;</a></h3>
<p>虽然当时Transformer还没出来，大规模模型的竞赛也还不像今天这么激烈，但是在多个领域中（文本、图像、音频），已经有不少工作反复证实了一件事：模型容量越大，能训出来的效果越好，上限越高。但是模型越大，需要的训练数据也就越多，二者共同作用下，就造成了训练开销基本是随着模型增大，以平方关系在增长。</p>
<p>在这个背景下就出现一些conditional computation，条件计算的工作来解决这个问题。conditional computation就是根据输入，有选择地只激活部分网络模块。那么MoE其实就是一种条件计算的实现。由于不用激活全部参数，训练所需的计算量就大大减小，整体计算成本就不用以平方速度增长。</p>
<p>虽然理论上计算量的成本下来了，不过实操起来还是会遇到几个问题：</p>
<ul>
<li>训练的时候，在MoE结构下，每个expert的batch size比整个模型的batch size小了。<br />
  比如模型的batch size是32，一共有16个expert，那实际上一次迭代平均每个expert只能分到2个训练样本。而batch size对训练效率影响是很大的，大的batch size摊小了参数传输和更新的成本。如果直接增大模型的batch size，又会受显存和通讯效率的限制。</li>
<li>训练数据量不足。<br />
  要训大模型就需要大量的数据，让模型参数充分学习。在当时的背景下，大规模的NLP数据是比较缺的。当然如今数据集多了很多，特别是预训练数据，这个问题现在来看没有那么突出了。</li>
<li>损失函数的设计。<br />
  如何使用合适的损失函数来训练模型，提升效果，并且使得模型的负载比较均衡，这是一个不容易解决的问题。</li>
<li>集群通讯问题。<br />
  一个GPU集群的计算能力可能比设备间网络带宽的总和高出数千倍，因此设备间的通讯很可能成为训练效率的瓶颈。为了计算效率，就要使得设备内计算量和所需的通讯量的比值，达到相应的比例。</li>
<li>GPU计算特点。<br />
  GPU做数学计算很快，但是并不擅长做branching（if/else），因此MoE的工作基本上都是用gating network来控制参数的激活。这个严格来说不算是新的挑战了，应该说是根据计算设备沿用下来的设计。</li>
</ul>
<p>要解决好这些问题，才能训出比较好的模型来。</p>
<h3 id="模型设计">模型设计<a class="anchor-link" href="#模型设计" title="Permanent link">&para;</a></h3>
<ol>
<li>整体结构</li>
</ol>
<p>先看下模型结构的设计。</p>
<p>论文里使用的是两个LSTM层，中间夹着一个MoE层，最上面和最下面分别还有一个embedding层和一个任务输出层，结构如下图所示</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-85a7f8cc6958cf82c354f5876d74dafe_1440w.jpg" /><br />
每个expert是一个简单的feed-forward neural network。一共有n个expert，gating network输出是一个稀疏的n维向量</p>
<p><span class="math-inline">\begin{aligned}y=\sum_{i=1}^nG(x)_iE_i(x)\end{aligned} \</span></p>
<p><span class="math-inline">E_{i}(x)</span> 是第 <span class="math-inline">i</span> 个expert的输出，<span class="math-inline">G(x)_{i}</span> 是gating network给出的第 <span class="math-inline">i</span> 个expert的权重。</p>
<p>如果 <span class="math-inline">G(x)_{i}</span> 为0，就不用计算对应的那个expert了，节省了计算。</p>
<p>如果expert的数量特别多，可以用two-level hierarchical MoE，即使用两层gating network，第一层的gating network先选择一个包含一批expert的分支，每个分支又有一个单独的gating network来选择具体的expert。类似word2vec训练所用的hierarchical softmax。这样做可以节省一些计算。</p>
<ol>
<li>gating network</li>
</ol>
<p>那具体gating network怎么设计呢？</p>
<p>如果对输入进行线性变换，再简单加上一个softmax，那得到的是一个非稀疏的gating function</p>
<p><span class="math-inline">\begin{aligned}G_\sigma(x)=Softmax(x\cdot W_g)\end{aligned} \</span></p>
<p>在这个基础上，使用一个topk函数，只保留最大的k个值，其他都设为﹣∞（softmax之后变成0），这样就能只选择部分expert，得到了稀疏性。</p>
<p>论文提到，虽然理论上这个形式的sparsity（topk）会造成gating function的不连续，不过在实操中暂时没有遇到相关问题。</p>
<p>在这个基础上，在输入再加上一个Gaussian noise，这个noise的大小由另外一个可学习的参数来控制。整体的计算公式如下</p>
<p><span class="math-inline">\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\end{aligned} \<div class="math-display">KeepTopK(v,k)<em>i=\begin{cases}v_i&amp;\text{if }v_i\text{ is in the top }k\text{ elements of }v.\-\infty&amp;\text{otherwise.}\end{cases} \</div>\begin{aligned}H(x)_i=(x\cdot W_g)_i+StandardNormal()\cdot Softplus((x\cdot W</em>{noise})_i)\end{aligned} \</span></p>
<p>其中用来调整noise的非线性函数softplus是个类似ReLU的激活函数，但是更为光滑，函数图像如下</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-828b390fde8f86e921d3a478dcd91c4b_1440w.jpg" /><br />
这里添加噪声的原因和负载均衡有关，下面来分析下负载均衡。</p>
<h3 id="负载均衡">负载均衡<a class="anchor-link" href="#负载均衡" title="Permanent link">&para;</a></h3>
<p>在MoE模型训练的实验中观察到，如果不对gating network进行干预，任由模型自由学习，那么最终模型会倾向于收敛到“总是选那几个固定的expert”的状态，而其他expert几乎不会被使用。这就是负载不均衡的状态，如果这些专家分布在不同的计算设备上，结果就是有些设备输入排队特别长，而有些设备基本处于闲置状态，这明显不是我们想要的。</p>
<p>这种负载不均衡的状态有自我加强的属性，因为一旦开始出现部分专家被较多选中激活，这些专家就会得到更充分的训练，从而获得更好的效果，进而又提升被选中激活的概率。</p>
<p>针对这种情况，之前有一些工作使用hard constraint来缓解，比如当某个expert激活次数达到上限，就把它从候选集合中移除。hard constraint明显会对模型效果有影响。而这篇论文使用的是一种soft constraint。</p>
<p>具体来说，对于每个expert，定义了一个它在当前这批输入数据里的重要性指标，如以下公式所示</p>
<p><span class="math-inline">Importance(X)=\sum_{x\in X}G(x) \</span></p>
<p><span class="math-inline">G(x)</span> 是gating network给出的权重，是一个维度等于expert数量的向量。</p>
<p>基于这个重要性指标，论文定义了一个辅助损失 <span class="math-inline">L_{importance}</span>，训练时和模型的交叉熵损失加到一起。<span class="math-inline">L_{importance}</span> 的计算方式如下</p>
<p><span class="math-inline">L_{importance}(X)=w_{importance}\cdot CV(Importance(X))^2 \</span></p>
<p>其中权重 <span class="math-inline">w_{importance}</span> 是手动设置的超参，实验的推荐值是0.1，CV是coefficient of variation。</p>
<p>coefficient of variation离散系数，是概率分布离散程度的一个归一化量度，定义为标准差 <span class="math-inline">\sigma</span> 和 均值 <span class="math-inline">\mu</span> 的比值。</p>
<p>对于MoE来说，确定激活的expert数之后，均值是固定的。如果expert的gating很不平衡，标准差就会很大，离散系数也会很大，使得 <span class="math-inline">L_{importance}</span> 变大。</p>
<p>但是这里还是有问题，虽然均衡的负载可以推导出 <span class="math-inline">L_{importance}</span> 较小的结论，但是 <span class="math-inline">L_{importance}</span> 较小却不能保证负载均衡。也就是说 <span class="math-inline">L_{importance}</span> 较小只是负载均衡一个必要不充分条件。</p>
<p>比如一个expert可能以很高的权重被分配到一个样本，而另一个expert可能以不太高的权重被分配到好几个样本。这种情况下对所有输入数据的gating权重进行求和，仍然可能呈现出均匀的表象（离散系数比较小），但这并不符合我们的要求。</p>
<p>为了解决这个问题，需要额外再加上一个损失 <span class="math-inline">L_{load}</span> 。这里就要用到添加在每个expert输出上的随机噪音了。</p>
<p>我们想要各个expert的负载均衡，也就是每个专家需要处理的样本数基本一致，但是分配到各个专家的样本数是个离散值，因此没有办法直接用于back propagation，而 <span class="math-inline">L_{load}</span> 就是对各个expert负载的一个平滑评估。</p>
<p>回想一下前面在设计MoE的时候，定义了 <span class="math-inline">H(x)</span> 为KeepTopK函数的输入</p>
<p><span class="math-inline">\begin{aligned}G(x)=Softmax(KeepTopK(H(x),k))\end{aligned} \<div class="math-display">\begin{aligned}H(x)<em>i=(x\cdot W_g)_i+StandardNormal()\cdot Softplus((x\cdot W</em>{noise})_i)\end{aligned} \</span></p>
<p>那么这里先定义一个 <span class="math-inline">kth_excluding(H(x),k,i)</span>，表示在除去 <span class="math-inline">H(x)</span> 中的第 <span class="math-inline">i</span> 个分量之后，排在第 <span class="math-inline">k</span> 大的值。基于这个，再定义 <span class="math-inline">P(x,i)</span> 为：固定其他分量已经选取好的noise，重新给第 <span class="math-inline">i</span> 个分量再添加一次noise，结果比 <span class="math-inline">kth_excluding(H(x),k,i)</span> 大的概率，公式如下</p>
<p><span class="math-inline">\begin{aligned}P(x,i)=Pr\Big((x\cdot W_g)<em>i+StandardNormal()\cdot Softplus((x\cdot W</em>{noise})_i)\&gt;kth_excluding(H(x),k,i)\Big)\end{aligned} \</span></p>
<p>通过这个noise，我们把“第 <span class="math-inline">i</span> 个专家是否处理这个输入”的离散值，变成“第 <span class="math-inline">i</span> 个专家处理这个输入的概率”这样一个平滑的估计，<span class="math-inline">P(x,i)</span> 就表示这个概率。这个概率可以简化写成</p>
<p><span class="math-inline">\begin{aligned}P(x,i)&amp;=\Phi\Big(\frac{(x\cdot W_g)<em>i-kth_excluding(H(x),k,i)}{Softplus((x\cdot W</em>{noise})_i)}\Big)\end{aligned} \</span></p>
<p>其中 <span class="math-inline">\Phi</span> 是标准正态分布的CDF。</p>
<p>接下来就可以把第 <span class="math-inline">i</span> 个expert的负载定义为</p>
<p><span class="math-inline">\begin{aligned}Load(X)<em>i=\sum</em>{x\in X}P(x,i)\end{aligned} \</span></p>
<p>有了每个expert的负载衡量，就可以和前面第一个负载均衡损失一样，计算新的负载均衡损失了</p>
<p><span class="math-inline">L_{load}(X)=w_{load}\cdot CV(Load(X))^2 \</span></p>
<p><span class="math-inline">w_{load}</span> 是手动设置的超参，实验的推荐值是0.1。</p>
<p>相比前面的 <span class="math-inline">L_{importance}(X)</span>，<span class="math-inline">Load(X)</span> 是对负载是否均衡更细粒度的评估。</p>
<p>论文中提到一个细节，在刚开始训练的时候，希望模型分配的expert尽量均衡，因此把 <span class="math-inline">W_g</span> 和 <span class="math-inline">W_{noise}</span> 都设为0，这样相当于没有信号，也没有噪音。</p>
<p>最终使用负载均衡之后的效果如下</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-64c99f350c0397e242b35ccb3c03b9a9_1440w.jpg" /><br />
使用这两个负载均衡损失之后，能达到接近完全平均分配的效果。</p>
<h3 id="实验">实验<a class="anchor-link" href="#实验" title="Permanent link">&para;</a></h3>
<ol>
<li>解决工程问题</li>
</ol>
<p>针对前面提出的一些工程问题，论文给出一些方案</p>
<p>（1）batch size减小</p>
<p>由于稀疏激活的原因，每个expert的batch size会变小。假设每次在n个expert中选择k个，模型训练的batch size为b，那么每个expert的batch size就是kb/n。论文通过以下这几种方法来提升每个expert的batch size：</p>
<ul>
<li>混合使用数据并行和模型并行。本来在使用数据并行的情况下，每个模型副本是异步处理各自的数据的。而这里做了优化，各个副本的batch是同步处理的，这样就可以把多个模型副本的batch组合起来。对于非MoE部分的参数，依然使用标准的数据并行机制；而对于每个expert，则在整个集群中只保留一个副本。如果模型分布在d个设备上，那每个expert就能得到一个kbd/n的batch size。</li>
<li>对于LSTM模型，在时间步上展开，就能把batch size提升相应的倍数。</li>
</ul>
<p>（2）集群通讯问题</p>
<p>另一个挑战就是平衡集群计算量和通讯量的关系。</p>
<p>对于每个expert来说，主要的通讯就是input和output的传输。而每个专家的主要计算量就是两个全连接层，大小分别为[input_size, hidden_size]和[hidden_size, output_size]。对于GPU来说，计算速度可能是通讯速度的1000倍，那我们就需要把计算量设计得足够大。最简单的做法就是把hidden_size提高，使得每个expert的内部计算量比通讯量大1000倍，以保证通讯不会成为训练的瓶颈。</p>
<ol>
<li>模型容量 &amp; 参数效率</li>
</ol>
<p>为了验证模型容量提升带来的收益，以及MoE模型的参数效率（即和dense模型同样推理计算量下能达到的效果），训练了包含4/32/256个expert的flat MoE模型，和包含256/1024/4096个expert的hierarchical MoE模型。每个expert大约是1M参数量，对于所有flat模型都是激活4个expert，而对于hierarchical MoE是每层gating激活2个。</p>
<p>效果如下图。左边的图显示，随着模型容量提升，测试的ppl有明显下降。右边的图将相近模型容量的dense模型和MoE模型的效果放在一起对比，可以看到MoE模型在相同模型容量下，效果更好</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-7421299902ee0d7a0e96cbe0502ef813_1440w.jpg" />  </p>
<ol>
<li>更大的模型</li>
</ol>
<p>前面几个模型训练用的数据量不是很大，模型最大也只有4B左右，训练不久就出现diminishing returns。</p>
<p>为了验证更大数据集 + 更大模型的收益，在100B token的语料上，分别训了包含32, 256, 1024，4096, 16384, 65536, 和131072个expert的MoE模型，最大的模型达到了137B的参数量。</p>
<p>各个模型对比如下表。整体来看，增加数据和模型容量，是可以继续获得提升的。</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-4a6fca8cd61812a165642d3cf817a0ad_1440w.jpg" /><br />
从这里还可以看出，在专家数量不太多时，提升专家数量效果有提升，但是收益会慢慢减小，甚至会出现专家数量太多，效果反而下降的情况。</p>
<ol>
<li>Expert Specialization</li>
</ol>
<p>按照MoE的设计思路，不同的专家应该学习到不同的子任务，但是实际上是否是这样呢？</p>
<p>论文里把模型中不同的专家分配到token拿出看，发现确实有比较强的specialization效果，不同的专家处理不同的内容，如下所示</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-3c103726216ff5705da6df64e238f0ba_1440w.jpg" />  </p>
<h2 id="gshard">GShard<a class="anchor-link" href="#gshard" title="Permanent link">&para;</a></h2>
<ol>
<li>简介</li>
</ol>
<p>2018年，随着Bert的发布，transformer结构彻底火了起来。2020年6月，Google发布《GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding》，把MoE用到了encoder-decoder结构的transformer模型上。MoE开始变成我们现在熟悉的样子了。</p>
<p>GShard这个工作做了很多的实验，训了很多规模巨大的MoE模型，最大的达到了600B。训练的一系列模型的参数如下表</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-5545840603e3234d7d7022e6ebbcdae5_1440w.jpg" /><br />
在expert数量的设计上，延续上面LSMT MoE工作的思路 -- expert越多，效果越好。（站在24年这个时间节点来看，太多的expert未必适合；但是也不能说这个思路一定错误，毕竟事物的发展是螺旋式的，就像ChatGPT出来之前大多数人都在魔改各种Bert，而GPT已经坐了几年冷板凳了。）</p>
<p>GShard论文中很大的篇幅在介绍工程实现和优化，这也是MoE模型训练最大的痛点。关于工程框架的内容比较硬核，因此这里不会展开讲太多，而是关注在模型算法层面上。</p>
<ol>
<li>模型设计</li>
</ol>
<p>先来看下模型设计。</p>
<p>Google在那段时间走的是encoder-decoder transfomer的技术路线，因此GShard也是基于encoder-decoder transfomer的模型结构。</p>
<p>GShard的模型设计是，在encoder和decoder中，每两层把其中一个FFN层替换成MoE层。对于总共有N层的模型，则有N/2个MoE层，如下图</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-28a240a2043418e84de1e59281f64ddb_1440w.jpg" /><br />
每层会选择最多top-2 expert来激活。为什么是最多，后面解释。</p>
<p>GShard在上面这篇LSTM MoE论文的基础上，改进了gating function和auxiliary loss function。</p>
<p>从公式来看，MoE层的具体计算如下</p>
<p><span class="math-inline">\begin{aligned} \mathcal{G}<em>{s,E}&amp; =\mathrm{GATE}(x_s)  \ \mathrm{FFN}_e(x_s)&amp; =wo_e\cdot\text{ReLU}(wi_e\cdot x_s)  \ y</em>{s}&amp; =\sum_{e=1}^E\mathcal{G}_{s,e}\cdot\mathrm{FFN}_e(x_s)  \end{aligned}\</span></p>
<p>其中 <span class="math-inline">x_s</span> 是MoE的输入token，<span class="math-inline">w_i</span> 和 <span class="math-inline">w_o</span> 分别是输入输出的线性变换矩阵。向量<span class="math-inline">\mathcal{G}_{s}</span> 就是gating function的输出。</p>
<p>GShard在gating function的设计上提出了两个要求：（1）负载均衡（2）高效扩展。</p>
<p>负载均衡和前面讲的一样，很好理解。而为什么要高效扩展，因为如果要对N个token分别进行E个expert的分配，在N能达到百万甚至千万级别，而E也有几百上千的情况下，就需要一个高效的分布式实现，以免其他计算资源等待gating function。</p>
<p>为了满足这些要求，gating function提出了以下机制</p>
<p>（1）专家容量 expert capacity</p>
<p>为了确保负载平衡，我们不希望有少量expert需要处理很多token，因此强制规定了每一个expert所负责处理的token数量有一个最大值，这个最大值就叫专家容量，在这里设置为2N/E，相当于平均分配的量。</p>
<p>这个expert capacity通过GATE(·)给每个expert维护一个计数器 <span class="math-inline">c_e</span> 来监控。如果一个token所选的两个专家当前处理量都已经超过设定的专家容量，那么这个token就不会被当前层的任何expert处理，而是直接通过残差链接透传到下一层。</p>
<p>（2）分组分配 Local group dispatching</p>
<p>给所有输入token分成了G组，不同的组并行处理，每个组相应地也把组内专家容量变成2N/EG。</p>
<p>这样做相当于在前向推理时，把大的batch拆分成小的batch，每个小的batch就是一个group。这样做的好处是通讯的时候（特别是all2all）只需要在每个group内进行就可以了，减少了通讯量。</p>
<p>而进行反向计算的时候这些group可以合起来一起用，相当于进行了gradient accumulation。</p>
<p>（3）辅助损失函数 Auxiliary loss</p>
<p>光设置专家容量并不能使得gating负载均衡，而且会导致大量溢出。参考前面LSTM MoE的工作，这里也定义了一个辅助损失函数，来帮助负载均衡。辅助损失函数设计如下</p>
<p><span class="math-inline">\ell_{aux}=\frac1E\sum_{e=1}^E\frac{c_e}S\cdot m_e \</span></p>
<p><span class="math-inline">S</span> 是token数，<span class="math-inline">E</span> 是专家数，<span class="math-inline">c_e</span> 是分配给第 <span class="math-inline">e</span> 个专家的token数，<span class="math-inline">m_e</span> 是第 <span class="math-inline">e</span> 个expert在 <span class="math-inline">S</span> 个token中获得的平均权重。</p>
<p>思路是，本来是要算 <span class="math-inline">\frac{c_e}S</span> 的平方的，但这是离散值不可导，因此把平方中的一个 <span class="math-inline">\frac{c_e}S</span> 换成了 <span class="math-inline">m_e</span> ， <span class="math-inline">m_e</span> 是第 <span class="math-inline">e</span> 个expert在 <span class="math-inline">S</span> 个token中获得的平均权重。在平均分配的情况下，这个loss达到最小。</p>
<p>相比前面的负载均衡损失，这个loss的设计就简单许多。</p>
<p>gating的整个算法如下</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-1c821b5993b3801cee328c3ba698676a_1440w.jpg" /><br />
（4）随机路由 Random routing</p>
<p>前面提到，每层会选择最多top-2 expert来激活，就是因为有随机路由的机制。直观来说，就是认为如果top-1专家的权重很高，而第二个专家的权重如果较小，那很有可能只用第一个专家就足够解决问题了。</p>
<p>随机路由的机制是top-1的专家永远会被激活，而第二个专家如果权重很小，就认为它可以被忽略。具体来说，会以与第二个专家的权重g2成比例的概率激活第二个专家。</p>
<ol>
<li>效果</li>
</ol>
<p>最后看一下模型在翻译任务上的效果</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-3f5766e40986db3faf1114ecb69d532f_1440w.jpg" />  </p>
<h2 id="switch-transformer">Switch Transformer<a class="anchor-link" href="#switch-transformer" title="Permanent link">&para;</a></h2>
<p>2022年4月，距离ChatGPT发布还有半年，Google发布了《Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity》（实际上2021年Google就提出Switch Transformer了）。</p>
<p>Switch Transformer和GShard一样，是encoder-decoder结构，基于T5开发的，具有1.6T的参数，2048个expert。</p>
<p>和前面的很多工作一样，Switch Transformer有一个出发点，那就是参数量越大，模型效果越好，并且可以通过稀疏激活来减少总计算量。</p>
<p>但是相比其他工作，Switch Transformer给出了一个更为具体的描述，那就是模型参数量可以是一个独立于总计算量的，单独的缩放轴。也就是说，在改变参数量的同时，（几乎）不改变训练和推理的计算量，就可以带来效果的提升。因此Switch Transformer关注在“同样的FLOPS/token的计算量”下，如何扩大模型，提升效果。</p>
<p>Switch Transformer所做的工作还是比较多的，包括：</p>
<p>（1）模型结构简化：简化了Transformer上的MoE架构，提出Switch Transformer架构。</p>
<p>（2）MoE to dense：把训出来的效果较好的MoE模型蒸馏到dense模型，在压缩MoE模型99%的参数的情况下，效果还是比直接训练dense模型好。</p>
<p>（3）训练和微调技术：</p>
<ul>
<li>首次使用bf16成功训练MoE模型</li>
<li>更适合MoE结构的模型初始化</li>
<li>增加的专家正则化，改善了稀疏模型的微调和多任务训练</li>
</ul>
<p>（4）训练框架：结合数据、模型和专家并行性，训练了超过1T参数的MoE模型。</p>
<p>（5）多语言：在多语言数据集上训练，发现101种语言效果普遍有提升。</p>
<p>（6）训练效率：在同样的FLOPS/token的计算量下，Switch Transformer模型收敛速度有数倍的提升。</p>
<h3 id="模型设计_1">模型设计<a class="anchor-link" href="#模型设计_1" title="Permanent link">&para;</a></h3>
<p>Switch Transformer的模型结构如下图，类似GShard，把transformer每层的FFN替换成MoE层</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-b47bb40179af97a5acd256dad42fea28_1440w.jpg" /><br />
Switch Transformer一个重要的改进点就是简化了gating function的做法（Switch Transformer论文里叫routing）。</p>
<p>之前的工作大多探索了选择k个expert的做法，而Switch Transformer则直接把gating简化为只选择1个expert，即k=1。这样的MoE层叫做Switch layer。</p>
<p>这样简化之后，routing的实现更简单，router的计算量小了，也减少了通讯量。</p>
<h3 id="负载均衡_1">负载均衡<a class="anchor-link" href="#负载均衡_1" title="Permanent link">&para;</a></h3>
<p>同GShard一样，Switch Transformer规定了一个专家容量expert capacity，来限制每个expert在一个batch里能处理的最大token数。</p>
<p>如果一个token被分配到了一个已经满载的expert，就会出现overflow，那这个token在本层就不会被处理，而是直接通过残差链接，透传给下一层。这点也同GShard一样。</p>
<p>在Switch Transformer，专家容量通过容量系数capacity factor来控制。</p>
<p><span class="math-inline">\text{expert capacity}=\left(\frac{\text{tokens per batch}}{\text{number of experts}}\right)\times\text{capacity factor}. \</span></p>
<p>一个大的capacity factor意味着每个expert能够处理更多的token，从而减少overflow情况的发生，但是计算量和通讯量的压力也会增大，所以这是一个需要权衡的参数。</p>
<p>下图给出了一个不同capacity factor下的例子</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-47e9ab8d8c706f03f8ae0e891f6c86b5_1440w.jpg" /><br />
那么如何设定expert capacity呢？</p>
<p>如果capacity factor为1的话，只有在完全平均分配的时候，才不会出现overflow的情况。而太大的capacity factor则可能造成算力和存储的浪费。</p>
<p>首先，实验中发现expert的数量和overflow的数量之间没有什么关系，所以在所有实验中，所有MoE和Switch Transformer模型都用128个专家。</p>
<p>不同的capacity factor对模型影响如下表。可以看到，大的容量系数相对来说能取得更好的效果（因为更少的overflow），但是相应地，大容量系数的模型处理速度就会慢一些。</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-28fcd2167d7a05e8d28ffa1f199185a5_1440w.jpg" /><br />
经验上，低的token丢弃率对模型的scaling很重要，想要训练超大规模的模型，就要解决这个问题。而通过负载均衡损失就可以确保良好的平衡，使得在使用较小容量系数的情况下，overflow尽量少，从而兼顾效果和计算速度。</p>
<p>关键问题来到负载均衡损失怎么设计。</p>
<p>给定 <span class="math-inline">N</span> 个expert，和包含 <span class="math-inline">T</span> 个token的batch <span class="math-inline">\mathcal{B}</span>，负载均衡损失是这么计算的</p>
<p><span class="math-inline">\begin{aligned}\text{loss}&amp;=\alpha\cdot N\cdot\sum_{i=1}^Nf_i\cdot P_i\end{aligned} \</span></p>
<p><span class="math-inline">f_{i}</span> 表示被分配到第 <span class="math-inline">i</span> 个expert的token数，这个不可导</p>
<p><span class="math-inline">\begin{aligned}f_i=\frac{1}{T}\sum_{x\in\mathcal{B}}\mathbb{1}{\text{argmax }p(x)=i}\end{aligned} \</span></p>
<p><span class="math-inline">P_i</span> 表示整个batch每个token分配给第<span class="math-inline">i</span> 个expert的概率的总和，这个可导</p>
<p><span class="math-inline">\begin{aligned}P_i=\frac{1}{T}\sum_{x\in\mathcal{B}}p_i(x).\end{aligned} \</span></p>
<p>这个损失的设计其实和GShard中的也是一样的。</p>
<p>在完美平均分配的情况下，<span class="math-inline">f</span> 和 <span class="math-inline">P</span> 这两个向量都是 <span class="math-inline">1/N</span>，这个时候负载均衡损失是最小的。</p>
<p><span class="math-inline">\alpha</span> 扫描了1e-5到1e-1，发现设为1e-2，已经足够大保持负载平衡，同时不过分影响模型收敛。</p>
<p>观察到 <span class="math-inline">\sum_{i=1}^N(f_i\cdot P_i)=\sum_{i=1}^N(\frac1N\cdot\frac1N)=\frac1N</span>，所以负载均衡loss还乘了个 <span class="math-inline">N</span>，这样可以保持无论使用多少个expert，在平均分配的情况下，loss都能保持相同的常数。</p>
<h3 id="实验_1">实验<a class="anchor-link" href="#实验_1" title="Permanent link">&para;</a></h3>
<ol>
<li>一些训练的trick</li>
</ol>
<p>（1）选择性地使用bf16</p>
<p>半精度训练会带来一些训练的不稳定。因此选择性地使用bf16，具体来说，routing function内部使用单精度，其他部分使用半精度，这样既不影响通讯，也能提高效果。</p>
<p>为什么选择在routing提高精度？因为softmax对误差特别敏感，exponential计算会极大放大输入中的rounding error，因此高精度对routing很重要。</p>
<p>（2）较小的参数初始化</p>
<p>从截断正态分布中抽取元素来初始化的模型参数，平均值 <span class="math-inline">\mu=0</span>，标准差<span class="math-inline">\sigma=\sqrt{s}/n</span>，其中s是超参，n是权重张量中的输入单元数量（e.g. fan-in）。</p>
<p>论文建议将默认的Transformer初始化尺度s=1.0减少10倍。这个方案在实验中既提高了质量又降低了训练不稳定性的可能性。初始化实验对比如下表</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-7a1640667e51321cfe07cc42d43c9940_1440w.jpg" /><br />
（3）增大dropout</p>
<p>由于Switch Transformer参数量很大，在微调的时候更容易过拟合，因此一个简单的方法就是增大dropout，效果如下</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-76ef17b2851cf0a3469d4c074d24c601_1440w.jpg" /><br />
可以看到大的dropout有效果，并且dense层保持0.1，只有expert层增大dropout效果更好。</p>
<ol>
<li>scaling</li>
</ol>
<p>对Switch Transformer结构预训练的scaling做了一些实验。</p>
<p>（1）Step-Basis</p>
<p>首先是验证在固定训练step的条件下，增大expert数量带来的提升，如下图所示。</p>
<p>左边是不同规模的模型在相同step下收敛的结果，可以看到在保持相同计算量的条件下，只通过增大专家数量来提升规模，就有明显的收益。右边则展示训练过程中，不同规模的模型在各个step下的效果。</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-da111e7667041e5f7cfe9f7dea3c50b7_1440w.jpg" /><br />
（2）Time-Basis</p>
<p>虽然Switch Transformer可以保持计算量不变的情况下提升模型规模，但是专家数量的增多会带来额外的通讯成本，所以即使训练的step数相同，实际的训练时间也不同。因此这里要回答的问题是，给定一个固定的训练时长，Switch Transformer是否相比dense模型仍有收益。</p>
<p>答案是肯定的。下图展示以训练时长为横轴，Switch Transformer和dense模型的效果对比。Switch Transformer收敛到dense模型最终效果的时间只有dense模型的1/7。</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-6916135555f6a69b375e51ace1e94bfb_1440w.jpg" /><br />
（3）和更大的dense模型对比</p>
<p>前面Switch Transformer和dense模型的比较，是基于相同计算量的前提。那么Switch Transformer是否具备超越更大规模dense模型的能力？</p>
<p>下图在Step-Basis和Time-Basis对比了64个专家的Switch Transformer和T5-Large。无论是相同step还是相同时间下，Switch Transformer都有明显优势。</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-cac33e6bafc73fd52dd4144930b0345b_1440w.jpg" />  </p>
<ol>
<li>SFT效果对比</li>
</ol>
<p>在GLUE和SuperGLUE等下游任务上微调，和dense模型对比。</p>
<p>对于各个模型，每两百步进行一次eval，选最好的效果，尽量保证公平。结果如下表，大部分任务都有明显的提升。</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-bea339b51b213e3d643a4b7b125c7940_1440w.jpg" />  </p>
<ol>
<li>模型蒸馏</li>
</ol>
<p>虽然Switch Transformer在相同计算量下效果更好，但是部署几百B甚至T级别的模型，还是不太方便，因此考虑把稀疏模型蒸馏到dense模型上来进行推理。</p>
<p>论文中给出了几个蒸馏的技巧：</p>
<ul>
<li>初始化的时候，把Switch Transformer模型中的非稀疏部分用于初始化dense模型</li>
<li>蒸馏所用的label，25%来自教师模型，75%来自ground truth，加权求和</li>
</ul>
<p>预训练模型的蒸馏效果如下，相比无蒸馏训练的dense模型，把同样计算量的稀疏模型蒸馏到dense模型，dense模型大约能获得Switch Transformer提升部分30%的增益。</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-a587cd3db6e83e5b07336f54be4feba7_1440w.jpg" /><br />
更进一步，用不同规模的稀疏模型下进行蒸馏，结果如下表，可以实现高达99%的压缩率</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-99054a78eccc1fe253d950b81e945a7e_1440w.jpg" /><br />
除了预训练模型，微调模型也可以蒸馏，效果如下，在SuperGLUE也有一定的提升</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-b9e4f91a58dc4aed9d4758205ee5f2c8_1440w.jpg" />  </p>
<h2 id="glam">GLaM<a class="anchor-link" href="#glam" title="Permanent link">&para;</a></h2>
<ol>
<li>简介</li>
</ol>
<p>2021年12月Google发表了《GLaM: Efficient Scaling of Language Models with Mixture-of-Experts》，训练出最大参数量为1.2T，每层包含64个专家，每个token激活参数量为96.6B的MoE模型。</p>
<p>相比Switch Transformer，GLaM的训练数据量要大得多，达到了1.6T token。</p>
<p>下表是论文中给出的，当时一些大规模模型的对比</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-9df297e975271469d0c3618eae967269_1440w.jpg" /><br />
虽然模型总参数量比GPT-3（175B）大很多，但是训练成本却比GPT-3低很多，推理速度也更快，而且在多个NLP任务上的效果都超越了GPT-3，如下所示。</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-c3d29a53309df589fefb5339a723b156_1440w.jpg" /><br />
<img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-7de874c1741b748e82cba1e52df6a238_1440w.jpg" />  </p>
<ol>
<li>模型设计</li>
</ol>
<p>模型设计上，和Switch Transformer一样，每两层把一个FFN替换成MoE层。但是和Switch Transformer不同，GLaM用回了每次激活两个expert的方案，模型结构如下图。</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-0997a228dffd8ab13d5e6e4b9ccd3cfe_1440w.jpg" /><br />
除此之外，模型在结构上海做了一些其他改动：</p>
<p>（1）位置编码</p>
<p>使用XLNET的相对位置编码。</p>
<p>（2）激活函数</p>
<blockquote>
<p>In the non-MoE Transformer feed-forward sub-layers, we replace the first linear projection and the activation function with the Gated Linear Unit，which computes the component-wise product of two linear transformation of the input, followed by a Gaussian Error Linear Unit.</p>
</blockquote>
<ol>
<li>实验</li>
</ol>
<p>训练中的一些trick：</p>
<p>（1）参考《Lingvo: a modular and scalable framework for sequence-to-sequence modeling》，在梯度出现NaN或者Inf的时候就跳过那一步更新。</p>
<p>（2）如果在BP更新的时候遇到NaN或者Inf，则重新加载更早的checkpoint并跳过有问题的数据来避免NaN或者Inf。</p>
<p>论文训了一系列模型来探索MoE，这些模型的设置如下表</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-58b4cc897684aefcf161ade7c54813d1_1440w.jpg" /><br />
GLaM和dense模型的评测结果如下</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-24f7a7092f340a048f5b0572b406ebb0_1440w.jpg" /><br />
可以看到GLaM MoE的有效参数效率一致高于dense模型。</p>
<h2 id="st-moe">ST-MoE<a class="anchor-link" href="#st-moe" title="Permanent link">&para;</a></h2>
<p>2022年2月，Google发表了《ST-MOE: DESIGNING STABLE AND TRANSFERABLE SPARSE EXPERT MODELS》。ST-MoE可以说不仅仅是一个MoE工作，对于模型结构、工程实现、训练策略等都做了很多分析，可以说是MoE的必读论文。</p>
<p>ST-MoE最大模型包含269B总参数量，和与32B dense模型相当的激活计算量。论文中把模型称为称为Stable Transferable Mixture-of-Experts，或者ST-MoE-32B。</p>
<p>在MoE层的使用上，ST-MoE比Switch Transformer更“节省”一点，每四层才替换1个MoE层。</p>
<p>论文中主要训了两个规模的ST-MoE模型，分别有4B和269B的总参数量。ST-MoE以及其他用于对比的模型参数如下表</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-cd2b5619db78203472143b3f554be5bd_1440w.jpg" />  </p>
<h3 id="稳定性与效果分析">稳定性与效果分析<a class="anchor-link" href="#稳定性与效果分析" title="Permanent link">&para;</a></h3>
<p>论文通过对乘性操作、噪音和裁剪这几个内容进行探索，来指导模型的设计。</p>
<ol>
<li>乘性操作对模型稳定性和效果的影响</li>
</ol>
<p>论文首先研究了乘性操作对模型的训练稳定性和最终效果的影响。</p>
<p>之前已经有一些工作表明更多的乘法对模型效果有收益。</p>
<blockquote>
<p>Some architectural improvements involve more multiplications than additions or do not sum many items at once</p>
</blockquote>
<p>（1）GELU Gated Linear Units (GEGLU)</p>
<p>第一个例子是关于激活函数的。GLU是一个对两个输入向量进行component-wise相乘的操作，之后被扩展成GELU-Linear FFN变体，用于替换transformer中的ReLU FFN变体，其计算如下</p>
<p><span class="math-inline">\begin{aligned}FFN_{GEGLU}(x,W,V,b,c)=GELU(xW+b)\odot(xV+c)\end{aligned} \</span></p>
<p>这样在一些其他工作里已经被证明了对模型效果有提升。</p>
<p>（2）RMSNorm</p>
<p>第二个例子是RMSNorm中的缩放参数，也就是下面公式的 <span class="math-inline">g</span>。</p>
<p><span class="math-inline">y_i=\frac{x_i}{\sqrt{\frac1d\sum_{i=1}^dx_i^2}}\cdot g_i \</span></p>
<p>ST-MoE针对GEGLU和RMSNorm这两个乘性操作，做了实验，结果如下表。</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-e41d291e11705ff4febed36a11c8eab1_1440w.jpg" /><br />
发现移除乘性操作可以使模型稳定性更好（训练中发散的情况减少），但是最终效果变差了。</p>
<p>（3）增加dense层</p>
<p>ST-MoE还验证了在expert层增加更多dense层的效果。结果发现增加更多的乘法交互（增加dense层），可以在带来效果收益的同时，基本不影响推理速度，如下表所示。</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-85212305a1f9ba29c6884df7862a4741_1440w.jpg" /><br />
（4）增加一个bias</p>
<p>在FFN层的第一个矩阵乘法后面增加一个可学习的bias B，分别通过加法和乘法加入</p>
<p><span class="math-inline">\text{FFN}<em>{\text{GEGLU}}+\text{Add Bias}(x)=[(\text{GELU}(xW</em>{11})\odot xW_{12})+B]W_2 \</div>\mathrm{FFN}<em>{\mathrm{GEGLU}}+\mathrm{Mult~Bias}(x)=[(\mathrm{GELU}(xW</em>{11})\odot xW_{12})\odot B]W_2 \</span></p>
<p>乘法的收敛速度更快，效果也更好。</p>
<p>上面这些实验显示，后续在模型效果的探索方向可以往多使用乘性操作去考虑。</p>
<ol>
<li>noise对模型稳定性和效果的影响</li>
</ol>
<p>接下来ST-MoE探索了“噪音可以提升模型稳定性”的假设。</p>
<p>通过input-jitter，给router的输入logits乘以一个在[1e-2, 1e2]之间的均匀随机变量来添加噪音。</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-91e7793e5ef4aeb73c8cd3ba4295e566_1440w.jpg" /><br />
结果是增加noise之后，有助于让模型的收敛更加稳定，但是对模型最终效果有负面影响。</p>
<p>这里论文还提到，小模型上的结果不一定能直接推广到更大的模型上，比如在小模型上稳定的配置，在大模型就可能就不稳定了。因此还是需要在大模型上也进行充分实验。</p>
<ol>
<li>限制激活值和梯度值对模型稳定性和效果的影响</li>
</ol>
<p>对activation和gradient进行限制是目前广泛应用的提升模型训练稳定性的手段。在反向传播过程中，通过裁剪梯度的范数来缓解梯度爆炸，就是一种常用的限制手段。</p>
<p>但是在ST-MoE训练269B的大规模模型时，发现裁剪会使得模型收敛的效果很差。</p>
<p>为了解决这个问题，ST-MoE在训练中引入了router z-loss，形式如下。</p>
<p><span class="math-inline">L_z(x)=\frac{1}{B}\sum_{i=1}^B\left(\log\sum_{j=1}^Ne^{x_j^{(i)}}\right)^2 \</span></p>
<p><span class="math-inline">B</span> 是token的数量，<span class="math-inline">N</span> 是专家数，<span class="math-inline">x\in\mathcal{R}^{B\times N}</span> 是router的输入。</p>
<p>z-loss会对进入router的较大的logits值进行惩罚，以达到尽量减少进入指数函数的较大误差的目的。什么意思呢？后面来解释，先看下使用z-loss的效果。</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-1be867465e6fbe7a034780539453812e_1440w.jpg" /><br />
ST-MoE认为，在模型训练过程中，由于精度不足或者其他问题，会产生很大的值，从而引入误差。而对梯度进行裁剪是在误差发生之后，并且裁剪本身也造成了数据的不连续性，某种程度上，裁剪本身也是一种误差。相反地，z-loss自然地鼓励模型产生较小的对数值，因此可以更精确地建模。</p>
<p>z-loss乘以一个权重超参 <span class="math-inline">c_z</span> 加入到模型训练的总损失中，如下式所示。</p>
<p><span class="math-inline">L_{tot}=L_{CE}+c_BL_B+c_zL_Z \</span></p>
<p>ST-MoE经过实验，选择了<span class="math-inline">c_z=0.001</span>。</p>
<p><span class="math-inline">L_B</span> 是 auxiliary load balance loss负载均衡损失，ST-MoE这里使用了和GShard/Switch Transformer所用的相同的损失计算，这里回顾一下：</p>
<p><span class="math-inline">\begin{aligned}\text{loss}&amp;=\alpha\cdot N\cdot\sum_{i=1}^Nf_i\cdot P_i\end{aligned} \<div class="math-display">\begin{aligned}f_i=\frac{1}{T}\sum_{x\in\mathcal{B}}\mathbb{1}{\text{argmax }p(x)=i}\end{aligned} \</div>\begin{aligned}P_i=\frac{1}{T}\sum_{x\in\mathcal{B}}p_i(x).\end{aligned} \</span></p>
<p><span class="math-inline">N</span> 是专家数， <span class="math-inline">\mathcal{B}</span> 包含 <span class="math-inline">T</span> 个token的batch。<span class="math-inline">f_{i}</span> 表示被分配到第 <span class="math-inline">i</span> 个expert的token数，这个不可导；<span class="math-inline">P_i</span> 表示整个batch每个token分配给第<span class="math-inline">i</span> 个expert的概率的总和，这个可导。</p>
<ol>
<li>数据精度对训练效率和训练效果的影响</li>
</ol>
<p>目前大部分的大模型训练都使用混合精度训练：模型权重以float32格式存储以进行梯度更新，然后在正向和反向传播的矩阵乘法中转换为bfloat16；此外，所有激活值都以bfloat16存储和操作，而allreduce通信可以在bfloat16或float32数值精度中进行。</p>
<p>对于ST-MoE-32B的训练，allreduce的数值使用半精度可以加速训练，然而这也会使训练变得不稳定，因此ST-MoE保持allreduce的数值精度为float32。</p>
<p>bfloat16和float32在不同范围的舍入误差如下表所示</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-c140c39092e6ceecae1788b434f092d2_1440w.jpg" /><br />
可以看到，表达的数值越大，舍入误差越大。而z-loss限制了数值大小，也就将误差值限制在比较小的范围。</p>
<p>MoE模型天生对舍入误差敏感，因为它们由于router的使用而有更多的指数函数，而指数函数会将小的输入误差放大很多，这就加剧舍入误差所导致的训练不稳定。</p>
<p>另外，ST-MoE有一个策略：只有当排第二的专家的权重大于等于第一的专家的1/5时，token才会被路由到其第二位专家，否则第二个专家就会被忽略。</p>
<p>因此虽然舍入误差不会改变softmax运算中各个概率的排序，但它确实会影响MoE中第二个专家的激活。</p>
<h3 id="模型设计_2">模型设计<a class="anchor-link" href="#模型设计_2" title="Permanent link">&para;</a></h3>
<p>dense模型的设计有scaling law进行指导，但是MoE模型的设计比dense模型多出几个要考虑的点：</p>
<p>（1）使用多少个expert</p>
<p>（2）怎么routing</p>
<p>（3）专家容量系数怎么定</p>
<p>（4）硬件的影响</p>
<p>（这里提到MoE模型的scaling law工作：《Unified scaling laws for routed language models》，可以了解一下）</p>
<ol>
<li>使用多少个expert</li>
</ol>
<p>ST-MoE认为，从以往的经验来看，在总专家数量较少的情况下（如8/16/32），提升专家数量，能有收益。但是在特别稀疏的情况下（如激活专家数量&lt;1%），或者总专家数较大（比如&gt;256）之后，提升专家数量收益就很小了。</p>
<p>从另一个角度来看，如果一个计算核心使用&gt;1个专家，那么就会出现比较大的加载参数张量的成本，因此建议每个计算核心使用&lt;=1个专家。</p>
<ol>
<li>routing和capacity factor</li>
</ol>
<p>论文做了一系列实验来探索capacity factor的选择，如下表所示</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-18699e0be1471dc5cb08779971306be4_1440w.jpg" /><br />
从这些实验中得到几个结论：</p>
<p>（1）训练和推理的capacity factor增大都会有收益</p>
<p>（2）如果硬件资源足够，推理的capacity facotr可以设得比训练的时候大，会有进一步提升</p>
<p>（3）激活的expert数量提升会有收益，但是收益随着capacity factor提升而越来越小</p>
<p>当然，选择capacity factor还要看硬件的特性，如果通讯很快，可以适当增大capacity factor，否则就不能选择太大的。</p>
<p>下表展示了不同capacity factor对推理速度的影响</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-c8ec4d49d5cc6d333457c11926c55134_1440w.jpg" />  </p>
<h3 id="实验_2">实验<a class="anchor-link" href="#实验_2" title="Permanent link">&para;</a></h3>
<ol>
<li>ST-MoE效果</li>
</ol>
<p>ST-MoE-32B在下游任务上和以往最佳结果对比如下表，ST-MoE-32B刷新了超过一半任务的最佳效果</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-5b738602c7bf66ae22dfeb2994f54685_1440w.jpg" />  </p>
<ol>
<li>Expert Specialization</li>
</ol>
<p>论文还对各个专家的专业化进行了追踪，发现decoder中几乎没有专业化的迹象，各种类型的token近乎随机分配给不同的专家。而在encoder中则表现出了高度专业化的特征，如下表</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-ba65dc1be729ca0ca44b4b1a93fbbf8a_1440w.jpg" /><br />
此外，还发现在多语言的模型的encoder中，专业化的情况并不想原先预想那样，按不同语言划分，而是每个专家都会处理一种语言的一部分token，如下表</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-a1229ff63794222a8f2a76f9374dfbdc_1440w.jpg" />  </p>
<h2 id="deepseekmoe">DeepseekMoE<a class="anchor-link" href="#deepseekmoe" title="Permanent link">&para;</a></h2>
<p>2024年1月，幻方量化(下的独立机构深度求索)开源了DeepseekMoE，是国内首个开源的MoE大模型。幻方还发布了论文《DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models》，给出了一些DeepSeekMoE的细节内容，颇为实在了。</p>
<p>DeepSeekMoE在其他MoE工作的基础上，进一步给出了2个模型设计的主要思路：</p>
<p>（1）对expert的粒度进行细分，以提供更多样的expert激活组合；</p>
<p>（2）对expert的类型进行区分，从所有expert中保留一部分作为shared expert共享专家，这部分专家对所有输入都保持激活。</p>
<p>这样的做法可以帮助每个expert达到更高程度的专业化(specialization)的水平，更好地学习不同的专业知识。</p>
<p>DeepSeekMoE先在2B的较小MoE模型上进行了充分的实验，然后把方案应用到16B参数的MoE模型上，并获得了较好的效果。其中DeepSeekMoE-16B不需要量化就可以在40GB显存的设备上运行。</p>
<p>DeepSeekMoE-2B模型具有和稠密2B模型相当的性能，而DeepSeekMoE-16B则具有和7B稠密模型相当的性能，且计算量仅为稠密模型的40%。</p>
<p>DeepSeekMoE-16B的参数效率相比稠密模型有明显的优势，如下图所示</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-f515f9747a9a886f4477ada593a5d8b9_1440w.jpg" /><br />
并且DeepSeekMoE-2B和16B模型都开源了。</p>
<p>在前面实验的基础上，幻方还训练了DeepSeekMoE-145B的超大MoE模型，具有和稠密的DeepSeek-67B模型相当的表现，但计算量更小。这个后续也有机会放出来。</p>
<h3 id="模型设计_3">模型设计<a class="anchor-link" href="#模型设计_3" title="Permanent link">&para;</a></h3>
<p>MoE，mixture of expert，顾名思义，一个最初始的motivation就是让不同expert学习不同的内容，然后再混合起来。</p>
<p>比如最上面提到的1991年的工作里，就是让不同的expert学习不同的元音特征，以此提升特征提取的准确率。</p>
<p>但是当前大部分的MoE架构都会遇到“knowledge hybridity”和“knowledge redundancy”的问题，即知识的杂糅和冗余：</p>
<p>（1）知识冗余</p>
<p>有些基础的常识在不同的领域都需要用到，每个expert就都会学一点，这样这些常识就被多个expert重复学习了。</p>
<p>（2）知识杂糅</p>
<p>在expert数量不够多的情况下，一个expert就可能要负责学习多个领域的内容。以学习高中知识为例，在只有两个expert的时候，只能一个expert学习理科知识，另一个学习文科知识；当我们有8个expert的时候，不同expert就可以分别学习语文、英语、历史、地理、物理、生物、化学、数学知识。显然后者所学知识的专业化程度更高。</p>
<p>知识的杂糅和冗余阻碍了专家专业化(expert specialization)的程度，也就阻碍了模型达到MoE结构理论上限性能。</p>
<p>我们期望每个expert能够学习到non-overlap &amp; foucusd knowledge的知识。</p>
<p>针对上面的问题，DeepSeekMoE的架构设计有2个主要策略：</p>
<p>（1）Fine-Grained Expert Segmentation</p>
<p>参数总量不变的情况下，将expert分成更细的粒度（每个expert更小）。这样可以带来更灵活的激活组合，让每个expert可以有更强的specialization。比如原本是16个expert选择激活2个，那么总的组合数是120种；如果把每个expert缩小为原来的1/4，那在总参数量和激活数量不变的情况下，是64个expert选择激活8个，那么总的排列组合数就是 <span class="math-inline">\binom{64}8=4,426,165,368</span> ，排列组合数比原来多了很多。</p>
<p>（2）Shared Expert Isolation</p>
<p>把部分expert分离出来，保持永远激活。我们期望这部分专家能够学到在多个领域间都通用的common knowledge。这样的策略同样可以使得其他expert能够提高专业化的程度，并且减少不同expert间的知识冗余。还是以学习高中知识为例，数学、物理和化学都需要算术能力，如果让学这三个领域的expert都学习算术技能，就会有冗余；我们可以把通用算术的技能剥离出来，由一个助手专门负责算术任务，相当于给他们发了一个计算器，这样学习数学、物理和化学的expert就能把更多的精力放在专业知识上，也就能达到更好的专业化效果。</p>
<p>下图展示了在传统MoE结构上增加Fine-Grained Expert Segmentation和Shared Expert Isolation策略的设计</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-c7d1e207a6e263ef5801c6115675d803_1440w.jpg" /><br />
（expert isolation的思路最早可以追溯到2022年1月发表的《DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale》，这里就不展开了。）</p>
<p>假设传统的MoE模型每层的expert数量为 <span class="math-inline">N</span>，激活expert数为 <span class="math-inline">K</span>，DeepSeekMoE使用的细粒度expert大小为原来的 <span class="math-inline">1/m</span>，那DeepSeekMoE每层就有 <span class="math-inline">mN</span> 个expert，激活的expert数量为 <span class="math-inline">mK</span> 。假设 <span class="math-inline">T</span> 为输入长度，<span class="math-inline">L</span> 为模型层数，<span class="math-inline">e_i^l</span> 表示第 <span class="math-inline">i</span> 个expert，DeepSeekMoE可以公式化为以下表示（忽略了layernorm）</p>
<p><span class="math-inline">\mathbf{u}<em>{1:T}^l=\text{Self-Att}\left(\mathbf{h}</em>{1:T}^{l-1}\right)+\mathbf{h}<em>{1:T}^{l-1} \<div class="math-display">\mathbf{h}_t^l=\sum</em>{i=1}^{mN}\left(g_{i,t}\text{ FFN}<em>i\left(\mathbf{u}_t^l\right)\right)+\mathbf{u}_t^l \</div>g</em>{i,t}=\begin{cases}s_{i,t},&amp;s_{i,t}\in\text{Topk}({s_{j,t}|1\leqslant j\leqslant mN},mK)\0,&amp;\text{otherwise,}\end{cases} \<div class="math-display">s_{i,t}=\mathrm{Softmax}_i\left({\mathbf{u}_t^l}^T\mathbf{e}_i^l\right) \</span></p>
<h3 id="负载均衡_2">负载均衡<a class="anchor-link" href="#负载均衡_2" title="Permanent link">&para;</a></h3>
<p>如之前工作反复提及的，如果任由MoE模型自主学习gating，可能会遇到两个问题</p>
<p>（1）routing collapse：专家分配的不均衡，也就是gating倾向于总是选择特定的少量expert，并且这种情况还会自我增强。</p>
<p>（2）计算效率问题：多设备间，不平衡的负载可能会成为计算效率的瓶颈。</p>
<p>针对routing collapse的问题，DeepSeekMoE引入一个expert-level balance loss，如下所示</p>
<p><span class="math-inline">\begin{aligned} \mathcal{L}<em>{\mathrm{ExpBal}}&amp; =\alpha_1\sum</em>{i=1}^{N'}f_iP_i \end{aligned}\</div>\begin{aligned} f_{i}&amp; =\frac{N^{\prime}}{K^{\prime}T}\sum_{t=1}^T\mathbb{1}(\text{Token }t\text{ selects Expert }i) \end{aligned}\<div class="math-display">\begin{aligned} P_{i}&amp; =\frac1T\sum_{t=1}^Ts_{i,t}  \end{aligned}\</span></p>
<p><span class="math-inline">\alpha_1</span> 叫做expert-level balance factor，是人工设定的超参。</p>
<p>而 <span class="math-inline">f_i</span> 和 <span class="math-inline">P_i</span> 和Switch Transformer里的设定基本一样。</p>
<p>在Switch Transformer里， <span class="math-inline">f_i</span> 表示分配到第 <span class="math-inline">i</span> 个expert的token数量。在DeepSeekMoE这里也是一样的含义，只是多乘了一个系数 <span class="math-inline">N'/K'</span> ，其中 <span class="math-inline">N'=mN-K_s</span>，<span class="math-inline">K'=mK-K_s</span>，<span class="math-inline">K_s</span> 是划分出来的共享expert的数量。这个系数是个常数，可以拿到求和符号外面，这样DeepSeekMoE里的 <span class="math-inline">f_i</span> 就和Switch Transformer里的完全一样了。</p>
<p><span class="math-inline">N'/K'</span> 这个系数可以使得在使用不同的数量的expert时，在完美平均分配的情况下，负载均衡loss都是相同的常数。</p>
<p><span class="math-inline">P_i</span> 表示所有每个token分配给第 <span class="math-inline">i</span> 个expert的权重的总和，和Switch Transformer里的含义一样。</p>
<p>注意这里 <span class="math-inline">f_i</span> 是不可导的，<span class="math-inline">P_i</span> 是可导的。</p>
<p>针对多设备间负载均衡的问题，DeepSeekMoE引入一个device-level balance loss，如下所示</p>
<p><span class="math-inline">\begin{aligned} \mathcal{L}<em>{\mathrm{DevBal}}&amp; =\alpha_2\sum</em>{i=1}^Df_i'P_i' \end{aligned}\</div>\begin{aligned} f_i^{\prime}&amp; =\frac1{|\mathcal{E}<em>i|}\sum</em>{j\in\mathcal{E}<em>i}f_j \end{aligned}\$$\begin{aligned} P</em>{i}^{\prime}&amp; =\sum_{j\in\mathcal{E}_i}P_j \end{aligned}\</span></p>
<p><span class="math-inline">\alpha_2</span> 叫做device-level balance factor，是人工设定的超参。</p>
<p><span class="math-inline">\mathcal{E}_i</span> 指第 <span class="math-inline">i</span> 个设备。</p>
<p>device-level balance loss形式上和expert-level balance loss一样，只是 <span class="math-inline">f_i</span> 和 <span class="math-inline">P_i</span> 对应的对象从单个expert变成单个设备了。</p>
<p>当我们的目标是缓解计算瓶颈时，我们不需要强制执行expert间的均匀分配，而只需确保设备之间计算量的平衡。比如我们每层有64个expert，均匀分布在8个设备上，我们只需要每个设备处理的token数平衡即可，在设备内部即使所有token都是同一个expert处理的，依然能满足设备间负载平衡的要求。</p>
<p>相比expert间严格的负载平衡，只要求设备间的平衡是更松的限制条件，这样缓解了因为过度的负载平衡而损害模型性能的问题。</p>
<h3 id="实验_3">实验<a class="anchor-link" href="#实验_3" title="Permanent link">&para;</a></h3>
<ol>
<li>小规模模型验证</li>
</ol>
<p>为了验证以上策略的有效性，先拿100B token的语料数据在DeepSeekMoE-2B模型做实验。词表也是通过BPE在语料上训练的8k词表，后面训练更大规模模型的时候再扩大词表。</p>
<p>DeepSeekMoE-2B模型参数初始化方差为0.006，使用multi-head attention，前向激活参数量约0.3B，具体参数如下表</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-2ed6a71bf898e0dc80d3e215b7b97fe5_1440w.jpg" /><br />
relative expert size指的是DeepSeekMoE所用的细粒度expert的大小和正常FFN层大小的比值。</p>
<p>训练的具体参数设置如下</p>
<table>
<thead>
<tr>
<th>属性</th>
<th>数值</th>
</tr>
</thead>
<tbody>
<tr>
<td>optimizer</td>
<td>AdamW</td>
</tr>
<tr>
<td>adam_beta_1</td>
<td>0.9</td>
</tr>
<tr>
<td>adam_beta_2</td>
<td>0.95</td>
</tr>
<tr>
<td>adam_weight_decay</td>
<td>0.1</td>
</tr>
<tr>
<td>warmup schedule</td>
<td>linear</td>
</tr>
<tr>
<td>warmup step</td>
<td>2000</td>
</tr>
<tr>
<td>max lr</td>
<td>1.08e-3</td>
</tr>
<tr>
<td>dropout</td>
<td>0</td>
</tr>
<tr>
<td>sequence length</td>
<td>2k</td>
</tr>
<tr>
<td>batch size</td>
<td>2k</td>
</tr>
<tr>
<td>total step</td>
<td>25,000</td>
</tr>
</tbody>
</table>
<p>其他训练细节：</p>
<ul>
<li>所有expert放在单个GPU上，没有使用device-level balance loss</li>
<li>expert-level balance factor设为0.01</li>
<li>训练到80%的时候，学习率乘以0.316，训练到90%的时候，再乘以0.316</li>
</ul>
<p>使用相同的100B训练数据，训了DeepSeekMoE-2B，在包含语言模型和下游任务的benchmark上和其他4个模型做对比：dense，hash layer（也是一种MoE模型，来自《Hash layers for large sparse models》），Switch Transformer，GShard。各模型参数和在benchmark的得分如下表 </p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-b30dc56c3a2f7de665dd84c1a5802ce6_1440w.jpg" /><br />
可以得到几个结论：</p>
<ul>
<li>更大的模型参数量和稀疏的架构，使得Hash Layer和Switch Transformer和具有同样激活参数的dense模型相比，有明显的优势。</li>
<li>同样的模型参数下，GSshard比Hash Layer和Switch Transformer有更多激活参数，效果也更好</li>
<li>同样的模型参数和激活参数下，DeepSeekMoE效果比GShard有明显优势。</li>
</ul>
<p>为了进一步探索DeepSeekMoE架构带来的收益，提升了dense模型和GShard模型的激活参数，直到效果和DeepSeekMoE-2B差不多。</p>
<p>结果dense模型和GShard模型需要分别扩大到16倍和1.5倍的参数量，才能达到DeepSeekMoE-2B相近的效果，如下表所示</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-a174a58d010921055cea09dd413ddfb2_1440w.jpg" /><br />
DeepSeekMoE的优势在更大规模的情况下，依然成立。训了DeepSeekMoE-13B, 对比参数量提升至1.2和1.5倍的GShard，DeepSeekMoE-13B依然能match，具体如下表</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-78eb0d561f909266a333856eaa830949_1440w.jpg" />  </p>
<ol>
<li>DeepSeekMoE架构消融实验</li>
</ol>
<p>针对DeepSeekMoE架构的两个主要设计，shared expert和fine-grained expert进行消融实验。使用不同数量的共享专家和不同粒度的expert进行效果对比，结果如下图。</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-ee511b1da09a6dfbfa5b5ae850cc074c_1440w.jpg" /><br />
（1）对比蓝色和橙色，可以看到增加共享专家带来了收益</p>
<p>（2）绿色和红色在橙色的基础上进一步把专家颗粒分得更细，效果进一步提升</p>
<p>（3）共享专家和路由专家的比例：在总共64个expert的情况下，对比了1/2/4个共享专家的情况，结果并没有显著差别，在pile上的loss分别是1.808,1.806,1.811。最终选择了共享专家和激活路由专家1:3（2+6）的比例。</p>
<ol>
<li>expert specialization的分析</li>
</ol>
<p>通过实验来验证DeepSeekMoE中expert specialization的优化。</p>
<p>（1）前面实验看到DeepSeekMoE-2B和1.5倍参数量的GShard模型效果相当。在这个基础上，通过禁用不同数量的top专家，而只能从次优的专家中选择进行回答。</p>
<p>实验结果如下</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-5519741d9b7504030cf3bb1d8110f2ed_1440w.jpg" /><br />
发现DeepSeekMoE的损失更大，说明DeepSeekMoE每个专家的专业化程度更好，必要性更高。</p>
<p>（2）另外，通过禁用DeepSeekMoE的共享专家，而额外激活一个路由专家，发现loss也大大提升。这个结果突出了共享专家的关键功能，并表明共享专家捕捉到了与路由专家不共享的基本且重要的知识，使得它无法被路由专家替代。</p>
<p>（3）DeepSeekMoE只激活更少专家，也能和GShard达到相同水平，这一观察结果支持了DeepSeekMoE可以更准确和高效地获取所需知识的观点。</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-ff8a2f2853bb4b830805afb5e6fc72ad_1440w.jpg" /><br />
此外还从零训了一个只用1个共享专家和3个激活专家的2b模型（正常是2个共享专家+6个激活专家），也比GShard好，说明DeepSeekMoE结构的有效参数效率更高</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-4f713af1040a345e8c85dc0c08b7edf4_1440w.jpg" />  </p>
<ol>
<li>DeepSeekMoE-16B</li>
</ol>
<p>DeepSeekMoE-16B模型使用了2T数据训练（和LLAMA2-7B对齐）训练，并使用了100k的词表。其他参数如下表所示</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-2ed6a71bf898e0dc80d3e215b7b97fe5_1440w.jpg" /><br />
论文中提到，除了第一层以外，其他层都使用了MoE层。</p>
<p>第一层不使用MoE是因为观察到第一层的负载均衡loss在训练中收敛得特别慢。</p>
<p>DeepSeekMoE-16B每层有64个专家，其中有2个作为共享专家保持永远激活，加上6个通过gating function选择激活的，每个token共使用8个专家。每个token会激活16.4B中的2.8B参数。</p>
<p>这里没有把专家的dimension再减小，是因为如果专家太小，计算效率就下降得太厉害。</p>
<p>训练中使用的其他设置：</p>
<ul>
<li>lr = 4.2e-4</li>
<li>训练进行到80%和90%的时候，lr都会缩小到0.316倍</li>
<li>batch size = 4.5k，训练窗口长度是4k，因此每个batch有18M token，2T数据差不多是10.6w步</li>
<li>使用了pipeline parallelism</li>
</ul>
<p>expert level balance loss的系数设得比较小，0.001，因为实验中发现设得再大并不能进一步优化负载平衡，反而会损害模型效果。</p>
<p>DeepSeekMoE-16B和DeepSeek-7B模型的对比如下</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-667a6a711338c4b2159192d1c7460a1f_1440w.jpg" /><br />
DeepSeekMoE-16B和LLAMA2-7B模型的对比如下</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-71c5d095ba4d6937d24445e0982ec67b_1440w.jpg" />  </p>
<ol>
<li>DeepSeekMoE-145B</li>
</ol>
<p>幻方还用245B的token训练了DeepSeekMoE-145B，模型效果上达到DeepSeek-67B的同等水平</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-d4aa2d90a069c2c7a7235a9ab9439eb7_1440w.jpg" />  </p>
<h2 id="dbrx">DBRX<a class="anchor-link" href="#dbrx" title="Permanent link">&para;</a></h2>
<p>2024年3月27日，Databricks开源了DBRX，一个拥有有132B参数，激活参数为36B的MoE模型。</p>
<p>结构上，DBRX使用了RoPE、GLU、GQA，采用了fine-grained expert的设计，每层有16个专家，每个token激活其中4个。相比Mixtral和Grok-1在8个专家中激活2个，DBRX有更多的专家组合方式。</p>
<p>DBRX训练的上下文长度为32k，并使用了12T文本和代码token进行训练。DBRX在3072个H100上完成预训练，加上post-training、效果评估、red-team优化，整个过程耗费3个月时间。</p>
<p>DBRX整体效果超过GPT-3.5，与Gemini 1.0 Pro相当，并且具有比较强的代码能力，甚至超过了在代码上专门优化过的模型，如CodeLLaMA-70B，如下图所示。</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-63237863cad1b7ae1007824b391521f8_1440w.jpg" /><br />
推理效率效率上，DBRX也领先于其他模型。</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-76e8ac883f3632b8fcbf8b80e59ba4ab_1440w.jpg" />  </p>
<h2 id="qwen15-moe">Qwen1.5-MoE<a class="anchor-link" href="#qwen15-moe" title="Permanent link">&para;</a></h2>
<p>2024年3月28日，阿里放出了Qwen1.5-MoE-A2.7B，以2.7B的模型参数，达到了Qwen1.5-7B模型的相近效果。</p>
<p>Qwen1.5-MoE-A2.7B参考了DeepSeekMoE和DBRX的工作，采用了fine-grained expert的做法，总共有64个专家，每个token激活8个专家，其中有4个为共享专家。</p>
<p>Qwen1.5-MoE-A2.7B使用Qwen-1.8B进行初始化，并在初始化阶段引入随机性，这样可以显著加快收敛速度，并得到更好的收敛结果。</p>
<p>Qwen1.5-MoE-A2.7B和其他模型效果对比如下</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-ba806c1f3d75de06e2221cb255683468_1440w.jpg" /><br />
虽然Qwen1.5-MoE-A2.7B总参数量较大，但激活的non-embedding参数量远小于7B模型，如下表所示</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-37512c37032ce548e0fa3cadad1f0796_1440w.jpg" /><br />
实践中，Qwen1.5-MoE-A2.7B相比于Qwen1.5-7B，训练成本降低了75%。</p>
<p>推理性能上，在A100-80G用vLLM部署Qwen1.5-7B和Qwen1.5-MoE-A2.7B模型进行了性能测试。</p>
<p>输入/输出token数都设置为1000，输出token数设置为1000，TPS和throughput如下</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-906cdf27998930e241786472c46481a5_1440w.jpg" /><br />
虽然MoE模型对内存需求更大，但是由于稀疏激活以及共享专家的设计，但是在速度和吞吐量上都比dense模型更好。Qwen1.5-MoE-A2.7B与Qwen1.5-7B相比，速度提高了约1.74倍。</p>
<h2 id="mistral">Mistral<a class="anchor-link" href="#mistral" title="Permanent link">&para;</a></h2>
<h3 id="mistral-8x7b">Mistral 8x7B<a class="anchor-link" href="#mistral-8x7b" title="Permanent link">&para;</a></h3>
<p>2023年12月11日，Mistral AI开源Mistral-8x7B，每个token激活8个专家中的2个。</p>
<p>Mistral-8x7B支持32k推理窗口和多语言，并且代码能力较好。和LLAM2-70B以及GPT-3.5的对比如下。</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-6a21215b6820974c9cce2d317d8edf9d_1440w.jpg" /><br />
Mistral-8x7B在大多数任务表现优于LLAM2-70B，且推理速度提高了6倍。</p>
<p>而和激活参数量相近的LLAM2-13B比，优势更为明显</p>
<p><img alt="" src="(20240620)MoE模型的前世今生_Linsight/v2-3988720d5e8bc015b22e86be5e5fce5c_1440w.jpg" />  </p>
<h3 id="mistral-8x22b">Mistral 8x22B<a class="anchor-link" href="#mistral-8x22b" title="Permanent link">&para;</a></h3>
<p>2024年4月17日，Mistral AI开源Mistral-8x22B模型，一个总参数为141B，激活参数为39B的超大MoE模型。</p>
<p>Mistral-8x22B支持多语言，并且具有较强的数学和代码能力。此外，推理窗口长度也从Mistral-8x7B的32k增加到64k。Mistral-8x22B还具备function call的能力。</p>
<p>在各个维度的评测结果如下</p>
<h2 id="moe">MoE++<a class="anchor-link" href="#moe" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/abs/2410.07348">MoE++: Accelerating Mixture-of-Experts Methods with Zero-Computation Experts</a></p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20241021192822.png" style="zoom: 60%;" /></div>

<ul>
<li>
<p><strong>降低计算成本</strong> ：MoE++允许每个Token使用可变数量的FFN专家，甚至可以完全跳过当前的MoE层。</p>
</li>
<li>
<p><strong>提升性能</strong> ：通过减少简单Token所需的FFN专家数量，MoE++使更多专家能够专注于复杂的Token，释放出比传统MoE更大的性能潜力。</p>
</li>
<li>
<p><strong>零计算量专家的参数极小</strong> ：可以在每个GPU上同时部署所有的零计算量专家，避免了分布式FFN专家部署带来的通信开销和专家负载不均的问题。</p>
</li>
<li>
<p><strong>跨任务专家负载</strong> </p>
</li>
<li>专家负载在不同层之间存在相关性，特别是在相邻层之间。例如，当第<span class="math-inline">j</span> 激活很大比例的FFN专家时，第<span class="math-inline">j + 1</span> 很可能也会以同样大的比例激活FFN专家。</li>
<li>浅层和最后一层的专家分配模式在不同任务之间的差异更大。</li>
<li>不同任务中每个Token激活的FFN专家数量存在显著差异，但并不一定是更简单的任务激活更少的FFN专家。</li>
<li>在所有专家类型中，Zero专家的平均激活次数最高，更简单的任务显示出更高的平均激活次数。</li>
<li>
<p>在MoE++模型的所有层中，不同任务主题的专家分配差异显著。</p>
</li>
<li>
<p><a href="https://mp.weixin.qq.com/s/WdLuK0Hk6S6EfnACACKXsA">颜水成袁粒提出新一代MoE架构：专家吞吐速度最高提升2.1倍！</a></p>
</li>
</ul>
<h2 id="cfm">CFM<a class="anchor-link" href="#cfm" title="Permanent link">&para;</a></h2>
<blockquote>
<p><a href="https://arxiv.org/pdf/2409.02877">Configurable Foundation Models:Building LLMs from a Modular Perspective</a></p>
</blockquote>
<ul>
<li><a href="https://mp.weixin.qq.com/s/9Yr5FFoBLcGQizTAbxCZVw">清华团队革新MoE架构！像搭积木一样构建大模型，提出新型类脑稀疏模块化架构</a></li>
</ul>
<h2 id="a-closer-look-into-mixture-of-experts-in-large-language-models">A Closer Look into Mixture-of-Experts in Large Language Models<a class="anchor-link" href="#a-closer-look-into-mixture-of-experts-in-large-language-models" title="Permanent link">&para;</a></h2>
<p><a href="https://mp.weixin.qq.com/s/koq-ytTb7CHxRIJCXtYV-g">清华等大学发表的深入探究大语言模型中的MoE</a></p>
<h2 id="deepseekmoe-towards-ultimate-expert-specialization-inmixture-of-experts-language-models">DeepSeekMoE: Towards Ultimate Expert Specialization inMixture-of-Experts Language Models<a class="anchor-link" href="#deepseekmoe-towards-ultimate-expert-specialization-inmixture-of-experts-language-models" title="Permanent link">&para;</a></h2>
<h2 id="harder-tasks-need-more-experts-dynamic-routing-in-moe-models">Harder Tasks Need More Experts: Dynamic Routing in MoE Models<a class="anchor-link" href="#harder-tasks-need-more-experts-dynamic-routing-in-moe-models" title="Permanent link">&para;</a></h2>
<h2 id="xmoe-sparse-models-with-fine-grained-and-adaptive-expert-selection">XMoE: Sparse Models with Fine-grained and Adaptive Expert Selection<a class="anchor-link" href="#xmoe-sparse-models-with-fine-grained-and-adaptive-expert-selection" title="Permanent link">&para;</a></h2>
<h2 id="hypermoe-towards-better-mixture-of-experts-via-transferring-amongexperts">HyperMoE: Towards Better Mixture of Experts via Transferring AmongExperts<a class="anchor-link" href="#hypermoe-towards-better-mixture-of-experts-via-transferring-amongexperts" title="Permanent link">&para;</a></h2>
<h2 id="小结">小结<a class="anchor-link" href="#小结" title="Permanent link">&para;</a></h2>
<ul>
<li>现有的工作都表明，MoE模型相比dense模型具有更高的参数效率，即同样的计算量下，MoE模型普遍能有更优的效果</li>
<li>因此MoE不仅能支持更大规模模型的训练，在较小规模模型上使用MoE架构也有很大收益</li>
<li>但是相比dense模型，MoE模型的训练也需要考虑更多内容，包括专家数量、激活数量和专家容量的设计，负载均衡的问题，如何在多设备上的并行等，训练难度更大</li>
<li>结构上，共享专家和细粒度专家目前被验证效果较好</li>
<li>负载均衡上，GShard和Switch Transformer的负载均衡损失被广泛采用</li>
<li>推理时需要对底层框架进行优化以适配MoE机制，否则难以发挥MoE的性能优势</li>
</ul>
<h2 id="reference">Reference<a class="anchor-link" href="#reference" title="Permanent link">&para;</a></h2>
<ul>
<li>Adaptive Mixtures of Local Experts <a href="https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf">https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf</a>  </li>
<li>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer <a href="https://arxiv.org/abs/1701.06538">https://arxiv.org/abs/1701.06538</a>  </li>
<li>GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding <a href="https://arxiv.org/abs/2006.16668">https://arxiv.org/abs/2006.16668</a>  </li>
<li>Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity <a href="https://arxiv.org/abs/2101.03961">https://arxiv.org/abs/2101.03961</a>  </li>
<li>GLaM: Efficient Scaling of Language Models with Mixture-of-Experts <a href="https://arxiv.org/abs/2112.06905">https://arxiv.org/abs/2112.06905</a>  </li>
<li>ST-MoE: Designing Stable and Transferable Sparse Expert Models <a href="https://arxiv.org/abs/2202.08906">https://arxiv.org/abs/2202.08906</a>  </li>
<li>DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models <a href="https://arxiv.org/abs/2401.06066">https://arxiv.org/abs/2401.06066</a>  </li>
<li>Introducing DBRX: A New State-of-the-Art Open LLM <a href="https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm">https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm</a>  </li>
<li>Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters <a href="https://qwenlm.github.io/zh/blog/qwen-moe/">https://qwenlm.github.io/zh/blog/qwen-moe/</a></li>
<li><a href="https://mp.weixin.qq.com/s/jhT4kv9c7fJp4xwSfckoag">MoE模型的前世今生</a></li>
</ul>
                </div>

                <!-- Comments Section (Giscus) -->
                <section class="comments-section">
                    <h3><i class="fas fa-comments"></i> 评论</h3>
                    <script src="https://giscus.app/client.js"
                        data-repo="Geeks-Z/Geeks-Z.github.io"
                        data-repo-id=""
                        data-category="Announcements"
                        data-category-id=""
                        data-mapping="pathname"
                        data-strict="0"
                        data-reactions-enabled="1"
                        data-emit-metadata="0"
                        data-input-position="bottom"
                        data-theme="preferred_color_scheme"
                        data-lang="zh-CN"
                        crossorigin="anonymous"
                        async>
                    </script>
                </section>
            </article>

            <footer class="post-footer">
                <p>© 2025 Hongwei Zhao. Built with ❤️</p>
            </footer>
        </main>
    </div>

    <!-- Theme Toggle Button -->
    <button class="theme-toggle" id="themeToggle" aria-label="切换主题">
        <i class="fas fa-moon"></i>
    </button>

    <script>
        // Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        const html = document.documentElement;
        const icon = themeToggle.querySelector('i');
        const hljsDark = document.getElementById('hljs-theme-dark');
        const hljsLight = document.getElementById('hljs-theme-light');
        
        // Check saved theme or system preference
        const savedTheme = localStorage.getItem('theme');
        const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
        
        if (savedTheme === 'dark' || (!savedTheme && prefersDark)) {
            html.setAttribute('data-theme', 'dark');
            icon.className = 'fas fa-sun';
            hljsDark.disabled = false;
            hljsLight.disabled = true;
        }
        
        themeToggle.addEventListener('click', () => {
            const isDark = html.getAttribute('data-theme') === 'dark';
            if (isDark) {
                html.removeAttribute('data-theme');
                icon.className = 'fas fa-moon';
                localStorage.setItem('theme', 'light');
                hljsDark.disabled = true;
                hljsLight.disabled = false;
            } else {
                html.setAttribute('data-theme', 'dark');
                icon.className = 'fas fa-sun';
                localStorage.setItem('theme', 'dark');
                hljsDark.disabled = false;
                hljsLight.disabled = true;
            }
            
            // Update Giscus theme
            const giscusFrame = document.querySelector('iframe.giscus-frame');
            if (giscusFrame) {
                giscusFrame.contentWindow.postMessage({
                    giscus: {
                        setConfig: {
                            theme: isDark ? 'light' : 'dark'
                        }
                    }
                }, 'https://giscus.app');
            }
        });
        
        // Highlight.js
        document.addEventListener('DOMContentLoaded', () => {
            hljs.highlightAll();
        });
        
        // KaTeX auto-render
        document.addEventListener('DOMContentLoaded', () => {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
        
        // TOC active state
        const tocLinks = document.querySelectorAll('.toc-container a');
        const headings = document.querySelectorAll('.post-content h2, .post-content h3, .post-content h4');
        
        function updateTocActive() {
            let current = '';
            headings.forEach(heading => {
                const rect = heading.getBoundingClientRect();
                if (rect.top <= 100) {
                    current = heading.getAttribute('id');
                }
            });
            
            tocLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === '#' + current) {
                    link.classList.add('active');
                }
            });
        }
        
        window.addEventListener('scroll', updateTocActive);
        updateTocActive();
    </script>
</body>
</html>
