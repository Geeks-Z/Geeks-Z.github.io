<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Untitled</title>
    <meta name="description" content="Untitled - Hongwei Zhao's Blog">
    <meta name="author" content="Hongwei Zhao">
    
    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <!-- Code Highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css" id="hljs-theme-dark">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css" id="hljs-theme-light" disabled>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    
    <style>
        :root {
            /* Light Theme */
            --primary-color: #2980b9;
            --primary-hover: #1a5276;
            --link-color: #c0392b;
            --text-color: #333;
            --text-light: #666;
            --text-muted: #999;
            --bg-color: #fff;
            --bg-secondary: #f8f9fa;
            --bg-code: #f5f5f5;
            --border-color: #e5e7eb;
            --shadow: 0 1px 3px rgba(0,0,0,0.1);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.1);
        }

        [data-theme="dark"] {
            --primary-color: #5dade2;
            --primary-hover: #85c1e9;
            --link-color: #e74c3c;
            --text-color: #e5e7eb;
            --text-light: #9ca3af;
            --text-muted: #6b7280;
            --bg-color: #1a1a2e;
            --bg-secondary: #16213e;
            --bg-code: #0f0f23;
            --border-color: #374151;
            --shadow: 0 1px 3px rgba(0,0,0,0.3);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.4);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 16px;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            transition: background-color 0.3s, color 0.3s;
        }

        a {
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--primary-hover);
            text-decoration: underline;
        }

        /* Layout */
        .page-wrapper {
            display: flex;
            max-width: 1400px;
            margin: 0 auto;
            padding: 2rem;
            gap: 3rem;
        }

        /* Sidebar TOC */
        .toc-sidebar {
            width: 260px;
            flex-shrink: 0;
            position: sticky;
            top: 2rem;
            height: fit-content;
            max-height: calc(100vh - 4rem);
            overflow-y: auto;
        }

        .toc-container {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 1.5rem;
            border: 1px solid var(--border-color);
        }

        .toc-container h3 {
            font-size: 0.85rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-muted);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .toc-container ul {
            list-style: none;
        }

        .toc-container li {
            margin-bottom: 0.5rem;
        }

        .toc-container a {
            font-size: 0.9rem;
            color: var(--text-light);
            display: block;
            padding: 0.25rem 0;
            border-left: 2px solid transparent;
            padding-left: 0.75rem;
            transition: all 0.2s;
        }

        .toc-container a:hover,
        .toc-container a.active {
            color: var(--primary-color);
            border-left-color: var(--primary-color);
            text-decoration: none;
        }

        .toc-container ul ul {
            margin-left: 1rem;
        }

        /* Main Content */
        .main-content {
            flex: 1;
            min-width: 0;
            max-width: 800px;
        }

        /* Header */
        .post-header {
            margin-bottom: 2rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
            margin-bottom: 1.5rem;
        }

        .back-link:hover {
            color: var(--primary-color);
            text-decoration: none;
        }

        .post-header h1 {
            font-size: 2.25rem;
            font-weight: 700;
            line-height: 1.3;
            margin-bottom: 1rem;
            color: var(--text-color);
        }

        .post-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
        }

        .post-meta span {
            display: flex;
            align-items: center;
            gap: 0.4rem;
        }

        .post-meta i {
            color: var(--text-muted);
        }

        .tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 1rem;
        }

        .tag {
            display: inline-block;
            font-size: 0.8rem;
            background: var(--bg-secondary);
            color: var(--text-light);
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            border: 1px solid var(--border-color);
            transition: all 0.2s;
        }

        .tag:hover {
            background: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }

        /* Article Content */
        .post-content {
            font-size: 1rem;
            line-height: 1.9;
        }

        .post-content h1,
        .post-content h2,
        .post-content h3,
        .post-content h4,
        .post-content h5,
        .post-content h6 {
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-weight: 600;
            line-height: 1.4;
            color: var(--text-color);
        }

        .post-content h1 { font-size: 1.875rem; }
        .post-content h2 { 
            font-size: 1.5rem; 
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }
        .post-content h3 { font-size: 1.25rem; }
        .post-content h4 { font-size: 1.125rem; }

        .post-content p {
            margin-bottom: 1.25rem;
        }

        .post-content ul,
        .post-content ol {
            margin: 1.25rem 0;
            padding-left: 1.5rem;
        }

        .post-content li {
            margin-bottom: 0.5rem;
        }

        .post-content blockquote {
            margin: 1.5rem 0;
            padding: 1rem 1.5rem;
            background: var(--bg-secondary);
            border-left: 4px solid var(--primary-color);
            border-radius: 0 8px 8px 0;
            color: var(--text-light);
            font-style: italic;
        }

        .post-content blockquote p:last-child {
            margin-bottom: 0;
        }

        /* Code */
        .post-content code {
            font-family: 'JetBrains Mono', 'Fira Code', Consolas, monospace;
            font-size: 0.9em;
            background: var(--bg-code);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            color: var(--link-color);
        }

        .post-content pre {
            margin: 1.5rem 0;
            padding: 1.25rem;
            background: var(--bg-code);
            border-radius: 10px;
            overflow-x: auto;
            border: 1px solid var(--border-color);
        }

        .post-content pre code {
            background: none;
            padding: 0;
            color: inherit;
            font-size: 0.875rem;
            line-height: 1.6;
        }

        /* Images */
        .post-content img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            margin: 1.5rem 0;
            box-shadow: var(--shadow-lg);
        }

        /* Tables */
        .post-content table {
            width: 100%;
            margin: 1.5rem 0;
            border-collapse: collapse;
            font-size: 0.95rem;
        }

        .post-content th,
        .post-content td {
            padding: 0.75rem 1rem;
            border: 1px solid var(--border-color);
            text-align: left;
        }

        .post-content th {
            background: var(--bg-secondary);
            font-weight: 600;
        }

        .post-content tr:nth-child(even) {
            background: var(--bg-secondary);
        }

        /* Math */
        .math-display {
            margin: 1.5rem 0;
            overflow-x: auto;
            padding: 1rem;
            background: var(--bg-secondary);
            border-radius: 8px;
        }

        /* Anchor links */
        .anchor-link {
            opacity: 0;
            margin-left: 0.5rem;
            color: var(--text-muted);
            font-weight: 400;
            transition: opacity 0.2s;
        }

        .post-content h2:hover .anchor-link,
        .post-content h3:hover .anchor-link,
        .post-content h4:hover .anchor-link {
            opacity: 1;
        }

        /* Theme Toggle */
        .theme-toggle {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            background: var(--primary-color);
            color: white;
            border: none;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.25rem;
            box-shadow: var(--shadow-lg);
            transition: transform 0.2s, background 0.2s;
            z-index: 1000;
        }

        .theme-toggle:hover {
            transform: scale(1.1);
            background: var(--primary-hover);
        }

        /* Comments Section */
        .comments-section {
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-color);
        }

        .comments-section h3 {
            font-size: 1.25rem;
            margin-bottom: 1.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        /* Footer */
        .post-footer {
            margin-top: 3rem;
            padding-top: 1.5rem;
            border-top: 1px solid var(--border-color);
            text-align: center;
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        /* Responsive */
        @media (max-width: 1024px) {
            .toc-sidebar {
                display: none;
            }
            
            .page-wrapper {
                padding: 1.5rem;
            }
        }

        @media (max-width: 768px) {
            .post-header h1 {
                font-size: 1.75rem;
            }
            
            .post-meta {
                flex-direction: column;
                gap: 0.5rem;
            }
            
            .theme-toggle {
                bottom: 1rem;
                right: 1rem;
                width: 44px;
                height: 44px;
            }
        }

        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-secondary);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--border-color);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--text-muted);
        }
    </style>
</head>
<body>
    <div class="page-wrapper">
        <!-- TOC Sidebar -->
        <aside class="toc-sidebar">
            <div class="toc-container">
                <h3><i class="fas fa-list"></i> 目录</h3>
                <div class="toc">
<ul>
<li><a href="#from-sparse-to-soft-mixtures-of-experts">From Sparse to Soft Mixtures of Experts</a></li>
<li><a href="#摘要">摘要</a></li>
<li><a href="#背景把离散优化问题变为可微的优化问题">背景：把离散优化问题变为可微的优化问题</a></li>
<li><a href="#参考">参考</a></li>
</ul>
</div>

            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <article>
                <header class="post-header">
                    <a href="../../../index.html" class="back-link">
                        <i class="fas fa-arrow-left"></i> 返回博客列表
                    </a>
                    <h1>Untitled</h1>
                    <div class="post-meta">
                        <span><i class="fas fa-calendar-alt"></i> 2026-01-28</span>
                        <span><i class="fas fa-folder"></i> AINotes/09.MoE</span>
                        <span><i class="fas fa-user"></i> Hongwei Zhao</span>
                    </div>
                    <div class="tags">
                        
                    </div>
                </header>

                <div class="post-content">
                    <h2 id="from-sparse-to-soft-mixtures-of-experts">From Sparse to Soft Mixtures of Experts<a class="anchor-link" href="#from-sparse-to-soft-mixtures-of-experts" title="Permanent link">&para;</a></h2>
<blockquote>
<p><a href="https://arxiv.org/pdf/2308.00951.pdf">From Sparse to Soft Mixtures of Experts</a></p>
</blockquote>
<h2 id="摘要">摘要<a class="anchor-link" href="#摘要" title="Permanent link">&para;</a></h2>
<p>Soft MoE 提出了一种新的<strong>可微稀疏混合专家模型</strong>，稀疏混合专家 (Sparse Mixture of Experts, MoE) 是一种在保证模型训练和推理的成本不显著增加的情况下，大幅度提升模型容量的方法。</p>
<p>MoE 方法已经有很长的一段历史了，是一种扩大模型容量的经典高效的做法，但是它的缺点是：</p>
<ol>
<li>训练不稳定</li>
<li>Token Dropping 的问题</li>
<li>较难扩展 Expert 的数量</li>
<li>低效率的微调</li>
</ol>
<p>造成以上问题的一个原因是 MoE 的端到端训练困难，因此，本文提出了一种可微的稀疏混合专家 Transformer 模型 (fully-differentiable sparse Transformer) Soft MoE 来解决端到端训练困难的问题，同时也能够保持 MoE 方法的优势，即以较低的推理成本更大的模型容量。Soft MoE 的特点是给每个专家输入不同 token 的权重混合。</p>
<p>视觉实验结果证明，Soft MoE 大大优于标准 ViT 和流行的 MoE 方法，比如 128 个 Expert，16 个 MoE 层的 Soft MoE-Huge/14 模型参数比 ViT-Huge/14 多 40 倍，但推理时间成本仅增长 2%，同时性能要好得多。</p>
<h2 id="背景把离散优化问题变为可微的优化问题">背景：把离散优化问题变为可微的优化问题<a class="anchor-link" href="#背景把离散优化问题变为可微的优化问题" title="Permanent link">&para;</a></h2>
<p>稀疏混合专家 (Sparse Mixture of Experts, MoE) 是一种在保证模型训练和推理的成本不显著增加的情况下，大幅度提升模型容量的方法。在视觉，语言和多模态任务中都取得了成功，代表像视觉的 V-MoE，文本的 Switch Transformer和多模态的 LIMoE。</p>
<p>如下图1左所示，稀疏 MoE Transformer 的核心是一个离散优化问题，即：模型需要决定每个输入 token 应该输入哪些 Expert 里面，这些 Expert 一般是 MLP 模块。输入 token 和 Expert 之间的匹配 (token-to-expert match) 是 MoE 中要考虑的很重要的问题之一，之前也有各种各样的方法尝试解决此问题，比如基于线性规划的，比如基于 RL 算法的，比如基于固定规则的，比如基于最优传输理论的，和基于贪婪匹配的。总之，解决好稀疏 MoE 的这个离散优化问题的确是件不容易的事情。稀疏 MoE 的缺点有：</p>
<ol>
<li>训练不稳定</li>
<li>Token Dropping 的问题</li>
<li>较难扩展 Expert 的数量</li>
<li>低效率的微调</li>
</ol>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20241105091345.png" style="zoom: 60%;" /></div>

<p>如图1右所示，Soft MoE 把稀疏 MoE Transformer 的这个离散优化问题变成了可微的优化问题。Soft MoE 觉得不必一定要 "hard" 地找到输入 token 和 Expert 之间的一一匹配，而是可以 "Soft" 地混合输入 token 并且分给每一个 Expert。Soft MoE 给每个 Expert 分配的不是某几个输入 token，而是所有输入 token 的加权平均值 (权重取决于 token 和 Expert)，然后由这个对应的 Expert 去处理这个加权平均值。</p>
<p><strong>1.2 变为可微的优化问题之后，解决了之前稀疏 MoE 的什么问题？</strong></p>
<p><strong>问题1</strong>：精心设计的 Expert-to-token 的路由机制通常并不比随机固定路由好。</p>
<p>Soft MoE 可以避免这个问题，因为每个路由的参数都是基于每个输入 token 直接更新的。</p>
<p><strong>问题2</strong>：训练不稳定 (LIMoE<a href="#ref_3">#ref_3</a>这个工作观察到在训练期间，可能有大部分 token 改变路由，给训练带来一定挑战) 导致很多稀疏 MoE 方法的 Expert 都不可以设置得很多。</p>
<p>Soft MoE 可以避免这个问题，扩展到数千个 Expert。</p>
<p><strong>1.3 Soft MoE 算法描述</strong></p>
<p><strong>参数配置</strong>：</p>
<p>设 <span class="math-inline">\mathbf{X} \in \mathbb{R}^{m \times d}</span> 为输入特征，其中 <span class="math-inline">m</span> 为序列长度， <span class="math-inline">d</span> 为特征维度。每个 MoE 层都使用 <span class="math-inline">n</span> 个 Expert： <span class="math-inline">{f_i: \mathbb{R}^d \rightarrow \mathbb{R}^d }_{1:n}</span> ，所有 Expert 都使用不同的参数应用相同的函数，通常是一个 MLP。</p>
<p>每个 Expert 处理 <span class="math-inline">p</span> 个 slots，每个 slot 包含 <span class="math-inline">d</span> 维的参数，所以这部分参数可以表示为 <span class="math-inline">\mathbf{\Phi} \in \mathbb{R}^{d \times (n \cdot p)}</span> 。</p>
<p><strong>中间计算</strong>：</p>
<p>输入特征 <span class="math-inline">\mathbf{X}</span> 首先计算分配权重 (Dispatch Weights) <span class="math-inline">\mathbf{D}</span> ，再根据权重重新混合输入特征，得到专家模型的输入 slots <span class="math-inline">\tilde{\mathbf{X}} \in \mathbb{R}^{(n \cdot p) \times d}</span> ：</p>
<p><div class="math-display">\begin{equation} \begin{gathered} \mathbf{D}<em>{ij} = \frac{\exp((\mathbf{X} \mathbf{\Phi})</em>{ij})}{\sum_{i'=1}^m \exp((\mathbf{X} \mathbf{\Phi})_{i'j})}\ \tilde{\mathbf{X}} = \mathbf{D}^\top \mathbf{X}. \end{gathered} \end{equation} \tag{1}</div><br />
其中的分配权重 (Dispatch Weights) <span class="math-inline">\mathbf{D}\in \mathbb{R}^{m\times (n \cdot p)}</span> 就是对 <span class="math-inline">\mathbf{X} \mathbf{\Phi}</span> 的逐列取 Softmax。</p>
<p>然后，在每个 slot 上应用相应的专家函数得到每个 Expert 的输出 slot <span class="math-inline">\tilde{\mathbf{Y}} \in \mathbb{R}^{(n \cdot p) \times d}</span> ：</p>
<p><div class="math-display">\begin{equation} \tilde{\mathbf{Y}}<em>i = f</em>{\left\lfloor{i / p}\right\rfloor}(\tilde{\mathbf{X}}_i). \end{equation} \tag{2}</div>  </p>
<p><strong>输出过程</strong>：</p>
<p>最后，输出 token 为所有 <span class="math-inline">n \cdot p</span> 个输出 slot <span class="math-inline">\mathbf{Y}</span> 的凸组合，其权重计算如下： <div class="math-display">\begin{equation} \begin{gathered} \mathbf{C}<em>{ij} = \frac{\exp((\mathbf{X} \mathbf{\Phi})</em>{ij})}{\sum_{j'=1}^{n \cdot p} \exp((\mathbf{X} \mathbf{\Phi})_{ij'})} \ \mathbf{Y} = \mathbf{C}\tilde{\mathbf{Y}}. \end{gathered} \end{equation} \tag{3}</div><br />
其中的组合权重 (Combine Weights) <span class="math-inline">\mathbf{C}\in \mathbb{R}^{m\times (n \cdot p)}</span> 就是对 <span class="math-inline">\mathbf{X} \mathbf{\Phi}</span> 的逐行取 Softmax，这部分参数可以表示为 <span class="math-inline">\mathbf{\Phi} \in \mathbb{R}^{d \times (n \cdot p)}</span> 。</p>
<p>整个过程如下图2所示。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202411042052161.jpg" />  </p>
<p>图2：Soft MoE 算法流程图</p>
<p>遵循稀疏 MoE 的常用设计思想，作者用 Soft MoE 块替换了 Transformer 的一部分 MoE 块。slot 的总数是 Soft MoE 的关键超参数，因为时间复杂度取决于 slot 的数量，而不是 Expert 的数量。比如，可以设置等于输入序列长度的 slot 数以匹配等效密集 Transformer 的 FLOP。</p>
<p>Soft MoE 的 JAX 代码：</p>
<pre><code>def soft_moe_layer(X, Phi, experts):
    # Compute the dispatch and combine weights.
    logits = jnp.einsum('md,dnp-&gt;mnp', X, Phi)
    D = jax.nn.softmax(logits, axis=(0,))
    C = jax.nn.softmax(logits, axis=(1, 2))
    # The input slots are a weighted average of all the input tokens,
    # given by the dispatch weights.
    Xs = jnp.einsum('md,mnp-&gt;npd', X, D)
    # Apply the corresponding expert function to each input slot.
    Ys = jnp.stack([
    f_i(Xs[i, :, :]) for i, f_i in enumerate(experts)],
    axis=0)
    # The output tokens are a weighted average of all the output slots,
    # given by the combine weights.
    Y = jnp.einsum('npd,mnp-&gt;md', Ys, C)
    return Y
</code></pre>
<p>全部代码：</p>
<p><a href="https://github.com/google-research/vmoe">https://github.com/google-research/vmoe</a>  </p>
<p><strong>1.4 Soft MoE 的一些关键性质</strong></p>
<p><strong>1) 完全可微</strong>：</p>
<p>Sparse MoE 算法的通病是 token 和 Expert 之间存在的分配问题，有时精心设计的 Expert-to-token 的路由机制通常并不比随机固定路由好。输入 token 和 Expert 之间的匹配 (token-to-expert match) 是 MoE 中要考虑的很重要的问题之一，之前也有各种各样的方法尝试解决此问题，比如基于线性规划的<a href="#ref_4">#ref_4</a>，比如基于 RL 算法的<a href="#ref_5">#ref_5</a>，比如基于固定规则的<a href="#ref_6">#ref_6</a>，比如基于最优传输理论的<a href="#ref_7">#ref_7</a>，和基于贪婪匹配的<a href="#ref_8">#ref_8</a><a href="#ref_9">#ref_9</a>。所有这些方法本质上都是离散，不可微的。</p>
<p>Soft MoE 可以避免这个问题，因为每个路由的参数都是基于每个输入 token 直接更新的。</p>
<p><strong>2) 可以避免掉 Token Dropping 和 Expert Unbalance 的问题</strong></p>
<p>MoE 算法里面每个 Expert 都会处理一些 token，很自然地就会带来 Token Dropping (有的 token 不会分配给任何一个 Expert) 和 Expert Unbalance (一些 Expert 会比另一些 Expert 分配到更多 token) 的问题。</p>
<p>Soft MoE 可以避免这个问题，因为每个 slot 的输入都是所有 token 的加权平均值。</p>
<p><strong>3) 运算速度快</strong></p>
<p>Soft MoE 的主要优点是完全避免了之前算法中的 token 排序或 top-k 操作，因为这些操作的速度慢，而且不太适合硬件加速器。因此，Soft MoE 明显快于大多数 Sparse MoE 算法。</p>
<p><strong>4) Soft MoE 算法是密集的 MoE 算法还是稀疏的 MoE 算法？</strong></p>
<p>要回答这个问题我们需要首先搞明白为什么 Sparse MoE 算法是稀疏的。Sparse MoE 是稀疏的这件事的根本原因是每个 Expert 的输入特征仅仅是一部分的 token，而 Soft MoE 的输入是所有输入 token 的加权平均值，因此不能算作是稀疏的。</p>
<p>Soft MoE 也不能算作是 Dense MoE 算法，因为每个 Expert 仅仅会处理输入 token 的子集。</p>
<p><strong>5) Soft MoE 算法需要归一化</strong></p>
<p>Transformers 中，MoE 层通常用于替换每个编码器块中的 FFN 层，因此如果去遵循大部分 Transformer 架构的 Pre-Normalization 方法，就需要使用归一化，这里 Soft MoE 针对 <span class="math-inline">\mathbf{X}</span> 的操作是：</p>
<pre><code>l2_normalize(X, axis=1)
</code></pre>
<p>权重 <span class="math-inline">\mathbf{\Phi}</span> 的做法是：</p>
<pre><code>scale * l2_normalize(Phi, axis=0)
</code></pre>
<p>其中，scale 是可学习的参数，l2_normalize 的定义是：</p>
<pre><code>def l2_normalize(x, axis, eps=1e-6):
    norm = jnp.sqrt(jnp.square(x).sum(axis=axis, keepdims=True))
    return x * jnp.reciprocal(norm + eps)
</code></pre>
<p><strong>6) Soft MoE 算法和注意力机制 (Multi-Head Self-Attention) 的区别和联系？</strong></p>
<p>首先相同点是二者都用了 Softmax 操作，Self-Attention 的 <span class="math-inline">h</span> 个 head 貌似和 Expert 有点像，但是仔细分析是不一样的。</p>
<p>不同点是如果 <span class="math-inline">m</span> 是序列长度，每个输入 token 的维度是 <span class="math-inline">d</span> ，则：</p>
<p>Self-Attention 里面的 <span class="math-inline">h</span> 个 Head 中的每一个都处理大小为 <span class="math-inline">d/h</span> 的 <span class="math-inline">m</span> 个向量。对每个向量施加不同的权重，然后将来自每个 Head 的结果 <span class="math-inline">d/h</span> 维向量 Concat 成一个 <span class="math-inline">d</span> 维度的向量。</p>
<p>Soft MoE 的专家是非线性的，每个专家输出都是一个 <span class="math-inline">d</span> 维度的向量，没有 Concat 操作。</p>
<p><strong>1.5 Soft MoE 算法的局限性</strong></p>
<ul>
<li><strong>自回归解码 (Auto-regressive decoding)</strong>：</li>
</ul>
<p>因为 Soft MoE 算法要在运行过程中合并所有的输入 token，因此很难实现自回归。因为自回归必须在训练期间保留过去的 token 和未来 token 之间的因果关系 (Causality)。</p>
<p>Self-Attention 解决这个问题的手段是依赖于注意力的掩码 (Mask) 机制。如果想在 Soft MoE 中实现这一点就需要特别小心 token 之间的依赖和相关关系。总之研究 Soft MoE 算法的自回归解码是个很有价值的方向。</p>
<ul>
<li><strong>内存消耗</strong></li>
</ul>
<p>Soft MoE 倾向于利用大量 Expert，而其成本和 Dense Backbone 类似，使得模型的内存需求可能变大。</p>
<p><strong>1.6 图像分类实验结果</strong></p>
<p><strong>训练数据集</strong></p>
<p><strong>预训练数据集</strong>：JFT-4B：一个私有数据集，其最新版本包含超过 4B 张图像，涵盖超过 29k 个类。预训练的过程中评价指标是 JFT-4B 上的上游验证精度 Precision-at-1 和 ImageNet 10-shot 精度 (冻结模型权重，并用一个新的权重来计算的，该数据集仅在包含来自 ImageNet-1K 的每个类包含 10 张图像的数据集上进行训练)。</p>
<p><strong>微调数据集</strong>：ImageNet-1K 训练集。</p>
<p><strong>验证集</strong>：ImageNet-1K 验证集。</p>
<p><strong>模型尺寸</strong>：</p>
<p>ViT-S/8, ViT-S/16, ViT-S/32, ViT-B/16, ViT-B/32, ViT-L/16, ViT-L/32, ViT-H/14。</p>
<p><strong>方法</strong>：</p>
<p>Token Choice, Expert Choice 和本文的 Soft MoE。</p>
<p><strong>训练策略</strong>：</p>
<p>300k steps, Batch Size 4096</p>
<p><strong>Pareto Model 实验结果</strong>：</p>
<p>如下图3所示是四种方法 Soft MoE, Experts Choice, Tokens Choice, Dense 在预训练过程中的 JFT-4B Precision-at-1 的结果和 ImageNet 10-shot 的精度的训练成本/性能帕累托边界。Soft MoE 算法在这两个指标上都优于之前的方法。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202411042052162.jpg" />  </p>
<p>图3：四种方法在预训练过程中的 JFT-4B Precision-at-1 的结果和 ImageNet 10-shot 的精度的训练成本/性能帕累托边界</p>
<p><strong>更长的训练结果</strong>：</p>
<p>本文还测试在更长的训练 step 下模型的性能如何，把从 Small 到 Huge 的模型训练了 4K steps，用 128 个 Expert 的 Soft MoE 替换 ViT S/16、B/16、L/16 和 H/14 中的最后一半 Block 中的 FFN，每个 Expert 使用一个 slot。</p>
<p>由于模型并行性所需的额外数据传输，Large Soft MoE 模型产生的 wall-clock time overhead 很小。所有变体都训练了 4M 步，除了 H/14，出于成本原因训练了 2M 步，实验结果如下图4和5所示。</p>
<p>如下图4所示是 Soft MoE 和 ViT 的 JFT-4B 精度、ImageNet 10-shot 精度和 ImageNet 微调精度与 ExaFLOPS 的训练成本。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202411042052163.jpg" />  </p>
<p>图4：不同模型更长的训练 step 下的 JFT-4B 精度</p>
<p>如下图5所示是所有结果。对于给定的计算预算，Soft MoE 模型大大优于 Vision Transformer 模型。比如 Soft MoE-S/16 在 JFT-4B 和 ImageNet 10-shot 上的表现优于 ViT-B/16，它还提高了完整 ImageNet 数据的微调分数，即使它的训练 (和推理) 成本要小得多。同样，Soft MoE-B/16 在上游任务 JFT-4B 和 ImageNet 10 shot 的表现优于 ViT-L/16，微调后仅落后 0.5，同时速度快 3 倍，所需的 FLOP 减少了近 4 倍。最后，Soft MoE-L/16 模型优于 Dense H/14 模型，同时在推理速度又快 3 倍左右。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202411042052164.jpg" />  </p>
<p>图5：不同模型更长的训练 step 下的实验结果</p>
<p>根据前面的实验结果，较小的 Soft MoE 的性能可以匹配较大的视觉 Transformer，作者因此继续训练小模型 Backbone，希望以非常低的训练成本获得更高质量的模型。</p>
<p>作者观察到对于 Soft MoE 方法而言，较长的 cooldown (学习率线性减小到零的时期) 可以很好地适用于 Soft MoE，因此将 cooldown 从 50k steps 增加到 500k steps。</p>
<p>实验结果如下图6和7所示。Soft MoE-B/16 训练了 1k TPUv3 Days，优于在相似时间预算上训练的 ViT-H/14，而 Soft MoE-B 模型的 FLOPs 要低 10 倍，wall-clock time 低 5.7 倍。即使将 ViT-H/14 的训练代价加倍，Soft MoE-B 模型的性能也可以与之相匹配。Soft MoE-L/16 模型的在推断上比 ViT H/14 快近 2 倍的同时性能大大优于所有模型。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202411042052165.jpg" />  </p>
<p>图6：不同训练代价和尺寸的 Soft MoE 模型和 ViT 的 JFT-4B Precision-at-1 性能和 ImageNet 10-shot 性能</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202411042052166.jpg" />  </p>
<p>图7：Soft MoE 模型和 ViT 的实验结果</p>
<p><strong>视觉-文本对比学习实验结果</strong></p>
<p>作者还验证了 Soft MoE 得到的模型在其他任务的性能。具体而言作者探索了一种流行的范式，即图像语言对比学习，这里遵循的是 LiT<a href="#ref_10">#ref_10</a> 方法，其中图像塔在图像分类任务上进行了预训练，然后在在图像-文本对数据集上训练文本编码器时冻结。</p>
<p>视觉编码器作者重用了在 JFT 上训练的模型，对比学习在 WebLI 上训练，这是一个专有数据集，由 10B 图像和从互联网上抓取的 ALT 文本组成。图像编码器被冻结，而文本编码器从头开始训练。实验结果如下图8所示，Soft MoE -L/16 在 Imagenet 和 Cifar-100 零样本上的性能分别比 ViT-L/16 高出 1% 和 2% 以上。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202411042052167.jpg" />  </p>
<p>图8：对比学习实验结果</p>
<h2 id="参考">参考<a class="anchor-link" href="#参考" title="Permanent link">&para;</a></h2>
<ol>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/650894166?theme=white">MoE 系列超详细解读 (一)：Soft MoE：一种完全可微的稀疏 Transformer</a></p>
</li>
<li>
<p><a href="#ref_1_0">#ref_1_0</a>Scaling Vision with Sparse Mixture of Experts</p>
</li>
<li><a href="#ref_2_0">#ref_2_0</a>Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</li>
<li>^<a href="#ref_3_0">#ref_3_0</a><a href="#ref_3_1">#ref_3_1</a>Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts</li>
<li>^<a href="#ref_4_0">#ref_4_0</a><a href="#ref_4_1">#ref_4_1</a>BASE Layers: Simplifying Training of Large, Sparse Models</li>
<li>^<a href="#ref_5_0">#ref_5_0</a><a href="#ref_5_1">#ref_5_1</a>Conditional Computation in Neural Networks for faster models</li>
<li>^<a href="#ref_6_0">#ref_6_0</a><a href="#ref_6_1">#ref_6_1</a>Hash Layers For Large Sparse Models</li>
<li>^<a href="#ref_7_0">#ref_7_0</a><a href="#ref_7_1">#ref_7_1</a>Sparsity-constrained optimal transport</li>
<li>^<a href="#ref_8_0">#ref_8_0</a><a href="#ref_8_1">#ref_8_1</a>Mixture-of-Experts with Expert Choice Routing</li>
<li><a href="#ref_9_0">#ref_9_0</a>Unified scaling laws for routed language models</li>
<li><a href="#ref_10_0">#ref_10_0</a>LiT: Zero-Shot Transfer with Locked-image Text Tuning</li>
</ol>
                </div>

                <!-- Comments Section (Giscus) -->
                <section class="comments-section">
                    <h3><i class="fas fa-comments"></i> 评论</h3>
                    <script src="https://giscus.app/client.js"
                        data-repo="Geeks-Z/Geeks-Z.github.io"
                        data-repo-id=""
                        data-category="Announcements"
                        data-category-id=""
                        data-mapping="pathname"
                        data-strict="0"
                        data-reactions-enabled="1"
                        data-emit-metadata="0"
                        data-input-position="bottom"
                        data-theme="preferred_color_scheme"
                        data-lang="zh-CN"
                        crossorigin="anonymous"
                        async>
                    </script>
                </section>
            </article>

            <footer class="post-footer">
                <p>© 2025 Hongwei Zhao. Built with ❤️</p>
            </footer>
        </main>
    </div>

    <!-- Theme Toggle Button -->
    <button class="theme-toggle" id="themeToggle" aria-label="切换主题">
        <i class="fas fa-moon"></i>
    </button>

    <script>
        // Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        const html = document.documentElement;
        const icon = themeToggle.querySelector('i');
        const hljsDark = document.getElementById('hljs-theme-dark');
        const hljsLight = document.getElementById('hljs-theme-light');
        
        // Check saved theme or system preference
        const savedTheme = localStorage.getItem('theme');
        const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
        
        if (savedTheme === 'dark' || (!savedTheme && prefersDark)) {
            html.setAttribute('data-theme', 'dark');
            icon.className = 'fas fa-sun';
            hljsDark.disabled = false;
            hljsLight.disabled = true;
        }
        
        themeToggle.addEventListener('click', () => {
            const isDark = html.getAttribute('data-theme') === 'dark';
            if (isDark) {
                html.removeAttribute('data-theme');
                icon.className = 'fas fa-moon';
                localStorage.setItem('theme', 'light');
                hljsDark.disabled = true;
                hljsLight.disabled = false;
            } else {
                html.setAttribute('data-theme', 'dark');
                icon.className = 'fas fa-sun';
                localStorage.setItem('theme', 'dark');
                hljsDark.disabled = false;
                hljsLight.disabled = true;
            }
            
            // Update Giscus theme
            const giscusFrame = document.querySelector('iframe.giscus-frame');
            if (giscusFrame) {
                giscusFrame.contentWindow.postMessage({
                    giscus: {
                        setConfig: {
                            theme: isDark ? 'light' : 'dark'
                        }
                    }
                }, 'https://giscus.app');
            }
        });
        
        // Highlight.js
        document.addEventListener('DOMContentLoaded', () => {
            hljs.highlightAll();
        });
        
        // KaTeX auto-render
        document.addEventListener('DOMContentLoaded', () => {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
        
        // TOC active state
        const tocLinks = document.querySelectorAll('.toc-container a');
        const headings = document.querySelectorAll('.post-content h2, .post-content h3, .post-content h4');
        
        function updateTocActive() {
            let current = '';
            headings.forEach(heading => {
                const rect = heading.getBoundingClientRect();
                if (rect.top <= 100) {
                    current = heading.getAttribute('id');
                }
            });
            
            tocLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === '#' + current) {
                    link.classList.add('active');
                }
            });
        }
        
        window.addEventListener('scroll', updateTocActive);
        updateTocActive();
    </script>
</body>
</html>
