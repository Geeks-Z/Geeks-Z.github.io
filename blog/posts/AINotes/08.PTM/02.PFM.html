<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Untitled</title>
    <meta name="description" content="Untitled - Hongwei Zhao's Blog">
    <meta name="author" content="Hongwei Zhao">
    
    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <!-- Code Highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css" id="hljs-theme-dark">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css" id="hljs-theme-light" disabled>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    
    <style>
        :root {
            /* Light Theme - 明亮清新配色 */
            --primary-color: #4A90D9;
            --primary-hover: #3678C2;
            --link-color: #E86B5F;
            --text-color: #2D2D2D;
            --text-light: #5A5A5A;
            --text-muted: #8A8A8A;
            --bg-color: #FFFFFF;
            --bg-secondary: #F5F7FA;
            --bg-code: #F8F9FC;
            --border-color: #E8ECF0;
            --shadow: 0 2px 8px rgba(0,0,0,0.06);
            --shadow-lg: 0 8px 24px rgba(0,0,0,0.08);
        }

        [data-theme="dark"] {
            --primary-color: #5dade2;
            --primary-hover: #85c1e9;
            --link-color: #e74c3c;
            --text-color: #e5e7eb;
            --text-light: #9ca3af;
            --text-muted: #6b7280;
            --bg-color: #1a1a2e;
            --bg-secondary: #16213e;
            --bg-code: #0f0f23;
            --border-color: #374151;
            --shadow: 0 1px 3px rgba(0,0,0,0.3);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.4);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 16px;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            transition: background-color 0.3s, color 0.3s;
        }

        a {
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--primary-hover);
            text-decoration: underline;
        }

        /* Layout */
        .page-wrapper {
            display: flex;
            max-width: 1400px;
            margin: 0 auto;
            padding: 2rem;
            gap: 3rem;
        }

        /* Sidebar TOC */
        .toc-sidebar {
            width: 260px;
            flex-shrink: 0;
            position: sticky;
            top: 2rem;
            height: fit-content;
            max-height: calc(100vh - 4rem);
            overflow-y: auto;
        }

        .toc-container {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 1.5rem;
            border: 1px solid var(--border-color);
        }

        .toc-container h3 {
            font-size: 0.85rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-muted);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .toc-container ul {
            list-style: none;
        }

        .toc-container li {
            margin-bottom: 0.5rem;
        }

        .toc-container a {
            font-size: 0.9rem;
            color: var(--text-light);
            display: block;
            padding: 0.25rem 0;
            border-left: 2px solid transparent;
            padding-left: 0.75rem;
            transition: all 0.2s;
        }

        .toc-container a:hover,
        .toc-container a.active {
            color: var(--primary-color);
            border-left-color: var(--primary-color);
            text-decoration: none;
        }

        .toc-container ul ul {
            margin-left: 1rem;
        }

        /* Main Content */
        .main-content {
            flex: 1;
            min-width: 0;
            max-width: 800px;
        }

        /* Header */
        .post-header {
            margin-bottom: 2rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
            margin-bottom: 1.5rem;
        }

        .back-link:hover {
            color: var(--primary-color);
            text-decoration: none;
        }

        .post-header h1 {
            font-size: 2.25rem;
            font-weight: 700;
            line-height: 1.3;
            margin-bottom: 1rem;
            color: var(--text-color);
        }

        .post-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
        }

        .post-meta span {
            display: flex;
            align-items: center;
            gap: 0.4rem;
        }

        .post-meta i {
            color: var(--text-muted);
        }

        .tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 1rem;
        }

        .tag {
            display: inline-block;
            font-size: 0.8rem;
            background: var(--bg-secondary);
            color: var(--text-light);
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            border: 1px solid var(--border-color);
            transition: all 0.2s;
        }

        .tag:hover {
            background: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }

        /* Article Content */
        .post-content {
            font-size: 1rem;
            line-height: 1.9;
        }

        .post-content h1,
        .post-content h2,
        .post-content h3,
        .post-content h4,
        .post-content h5,
        .post-content h6 {
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-weight: 600;
            line-height: 1.4;
            color: var(--text-color);
        }

        .post-content h1 { font-size: 1.875rem; }
        .post-content h2 { 
            font-size: 1.5rem; 
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }
        .post-content h3 { font-size: 1.25rem; }
        .post-content h4 { font-size: 1.125rem; }

        .post-content p {
            margin-bottom: 1.25rem;
        }

        .post-content ul,
        .post-content ol {
            margin: 1.25rem 0;
            padding-left: 1.5rem;
        }

        .post-content li {
            margin-bottom: 0.5rem;
        }

        .post-content blockquote {
            margin: 1.5rem 0;
            padding: 1rem 1.5rem;
            background: var(--bg-secondary);
            border-left: 4px solid var(--primary-color);
            border-radius: 0 8px 8px 0;
            color: var(--text-light);
            font-style: italic;
        }

        .post-content blockquote p:last-child {
            margin-bottom: 0;
        }

        /* Code */
        .post-content code {
            font-family: 'JetBrains Mono', 'Fira Code', Consolas, monospace;
            font-size: 0.9em;
            background: var(--bg-code);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            color: var(--link-color);
        }

        .post-content pre {
            margin: 1.5rem 0;
            padding: 1.25rem;
            background: var(--bg-code);
            border-radius: 10px;
            overflow-x: auto;
            border: 1px solid var(--border-color);
        }

        .post-content pre code {
            background: none;
            padding: 0;
            color: inherit;
            font-size: 0.875rem;
            line-height: 1.6;
        }

        /* Images */
        .post-content img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            margin: 1.5rem 0;
            box-shadow: var(--shadow-lg);
        }

        /* Tables */
        .post-content table {
            width: 100%;
            margin: 1.5rem 0;
            border-collapse: collapse;
            font-size: 0.95rem;
        }

        .post-content th,
        .post-content td {
            padding: 0.75rem 1rem;
            border: 1px solid var(--border-color);
            text-align: left;
        }

        .post-content th {
            background: var(--bg-secondary);
            font-weight: 600;
        }

        .post-content tr:nth-child(even) {
            background: var(--bg-secondary);
        }

        /* Math */
        .math-display {
            margin: 1.5rem 0;
            overflow-x: auto;
            padding: 1rem;
            background: var(--bg-secondary);
            border-radius: 8px;
        }

        /* Anchor links */
        .anchor-link {
            opacity: 0;
            margin-left: 0.5rem;
            color: var(--text-muted);
            font-weight: 400;
            transition: opacity 0.2s;
        }

        .post-content h2:hover .anchor-link,
        .post-content h3:hover .anchor-link,
        .post-content h4:hover .anchor-link {
            opacity: 1;
        }

        /* Theme Toggle */
        .theme-toggle {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            background: var(--primary-color);
            color: white;
            border: none;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.25rem;
            box-shadow: var(--shadow-lg);
            transition: transform 0.2s, background 0.2s;
            z-index: 1000;
        }

        .theme-toggle:hover {
            transform: scale(1.1);
            background: var(--primary-hover);
        }

        /* Comments Section */
        .comments-section {
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-color);
        }

        .comments-section h3 {
            font-size: 1.25rem;
            margin-bottom: 1.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        /* Footer */
        .post-footer {
            margin-top: 3rem;
            padding-top: 1.5rem;
            border-top: 1px solid var(--border-color);
            text-align: center;
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        /* Responsive */
        @media (max-width: 1024px) {
            .toc-sidebar {
                display: none;
            }
            
            .page-wrapper {
                padding: 1.5rem;
            }
        }

        @media (max-width: 768px) {
            .post-header h1 {
                font-size: 1.75rem;
            }
            
            .post-meta {
                flex-direction: column;
                gap: 0.5rem;
            }
            
            .theme-toggle {
                bottom: 1rem;
                right: 1rem;
                width: 44px;
                height: 44px;
            }
        }

        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-secondary);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--border-color);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--text-muted);
        }
    </style>
</head>
<body>
    <div class="page-wrapper">
        <!-- TOC Sidebar -->
        <aside class="toc-sidebar">
            <div class="toc-container">
                <h3><i class="fas fa-list"></i> 目录</h3>
                <div class="toc">
<ul>
<li><a href="#kimi全文翻译-arrow_down">Kimi全文翻译 :arrow_down:</a></li>
<li><a href="#0-摘要">0. 摘要</a></li>
<li><a href="#1-引言">1. 引言</a><ul>
<li><a href="#11-pfms-与预训练">1.1 PFMs 与预训练</a></li>
<li><a href="#12-贡献与组织结构">1.2 贡献与组织结构</a></li>
</ul>
</li>
<li><a href="#2-基础组件">2. 基础组件</a><ul>
<li><a href="#21-transformer-用于-pfms">2.1 Transformer 用于 PFMs</a></li>
<li><a href="#22-pfms-的学习机制">2.2 PFMs 的学习机制</a></li>
<li><a href="#23-pfms-的预训练任务">2.3 PFMs 的预训练任务</a><ul>
<li><a href="#231-nlp-中的预训练任务">2.3.1 NLP 中的预训练任务</a></li>
<li><a href="#232-cv-中的预训练任务">2.3.2 CV 中的预训练任务</a></li>
<li><a href="#233-gl-中的预训练任务">2.3.3 GL 中的预训练任务</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#4-pfms-for-computer-vision">4 PFMs for Computer Vision</a><ul>
<li><a href="#41-通过特定预文本任务学习">4.1 通过特定预文本任务学习</a></li>
<li><a href="#42-通过帧顺序学习">4.2 通过帧顺序学习</a></li>
<li><a href="#43-通过生成学习">4.3 通过生成学习</a></li>
<li><a href="#44-通过重建学习">4.4 通过重建学习</a></li>
<li><a href="#45-通过记忆库学习">4.5 通过记忆库学习</a></li>
<li><a href="#46-通过共享学习">4.6 通过共享学习</a></li>
<li><a href="#47-通过聚类学习">4.7 通过聚类学习</a></li>
<li><a href="#48-总结">4.8 总结</a></li>
</ul>
</li>
</ul>
</div>

            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <article>
                <header class="post-header">
                    <a href="../../../index.html" class="back-link">
                        <i class="fas fa-arrow-left"></i> 返回博客列表
                    </a>
                    <h1>Untitled</h1>
                    <div class="post-meta">
                        <span><i class="fas fa-calendar-alt"></i> 2026-02-04</span>
                        <span><i class="fas fa-folder"></i> AINotes/08.PTM</span>
                        <span><i class="fas fa-user"></i> Hongwei Zhao</span>
                    </div>
                    <div class="tags">
                        
                    </div>
                </header>

                <div class="post-content">
                    <h2 id="kimi全文翻译-arrow_down">Kimi全文翻译 :arrow_down:<a class="anchor-link" href="#kimi全文翻译-arrow_down" title="Permanent link">&para;</a></h2>
<h2 id="0-摘要">0. 摘要<a class="anchor-link" href="#0-摘要" title="Permanent link">&para;</a></h2>
<p><strong>预训练基础模型（Pretrained Foundation Models, PFMs）被视为不同数据模态的各种下游任务的基础。PFM（例如，BERT、ChatGPT 和 GPT-4）是在大规模数据上训练的，为广泛的下游应用提供了合理的参数初始化</strong>。与早期使用卷积和循环模块提取特征的方法不同，BERT 学习了 Transformer 的双向编码器表示，这些是在大型数据集上作为上下文语言模型训练的。同样，生成预训练Transformer（Generative Pretrained Transformer, GPT）方法采用 Transformer 作为特征提取器，并使用自回归范式在大型数据集上进行训练。最近，ChatGPT 在大型语言模型上取得了有希望的成功，它应用了自回归语言模型与零样本或少样本提示。PFM 的显著成就为近年来人工智能的各个领域带来了重大突破。众多研究提出了不同的方法、数据集和评估指标，提高了对最新调查的需求。本研究提供了对文本、图像、图形以及其他数据模态的 PFMs 最新研究进展、挑战和机遇的全面回顾。该回顾涵盖了用于自然语言处理、计算机视觉和图学习的基本组件和现有的预训练方法。此外，它还探讨了用于不同数据模态的高级 PFMs 以及考虑数据质量和数量的统一 PFMs。该回顾还讨论了与 PFMs 基础相关的研究，例如模型效率和压缩、安全性和隐私。最后，研究提供了关键的影响、未来研究方向、挑战和领域中未解决的问题。总体而言，本调查旨在为 PFMs 在可扩展性、安全性、逻辑推理能力、跨域学习能力以及对人工通用智能的友好交互能力方面的研究提供启示。</p>
<h2 id="1-引言">1. 引言<a class="anchor-link" href="#1-引言" title="Permanent link">&para;</a></h2>
<p>预训练基础模型（PFMs）被视为大数据时代人工智能的重要组成部分。基础模型的概念首次在文献[1]中提出，它指的是一类更广泛的模型及其功能。PFMs 在 AI 的三个主要领域中得到了广泛研究：自然语言处理（NLP）[2]、计算机视觉（CV）[3]和图学习（GL）[4]。PFMs 是强大的通用模型，在各个领域或跨领域中都表现出色。它们在各种学习任务中学习特征表示方面展现出巨大潜力，例如文本分类[5]、文本生成[6]、图像分类[7]、目标检测[8]和图分类[9]。PFMs 在大规模语料库上训练，并微调到类似的小规模任务上，表现出优越的性能，使得快速数据处理成为可能。</p>
<h3 id="11-pfms-与预训练">1.1 PFMs 与预训练<a class="anchor-link" href="#11-pfms-与预训练" title="Permanent link">&para;</a></h3>
<p>PFMs 建立在预训练技术之上，该技术的目标是使用大量数据和任务训练一个通用模型，该模型可以在不同的下游应用中轻松微调。预训练的概念起源于计算机视觉（CV）任务中的迁移学习[10]。认识到预训练在 CV 领域的有效性后，人们开始在其他领域使用预训练技术来增强模型性能。当预训练技术应用于 NLP 领域时，训练良好的语言模型（LMs）可以捕获对下游任务有益的丰富知识，例如长期依赖关系、层次关系等。此外，预训练在 NLP 领域的一个重要优势是训练数据可以从任何未标记的文本语料库中获取，即预训练过程中有无限的训练数据。早期的预训练是静态技术，例如 NNLM[11] 和 Word2vec[12]，但静态方法很难适应不同的语义环境。因此，提出了动态预训练技术，例如 BERT[13]、XLNet[14] 等。图 1 描述了 PFMs 在 NLP、CV 和 GL 领域的发展历程和演变。基于预训练技术的 PFMs 使用大型语料库学习通用的语义表示。随着这些开创性工作的引入，出现了各种各样的 PFMs，并将其应用于下游任务和应用。</p>
<h3 id="12-贡献与组织结构">1.2 贡献与组织结构<a class="anchor-link" href="#12-贡献与组织结构" title="Permanent link">&para;</a></h3>
<p>有几项调查研究[37, 8, 5, 6, 7, 1]已经回顾了特定领域的预训练模型，例如文本生成[6]、视觉Transformer[7]、目标检测[8]。Bommasani 等人[1]总结了基础模型的机会和风险。然而，现有的工作没有实现对不同领域（例如 CV、NLP、GL）的 PFMs 以及不同方面如预训练任务、效率、效能和隐私的全面回顾。在这项调查中，我们特别追踪了 NLP 领域中 PFMs 的演变，以及预训练是如何转移到并被 CV 和 GL 采用的。与其他调查相比，没有对来自这三个领域的现有 PFMs 进行全面介绍和分析。与以往对预训练模型的回顾不同，我们总结了从传统模型到具有最近工作的三个领域中的 PFMs 的现有模型。传统模型强调静态特征学习。动态 PFMs 引入了结构，这是主流研究。我们进一步介绍了一些其他 PFMs 的研究，包括其他高级和统一的 PFMs、模型效率和压缩以及安全性和隐私。最后，我们总结了不同领域 PFMs 的主要挑战，并在第 8 节中概述了未来的研究方向和开放问题。我们还全面地在附录 F 和 G 中介绍了相关的评估指标和数据集。总之，主要贡献如下：</p>
<ul>
<li>我们提供了 NLP、CV 和 GL 中 PFM 发展的扎实且最新的回顾。在回顾中，我们讨论并提供了关于这三个主要应用领域中通用 PFM 设计和预训练方法的见解。</li>
<li>我们总结了 PFMs 在其他多媒体领域的发展，如语音和视频。此外，我们讨论了关于 PFMs 的高级主题，包括统一 PFMs、模型效率和压缩以及安全性和隐私。</li>
<li>通过回顾不同模态的 PFMs 在不同任务中的应用，我们讨论了大数据时代未来对非常大的模型进行研究的主要挑战和机遇，这指导了基于 PFMs 的新一代协作和交互式智能。</li>
</ul>
<p>调查的其余部分组织如下。第 2 节介绍基本组件。第 3、4 和 5 节分别总结了 NLP、CV 和 GL 中现有的 PFMs。第 6 和 7 节介绍了 PFMs 的其他高级研究，包括高级和统一的 PFMs、模型效率和压缩以及安全性和隐私。此外，我们在第 8 节总结了 PFMs 的主要挑战，然后在第 9 节结束调查。</p>
<h2 id="2-基础组件">2. 基础组件<a class="anchor-link" href="#2-基础组件" title="Permanent link">&para;</a></h2>
<p>图 2 展示了 PFMs 的一般概念架构。PFMs 是庞大的神经网络模型，它们处理的都是神经信息。PFMs 的具体设计根据不同领域中的数据模态和任务需求而变化。Transformer 是许多领域（如 NLP 和 CV）PFMs 的主流模型架构设计。训练大型模型需要有各种数据集进行模型预训练。训练 PFMs 后，模型应该进行微调以满足下游需求，如有效性、效率和隐私性。在这一部分，我们介绍了 NLP、CV 和 GL 领域中 PFMs 的基本模型架构、概念和设置。有关更详细组件的介绍，请参阅附录 A。</p>
<h3 id="21-transformer-用于-pfms">2.1 Transformer 用于 PFMs<a class="anchor-link" href="#21-transformer-用于-pfms" title="Permanent link">&para;</a></h3>
<p>Transformer [38] 是一种创新的架构，它促进了不同神经单元之间加权表示知识的转移。它完全依赖于注意力机制，并且不使用递归或卷积架构。注意力机制是 Transformer 的关键组成部分，它为所有编码输入表示分配权重，并学习输入数据中最重要的部分。注意力的输出是通过取值的加权和得到的，权重是使用查询与相应键的兼容性函数计算的 [38]。在大型模型中开发了多种注意力机制 [39]。例如，在自然语言处理中，自注意力被创建以连接单个序列中的不同位置，以生成相同序列的表示。</p>
<p>Transformer 是 NLP、CV 和 GL 领域中 PFMs 的重要结构。对于 NLP，Transformer 可以帮助解决处理序列输入数据时的长期依赖问题。例如，GPT3 [20] 是基于 Transformer 的生成模型。对于 CV，视觉 Transformer (ViT) [40] 被提出，将图像表示为一系列图像块，类似于一系列单词嵌入。对于 GL，图 Transformer 网络 (GTN) [41] 被采用来学习新的图结构和强大的节点表示，而无需领域知识。由于 Transformer 结构实现了更高的并行性，Transformer 变得足够可扩展，以驱动 PFMs 的突破性能力。例如，ViT-22B 模型 [42] 有大约 22B 个参数，最大的语言模型可以有超过 100B 个参数（例如，GPT-3 有 175B，PaLM [43] 有 540B 个参数）。</p>
<h3 id="22-pfms-的学习机制">2.2 PFMs 的学习机制<a class="anchor-link" href="#22-pfms-的学习机制" title="Permanent link">&para;</a></h3>
<p>在计算机视觉中，深度学习模型在大多数任务中显示出比传统学习模型更大的优势，包括常见的分类、识别、检测和分割任务，以及特定的匹配、跟踪和序列预测任务。这些学习方法不仅在 CV 中可用，而且在 NLP 和 GL 中也适用。</p>
<p><strong>监督学习</strong> 假设我们有一个训练数据集 <span class="math-inline">X</span> 包含 <span class="math-inline">{ (x_i, y_i) }^n_{i=1}</span> 来表示训练数据集中的原始数据，其中 <span class="math-inline">x_i</span> 表示第 <span class="math-inline">i</span> 个训练样本，<span class="math-inline">y_i</span> 表示相应的标签。完整的网络是通过最小化目标函数来学习一个函数 <span class="math-inline">f(x; \theta)</span>，如下所示：<br />
<div class="math-display"><br />
    \arg \min_{\theta} \frac{1}{n} \sum_{i=1}^{n} L(f(x_i; \theta), y_i) + \lambda \Omega(\theta),<br />
</div><br />
其中 <span class="math-inline">L</span> 和 <span class="math-inline">\Omega</span> 分别代表预定义的损失函数和正则化项。函数 <span class="math-inline">f</span> 具有嵌套形式，如下所示：<br />
<div class="math-display"><br />
    h_1(x_i) = g(x_i^\top \omega_1 + b_1),<br />
</div></p>
<p><div class="math-display"><br />
    h_{l+1}(x_i) = g(h_l(x_i)^\top \omega_l + b_l), \quad l = 1, 2, \ldots, N,<br />
</div><br />
其中 <span class="math-inline">l</span> 是深度学习模型中层的索引，<span class="math-inline">N</span> 是层数，这意味着 <span class="math-inline">\theta = {\omega_l, b_l, l = 1, 2, \ldots, N}</span>。</p>
<p><strong>半监督学习</strong> 假设除了之前标记的人类数据集外，我们还有更多的未标记数据集 <span class="math-inline">Z = {z_i}^m_{i=1}</span>。如果我们想利用这两个数据集来学习理想的网络，学习过程可以表述为：<br />
<div class="math-display"><br />
    \arg \min_{\theta} \left( \frac{1}{n} \sum_{i=1}^{n} L(f(x_i; \theta), y_i) + \frac{1}{m} \sum_{i=1}^{m} L'(f'(z_i; \theta'), R(z_i, X)) \right) + \lambda \Omega(\theta),<br />
</div><br />
其中 <span class="math-inline">R</span> 是定义未标记数据的目标的函数，然后将这些伪标签集成到端到端的训练过程中。<span class="math-inline">f'</span> 是一个编码器，用于学习数据集 <span class="math-inline">Z</span> 中原始数据的新表示。具体来说，如果在训练过程中没有任何数据的标签，我们可以通过内部距离或设计的预文本任务从数据本身的属性中学习，这分别被称为无监督学习和自监督学习（SSL），我们将在第 4.3 节中详细讨论后者。</p>
<p><strong>弱监督学习</strong> 弱监督方法根据对人类标签的依赖程度，介于全监督学习和 SSL 之间。SSL 设计特殊的预文本任务作为监督学习，但全监督学习利用附加在数据上的现有标签。然而，它们都可以学习良好的视觉特征，并在特定的下游任务上表现良好。假设数据集中有不准确的 K 个标签，任何标签都可以附加到数据样本上。因此，我们表示图像 <span class="math-inline">x_i</span> 的真实标签为 <span class="math-inline">y_i \in {0, 1}^K</span>，<span class="math-inline">i = 1, 2, \ldots, n</span>，并且 <span class="math-inline">y_i</span> 的任何条目可以是 0 或 1。这里我们需要最小化总共 <span class="math-inline">nK</span> 个损失项，公式如下：<br />
<div class="math-display"><br />
    \arg \min_{\theta} \sum_{i=1}^{nK} L(f(x_i; \theta), y_{k_i}) + \lambda \Omega(\theta),<br />
</div><br />
其中 <span class="math-inline">\mathbf{y} = (y_1, y_2, \ldots, y_K) = y_i</span>，并且 <span class="math-inline">L</span> 是适合二元分类问题的损失函数。对于 <span class="math-inline">y_i</span> 的任何条目，需要计算一对一二元分类的损失函数。</p>
<p><strong>自监督学习</strong> SSL 利用数据本身的信息来学习不同任务的基本特征表示。通过应用自定义的伪标签，它可以避免手动标记大型数据集的成本。在 NLP 中，语言模型可以通过预测掩蔽的字符、单词或句子来训练。变分自编码器（VAE）和生成对抗网络（GAN）是两种类型的生成 SSL 方法，它们用于重建数据本身。此外，对比学习作为判别性 SSL 方法，在 CV、NLP 和 GL 中得到了广泛应用。对比学习的主要思想是通过数据增强等方法学习数据本身的先验知识分布。这样，对比学习可以学习一个模型，使得在投影空间中相似的实例更接近，不相似的实例更远。这里我们展示了对比损失的一个简单版本：<br />
<div class="math-display"><br />
    L_c(x_i, x_j, \theta) = m | f_\theta(x_i) - f_\theta(x_j) |<em>2^2 + (1 - m) \max(0, \epsilon - | f</em>\theta(x_i) - f_\theta(x_j) |_2^2),<br />
</div><br />
其中 <span class="math-inline">m</span> 是 1 如果两个样本具有相同的标签，否则为 0，<span class="math-inline">\epsilon</span> 是上界距离。</p>
<p><strong>强化学习</strong> RL 是另一种学习范式，它将学习过程建模为代理和环境之间的顺序交互，其中 RL 代理寻求学习顺序决策问题的最优策略。具体来说，在每个交互步骤 <span class="math-inline">t</span> 时，代理接收状态空间 <span class="math-inline">S</span> 中的状态 <span class="math-inline">s_t</span>，并根据策略 <span class="math-inline">\pi_\theta(a_t | s_t) : A \rightarrow S</span> 选择动作 <span class="math-inline">a_t</span>，该策略由 <span class="math-inline">\theta</span> 参数化。然后代理根据环境动态接收即时奖励 <span class="math-inline">r_t = r(s_t, a_t)</span> 和下一个状态 <span class="math-inline">s_{t+1}</span>，其中 <span class="math-inline">r(s, a)</span> 是奖励函数。对于每个情节，这个过程一直持续到代理达到终止状态。在每个情节结束后，RL 代理将重新开始一个新的情节。每个状态的回报是累积奖励的折扣因子 <span class="math-inline">\gamma \in (0, 1]</span>，<span class="math-inline">R_t = R(s_t, a_t) = \sum_{k=0}^{\infty} \gamma^k r_{t+k}</span>。代理的目标是最大化每个状态的长期回报的期望值，<br />
<div class="math-display"><br />
    \max_{\theta} \mathbb{E}[R_t | s_t, a_t = \pi_\theta(s_t)].<br />
</div></p>
<h3 id="23-pfms-的预训练任务">2.3 PFMs 的预训练任务<a class="anchor-link" href="#23-pfms-的预训练任务" title="Permanent link">&para;</a></h3>
<p>预训练是一个初始化框架，通常需要与下游任务的微调结合使用。在预训练和微调的过程中，模型参数通过预设任务训练，以捕获特定的属性、结构和社区信息。预训练得到的特征可以辅助下游任务，提供充分信息，并加速模型的收敛。</p>
<h4 id="231-nlp-中的预训练任务">2.3.1 NLP 中的预训练任务<a class="anchor-link" href="#231-nlp-中的预训练任务" title="Permanent link">&para;</a></h4>
<p>根据学习方法，预训练任务可分为五类：掩码语言建模（MLM）、去噪自编码器（DAE）、替换令牌检测（RTD）、下一句预测（NSP）和句子顺序预测（SOP）。RTD、NSP 和 SOP 是对比学习方法，它们假设观察到的样本比随机样本在语义上更为相似。</p>
<p><strong>掩码语言建模 (MLM)</strong><br />
MLM 随机擦除输入序列中的一些单词，然后在预训练期间预测这些被擦除的单词。典型的例子包括 BERT [13] 和 SpanBERT [44]。</p>
<p><strong>去噪自编码器 (DAE)</strong><br />
DAE 用于向原始语料库添加噪声，然后使用含噪声的语料库重构原始输入。BART [45] 是一个代表性的例子。</p>
<p><strong>替换令牌检测 (RTD)</strong><br />
RTD 是一个辨别任务，用于确定语言模型是否替换了当前的令牌。这个任务在 ELECTRA [46] 中引入。通过训练模型区分一个令牌是否被替换，模型可以获得语言知识。</p>
<p><strong>下一句预测 (NSP)</strong><br />
为了让模型理解两个句子之间的关联，并捕获句子级表示，引入了 NSP 任务。PFM 输入来自不同文档的两个句子，并检查句子的顺序是否正确。一个典型的例子是 BERT。</p>
<p><strong>句子顺序预测 (SOP)</strong><br />
与 NSP 不同，SOP 使用文档中的两个连续片段作为正样本，并将两个片段的交换顺序作为负样本。PFM 可以更好地建模句子之间的关联，例如 ALBERT [47]。</p>
<h4 id="232-cv-中的预训练任务">2.3.2 CV 中的预训练任务<a class="anchor-link" href="#232-cv-中的预训练任务" title="Permanent link">&para;</a></h4>
<p>为了学习特征空间，计算机视觉（CV）中创建了许多预训练任务，这些任务基于自监督学习（SSL）。它利用包含人工设计标签的预文本任务，例如拼图或比较图像中的不同补丁。这使得学习到的表示能够泛化到各种下游任务，包括分类、检测、识别、分割等。</p>
<p><strong>特定预文本任务</strong><br />
预文本任务，也称为预定义任务，是在预训练阶段为编码器网络执行的。网络通过预测特殊预文本任务的答案进行训练。基于数据的特定特征，为虚构任务生成伪标签。然后，使用引导学习技术训练编码器网络解决预文本任务。例如，修复（inpainting）旨在通过预测缺失的中心部分来预训练模型。</p>
<p><strong>帧顺序学习任务</strong><br />
从视频中学习帧顺序涉及通过时间步骤处理帧，这可以作为 CV 的预训练任务。这个问题通常涉及完成可以帮助获取视觉时间表示的预文本练习。</p>
<p><strong>数据生成任务</strong><br />
生成对抗网络（GANs）中的表示能力也可以用于预训练任务。将数据投影回潜在空间，正如 BiGANs [48] 所演示的，有助于通过作为特征表示来辅助监督判别任务。</p>
<p><strong>数据重建任务</strong><br />
由于图像可以被划分为受自然语言启发的补丁，一些用于 NLP 的预训练任务也可以用于 CV，例如基于自编码器的掩码预测。原始图像首先被划分为几个补丁，使用离散视觉标记来编码每个补丁。然后在第二阶段，从掩码补丁中输出的视觉标记与固定标记器中的视觉标记相匹配。</p>
<p><strong>其他</strong><br />
为了在 CV 中训练 PFMs，提出了其他一些预训练任务。例如，基于对比学习的编码器网络在各种数据增强上进行预训练。参数通过最大化负对（例如，具有不同标签的对）之间的距离和最小化正对（例如，具有相同标签的对）之间的距离来进行训练。为了预训练主干网络的参数，DeepClustering [49] 方法将表示划分为不同的簇，并将这些簇标记为监督信号。</p>
<h4 id="233-gl-中的预训练任务">2.3.3 GL 中的预训练任务<a class="anchor-link" href="#233-gl-中的预训练任务" title="Permanent link">&para;</a></h4>
<p>在 GL 中预设的任务类似于其他预文本任务。然而，它们可以是监督的或非监督的，这取决于设计。根据 GL 中的预训练目的和潜在动机，这些任务可以分为以下几类：</p>
<p><strong>图信息补全</strong><br />
这个任务指的是首先掩盖输入图中的部分信息，然后根据剩余信息的分布恢复被掩盖的信息。在 CV 和 NLP 中也存在类似的任务，它们的目标分别是填补隐藏的像素或单词。</p>
<p><strong>图属性预测</strong><br />
与直接对输入图的信息进行建模不同，这个任务旨在通过挖掘输入图的潜在属性来提供各种自监督信号。具体来说，一方面，它考虑节点属性、局部子结构和连接信息来提供预测回归任务；另一方面，它通过诸如聚类、结构密度和属性相似性等信息为节点分配伪标签，以提供分类任务。</p>
<p><strong>图一致性分析</strong><br />
这个任务的目标是在具有相似语义信息的图嵌入样本之间最大化一致性，并最小化具有无关语义信息的样本之间的一致性。在实际情况中，根据不同的模型训练策略，它可以被划分为上下文/自我/跨尺度的一致性分析。</p>
<p><strong>其他</strong><br />
与仅使用一个预文本任务相比，一些方法设计了一些集成机制，将多个预文本任务的优势整合到一个统一框架中。此外，某些特定领域的图数据具有具有实际意义的独特自监督信号，可以在有针对性的设计下用于预训练。</p>
<p>总之，Transformer 是大模型架构中的重要组成部分，有助于学习数据中的重要特征并挖掘内在结构。根据数据集和具体任务的不同，可以使用不同的学习机制来训练PFMs（预训练模型）。特别地，自监督学习（SSL）是一种有前途的机制，考虑到在各个领域中存在的大量未标注数据，它可以从数据中学习知识嵌入。强化学习（RL）则通过优化与奖励模型相对的策略（模型），为下游任务的微调提供了一种新途径。如何设计有效且高效的任务，使PFMs掌握数据背后的知识，是一个重要的研究课题。</p>
<h2 id="4-pfms-for-computer-vision">4 PFMs for Computer Vision<a class="anchor-link" href="#4-pfms-for-computer-vision" title="Permanent link">&para;</a></h2>
<p>随着在自然语言处理中使用PFM的普及，激励了研究人员开始探索计算机视觉（CV）中的PFM。在深度学习研究中，“预训练”这个术语并没有被清晰地定义。当在更通用的数据集（如ImageNet）上调整参数时，这个词首次被使用，这可以使其他任务以预热初始化开始训练，从而更快地收敛。与早期依赖于有监督信号的预训练数据集的基于卷积神经网络（CNN）的迁移学习技术不同，我们对PFM的考察集中在使用人工设计的标签进行自监督学习（SSL），例如拼图或比较图像中不同补丁作为预文本任务。这允许学习到的表示能够泛化到各种下游任务，包括分类、检测、识别、分割等。</p>
<p>然而，当学习任务变得更加复杂时，依赖数据注释变得成本高昂，标签过程比实际学习过程更加繁琐和耗时，这正是SSL迫切需要的地方，以及它如何进一步推动深度学习方法的进展。为了减少对数据标签的依赖，未标记数据通过SSL进行自监督训练，通过匹配、对比或生成。</p>
<p>图6展示了SSL的一般流程。在预训练阶段，为编码器网络设计了一个预文本任务来解决。这个预文本任务的人工标签基于数据的特定属性自动生成，例如来自同一来源的图像补丁被标记为“正样本”，而来自不同来源的补丁被标记为“负样本”。然后，使用监督学习方法训练编码器网络来解决预文本任务。由于浅层提取细粒度细节，如边缘、角度和纹理，而更深层捕获与任务相关的高级特征，如语义信息或图像内容，因此在预文本任务上训练的编码器可以迁移到下游的监督任务。在这个阶段，主干网络的参数是固定的，只需要学习一个简单的分类器，例如两层的多层感知器（MLP）。考虑到下游训练阶段的工作量有限，这个学习过程通常被称为微调。总结来说，SSL中预训练阶段学到的表示可以在其他下游任务上重用，并取得可比的结果。</p>
<p>在本节中，我们介绍了在CV中预训练PFM的不同任务。PFM可以通过特定的预文本任务、帧顺序、生成、重建、记忆库、共享、聚类等方式进行训练。我们在表2中总结了CV中提出的PFM。</p>
<h3 id="41-通过特定预文本任务学习">4.1 通过特定预文本任务学习<a class="anchor-link" href="#41-通过特定预文本任务学习" title="Permanent link">&para;</a></h3>
<p>在无监督学习的早期阶段，网络通过设计一个特殊的预文本任务并预测这个任务的答案来进行训练。Dosovitskiy等人[134, 135]预训练Exemplar CNN来区分未标记数据中的不同补丁。实验证明这些设计可以学习有用的表示，这些表示可以迁移到标准识别任务中。在基于上下文预测的方法[136]中，关于位置信息的手动生成的监督信号作为成对分类的标签。修复[137]旨在通过预测缺失的中心部分来预训练模型。由于修复是基于语义的预测，另一种解码器与上下文编码器相连。此外，解码器的标准逐像素重建过程可以迁移到任何其他下游修复任务中。具体来说，着色[138]是一种评估颜色着色作为预文本任务如何帮助学习语义表示以用于下游任务的方法。它也被称为跨通道编码，因为不同的图像通道作为输入，输出被区分。类似地，Split-Brain Autoencoder[139]也通过迫使网络解决跨通道预测任务来以自监督方式学习表示。拼图[140]被提出以自监督方式预训练设计的无上下文网络（CFN），首先设计拼图作为预文本任务。完整的损坏拼图（CDJP）[141]通过进一步复杂化预文本任务来学习图像表示，在这种情况下，拼图中缺少一个片段，其他片段包含不完整的颜色。在设计高效有效的预文本任务的思想指导下，Noroozi等人[142]使用计数视觉基元作为特殊的预文本任务，并在常规基准测试上超越了以前的最先进模型。NAT[143]通过将输出与低维噪声对齐来学习表示。RotNet[144]旨在预测图像的不同旋转。</p>
<h3 id="42-通过帧顺序学习">4.2 通过帧顺序学习<a class="anchor-link" href="#42-通过帧顺序学习" title="Permanent link">&para;</a></h3>
<p>序列数据如视频的学习通常涉及通过时间步骤处理帧。这个问题通常与解决有助于学习视觉时间表示的预文本任务相关。对比预测编码（Contrastive Predictive Coding, CPC）[145] 是第一个通过预测潜在空间中的未来来学习数据表示的模型。这个模型可以处理任何模态的数据，如语音、图像、文本等。CPC 的组成部分如图 7[145] 所示，其中 <span class="math-inline">x_t</span> 表示观察序列的输入，<span class="math-inline">z_t</span> 是经过编码器 <span class="math-inline">g_{enc}</span> 后的潜在表示序列，<span class="math-inline">c_t</span> 是在自回归模型 <span class="math-inline">g_{ar}</span> 后总结所有潜在序列 <span class="math-inline">z \leq t</span> 的上下文潜在表示。与传统模型通过生成模型 <span class="math-inline">p_k(x_{t+k}|c_t)</span> 预测未来帧 <span class="math-inline">x_{t+k}</span> 不同，CPC 模型一个 " 密度比 " <span class="math-inline">f_k</span> 来表示上下文潜在表示 <span class="math-inline">c_t</span> 和未来帧 <span class="math-inline">x_{t+k}</span> 之间的互信息：<br />
<div class="math-display"><br />
    f_k(x_{t+k}, c_t) \propto p(x_{t+k}|c_t) / x_{t+k}. \quad (10)<br />
</div><br />
在循环神经网络编码后，<span class="math-inline">z_t</span> 和 <span class="math-inline">c_t</span> 可以根据需要选择用于下游任务。编码器和自回归模型通过 InfoNCE [145] 进行训练，如下所示：<br />
<div class="math-display"><br />
    L = -\mathbb{E}<em>X[\log f_k(x</em>{t+k}, c_t) / \sum_{x_j \in X} f_k(x_j, c_t)], \quad (11)<br />
</div><br />
其中 <span class="math-inline">X</span> 表示包含正样本和负样本的训练数据集。密度比 <span class="math-inline">f_k</span> 可以通过优化 <span class="math-inline">L</span> 来估计。CPC v2 [146] 重新审视并改进了 CPC，通过在无监督表示上进行预训练，其表示的泛化性可以迁移到数据高效的下游任务。</p>
<h3 id="43-通过生成学习">4.3 通过生成学习<a class="anchor-link" href="#43-通过生成学习" title="Permanent link">&para;</a></h3>
<p>尽管在基于生成对抗网络（GAN）的方法发展后，许多现有应用变得流行，但由于缺乏特征编码器，GANs 内部的表示能力并未完全被开发。因此，双向生成对抗网络（Bidirectional Generative Adversarial Networks, BiGANs）[48] 被提出，将数据投影回潜在空间，这在辅助监督判别任务中通过作为特征表示非常有用。</p>
<p>基于 BiGANs，BigBiGAN [147] 通过添加编码器并修改判别器，首次在 ImageNet 上实现了无监督表示学习的最先进性能。如图 8[147] 所示，GANs 的传统组件（编码器 <span class="math-inline">E</span> 和生成器 <span class="math-inline">G</span>）用于产生数据 - 潜在对，表示为 <span class="math-inline">(x \sim P_x, \hat{z} \sim E(x))</span> 和 <span class="math-inline">(\hat{x} \sim G(z), z \sim P_z)</span>。最终损失 <span class="math-inline">\ell</span> 定义为数据特定项 <span class="math-inline">s_x, s_z</span> 和数据联合项 <span class="math-inline">s_{xz}</span> 的和。引入的判别器 <span class="math-inline">D</span>（对抗性学习推理（Adversarially Learned Inference, ALI）[148] 或 BiGAN[48]）学习区分来自原始数据、潜在分布和编码向量的对。</p>
<h3 id="44-通过重建学习">4.4 通过重建学习<a class="anchor-link" href="#44-通过重建学习" title="Permanent link">&para;</a></h3>
<p>iGPT [149] 和 ViT [40] 模型已经展示了将自编码器的掩码预测预文本任务从语言适应到图像数据的可行性。BEiT [150] 是第一个展示基于自编码器的掩码预测可以超越 DINO [151] 等传统最先进方法，无需预训练技术。具体来说，BEiT 包括两个阶段：使用离散变分自编码器（discrete variational autoencoder, dVAE）[152] 的令牌嵌入，以及使用掩码图像预测的分词器训练。在第一阶段，原始图像被分割成一些补丁，并使用离散令牌进行编码，这与 BERT 不同，因为图像补丁没有像 NLP 中的单词那样的现成令牌。在第二阶段，BEiT 编码器获取包含未掩码和掩码补丁的损坏图像，然后输出掩码补丁的视觉令牌，以匹配固定分词器中相应的视觉令牌。尽管 BEiT 取得了成功，但掩码预测和自编码器训练之间的分离导致整个框架不是端到端的，阻碍了学习效果和效率。</p>
<p>为了解决这个问题，MAE [154] 提出了一个端到端的简单解决方案，直接从未掩码的补丁预测掩码补丁，使用均方误差（Mean Squared Error, MSE）损失。值得注意的是，MAE 使用了 75% 的掩码比率，这比 BERT（通常为 15%）高得多。消融研究表明，更高的掩码比率对微调和线性探测都是有益的。同时，SimMIM [155] 提出了一个与 MAE 类似的基于自编码器的解决方案，他们还证实了更高的标记比率和利用随机掩码策略有助于提高性能。主要的区别在于他们如何在自编码器中分配表示编码和预文本预测的责任。由于 SimMIM 的解码器很简单，SimMIM 的编码器同时进行这两项工作。相反，MAE 中的编码器只承担表示编码的角色，解码器负责预文本预测。最近，Meta AI 宣布了 Segment Anything Model (SAM) [156]，它提示用户指定要在图像中分割的内容，允许在不需要额外训练的情况下进行广泛的分割任务。SAM 使用了预训练的 ViT-H [40] 图像编码器的 MAE，该编码器每次处理一张图像并产生图像嵌入，以及一个提示编码器，它嵌入点击或框等输入提示。随后，一个轻量级的基于 Transformer 的掩码解码器从图像和提示嵌入中预测对象掩码。结果表明，SAM 可以从单个前景点生成高质量的掩码，通常只比手动注释的真值略逊一筹。它在使用零样本迁移方法和提示工程的广泛下游任务上常规地实现了强大的定量和定性结果。</p>
<p>利用 ViT 在 MAE 中引入了一个严重的效率问题，减小补丁大小会导致计算资源的二次增加。为了解决这个问题，有两个重要的解决方案：(1) 分层 ViT 和 (2) 局部注意力。在第一个方向上，分层 ViT（hViT）被引入，它使用收缩金字塔结构和像移位窗口 [157] 这样的技术来减少计算需求。不幸的是，hViT 不能直接应用于启用 MAE 预训练，因为 hViT 中使用的局部窗口注意力使得它难以处理 MAE 中像随机掩码这样的补丁。最近，Uniform Masking MAE (UM-MAE) [158] 被提出，以赋予 MAE 以 hViT 的能力，它引入了一个两阶段的流程：采样和掩码。它首先从每个块中随机采样一部分补丁（论文中报告为 25%），然后在采样的补丁上再掩码额外的补丁。第一步有助于在不同的局部窗口之间保持共同元素，而第二步防止了从附近低级特征的像素重建的捷径，使任务更加困难。另一个提高效率的方向集中在通过将网络的注意力放在图像的一些局部小窗口上来减小输入大小。受到局部知识足以重建掩码补丁的观察启发，Local masked reconstruction (LoMaR) [159] 被提出。与使用整个图像进行掩码重建不同，LoMaR 采样一些小型窗口并关注局部区域，这在下游任务中的学习效率方面优于 MAE。</p>
<h3 id="45-通过记忆库学习">4.5 通过记忆库学习<a class="anchor-link" href="#45-通过记忆库学习" title="Permanent link">&para;</a></h3>
<p>非参数实例判别（Non-Parametric Instance Discrimination, NPID）[153] 是第一个利用实例来学习下游任务表示的方法。详细的流程如图 9 所示。特征表示存储在记忆库中，以便于计算，因为实例级分类目标需要训练数据集中的所有图像。对于任何图像 <span class="math-inline">x</span> 具有特征表示 <span class="math-inline">v = f_\theta(x)</span>，其被识别为第 <span class="math-inline">i</span> 个样本的概率为：<br />
<div class="math-display"><br />
    P(i|v) = \frac{\exp(v^T i v / \tau)}{\sum_{j=1}^{n} \exp(v^T j v / \tau)}, \quad (12)<br />
</div><br />
其中 <span class="math-inline">v_i</span> 或 <span class="math-inline">v_j</span> 是第 <span class="math-inline">i</span> 或第 <span class="math-inline">j</span> 个样本的表示，它作为参数类原型（即分类器的权重）的替代。此外，<span class="math-inline">\tau</span> 是从知识蒸馏 [160] 中借用的温度参数。</p>
<p>局部聚合（Local Aggregation, LA）[161] 是另一种方法，它训练一个 CNN 编码器将原始图像嵌入到较低维度空间——嵌入空间。当局部聚合的度量最大化时，相似的数据实例在嵌入空间中聚集在一起，而不相似的实例则分散开来。</p>
<p>基于 NPID，预文本不变表示学习（Pretext Invariant Representation</p>
<p>Learning, PIRL，发音为“pearl”）[162] 被提出，认为语义表示在预文本转换任务下是不变的。假设图像的原始视图和转换视图分别表示为 <span class="math-inline">I</span> 和 <span class="math-inline">I_t</span>。这些样本视图被输入到 CNN 编码器中，训练数据集 <span class="math-inline">D</span> 上的总经验损失可以定义为：<br />
<div class="math-display"><br />
    L_{\text{total}}(\theta; D) = \mathbb{E}<em>t \left[ \frac{1}{|D|} \sum</em>{I \in D} L(V_I, V_{I_t}) \right], \quad (13)<br />
</div><br />
其中 <span class="math-inline">T</span> 表示图像的不同转换。损失鼓励图像 <span class="math-inline">I</span> 的表示与 <span class="math-inline">I_t</span> 的表示相似，而 <span class="math-inline">I_t</span> 的表示与不同图像 <span class="math-inline">I'</span> 的表示不相似，如图 10 中的虚线框所示。因此，更多的负样本对有助于提高梯度的可扩展性，并导致最终学习到的编码器具有更强的表示能力。这就是为什么引入记忆库来存储更多先前表示以供后续比较的原因。</p>
<h3 id="46-通过共享学习">4.6 通过共享学习<a class="anchor-link" href="#46-通过共享学习" title="Permanent link">&para;</a></h3>
<p>SSL 倾向于使用两个编码器网络对不同的数据增强，然后通过最大化负对之间的距离或最小化正对之间的距离来预训练参数。图 10 显示了所有对比学习框架的两流模型。原始输入图像 <span class="math-inline">I</span> 的转换 <span class="math-inline">t</span> 生成了视图 <span class="math-inline">v</span>，同样，它的对应物 <span class="math-inline">t'</span> 生成了 <span class="math-inline">v'</span>。通常，两个不同或相同的编码器 <span class="math-inline">f_\theta</span> 和 <span class="math-inline">f'<em>\xi</span> 用于提取对比表示。随后的 MLP 头 <span class="math-inline">g</em>\theta</span> 和 <span class="math-inline">g'<em>\xi</span> 用于学习更多有利于对比损失的组合。注意，在不同设置下，MLP 和记忆库可能被移除或保留。在共享编码器方面，SSL 可以分为两类：1) 软共享，两个编码器共享相似但不同的参数（<span class="math-inline">f</em>\theta \neq f'<em>\xi</span>）；2) 硬共享，两个编码器保持相同的架构和参数（<span class="math-inline">f</em>\theta = f'_\xi</span>）。</p>
<p><strong>软共享（Soft Sharing）</strong><br />
Facebook AI Research (FAIR) 通过使用动量来控制两个编码器之间的微小差异，提出了 Momentum Contrast (MoCo) [163]。如图 11 所示，其中一个编码器作为字典查找任务，生成一系列编码数据样本的队列 {k0, k1, ...}。另一个编码器生成训练批次更新的编码查询 {q0, q1, ...}。相似性是通过新来的编码查询 q 与存储在字典队列中的编码键的点积来衡量的。假设在新键到来之前，队列中存储了 K 个键。这 K 个键被视为查询新键的负样本。为了结合正样本和负样本上的对比损失，MoCo 中使用了 InfoNCE Loss [145] 进行预训练。MoCo 中软参数共享的关键设计是动量更新。He 等人 [163] 建议，直接将关键编码器（即动量编码器）的参数变化到查询编码器会失去必要的一致性，导致结果不佳。动量编码器的参数 θk 更新如下：<br />
<div class="math-display"><br />
    \theta_k = m \theta_k + (1 - m) \theta_q, \quad (14)<br />
</div><br />
其中查询编码器的参数 θq 直接从新实例的梯度中学习，m ∈ [0, 1) 是一个超参数，控制一致性（如果 m 更接近 1，则 θk 更一致）。</p>
<p>受到 SimCLR [164] 设计的启发，在 MoCo v2 [164] 中，FAIR 团队在编码器后引入了一个 MLP 投影头，并使用了更多的数据增强技术来提高性能。进一步的改进来自于：1) 嵌入式线性分类器弥合了无监督和监督预训练表示之间的差距；2) 更大的训练批次和更强的数据增强使得更多的对比样本成为可能。</p>
<p>DeepMind 提出了 Bootstrap Your Own Latent (BYOL) [165]，它包含表示、投影和判别阶段，无需使用负样本就能实现新的最先进性能。他们将原始图像的不同视图之间的判别视为防止预训练过程中崩溃的必要预防措施。然而，他们认为许多负样本并不是防止这种崩溃所必需的。如图 10 左部分所示，BYOL 中有两个不同参数的流。在线网络（顶部绿色）通过将自己生成的预测与目标网络提供的回归目标进行比较来更新参数。然后目标模型（底部红色）的参数更新与等式 (14) 相同，即 ξ ← τξ + (1 − τ)θ，其中 τ 是目标衰减率，控制目标网络中参数变化的程度。因此，目标网络也可以理解为动量编码器。这里，目标模型中的 ξ 是动量编码器中的参数 θk，而在线网络中的 θ 表示查询编码器中的参数 θq。</p>
<p><strong>硬共享（Hard Sharing）</strong><br />
Google Research 的 Brain Team 提出了 SimCLR [166]，它利用硬参数共享架构。这个简单的框架也可以在图 10 中总结出来，我们可以看到，不同视图的相同图像的表示在网络 f(·) 中学习。这个基础编码器彼此之间共享参数。因此，不需要记忆库和动量设置来学习关键和查询编码器，这有助于简化后端架构，使学习策略更简单。定义为最大化相同图像的不同视图（正样本）之间的相似性的损失函数如下：<br />
<div class="math-display"><br />
    \ell_{i,j} = -\log \frac{\exp(sim(z_i, z_j)/\tau)}{\sum_{k=1}^{2N} [k=i] \exp(sim(z_i, z_k)/\tau)} \quad (15)<br />
</div><br />
其中 (i, j) 是正样本对，τ 是引入的超参数，称为温度参数 [153]，1[k=i] ∈ {0, 1} 是一个指示函数，控制分母只包含负样本。</p>
<p>为了减少对大量显式成对特征比较的依赖，Swapping Assignments between multiple Views of the same image (SwAV) [167] 被提出作为在线算法，由 Inria 和 FAIR 开发。SwAV 引入了聚类来替代之前的成对比较，这在非队列架构的帮助下获得了更多的内存。在这种方法中，聚类原型加入了定义损失函数的计算。这个原型被编码为通过 CNN 反向传播学习的向量的串联。因此，SwAV 不需要比较不同视图的编码表示。</p>
<p>基于现有的 SwAV，一个名为 SElf-supERvised (SEER) [168] 的新模型旨在从任何随机图像和无界数据集中学习预训练编码器。基础网络是使用 SwAV SSL 方法 [167] 训练的 RegNetY 架构 [169]。这种方法证明了 SSL 不特定于像 ImageNet 这样的策划数据集，并且最近的 RegNet 发布解除了传统骨干网络（如 ResNet）的限制。此外，这种方法鼓励研究社区探索更多适合通用 SSL 的骨干网络。</p>
<p>在最近的 SSL 中引起关注的是，FAIR 通过使用 Simple Siamese (SimSiam) 网络的结构进行了实证实验。这种方法 [170] 可以避免传统对比学习中负样本对的设计、大批量（或记忆库）和动量编码器。图 10 中的两个编码器，它们具有相同的参数，处理图像 x 的两个不同视图 t 和 t'，被唯一的连体网络替代。MLP 预测器 g 用于其中一个视图表示，然后对另一个视图表示应用了 stop-gradient 操作。</p>
<h3 id="47-通过聚类学习">4.7 通过聚类学习<a class="anchor-link" href="#47-通过聚类学习" title="Permanent link">&para;</a></h3>
<p>DeepCluster [49] 是第一个采用聚类算法进行大规模数据集学习的模型。这种方法将表示分组到不同的聚类中，并将这些聚类标记为监督信号以预训练参数。它在广泛的标准迁移任务上展示了最先进的性能，这些任务用于无监督学习。</p>
<p>当涉及到对比学习和聚类的联系时，SwAV [167] 已经利用原型作为聚类中心来帮助在预训练期间对样本对进行分类，而 Prototypical Contrastive Learning (PCL) [171] 首次针对将对比学习与聚类联系起来。与实例判别作为预文本任务学习低级表示不同，聚类可以帮助编码更多的语义信息。然后，更多基于语义的下游任务将从受益。如图 12 所示，原型对比学习使用原型来替代 NCE 损失中的一个视图生成样本（等式 (15)），这是 PCL 中提出的 ProtoNCE 损失。此外，PCL 也是建立在软参数共享基础上的方法，其中动量编码器的更新如等式 (14) 所示。</p>
<h3 id="48-总结">4.8 总结<a class="anchor-link" href="#48-总结" title="Permanent link">&para;</a></h3>
<p>本节广泛调查了图像表示学习的 PFMs 的最新进展，从早期为自标记设计的预文本任务的角度，到现在基于对比损失的 SSL。主要方法的流程被清晰地阐述。我们希望本节能够为新入行的研究人员提供对这个新兴领域基本理解的准备，以及一些值得研究的方向。我们相信 PFMs 强大的泛化能力将极大地减少通过“预训练一次，永远迁移”的培训计算开销。最近的基于 Transformer 的 PFMs 已经逐渐在目标数据集上超越了从头开始的传统训练。这一发现将激发对这个激动人心领域的进一步探索和研究。</p>
                </div>

                <!-- Comments Section (Giscus) -->
                <section class="comments-section">
                    <h3><i class="fas fa-comments"></i> 评论</h3>
                    <script src="https://giscus.app/client.js"
                        data-repo="Geeks-Z/Geeks-Z.github.io"
                        data-repo-id=""
                        data-category="Announcements"
                        data-category-id=""
                        data-mapping="pathname"
                        data-strict="0"
                        data-reactions-enabled="1"
                        data-emit-metadata="0"
                        data-input-position="bottom"
                        data-theme="preferred_color_scheme"
                        data-lang="zh-CN"
                        crossorigin="anonymous"
                        async>
                    </script>
                </section>
            </article>

            <footer class="post-footer">
                <p>© 2025 Hongwei Zhao. Built with ❤️</p>
            </footer>
        </main>
    </div>

    <!-- Theme Toggle Button -->
    <button class="theme-toggle" id="themeToggle" aria-label="切换主题">
        <i class="fas fa-moon"></i>
    </button>

    <script>
        // Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        const html = document.documentElement;
        const icon = themeToggle.querySelector('i');
        const hljsDark = document.getElementById('hljs-theme-dark');
        const hljsLight = document.getElementById('hljs-theme-light');
        
        // Check saved theme or system preference
        const savedTheme = localStorage.getItem('theme');
        const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
        
        if (savedTheme === 'dark' || (!savedTheme && prefersDark)) {
            html.setAttribute('data-theme', 'dark');
            icon.className = 'fas fa-sun';
            hljsDark.disabled = false;
            hljsLight.disabled = true;
        }
        
        themeToggle.addEventListener('click', () => {
            const isDark = html.getAttribute('data-theme') === 'dark';
            if (isDark) {
                html.removeAttribute('data-theme');
                icon.className = 'fas fa-moon';
                localStorage.setItem('theme', 'light');
                hljsDark.disabled = true;
                hljsLight.disabled = false;
            } else {
                html.setAttribute('data-theme', 'dark');
                icon.className = 'fas fa-sun';
                localStorage.setItem('theme', 'dark');
                hljsDark.disabled = false;
                hljsLight.disabled = true;
            }
            
            // Update Giscus theme
            const giscusFrame = document.querySelector('iframe.giscus-frame');
            if (giscusFrame) {
                giscusFrame.contentWindow.postMessage({
                    giscus: {
                        setConfig: {
                            theme: isDark ? 'light' : 'dark'
                        }
                    }
                }, 'https://giscus.app');
            }
        });
        
        // Highlight.js
        document.addEventListener('DOMContentLoaded', () => {
            hljs.highlightAll();
        });
        
        // KaTeX auto-render
        document.addEventListener('DOMContentLoaded', () => {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
        
        // TOC active state
        const tocLinks = document.querySelectorAll('.toc-container a');
        const headings = document.querySelectorAll('.post-content h2, .post-content h3, .post-content h4');
        
        function updateTocActive() {
            let current = '';
            headings.forEach(heading => {
                const rect = heading.getBoundingClientRect();
                if (rect.top <= 100) {
                    current = heading.getAttribute('id');
                }
            });
            
            tocLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === '#' + current) {
                    link.classList.add('active');
                }
            });
        }
        
        window.addEventListener('scroll', updateTocActive);
        updateTocActive();
    </script>
</body>
</html>
