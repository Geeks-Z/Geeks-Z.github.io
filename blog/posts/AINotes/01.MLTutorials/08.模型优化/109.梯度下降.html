<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Untitled</title>
    <meta name="description" content="Untitled - Hongwei Zhao's Blog">
    <meta name="author" content="Hongwei Zhao">
    
    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <!-- Code Highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css" id="hljs-theme-dark">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css" id="hljs-theme-light" disabled>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    
    <style>
        :root {
            /* Light Theme - 明亮清新配色 */
            --primary-color: #4A90D9;
            --primary-hover: #3678C2;
            --link-color: #E86B5F;
            --text-color: #2D2D2D;
            --text-light: #5A5A5A;
            --text-muted: #8A8A8A;
            --bg-color: #FFFFFF;
            --bg-secondary: #F5F7FA;
            --bg-code: #F8F9FC;
            --border-color: #E8ECF0;
            --shadow: 0 2px 8px rgba(0,0,0,0.06);
            --shadow-lg: 0 8px 24px rgba(0,0,0,0.08);
        }

        [data-theme="dark"] {
            --primary-color: #5dade2;
            --primary-hover: #85c1e9;
            --link-color: #e74c3c;
            --text-color: #e5e7eb;
            --text-light: #9ca3af;
            --text-muted: #6b7280;
            --bg-color: #1a1a2e;
            --bg-secondary: #16213e;
            --bg-code: #0f0f23;
            --border-color: #374151;
            --shadow: 0 1px 3px rgba(0,0,0,0.3);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.4);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 16px;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            transition: background-color 0.3s, color 0.3s;
        }

        a {
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--primary-hover);
            text-decoration: underline;
        }

        /* Layout */
        .page-wrapper {
            display: flex;
            max-width: 1400px;
            margin: 0 auto;
            padding: 2rem;
            gap: 3rem;
        }

        /* Sidebar TOC */
        .toc-sidebar {
            width: 260px;
            flex-shrink: 0;
            position: sticky;
            top: 2rem;
            height: fit-content;
            max-height: calc(100vh - 4rem);
            overflow-y: auto;
        }

        .toc-container {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 1.5rem;
            border: 1px solid var(--border-color);
        }

        .toc-container h3 {
            font-size: 0.85rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-muted);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .toc-container ul {
            list-style: none;
        }

        .toc-container li {
            margin-bottom: 0.5rem;
        }

        .toc-container a {
            font-size: 0.9rem;
            color: var(--text-light);
            display: block;
            padding: 0.25rem 0;
            border-left: 2px solid transparent;
            padding-left: 0.75rem;
            transition: all 0.2s;
        }

        .toc-container a:hover,
        .toc-container a.active {
            color: var(--primary-color);
            border-left-color: var(--primary-color);
            text-decoration: none;
        }

        .toc-container ul ul {
            margin-left: 1rem;
        }

        /* Main Content */
        .main-content {
            flex: 1;
            min-width: 0;
            max-width: 800px;
        }

        /* Header */
        .post-header {
            margin-bottom: 2rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
            margin-bottom: 1.5rem;
        }

        .back-link:hover {
            color: var(--primary-color);
            text-decoration: none;
        }

        .post-header h1 {
            font-size: 2.25rem;
            font-weight: 700;
            line-height: 1.3;
            margin-bottom: 1rem;
            color: var(--text-color);
        }

        .post-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
        }

        .post-meta span {
            display: flex;
            align-items: center;
            gap: 0.4rem;
        }

        .post-meta i {
            color: var(--text-muted);
        }

        .tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 1rem;
        }

        .tag {
            display: inline-block;
            font-size: 0.8rem;
            background: var(--bg-secondary);
            color: var(--text-light);
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            border: 1px solid var(--border-color);
            transition: all 0.2s;
        }

        .tag:hover {
            background: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }

        /* Article Content */
        .post-content {
            font-size: 1rem;
            line-height: 1.9;
        }

        .post-content h1,
        .post-content h2,
        .post-content h3,
        .post-content h4,
        .post-content h5,
        .post-content h6 {
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-weight: 600;
            line-height: 1.4;
            color: var(--text-color);
        }

        .post-content h1 { font-size: 1.875rem; }
        .post-content h2 { 
            font-size: 1.5rem; 
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }
        .post-content h3 { font-size: 1.25rem; }
        .post-content h4 { font-size: 1.125rem; }

        .post-content p {
            margin-bottom: 1.25rem;
        }

        .post-content ul,
        .post-content ol {
            margin: 1.25rem 0;
            padding-left: 1.5rem;
        }

        .post-content li {
            margin-bottom: 0.5rem;
        }

        .post-content blockquote {
            margin: 1.5rem 0;
            padding: 1rem 1.5rem;
            background: var(--bg-secondary);
            border-left: 4px solid var(--primary-color);
            border-radius: 0 8px 8px 0;
            color: var(--text-light);
            font-style: italic;
        }

        .post-content blockquote p:last-child {
            margin-bottom: 0;
        }

        /* Code */
        .post-content code {
            font-family: 'JetBrains Mono', 'Fira Code', Consolas, monospace;
            font-size: 0.9em;
            background: var(--bg-code);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            color: var(--link-color);
        }

        .post-content pre {
            margin: 1.5rem 0;
            padding: 1.25rem;
            background: var(--bg-code);
            border-radius: 10px;
            overflow-x: auto;
            border: 1px solid var(--border-color);
        }

        .post-content pre code {
            background: none;
            padding: 0;
            color: inherit;
            font-size: 0.875rem;
            line-height: 1.6;
        }

        /* Images */
        .post-content img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            margin: 1.5rem 0;
            box-shadow: var(--shadow-lg);
        }

        /* Tables */
        .post-content table {
            width: 100%;
            margin: 1.5rem 0;
            border-collapse: collapse;
            font-size: 0.95rem;
        }

        .post-content th,
        .post-content td {
            padding: 0.75rem 1rem;
            border: 1px solid var(--border-color);
            text-align: left;
        }

        .post-content th {
            background: var(--bg-secondary);
            font-weight: 600;
        }

        .post-content tr:nth-child(even) {
            background: var(--bg-secondary);
        }

        /* Math */
        .math-display {
            margin: 1.5rem 0;
            overflow-x: auto;
            padding: 1rem;
            background: var(--bg-secondary);
            border-radius: 8px;
        }

        /* Anchor links */
        .anchor-link {
            opacity: 0;
            margin-left: 0.5rem;
            color: var(--text-muted);
            font-weight: 400;
            transition: opacity 0.2s;
        }

        .post-content h2:hover .anchor-link,
        .post-content h3:hover .anchor-link,
        .post-content h4:hover .anchor-link {
            opacity: 1;
        }

        /* Theme Toggle */
        .theme-toggle {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            background: var(--primary-color);
            color: white;
            border: none;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.25rem;
            box-shadow: var(--shadow-lg);
            transition: transform 0.2s, background 0.2s;
            z-index: 1000;
        }

        .theme-toggle:hover {
            transform: scale(1.1);
            background: var(--primary-hover);
        }

        /* Comments Section */
        .comments-section {
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-color);
        }

        .comments-section h3 {
            font-size: 1.25rem;
            margin-bottom: 1.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        /* Footer */
        .post-footer {
            margin-top: 3rem;
            padding-top: 1.5rem;
            border-top: 1px solid var(--border-color);
            text-align: center;
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        /* Responsive */
        @media (max-width: 1024px) {
            .toc-sidebar {
                display: none;
            }
            
            .page-wrapper {
                padding: 1.5rem;
            }
        }

        @media (max-width: 768px) {
            .post-header h1 {
                font-size: 1.75rem;
            }
            
            .post-meta {
                flex-direction: column;
                gap: 0.5rem;
            }
            
            .theme-toggle {
                bottom: 1rem;
                right: 1rem;
                width: 44px;
                height: 44px;
            }
        }

        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-secondary);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--border-color);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--text-muted);
        }
    </style>
</head>
<body>
    <div class="page-wrapper">
        <!-- TOC Sidebar -->
        <aside class="toc-sidebar">
            <div class="toc-container">
                <h3><i class="fas fa-list"></i> 目录</h3>
                <div class="toc">
<ul>
<li><a href="#梯度下降">梯度下降</a></li>
<li><a href="#特征缩放">特征缩放</a><ul>
<li><a href="#为什么要这样做">为什么要这样做？</a></li>
<li><a href="#怎么做缩放">怎么做缩放？</a></li>
</ul>
</li>
<li><a href="#梯度下降的理论基础">梯度下降的理论基础</a><ul>
<li><a href="#问题">问题</a></li>
</ul>
</li>
<li><a href="#数学理论">数学理论</a><ul>
<li><a href="#泰勒展开式">泰勒展开式</a><ul>
<li><a href="#定义">定义</a></li>
<li><a href="#多变量泰勒展开式">多变量泰勒展开式</a></li>
</ul>
</li>
<li><a href="#利用泰勒展开式简化">利用泰勒展开式简化</a></li>
</ul>
</li>
<li><a href="#梯度下降的限制">梯度下降的限制</a></li>
<li><a href="#vanishing--exploding-gradients">Vanishing / Exploding gradients</a></li>
<li><a href="#weight-initialization-for-deep-networksvanishing--exploding-gradients">Weight Initialization for Deep NetworksVanishing / Exploding gradients</a></li>
<li><a href="#numerical-approximation-of-gradients">Numerical approximation of gradients</a></li>
<li><a href="#gradient-checking">Gradient checking</a></li>
<li><a href="#gradient-checking-implementation-notes">Gradient Checking Implementation Notes</a></li>
<li><a href="#bgdbatch-gradient-descent">BGD(Batch gradient descent)</a><ul>
<li><a href="#theory">Theory</a></li>
<li><a href="#summary">Summary</a></li>
</ul>
</li>
</ul>
</div>

            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <article>
                <header class="post-header">
                    <a href="../../../../index.html" class="back-link">
                        <i class="fas fa-arrow-left"></i> 返回博客列表
                    </a>
                    <h1>Untitled</h1>
                    <div class="post-meta">
                        <span><i class="fas fa-calendar-alt"></i> 2026-02-04</span>
                        <span><i class="fas fa-folder"></i> AINotes/01.MLTutorials/08.模型优化</span>
                        <span><i class="fas fa-user"></i> Hongwei Zhao</span>
                    </div>
                    <div class="tags">
                        
                    </div>
                </header>

                <div class="post-content">
                    <h2 id="梯度下降">梯度下降<a class="anchor-link" href="#梯度下降" title="Permanent link">&para;</a></h2>
<p>解决下面的最优化问题：<br />
<div class="math-display"><br />
\theta^∗= \underset{ \theta }{\operatorname{arg\ min}}  L(\theta) \tag1<br />
</div></p>
<ul>
<li><span class="math-inline">L</span> :lossfunction（损失函数）</li>
<li><span class="math-inline">\theta</span> :parameters（参数）</li>
</ul>
<p>这里的parameters是复数，即 <span class="math-inline">\theta</span> 指代一系列参数，比如 <span class="math-inline">w</span> 和 <span class="math-inline">b</span> 。</p>
<p>我们要找一组参数 <span class="math-inline">\theta</span> ，让损失函数越小越好，这个问题可以用梯度下降法解决：</p>
<p>假设 <span class="math-inline">\theta</span> 有里面有两个参数 <span class="math-inline">\theta_1, \theta_2</span><br />
随机选取初始值</p>
<p><div class="math-display"><br />
\theta^0 = \begin{bmatrix}<br />
\theta_1^0 \<br />
\theta_2^0<br />
\end{bmatrix} \tag2<br />
</div></p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202409130917139.png" /></p>
<p>然后分别计算初始点处，两个参数对 <span class="math-inline">L</span> 的偏微分，然后 <span class="math-inline">\theta^0</span> 减掉 <span class="math-inline">\eta</span> 乘上偏微分的值，得到一组新的参数。同理反复进行这样的计算。黄色部分为简洁的写法，<span class="math-inline">\triangledown L(\theta)</span> 即为梯度。</p>
<blockquote>
<p><span class="math-inline">\eta</span> 叫做Learning rates（学习速率）</p>
</blockquote>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202409130917140.png" /></p>
<p>上图举例将梯度下降法的计算过程进行可视化。</p>
<h2 id="特征缩放">特征缩放<a class="anchor-link" href="#特征缩放" title="Permanent link">&para;</a></h2>
<p>比如有个函数：</p>
<p><div class="math-display">y=b+w_1x_1+w_2x_2 \tag{12}</div><br />
两个输入的分布的范围很不一样，建议把他们的范围缩放，使得不同输入的范围是一样的。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202409130917151.png" /></p>
<h3 id="为什么要这样做">为什么要这样做？<a class="anchor-link" href="#为什么要这样做" title="Permanent link">&para;</a></h3>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202409130917152.png" /></p>
<p>上图左边是 <span class="math-inline">x_1</span> 的scale比 <span class="math-inline">x_2</span> 要小很多，所以当 <span class="math-inline">w_1</span> 和 <span class="math-inline">w_2</span> 做同样的变化时，<span class="math-inline">w_1</span> 对 <span class="math-inline">y</span> 的变化影响是比较小的，<span class="math-inline">x_2</span> 对 <span class="math-inline">y</span> 的变化影响是比较大的。</p>
<p>坐标系中是两个参数的error surface（现在考虑左边蓝色），因为 <span class="math-inline">w_1</span> 对 <span class="math-inline">y</span> 的变化影响比较小，所以 <span class="math-inline">w_1</span> 对损失函数的影响比较小，<span class="math-inline">w_1</span> 对损失函数有比较小的微分，所以 <span class="math-inline">w_1</span> 方向上是比较平滑的。同理 <span class="math-inline">x_2</span> 对 <span class="math-inline">y</span> 的影响比较大，所以 <span class="math-inline">x_2</span> 对损失函数的影响比较大，所以在 <span class="math-inline">x_2</span> 方向有比较尖的峡谷。</p>
<p>上图右边是两个参数scaling比较接近，右边的绿色图就比较接近圆形。</p>
<p>对于左边的情况，上面讲过这种狭长的情形不过不用Adagrad的话是比较难处理的，两个方向上需要不同的学习率，同一组学习率会搞不定它。而右边情形更新参数就会变得比较容易。左边的梯度下降并不是向着最低点方向走的，而是顺着等高线切线法线方向走的。但绿色就可以向着圆心（最低点）走，这样做参数更新也是比较有效率。</p>
<h3 id="怎么做缩放">怎么做缩放？<a class="anchor-link" href="#怎么做缩放" title="Permanent link">&para;</a></h3>
<p>方法非常多，这里举例一种常见的做法：</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202409130917153.png" /></p>
<p>上图每一列都是一个例子，里面都有一组特征。</p>
<p>对每一个维度 <span class="math-inline">i</span>（绿色框）都计算平均数，记做 <span class="math-inline">m_i</span>；还要计算标准差，记做 <span class="math-inline">\sigma _i</span>。</p>
<p>然后用第 <span class="math-inline">r</span> 个例子中的第 <span class="math-inline">i</span> 个输入，减掉平均数 <span class="math-inline">m_i</span>，然后除以标准差 <span class="math-inline">\sigma _i</span>，得到的结果是所有的维数都是 <span class="math-inline">0</span>，所有的方差都是 <span class="math-inline">1</span></p>
<h2 id="梯度下降的理论基础">梯度下降的理论基础<a class="anchor-link" href="#梯度下降的理论基础" title="Permanent link">&para;</a></h2>
<h3 id="问题">问题<a class="anchor-link" href="#问题" title="Permanent link">&para;</a></h3>
<p>当用梯度下降解决问题：</p>
<p><div class="math-display">\theta^∗= \underset{ \theta }{\operatorname{arg\ min}}  L(\theta) \tag1</div></p>
<p>每次更新参数 <span class="math-inline">\theta</span>，都得到一个新的 <span class="math-inline">\theta</span>，它都使得损失函数更小。即：</p>
<p><div class="math-display">L(\theta^0) &gt;L(\theta^1)&gt;L(\theta^2)&gt;···\tag{13}</div></p>
<p>上述结论正确吗？</p>
<p>结论是不正确的。。。</p>
<h2 id="数学理论">数学理论<a class="anchor-link" href="#数学理论" title="Permanent link">&para;</a></h2>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202409130917154.png" /></p>
<p>比如在 <span class="math-inline">\theta^0</span> 处，可以在一个小范围的圆圈内找到损失函数细小的 <span class="math-inline">\theta^1</span>，不断的这样去寻找。</p>
<p>接下来就是如果在小圆圈内快速的找到最小值？</p>
<h3 id="泰勒展开式">泰勒展开式<a class="anchor-link" href="#泰勒展开式" title="Permanent link">&para;</a></h3>
<h4 id="定义">定义<a class="anchor-link" href="#定义" title="Permanent link">&para;</a></h4>
<p>若 <span class="math-inline">h(x)</span> 在 <span class="math-inline">x=x_0</span> 点的某个领域内有无限阶导数（即无限可微分，infinitely differentiable），那么在此领域内有：<br />
<div class="math-display"><br />
h(x)  = \sum_{k=0}^{\infty }\frac{h^k(x_0)}{k!}(x-x_0)^k\=h(x_0)+{h}'(x_0)(x−x_0)+\frac{h''(x_0)}{2!}(x−x_0)^2+⋯<br />
</div></p>
<p>当 <span class="math-inline">x</span> 很接近 <span class="math-inline">x_0</span> 时，有 <span class="math-inline">h(x)≈h(x_0)+{h}'(x_0)(x−x_0)</span><br />
式14 就是函数 <span class="math-inline">h(x)</span> 在 <span class="math-inline">x=x_0</span> 点附近关于 <span class="math-inline">x</span> 的幂函数展开式，也叫泰勒展开式。</p>
<p>举例：</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202409130917155.png" /></p>
<p>图中3条蓝色线是把前3项作图，橙色线是 <span class="math-inline">sin(x)</span>。</p>
<h4 id="多变量泰勒展开式">多变量泰勒展开式<a class="anchor-link" href="#多变量泰勒展开式" title="Permanent link">&para;</a></h4>
<p>下面是两个变量的泰勒展开式</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202409130917156.png" /></p>
<h3 id="利用泰勒展开式简化">利用泰勒展开式简化<a class="anchor-link" href="#利用泰勒展开式简化" title="Permanent link">&para;</a></h3>
<p>回到之前如何快速在圆圈内找到最小值。基于泰勒展开式，在 <span class="math-inline">(a,b)</span> 点的红色圆圈范围内，可以将损失函数用泰勒展开式进行简化：</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202409130917157.png" /></p>
<p>将问题进而简化为下图：</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202409130917158.png" /></p>
<p>不考虑s的话，可以看出剩下的部分就是两个向量<span class="math-inline">(\triangle \theta_1,\triangle \theta_2)</span> 和  <span class="math-inline">(u,v)</span> 的内积，那怎样让它最小，就是和向量 <span class="math-inline">(u,v)</span> 方向相反的向量</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202409130917159.png" /></p>
<p>然后将u和v带入。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202409130917160.png" /><br />
<div class="math-display">L(\theta)\approx s+u(\theta_1 - a)+v(\theta_2 - b) \tag{14}</div></p>
<p>发现最后的式子就是梯度下降的式子。但这里用这种方法找到这个式子有个前提，泰勒展开式给的损失函数的估算值是要足够精确的，而这需要红色的圈圈足够小（也就是学习率足够小）来保证。所以理论上每次更新参数都想要损失函数减小的话，即保证式1-2 成立的话，就需要学习率足够足够小才可以。</p>
<p>所以实际中，当更新参数的时候，如果学习率没有设好，是有可能式1-2是不成立的，所以导致做梯度下降的时候，损失函数没有越来越小。</p>
<p>式1-2只考虑了泰勒展开式的一次项，如果考虑到二次项（比如牛顿法），在实际中不是特别好，会涉及到二次微分等，多很多的运算，性价比不好。</p>
<h2 id="梯度下降的限制">梯度下降的限制<a class="anchor-link" href="#梯度下降的限制" title="Permanent link">&para;</a></h2>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202409130917161.png" /></p>
<p>容易陷入局部极值<br />
还有可能卡在不是极值，但微分值是0的地方<br />
还有可能实际中只是当微分值小于某一个数值就停下来了，但这里只是比较平缓，并不是极值点</p>
<h2 id="vanishing--exploding-gradients">Vanishing / Exploding gradients<a class="anchor-link" href="#vanishing--exploding-gradients" title="Permanent link">&para;</a></h2>
<p>训练神经网络，尤其是深度神经所面临的一个问题就是梯度消失或梯度爆炸，也就是你训练神经网络的时候，导数或坡度有时会变得非常大，或者非常小，甚至于以指数方式变小，这加大了训练的难度。</p>
<p>这节课，你将会了解梯度消失或梯度爆炸的真正含义，以及如何更明智地选择随机初始化权重，从而避免这个问题。<br />
假设你正在训练这样一个极深的神经网络，为了节约幻灯片上的空间，我画的神经网络每层只有两个隐藏单元，但它可能含有更多，但这个神经网络会有参数<span class="math-inline">W^{[1]}</span>，<span class="math-inline">W^{[2]}</span>，<span class="math-inline">W^{[3]}</span> 等，直到<span class="math-inline">W^{[l]}</span>，为了简单起见，假设我们使用激活函数<span class="math-inline">g(z)=z</span>，也就是线性激活函数，我们忽略<span class="math-inline">b</span>，假设<span class="math-inline">b^{[l]}</span>=0，如果那样的话，输出<span class="math-inline">y=W^{[l]}W^{[L -1]}W^{[L - 2]}\ldots W^{[3]}W^{[2]}W^{[1]}x</span>，如果你想考验我的数学水平，<span class="math-inline">W^{[1]} x = z^{[1]}</span>，因为<span class="math-inline">b=0</span>，所以我想<span class="math-inline">z^{[1]} =W^{[1]} x</span>，<span class="math-inline">a^{[1]} = g(z^{[1]})</span>，因为我们使用了一个线性激活函数，它等于<span class="math-inline">z^{[1]}</span>，所以第一项<span class="math-inline">W^{[1]} x = a^{[1]}</span>，通过推理，你会得出<span class="math-inline">W^{[2]}W^{[1]}x =a^{[2]}</span>，因为<span class="math-inline">a^{[2]} = g(z^{[2]})</span>，还等于<span class="math-inline">g(W^{[2]}a^{[1]})</span>，可以用<span class="math-inline">W^{[1]}x</span> 换<span class="math-inline">a^{[1]}</span>，所以这一项就等于<span class="math-inline">a^{[2]}</span>，这个就是<span class="math-inline">a^{[3]}</span>(<span class="math-inline">W^{[3]}W^{[2]}W^{[1]}x</span>)。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208160954203.png" /></p>
<p>所有这些矩阵数据传递的协议将给出<span class="math-inline">\hat y</span> 不是<span class="math-inline">y</span> 值。</p>
<p>假设每个权重矩阵<span class="math-inline">W^{[l]} = \begin{bmatrix} 1.5 &amp; 0 \0 &amp; 1.5 \\end{bmatrix}</span>，从技术上来讲，最后一项有不同维度，可能它就是余下的权重矩阵，<span class="math-inline">y= W^{[1]}\begin{bmatrix} 1.5 &amp; 0 \ 0 &amp; 1.5 \\end{bmatrix}^{(L -1)}x</span>，因为我们假设所有矩阵都等于它，它是1.5倍的单位矩阵，最后的计算结果就是<span class="math-inline">\hat{y}</span>，<span class="math-inline">\hat{y}</span> 就是等于<span class="math-inline">{1.5}^{(L-1)}x</span>。如果对于一个深度神经网络来说<span class="math-inline">L</span> 较大，那么<span class="math-inline">\hat{y}</span> 值也会非常大，实际上它呈指数级增长的，它增长的比率是<span class="math-inline">{1.5}^{L}</span>，因此对于一个深度神经网络，<span class="math-inline">y</span> 值将爆炸式增长。</p>
<p>相反的，如果权重是0.5，<span class="math-inline">W^{[l]} = \begin{bmatrix} 0.5&amp; 0 \ 0 &amp; 0.5 \ \end{bmatrix}</span>，它比1小，这项也就变成了<span class="math-inline">{0.5}^{L}</span>，矩阵<span class="math-inline">y= W^{[1]}\begin{bmatrix} 0.5 &amp; 0 \ 0 &amp; 0.5 \\end{bmatrix}^{(L - 1)}x</span>，再次忽略<span class="math-inline">W^{[L]}</span>，因此每个矩阵都小于1，假设<span class="math-inline">x_{1}</span> <span class="math-inline">x_{2}</span> 是1，激活函数将变成<span class="math-inline">\frac{1}{2}</span>，<span class="math-inline">\frac{1}{2}</span>，<span class="math-inline">\frac{1}{4}</span>，<span class="math-inline">\frac{1}{4}</span>，<span class="math-inline">\frac{1}{8}</span>，<span class="math-inline">\frac{1}{8}</span> ，直到最后一项变成<span class="math-inline">\frac{1}{2^{L}}</span>，所以作为自定义函数，激活函数的值将以指数级下降，它是与网络层数数量<span class="math-inline">L</span> 关的函数，在深度网络中，激活函数以指数级递减。</p>
<p>我希望你得到的直观理解是，权重<span class="math-inline">W</span> 比1略大一点，或者说只是比单位矩阵大一点，深度神经网络的激活函数将爆炸式增长，如果<span class="math-inline">W</span> 1略小一点，可能是<span class="math-inline">\begin{bmatrix}0.9 &amp; 0 \ 0 &amp; 0.9 \ \end{bmatrix}</span>。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208160954072.png" /></p>
<p>在深度神经网络中，激活函数将以指数级递减，虽然我只是讨论了激活函数以与<span class="math-inline">L</span> 关的指数级数增长或下降，它也适用于与层数<span class="math-inline">L</span> 关的导数或梯度函数，也是呈指数级增长或呈指数递减。</p>
<p>对于当前的神经网络，假设<span class="math-inline">L=150</span>，最近<strong>Microsoft</strong>对152层神经网络的研究取得了很大进展，在这样一个深度神经网络中，如果激活函数或梯度函数以与<span class="math-inline">L</span> 关的指数增长或递减，它们的值将会变得极大或极小，从而导致训练难度上升，尤其是梯度指数小于<span class="math-inline">L</span> ，梯度下降算法的步长会非常非常小，梯度下降算法将花费很长时间来学习。</p>
<p>总结一下，我们讲了深度神经网络是如何产生梯度消失或爆炸问题的，实际上，在很长一段时间内，它曾是训练深度神经网络的阻力，虽然有一个不能彻底解决此问题的解决方案，但是已在如何选择初始化权重问题上提供了很多帮助。</p>
<h2 id="weight-initialization-for-deep-networksvanishing--exploding-gradients">Weight Initialization for Deep NetworksVanishing / Exploding gradients<a class="anchor-link" href="#weight-initialization-for-deep-networksvanishing--exploding-gradients" title="Permanent link">&para;</a></h2>
<p>上节课，我们学习了深度神经网络如何产生梯度消失和梯度爆炸问题，最终针对该问题，我们想出了一个不完整的解决方案，虽然不能彻底解决问题，却很有用，有助于我们为神经网络更谨慎地选择随机初始化参数，为了更好地理解它，我们先举一个神经单元初始化地例子，然后再演变到整个深度网络。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208160954833.png" /></p>
<p>我们来看看只有一个神经元的情况，然后才是深度网络。</p>
<p>单个神经元可能有4个输入特征，从<span class="math-inline">x_{1}</span> <span class="math-inline">x_{4}</span>，经过<span class="math-inline">a=g(z)</span> 理，最终得到<span class="math-inline">\hat{y}</span>，稍后讲深度网络时，这些输入表示为<span class="math-inline">a^{[l]}</span>，暂时我们用<span class="math-inline">x</span> 示。</p>
<p><span class="math-inline">z = w_{1}x_{1} + w_{2}x_{2} + \ldots +w_{n}x_{n}</span>，<span class="math-inline">b=0</span>，暂时忽略<span class="math-inline">b</span>，为了预防<span class="math-inline">z</span> 过大或过小，你可以看到<span class="math-inline">n</span> 大，你希望<span class="math-inline">w_{i}</span> 小，因为<span class="math-inline">z</span> <span class="math-inline">w_{i}x_{i}</span> 和，如果你把很多此类项相加，希望每项值更小，最合理的方法就是设置<span class="math-inline">w_{i}=\frac{1}{n}</span>，<span class="math-inline">n</span> 示神经元的输入特征数量，实际上，你要做的就是设置某层权重矩阵<span class="math-inline">w^{[l]} = np.random.randn( \text{shape})*\text{np.}\text{sqrt}(\frac{1}{n^{[l-1]}})</span>，<span class="math-inline">n^{[l - 1]}</span> 是我喂给第<span class="math-inline">l</span> 神经单元的数量（即第<span class="math-inline">l-1</span> 神经元数量）。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208160954742.png" /></p>
<p>结果，如果你是用的是<strong>Relu</strong>激活函数，而不是<span class="math-inline">\frac{1}{n}</span>，方差设置为<span class="math-inline">\frac{2}{n}</span>，效果会更好。你常常发现，初始化时，尤其是使用<strong>Relu</strong>激活函数时，<span class="math-inline">g^{[l]}(z) =Relu(z)</span>,它取决于你对随机变量的熟悉程度，这是高斯随机变量，然后乘以它的平方根，也就是引用这个方差<span class="math-inline">\frac{2}{n}</span>。这里，我用的是<span class="math-inline">n^{[l - 1]}</span>，因为本例中，逻辑回归的特征是不变的。但一般情况下<span class="math-inline">l</span> 上的每个神经元都有<span class="math-inline">n^{[l - 1]}</span> 输入。如果激活函数的输入特征被零均值和标准方差化，方差是1，<span class="math-inline">z</span> 会调整到相似范围，这就没解决问题（梯度消失和爆炸问题）。但它确实降低了梯度消失和爆炸问题，因为它给权重矩阵<span class="math-inline">w</span> 置了合理值，你也知道，它不能比1大很多，也不能比1小很多，所以梯度没有爆炸或消失过快。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208160954489.png" /></p>
<p>我提到了其它变体函数，刚刚提到的函数是<strong>Relu</strong>激活函数，一篇由<strong>Herd</strong>等人撰写的论文曾介绍过。对于几个其它变体函数，如<strong>tanh</strong>激活函数，有篇论文提到，常量1比常量2的效率更高，对于<strong>tanh</strong>函数来说，它是<span class="math-inline">\sqrt{\frac{1}{n^{[l-1]}}}</span>，这里平方根的作用与这个公式作用相同(<span class="math-inline">\text{np.}\text{sqrt}(\frac{1}{n^{[l-1]}})</span>)，它适用于<strong>tanh</strong>激活函数，被称为<strong>Xavier</strong>初始化。<strong>Yoshua Bengio</strong>和他的同事还提出另一种方法，你可能在一些论文中看到过，它们使用的是公式<span class="math-inline">\sqrt{\frac{2}{n^{[l-1]} + n^{\left[l\right]}}}</span>。其它理论已对此证明，但如果你想用<strong>Relu</strong>激活函数，也就是最常用的激活函数，我会用这个公式<span class="math-inline">\text{np.}\text{sqrt}(\frac{2}{n^{[l-1]}})</span>，如果使用<strong>tanh</strong>函数，可以用公式<span class="math-inline">\sqrt{\frac{1}{n^{[l-1]}}}</span>，有些作者也会使用这个函数。</p>
<p>实际上，我认为所有这些公式只是给你一个起点，它们给出初始化权重矩阵的方差的默认值，如果你想添加方差，方差参数则是另一个你需要调整的超级参数，可以给公式<span class="math-inline">\text{np.}\text{sqrt}(\frac{2}{n^{[l-1]}})</span> 加一个乘数参数，调优作为超级参数激增一份子的乘子参数。有时调优该超级参数效果一般，这并不是我想调优的首要超级参数，但我发现调优过程中产生的问题，虽然调优该参数能起到一定作用，但考虑到相比调优，其它超级参数的重要性，我通常把它的优先级放得比较低。</p>
<p>希望你现在对梯度消失或爆炸问题以及如何为权重初始化合理值已经有了一个直观认识，希望你设置的权重矩阵既不会增长过快，也不会太快下降到0，从而训练出一个权重或梯度不会增长或消失过快的深度网络。我们在训练深度网络时，这也是一个加快训练速度的技巧。</p>
<h2 id="numerical-approximation-of-gradients">Numerical approximation of gradients<a class="anchor-link" href="#numerical-approximation-of-gradients" title="Permanent link">&para;</a></h2>
<p>在实施<strong>backprop</strong>时，有一个测试叫做梯度检验，它的作用是确保<strong>backprop</strong>正确实施。因为有时候，你虽然写下了这些方程式，却不能100%确定，执行<strong>backprop</strong>的所有细节都是正确的。为了逐渐实现梯度检验，我们首先说说如何计算梯度的数值逼近，下节课，我们将讨论如何在<strong>backprop</strong>中执行梯度检验，以确保<strong>backprop</strong>正确实施。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208160954895.png" /></p>
<p>我们先画出函数<span class="math-inline">f</span>，标记为<span class="math-inline">f\left( \theta \right)</span>，<span class="math-inline">f\left( \theta \right)=\theta^{3}</span>，先看一下<span class="math-inline">\theta</span> 值，假设<span class="math-inline">\theta=1</span>，不增大<span class="math-inline">\theta</span> 值，而是在<span class="math-inline">\theta</span> 右侧，设置一个<span class="math-inline">\theta +\varepsilon</span>，在<span class="math-inline">\theta</span> 侧，设置<span class="math-inline">\theta -\varepsilon</span>。因此<span class="math-inline">\theta=1</span>，<span class="math-inline">\theta +\varepsilon =1.01,\theta -\varepsilon =0.99</span>,，跟以前一样，<span class="math-inline">\varepsilon</span> 值为0.01，看下这个小三角形，计算高和宽的比值，就是更准确的梯度预估，选择<span class="math-inline">f</span> 数在<span class="math-inline">\theta -\varepsilon</span> 的这个点，用这个较大三角形的高比上宽，技术上的原因我就不详细解释了，较大三角形的高宽比值更接近于<span class="math-inline">\theta</span> 导数，把右上角的三角形下移，好像有了两个三角形，右上角有一个，左下角有一个，我们通过这个绿色大三角形同时考虑了这两个小三角形。所以我们得到的不是一个单边公差而是一个双边公差。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208160954247.png" /></p>
<p>我们写一下数据算式，图中绿色三角形上边的点的值是<span class="math-inline">f( \theta +\varepsilon )</span>，下边的点是<span class="math-inline">f( \theta-\varepsilon)</span>，这个三角形的高度是<span class="math-inline">f( \theta +\varepsilon)-f(\theta -\varepsilon)</span>，这两个宽度都是ε，所以三角形的宽度是<span class="math-inline">2\varepsilon</span>，高宽比值为<span class="math-inline">\frac{f(\theta + \varepsilon ) - (\theta -\varepsilon)}{2\varepsilon}</span>，它的期望值接近<span class="math-inline">g( \theta)</span>，<span class="math-inline">f( \theta)=\theta^{3}</span> 入参数值，<span class="math-inline">\frac {f\left( \theta + \varepsilon \right) - f(\theta -\varepsilon)}{2\varepsilon} = \frac{{(1.01)}^{3} - {(0.99)}^{3}}{2 \times0.01}</span>，大家可以暂停视频，用计算器算算结果，结果应该是3.0001，而前面一张幻灯片上面是，当<span class="math-inline">\theta =1</span> ，<span class="math-inline">g( \theta)=3\theta^{2} =3</span>，所以这两个<span class="math-inline">g(\theta)</span> 非常接近，逼近误差为0.0001，前一张幻灯片，我们只考虑了单边公差，即从<span class="math-inline">\theta </span> <span class="math-inline">\theta +\varepsilon</span> 间的误差，<span class="math-inline">g( \theta)</span> 值为3.0301，逼近误差是0.03，不是0.0001，所以使用双边误差的方法更逼近导数，其结果接近于3，现在我们更加确信，<span class="math-inline">g( \theta)</span> 能是<span class="math-inline">f</span> 数的正确实现，在梯度检验和反向传播中使用该方法时，最终，它与运行两次单边公差的速度一样，实际上，我认为这种方法还是非常值得使用的，因为它的结果更准确。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208160954531.png" /></p>
<p>这是一些你可能比较熟悉的微积分的理论，如果你不太明白我讲的这些理论也没关系，导数的官方定义是针对值很小的<span class="math-inline">\varepsilon</span>，导数的官方定义是<span class="math-inline">f^{'}\theta) = \operatorname{}\frac{f( \theta + \varepsilon) -f(\theta -\varepsilon)}{2\varepsilon}</span>，如果你上过微积分课，应该学过无穷尽的定义，我就不在这里讲了。</p>
<p>对于一个非零的<span class="math-inline">\varepsilon</span>，它的逼近误差可以写成<span class="math-inline">O(\varepsilon^{2})</span>，ε值非常小，如果<span class="math-inline">\varepsilon=0.01</span>，<span class="math-inline">\varepsilon^{2}=0.0001</span>，大写符号<span class="math-inline">O</span> 含义是指逼近误差其实是一些常量乘以<span class="math-inline">\varepsilon^{2}</span>，但它的确是很准确的逼近误差，所以大写<span class="math-inline">O</span> 常量有时是1。然而，如果我们用另外一个公式逼近误差就是<span class="math-inline">O(\varepsilon)</span>，当<span class="math-inline">\varepsilon</span> 于1时，实际上<span class="math-inline">\varepsilon</span> <span class="math-inline">\varepsilon^{2}</span> 很多，所以这个公式近似值远没有左边公式的准确，所以在执行梯度检验时，我们使用双边误差，即<span class="math-inline">\frac{f\left(\theta + \varepsilon \right) - f(\theta -\varepsilon)}{2\varepsilon}</span>，而不使用单边公差，因为它不够准确。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208160954955.png" /></p>
<p>如果你不理解上面两条结论，所有公式都在这儿，不用担心，如果你对微积分和数值逼近有所了解，这些信息已经足够多了，重点是要记住，双边误差公式的结果更准确，下节课我们做梯度检验时就会用到这个方法。</p>
<p>今天我们讲了如何使用双边误差来判断别人给你的函数<span class="math-inline">g( \theta)</span>，是否正确实现了函数<span class="math-inline">f</span> 偏导，现在我们可以使用这个方法来检验反向传播是否得以正确实施，如果不正确，它可能有bug需要你来解决。</p>
<h2 id="gradient-checking">Gradient checking<a class="anchor-link" href="#gradient-checking" title="Permanent link">&para;</a></h2>
<p>梯度检验帮我们节省了很多时间，也多次帮我发现<strong>backprop</strong>实施过程中的bug，接下来，我们看看如何利用它来调试或检验<strong>backprop</strong>的实施是否正确。</p>
<p>假设你的网络中含有下列参数，<span class="math-inline">W^{[1]}</span> <span class="math-inline">b^{[1]}</span>……<span class="math-inline">W^{[l]}</span> <span class="math-inline">b^{[l]}</span>，为了执行梯度检验，首先要做的就是，把所有参数转换成一个巨大的向量数据，你要做的就是把矩阵<span class="math-inline">W</span> 换成一个向量，把所有<span class="math-inline">W</span> 阵转换成向量之后，做连接运算，得到一个巨型向量<span class="math-inline">\theta</span>，该向量表示为参数<span class="math-inline">\theta</span>，代价函数<span class="math-inline">J</span> 所有<span class="math-inline">W</span> <span class="math-inline">b</span> 函数，现在你得到了一个<span class="math-inline">\theta</span> 代价函数<span class="math-inline">J</span>（即<span class="math-inline">J(\theta)</span>）。接着，你得到与<span class="math-inline">W</span> <span class="math-inline">b</span> 序相同的数据，你同样可以把<span class="math-inline">dW^{[1]}</span> <span class="math-inline">{db}^{[1]}</span>……<span class="math-inline">{dW}^{[l]}</span> <span class="math-inline">{db}^{[l]}</span> 换成一个新的向量，用它们来初始化大向量<span class="math-inline">d\theta</span>，它与<span class="math-inline">\theta</span> 有相同维度。</p>
<p>同样的，把<span class="math-inline">dW^{[1]}</span> 换成矩阵，<span class="math-inline">db^{[1]}</span> 经是一个向量了，直到把<span class="math-inline">{dW}^{[l]}</span> 换成矩阵，这样所有的<span class="math-inline">dW</span> 已经是矩阵，注意<span class="math-inline">dW^{[1]}</span> <span class="math-inline">W^{[1]}</span> 有相同维度，<span class="math-inline">db^{[1]}</span> <span class="math-inline">b^{[1]}</span> 有相同维度。经过相同的转换和连接运算操作之后，你可以把所有导数转换成一个大向量<span class="math-inline">d\theta</span>，它与<span class="math-inline">\theta</span> 有相同维度，现在的问题是<span class="math-inline">d\theta</span> 代价函数<span class="math-inline">J</span> 梯度或坡度有什么关系？</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208160954475.png" /></p>
<p>这就是实施梯度检验的过程，英语里通常简称为“<strong>grad check</strong>”，首先，我们要清楚<span class="math-inline">J</span> 超参数<span class="math-inline">\theta</span> 一个函数，你也可以将J函数展开为<span class="math-inline">J(\theta_{1},\theta_{2},\theta_{3},\ldots\ldots)</span>，不论超级参数向量<span class="math-inline">\theta</span> 维度是多少，为了实施梯度检验，你要做的就是循环执行，从而对每个<span class="math-inline">i</span> 就是对每个<span class="math-inline">\theta</span> 成元素计算<span class="math-inline">d\theta_{\text{approx}}[i]</span> 值，我使用双边误差，也就是</p>
<p><span class="math-inline">d\theta_{\text{approx}}\left[i \right] = \frac{J\left( \theta_{1},\theta_{2},\ldots\theta_{i} + \varepsilon,\ldots \right) - J\left( \theta_{1},\theta_{2},\ldots\theta_{i} - \varepsilon,\ldots \right)}{2\varepsilon}</span></p>
<p>只对<span class="math-inline">\theta_{i}</span> 加<span class="math-inline">\varepsilon</span>，其它项保持不变，因为我们使用的是双边误差，对另一边做同样的操作，只不过是减去<span class="math-inline">\varepsilon</span>，<span class="math-inline">\theta</span> 它项全都保持不变。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208160954918.png" /></p>
<p>从上节课中我们了解到这个值（<span class="math-inline">d\theta_{\text{approx}}\left[i \right]</span>）应该逼近<span class="math-inline">d\theta\left[i \right]</span>=<span class="math-inline">\frac{\partial J}{\partial\theta_{i}}</span>，<span class="math-inline">d\theta\left[i \right]</span> 代价函数的偏导数，然后你需要对i的每个值都执行这个运算，最后得到两个向量，得到<span class="math-inline">d\theta</span> 逼近值<span class="math-inline">d\theta_{\text{approx}}</span>，它与<span class="math-inline">d\theta</span> 有相同维度，它们两个与<span class="math-inline">\theta</span> 有相同维度，你要做的就是验证这些向量是否彼此接近。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208160954254.png" /></p>
<p>具体来说，如何定义两个向量是否真的接近彼此？我一般做下列运算，计算这两个向量的距离，<span class="math-inline">d\theta_{\text{approx}}\left[i \right] - d\theta[i]</span> 欧几里得范数，注意这里（<span class="math-inline">{||d\theta_{\text{approx}} -d\theta||}_{2}</span>）没有平方，它是误差平方之和，然后求平方根，得到欧式距离，然后用向量长度归一化，使用向量长度的欧几里得范数。分母只是用于预防这些向量太小或太大，分母使得这个方程式变成比率，我们实际执行这个方程式，<span class="math-inline">\varepsilon</span> 能为<span class="math-inline">10^{-7}</span>，使用这个取值范围内的<span class="math-inline">\varepsilon</span>，如果你发现计算方程式得到的值为<span class="math-inline">10^{-7}</span> 更小，这就很好，这就意味着导数逼近很有可能是正确的，它的值非常小。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208160954604.png" /></p>
<p>如果它的值在<span class="math-inline">10^{-5}</span> 围内，我就要小心了，也许这个值没问题，但我会再次检查这个向量的所有项，确保没有一项误差过大，可能这里有<strong>bug</strong>。</p>
<p>如果左边这个方程式结果是<span class="math-inline">10^{-3}</span>，我就会担心是否存在<strong>bug</strong>，计算结果应该比<span class="math-inline">10^{- 3}</span> 很多，如果比<span class="math-inline">10^{-3}</span> 很多，我就会很担心，担心是否存在<strong>bug</strong>。这时应该仔细检查所有<span class="math-inline">\theta</span> ，看是否有一个具体的<span class="math-inline">i</span> ，使得<span class="math-inline">d\theta_{\text{approx}}\left[i \right]</span> <span class="math-inline"> d\theta[i]</span> 不相同，并用它来追踪一些求导计算是否正确，经过一些调试，最终结果会是这种非常小的值（<span class="math-inline">10^{-7}</span>），那么，你的实施可能是正确的。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208160954036.png" /></p>
<p>在实施神经网络时，我经常需要执行<strong>foreprop</strong>和<strong>backprop</strong>，然后我可能发现这个梯度检验有一个相对较大的值，我会怀疑存在<strong>bug</strong>，然后开始调试，调试，调试，调试一段时间后，我得到一个很小的梯度检验值，现在我可以很自信的说，神经网络实施是正确的。</p>
<p>现在你已经了解了梯度检验的工作原理，它帮助我在神经网络实施中发现了很多<strong>bug</strong>，希望它对你也有所帮助。</p>
<h2 id="gradient-checking-implementation-notes">Gradient Checking Implementation Notes<a class="anchor-link" href="#gradient-checking-implementation-notes" title="Permanent link">&para;</a></h2>
<p>这节课，分享一些关于如何在神经网络实施梯度检验的实用技巧和注意事项。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208160954558.png" /></p>
<p>首先，不要在训练中使用梯度检验，它只用于调试。我的意思是，计算所有<span class="math-inline">i</span> 的<span class="math-inline">d\theta_{\text{approx}}\left[i\right]</span> 一个非常漫长的计算过程，为了实施梯度下降，你必须使用<span class="math-inline">W</span> <span class="math-inline">b</span> <strong>backprop</strong>来计算<span class="math-inline">d\theta</span>，并使用<strong>backprop</strong>来计算导数，只要调试的时候，你才会计算它，来确认数值是否接近<span class="math-inline">d\theta</span>。完成后，你会关闭梯度检验，梯度检验的每一个迭代过程都不执行它，因为它太慢了。</p>
<p>第二点，如果算法的梯度检验失败，要检查所有项，检查每一项，并试着找出<strong>bug</strong>，也就是说，如果<span class="math-inline">d\theta_{\text{approx}}\left[i\right]</span> dθ[i]的值相差很大，我们要做的就是查找不同的i值，看看是哪个导致<span class="math-inline">d\theta_{\text{approx}}\left[i\right]</span> <span class="math-inline">d\theta\left[i\right]</span> 值相差这么多。举个例子，如果你发现，相对某些层或某层的<span class="math-inline">\theta</span> <span class="math-inline">d\theta</span> 值相差很大，但是<span class="math-inline">\text{dw}^{[l]}</span> 各项非常接近，注意<span class="math-inline">\theta</span> 各项与<span class="math-inline">b</span> <span class="math-inline">w</span> 各项都是一一对应的，这时，你可能会发现，在计算参数<span class="math-inline">b</span> 导数<span class="math-inline">db</span> 过程中存在<strong>bug</strong>。反过来也是一样，如果你发现它们的值相差很大，<span class="math-inline">d\theta_{\text{approx}}\left[i\right]</span> 值与<span class="math-inline">d\theta\left[i\right]</span> 值相差很大，你会发现所有这些项目都来自于<span class="math-inline">dw</span> 某层的<span class="math-inline">dw</span>，可能帮你定位bug的位置，虽然未必能够帮你准确定位bug的位置，但它可以帮助你估测需要在哪些地方追踪<strong>bug</strong>。</p>
<p>第三点，在实施梯度检验时，如果使用正则化，请注意正则项。如果代价函数<span class="math-inline">J(\theta) = \frac{1}{m}\sum_{}^{}{L(\hat y^{(i)},y^{(i)})} + \frac{\lambda}{2m}\sum_{}^{}{||W^{[l]}||}^{2}</span>，这就是代价函数<span class="math-inline">J</span> 定义，<span class="math-inline">d\theta</span> 于与<span class="math-inline">\theta</span> 关的<span class="math-inline">J</span> 数的梯度，包括这个正则项，记住一定要包括这个正则项。</p>
<p>第四点，梯度检验不能与<strong>dropout</strong>同时使用，因为每次迭代过程中，<strong>dropout</strong>会随机消除隐藏层单元的不同子集，难以计算<strong>dropout</strong>在梯度下降上的代价函数<span class="math-inline">J</span>。因此<strong>dropout</strong>可作为优化代价函数<span class="math-inline">J</span> 一种方法，但是代价函数J被定义为对所有指数极大的节点子集求和。而在任何迭代过程中，这些节点都有可能被消除，所以很难计算代价函数<span class="math-inline">J</span>。你只是对成本函数做抽样，用<strong>dropout</strong>，每次随机消除不同的子集，所以很难用梯度检验来双重检验<strong>dropout</strong>的计算，所以我一般不同时使用梯度检验和<strong>dropout</strong>。如果你想这样做，可以把<strong>dropout</strong>中的<strong>keepprob</strong>设置为1.0，然后打开<strong>dropout</strong>，并寄希望于<strong>dropout</strong>的实施是正确的，你还可以做点别的，比如修改节点丢失模式确定梯度检验是正确的。实际上，我一般不这么做，我建议关闭<strong>dropout</strong>，用梯度检验进行双重检查，在没有<strong>dropout</strong>的情况下，你的算法至少是正确的，然后打开<strong>dropout</strong>。</p>
<p>最后一点，也是比较微妙的一点，现实中几乎不会出现这种情况。当<span class="math-inline">w</span> <span class="math-inline">b</span> 近0时，梯度下降的实施是正确的，在随机初始化过程中……，但是在运行梯度下降时，<span class="math-inline">w</span> <span class="math-inline">b</span> 得更大。可能只有在<span class="math-inline">w</span> <span class="math-inline">b</span> 近0时，<strong>backprop</strong>的实施才是正确的。但是当<span class="math-inline">W</span> <span class="math-inline">b</span> 大时，它会变得越来越不准确。你需要做一件事，我不经常这么做，就是在随机初始化过程中，运行梯度检验，然后再训练网络，<span class="math-inline">w</span> <span class="math-inline">b</span> 有一段时间远离0，如果随机初始化值比较小，反复训练网络之后，再重新运行梯度检验。</p>
<p>这就是梯度检验，恭喜大家，这是本周最后一课了。回顾这一周，我们讲了如何配置训练集，验证集和测试集，如何分析偏差和方差，如何处理高偏差或高方差以及高偏差和高方差并存的问题，如何在神经网络中应用不同形式的正则化，如<span class="math-inline">L2</span> 则化和<strong>dropout</strong>，还有加快神经网络训练速度的技巧，最后是梯度检验。这一周我们学习了很多内容，你可以在本周编程作业中多多练习这些概念。祝你好运，期待下周再见。</p>
<hr />
<h2 id="bgdbatch-gradient-descent">BGD(Batch gradient descent)<a class="anchor-link" href="#bgdbatch-gradient-descent" title="Permanent link">&para;</a></h2>
<h3 id="theory">Theory<a class="anchor-link" href="#theory" title="Permanent link">&para;</a></h3>
<p>假设<span class="math-inline">f\left(x\right)</span> <span class="math-inline">R^{n}</span> 具有一阶连续偏导数的函数。求解<br />
<div class="math-display"><br />
\begin{align<em>} \ &amp; \min_{x \in R^{n}} f \left( x \right) \end{align</em>}<br />
</div><br />
 无约束最优化问题。<span class="math-inline">f^{*}</span> 示目标函数<span class="math-inline">f\left(x\right)</span> 极小点。</p>
<p>由于<span class="math-inline">f\left(x\right)</span> 有一阶连续偏导数，若第<span class="math-inline">k</span> 迭代值为<span class="math-inline">x^{\left(k\right)}</span>，则可将<span class="math-inline">f\left(x\right)</span> <span class="math-inline">x^{\left(k\right)}</span> 近进行一阶泰勒展开<br />
<div class="math-display"><br />
\begin{align<em>} \ &amp; f\left(x\right) ＝ f\left(x^{\left(k\right)}\right) + g_{k}^{T} \left(x-x^{\left(k\right)}\right)\end{align</em>}<br />
</div></p>
<p>其中，<span class="math-inline">g_{k}=g\left( x^{\left(k\right)} \right)=\nabla f \left( x^{\left(k\right)}\right)</span> <span class="math-inline">f\left(x\right)</span> <span class="math-inline">x^{\left(k\right)}</span> 梯度。</p>
<p>第<span class="math-inline">k+1</span> 迭代值<br />
<div class="math-display"><br />
\begin{align<em>} \ &amp; x^{\left( k+1 \right)} \leftarrow x^{\left( k \right)} + \lambda_{k} p_{k} \end{align</em>} <br />
</div><br />
其中，<span class="math-inline">p_{k}</span> 搜索方向，取负梯度方向<span class="math-inline">p_{k}= - \nabla f \left( x^{\left(k\right)} \right)</span>，<span class="math-inline">\lambda_{k}</span> 步长，由一维搜索确定，即<span class="math-inline">\lambda_{k}</span> 得<br />
<div class="math-display"><br />
\begin{align<em>} \ &amp; f \left( x^{\left(k\right)}+\lambda_{k}p_{k} \right)=\min_{\lambda \geq 0} f \left( x^{\left(k\right)}+\lambda p_{k} \right) \end{align</em>}<br />
</div><br />
梯度下降算法：<br />
输入：目标函数<span class="math-inline">f \left( x \right) </span>，梯度函数<span class="math-inline">g\left( x^{\left(k\right)} \right)=\nabla f \left( x^{\left(k\right)}\right)</span>，计算精度<span class="math-inline">\varepsilon</span><br />
输出：<span class="math-inline">f \left( x \right) </span> 极小点<span class="math-inline">x^{*}</span></p>
<ol>
<li>
<p>取初值<span class="math-inline">x^{\left(0\right)} \in R^{n}</span>，置<span class="math-inline">k=0</span></p>
</li>
<li>
<p>计算<span class="math-inline">f \left( x^{\left(k\right)}\right)</span></p>
</li>
<li>
<p>计算梯度<span class="math-inline">g_{k}=g\left( x^{\left(k\right)} \right)</span>，当<span class="math-inline">| g_{k} |  &lt; \varepsilon </span> ，停止迭代，令<span class="math-inline">x^{*}=x^{\left(k\right)}</span>；否则，令<span class="math-inline">p_{k}=-g\left(x^{\left(k\right)}\right)</span>，求<span class="math-inline">\lambda_{k}</span>，使</p>
</li>
</ol>
<p><div class="math-display"><br />
  \begin{align<em>} \ &amp; f \left( x^{\left(k\right)}+\lambda_{k}p_{k} \right)=\min_{\lambda \geq 0} f \left( x^{\left(k\right)}+\lambda p_{k} \right) \end{align</em>} <br />
</div></p>
<ol start="4">
<li>
<p>置<span class="math-inline">x^{\left(k+1\right)}= x^{\left(k\right)}+\lambda_{k}p_{k}</span>，计算<span class="math-inline">f \left( x^{\left(k＋1\right)} \right)</span><br />
   当<span class="math-inline">| f \left( x^{\left(k＋1\right)} \right) - f \left( x^{\left(k\right)} \right) | &lt; \varepsilon </span> <span class="math-inline">|  x^{\left(k＋1\right)} -  x^{\left(k\right)}  | &lt; \varepsilon </span> ，停止迭代，令<span class="math-inline">x^{*}=x^{k+1}</span></p>
</li>
<li>
<p>否则，置<span class="math-inline">k=k+1</span>，转3.</p>
</li>
</ol>
<h3 id="summary">Summary<a class="anchor-link" href="#summary" title="Permanent link">&para;</a></h3>
<ul>
<li>BGD(批量梯度下降）:更新每一参数都用所有样本更新。</li>
<li>优点：每次迭代都需要把所有样本都送入，这样的好处是每次迭代都顾及了全部的样本，能保证做的是全局最优化。</li>
<li>缺点：由于这种方法是在一次更新中，就对整个数据集计算梯度，所以计算起来非常慢，遇到很大量的数据集也会非常棘手，而且不能投入新数据实时更新模型。</li>
</ul>
                </div>

                <!-- Comments Section (Giscus) -->
                <section class="comments-section">
                    <h3><i class="fas fa-comments"></i> 评论</h3>
                    <script src="https://giscus.app/client.js"
                        data-repo="Geeks-Z/Geeks-Z.github.io"
                        data-repo-id=""
                        data-category="Announcements"
                        data-category-id=""
                        data-mapping="pathname"
                        data-strict="0"
                        data-reactions-enabled="1"
                        data-emit-metadata="0"
                        data-input-position="bottom"
                        data-theme="preferred_color_scheme"
                        data-lang="zh-CN"
                        crossorigin="anonymous"
                        async>
                    </script>
                </section>
            </article>

            <footer class="post-footer">
                <p>© 2025 Hongwei Zhao. Built with ❤️</p>
            </footer>
        </main>
    </div>

    <!-- Theme Toggle Button -->
    <button class="theme-toggle" id="themeToggle" aria-label="切换主题">
        <i class="fas fa-moon"></i>
    </button>

    <script>
        // Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        const html = document.documentElement;
        const icon = themeToggle.querySelector('i');
        const hljsDark = document.getElementById('hljs-theme-dark');
        const hljsLight = document.getElementById('hljs-theme-light');
        
        // Check saved theme or system preference
        const savedTheme = localStorage.getItem('theme');
        const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
        
        if (savedTheme === 'dark' || (!savedTheme && prefersDark)) {
            html.setAttribute('data-theme', 'dark');
            icon.className = 'fas fa-sun';
            hljsDark.disabled = false;
            hljsLight.disabled = true;
        }
        
        themeToggle.addEventListener('click', () => {
            const isDark = html.getAttribute('data-theme') === 'dark';
            if (isDark) {
                html.removeAttribute('data-theme');
                icon.className = 'fas fa-moon';
                localStorage.setItem('theme', 'light');
                hljsDark.disabled = true;
                hljsLight.disabled = false;
            } else {
                html.setAttribute('data-theme', 'dark');
                icon.className = 'fas fa-sun';
                localStorage.setItem('theme', 'dark');
                hljsDark.disabled = false;
                hljsLight.disabled = true;
            }
            
            // Update Giscus theme
            const giscusFrame = document.querySelector('iframe.giscus-frame');
            if (giscusFrame) {
                giscusFrame.contentWindow.postMessage({
                    giscus: {
                        setConfig: {
                            theme: isDark ? 'light' : 'dark'
                        }
                    }
                }, 'https://giscus.app');
            }
        });
        
        // Highlight.js
        document.addEventListener('DOMContentLoaded', () => {
            hljs.highlightAll();
        });
        
        // KaTeX auto-render
        document.addEventListener('DOMContentLoaded', () => {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
        
        // TOC active state
        const tocLinks = document.querySelectorAll('.toc-container a');
        const headings = document.querySelectorAll('.post-content h2, .post-content h3, .post-content h4');
        
        function updateTocActive() {
            let current = '';
            headings.forEach(heading => {
                const rect = heading.getBoundingClientRect();
                if (rect.top <= 100) {
                    current = heading.getAttribute('id');
                }
            });
            
            tocLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === '#' + current) {
                    link.classList.add('active');
                }
            });
        }
        
        window.addEventListener('scroll', updateTocActive);
        updateTocActive();
    </script>
</body>
</html>
