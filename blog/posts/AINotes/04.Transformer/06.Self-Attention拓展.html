<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Untitled</title>
    <meta name="description" content="Untitled - Hongwei Zhao's Blog">
    <meta name="author" content="Hongwei Zhao">
    
    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <!-- Code Highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css" id="hljs-theme-dark">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css" id="hljs-theme-light" disabled>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    
    <style>
        :root {
            /* Light Theme */
            --primary-color: #2980b9;
            --primary-hover: #1a5276;
            --link-color: #c0392b;
            --text-color: #333;
            --text-light: #666;
            --text-muted: #999;
            --bg-color: #fff;
            --bg-secondary: #f8f9fa;
            --bg-code: #f5f5f5;
            --border-color: #e5e7eb;
            --shadow: 0 1px 3px rgba(0,0,0,0.1);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.1);
        }

        [data-theme="dark"] {
            --primary-color: #5dade2;
            --primary-hover: #85c1e9;
            --link-color: #e74c3c;
            --text-color: #e5e7eb;
            --text-light: #9ca3af;
            --text-muted: #6b7280;
            --bg-color: #1a1a2e;
            --bg-secondary: #16213e;
            --bg-code: #0f0f23;
            --border-color: #374151;
            --shadow: 0 1px 3px rgba(0,0,0,0.3);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.4);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 16px;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            transition: background-color 0.3s, color 0.3s;
        }

        a {
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--primary-hover);
            text-decoration: underline;
        }

        /* Layout */
        .page-wrapper {
            display: flex;
            max-width: 1400px;
            margin: 0 auto;
            padding: 2rem;
            gap: 3rem;
        }

        /* Sidebar TOC */
        .toc-sidebar {
            width: 260px;
            flex-shrink: 0;
            position: sticky;
            top: 2rem;
            height: fit-content;
            max-height: calc(100vh - 4rem);
            overflow-y: auto;
        }

        .toc-container {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 1.5rem;
            border: 1px solid var(--border-color);
        }

        .toc-container h3 {
            font-size: 0.85rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-muted);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .toc-container ul {
            list-style: none;
        }

        .toc-container li {
            margin-bottom: 0.5rem;
        }

        .toc-container a {
            font-size: 0.9rem;
            color: var(--text-light);
            display: block;
            padding: 0.25rem 0;
            border-left: 2px solid transparent;
            padding-left: 0.75rem;
            transition: all 0.2s;
        }

        .toc-container a:hover,
        .toc-container a.active {
            color: var(--primary-color);
            border-left-color: var(--primary-color);
            text-decoration: none;
        }

        .toc-container ul ul {
            margin-left: 1rem;
        }

        /* Main Content */
        .main-content {
            flex: 1;
            min-width: 0;
            max-width: 800px;
        }

        /* Header */
        .post-header {
            margin-bottom: 2rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
            margin-bottom: 1.5rem;
        }

        .back-link:hover {
            color: var(--primary-color);
            text-decoration: none;
        }

        .post-header h1 {
            font-size: 2.25rem;
            font-weight: 700;
            line-height: 1.3;
            margin-bottom: 1rem;
            color: var(--text-color);
        }

        .post-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
        }

        .post-meta span {
            display: flex;
            align-items: center;
            gap: 0.4rem;
        }

        .post-meta i {
            color: var(--text-muted);
        }

        .tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 1rem;
        }

        .tag {
            display: inline-block;
            font-size: 0.8rem;
            background: var(--bg-secondary);
            color: var(--text-light);
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            border: 1px solid var(--border-color);
            transition: all 0.2s;
        }

        .tag:hover {
            background: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }

        /* Article Content */
        .post-content {
            font-size: 1rem;
            line-height: 1.9;
        }

        .post-content h1,
        .post-content h2,
        .post-content h3,
        .post-content h4,
        .post-content h5,
        .post-content h6 {
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-weight: 600;
            line-height: 1.4;
            color: var(--text-color);
        }

        .post-content h1 { font-size: 1.875rem; }
        .post-content h2 { 
            font-size: 1.5rem; 
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }
        .post-content h3 { font-size: 1.25rem; }
        .post-content h4 { font-size: 1.125rem; }

        .post-content p {
            margin-bottom: 1.25rem;
        }

        .post-content ul,
        .post-content ol {
            margin: 1.25rem 0;
            padding-left: 1.5rem;
        }

        .post-content li {
            margin-bottom: 0.5rem;
        }

        .post-content blockquote {
            margin: 1.5rem 0;
            padding: 1rem 1.5rem;
            background: var(--bg-secondary);
            border-left: 4px solid var(--primary-color);
            border-radius: 0 8px 8px 0;
            color: var(--text-light);
            font-style: italic;
        }

        .post-content blockquote p:last-child {
            margin-bottom: 0;
        }

        /* Code */
        .post-content code {
            font-family: 'JetBrains Mono', 'Fira Code', Consolas, monospace;
            font-size: 0.9em;
            background: var(--bg-code);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            color: var(--link-color);
        }

        .post-content pre {
            margin: 1.5rem 0;
            padding: 1.25rem;
            background: var(--bg-code);
            border-radius: 10px;
            overflow-x: auto;
            border: 1px solid var(--border-color);
        }

        .post-content pre code {
            background: none;
            padding: 0;
            color: inherit;
            font-size: 0.875rem;
            line-height: 1.6;
        }

        /* Images */
        .post-content img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            margin: 1.5rem 0;
            box-shadow: var(--shadow-lg);
        }

        /* Tables */
        .post-content table {
            width: 100%;
            margin: 1.5rem 0;
            border-collapse: collapse;
            font-size: 0.95rem;
        }

        .post-content th,
        .post-content td {
            padding: 0.75rem 1rem;
            border: 1px solid var(--border-color);
            text-align: left;
        }

        .post-content th {
            background: var(--bg-secondary);
            font-weight: 600;
        }

        .post-content tr:nth-child(even) {
            background: var(--bg-secondary);
        }

        /* Math */
        .math-display {
            margin: 1.5rem 0;
            overflow-x: auto;
            padding: 1rem;
            background: var(--bg-secondary);
            border-radius: 8px;
        }

        /* Anchor links */
        .anchor-link {
            opacity: 0;
            margin-left: 0.5rem;
            color: var(--text-muted);
            font-weight: 400;
            transition: opacity 0.2s;
        }

        .post-content h2:hover .anchor-link,
        .post-content h3:hover .anchor-link,
        .post-content h4:hover .anchor-link {
            opacity: 1;
        }

        /* Theme Toggle */
        .theme-toggle {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            background: var(--primary-color);
            color: white;
            border: none;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.25rem;
            box-shadow: var(--shadow-lg);
            transition: transform 0.2s, background 0.2s;
            z-index: 1000;
        }

        .theme-toggle:hover {
            transform: scale(1.1);
            background: var(--primary-hover);
        }

        /* Comments Section */
        .comments-section {
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-color);
        }

        .comments-section h3 {
            font-size: 1.25rem;
            margin-bottom: 1.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        /* Footer */
        .post-footer {
            margin-top: 3rem;
            padding-top: 1.5rem;
            border-top: 1px solid var(--border-color);
            text-align: center;
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        /* Responsive */
        @media (max-width: 1024px) {
            .toc-sidebar {
                display: none;
            }
            
            .page-wrapper {
                padding: 1.5rem;
            }
        }

        @media (max-width: 768px) {
            .post-header h1 {
                font-size: 1.75rem;
            }
            
            .post-meta {
                flex-direction: column;
                gap: 0.5rem;
            }
            
            .theme-toggle {
                bottom: 1rem;
                right: 1rem;
                width: 44px;
                height: 44px;
            }
        }

        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-secondary);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--border-color);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--text-muted);
        }
    </style>
</head>
<body>
    <div class="page-wrapper">
        <!-- TOC Sidebar -->
        <aside class="toc-sidebar">
            <div class="toc-container">
                <h3><i class="fas fa-list"></i> 目录</h3>
                <div class="toc">
<ul>
<li><a href="#self-attention-vs-cnn3">Self-attention v.s. CNN3</a></li>
<li><a href="#self-attention-vs-rnn">Self-attention v.s. RNN</a></li>
<li><a href="#self-attention-for-graph">Self-attention for Graph</a></li>
<li><a href="#more">More</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <article>
                <header class="post-header">
                    <a href="../../../index.html" class="back-link">
                        <i class="fas fa-arrow-left"></i> 返回博客列表
                    </a>
                    <h1>Untitled</h1>
                    <div class="post-meta">
                        <span><i class="fas fa-calendar-alt"></i> 2026-01-28</span>
                        <span><i class="fas fa-folder"></i> AINotes/04.Transformer</span>
                        <span><i class="fas fa-user"></i> Hongwei Zhao</span>
                    </div>
                    <div class="tags">
                        
                    </div>
                </header>

                <div class="post-content">
                    <h3 id="self-attention-vs-cnn3">Self-attention v.s. CNN3<a class="anchor-link" href="#self-attention-vs-cnn3" title="Permanent link">&para;</a></h3>
<p>我们可以来比较一下，Self-attention 跟 CNN 之间，有什么样的差异或者是关联性</p>
<p>如果我们今天，是用 Self-attention 来处理一张图片，代表说，假设这个是你要考虑的 pixel，那它产生 query，其他 pixel 产生 key，</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161002284.png" alt="image-20210412214915856" /></div>

<p>你今天在做 inner product 的时候，你考虑的不是一个小的receptive field的信息，而是整张影像的资讯</p>
<p>但是今天在做 CNN 的时候，，会画出一个 receptive field，每一个 filter，每一个 neural，只考虑 receptive field 范围里面的资讯</p>
<p><img alt="image-20221126153624038" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202211261536068.png" /></p>
<ul>
<li>
<p>所以如果我们比较 CNN 跟 Self-attention 的话，<strong>CNN 可以看作是一种简化版的 Self-attention</strong>，因为在做CNN的时候，我们只考虑 receptive field 里面的资讯，而在做 Self-attention 的时候，我们是考虑整张图片的资讯，所以 CNN，是简化版的 Self-attention</p>
</li>
<li>
<p>或者是你可以反过来说，<strong>Self-attention 是一个复杂化的 CNN</strong></p>
</li>
</ul>
<p>在 CNN 里面，我们要划定 receptive field，每一个 neural，只考虑 receptive field 里面的资讯，而 <strong>receptive field 的范围跟大小，是人决定的，</strong></p>
<p>而对 Self-attention 而言，我们用 attention，去找出相关的 pixel，就好像是 <strong>receptive field 是自动被学出的</strong>，network 自己决定说，receptive field 的形状长什么样子，network 自己决定说，以这个 pixel 为中心，哪些 pixel 是我们真正需要考虑的，那些 pixel 是相关的</p>
<p><strong>所以 receptive field 的范围，不再是人工划定，而是让机器自己学出来</strong></p>
<p>其实你可以读一篇 paper，叫做 On the Relationship，between Self-attention and Convolutional Layers</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161002915.png" alt="image-20210412215841085" style="zoom:50%;" /></div>

<p>在这篇 paper 里面，会用数学的方式严谨的告诉你说，其实这个 <strong>CNN就是 Self-attention 的特例，Self-attention 只要设定合适的参数，它可以做到跟 CNN 一模一样的事情</strong>。所以 self attention，是更 flexible 的 CNN，而 CNN 是有受限制的 Self-attention，Self-attention 只要透过某些设计，某些限制，它就会变成 CNN。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161002303.png" alt="image-20210412220020641" /></div>

<p>既然Self-attention 比较 flexible，之前有讲说<strong>比较 flexible 的 model，比较需要更多的 data，如果你 data 不够，就有可能 overfitting</strong></p>
<p>而小的 model，而比较有限制的 model，它适合在 data 小的，少的时候，它可能比较不会 overfitting，那如果你这个限制设的好，也会有不错的结果</p>
<p>如果你今天用不同的 data 量，来训练 CNN 跟 Self-attention，你确实可以看到我刚才讲的现象</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161002876.png" alt="image-20210412220707729" /></div>

<p>那这个实验结果，来自于 An image is worth 16 乘以 16 的 words，这个是 Google 的 paper，它就是把这个 Self-attention，apply 在影像上面</p>
<p>那其实<strong>把一张影像呢，拆成 16 乘以 16 个 patch，它把每一个 patch想像成是一个 word</strong>，因为一般我们这个 Self-attention，比较常用在 NLP 上面，所以他就说，想像每一个 patch 其实就是一个 word，所以他就取了一个很 fancy 的 title，叫做一张图呢，值 16 乘以 16 个文字</p>
<p>横轴是训练的影像的量，那你发现说，对 Google 来说 用的，所谓的资料量比较少，也是你没有办法用的资料量啦这边有 10 个 million 就是，1000 万张图，是资料量比较小的 setting，然后资料量比较大的 setting 呢，有 3 亿张图片，在这个实验里面呢，比较了 Self-attention 是浅蓝色的这一条线，跟 CNN 是深灰色的这条线</p>
<p>就会发现说，<strong>随著资料量越来越多，那 Self-attention 的结果就越来越好，最终在资料量最多的时候，Self-attention 可以超过 CNN，但在资料量少的时候，CNN 它是可以比 Self-attention，得到更好的结果的</strong></p>
<p>那为什么会这样，你就可以从 CNN 跟 Self-attention，它们的弹性来加以解释</p>
<ul>
<li>Self-attention 它弹性比较大，所以需要比较多的训练资料，训练资料少的时候，就会 overfitting</li>
<li>而 CNN 它弹性比较小，在训练资料少的时候，结果比较好，但训练资料多的时候，它没有办法从更大量的训练资料得到好处</li>
</ul>
<p>所以这个就是 Self-attention 跟 CNN 的比较，那 Self-attention 跟 CNN，谁比较好呢，<strong>我应该选哪一个呢，事实上你也可以都用</strong>，在我们作业四里面，如果你要做 strong baseline 的话，就特别给你一个提示，就是用 conformer，里面就是有用到 Self-attention，也有用到 CNN</p>
<h3 id="self-attention-vs-rnn">Self-attention v.s. RNN<a class="anchor-link" href="#self-attention-vs-rnn" title="Permanent link">&para;</a></h3>
<p>我们来比较一下，Self-attention 跟 RNN，RNN就是 recurrent neural network，这门课里面现在就不会讲到 recurrent neural network，因为 recurrent neural network 的角色，很大一部分都可以用 Self-attention 来取代了，</p>
<p>但是 RNN 是什么呢，假设你想知道的话，那这边很快地三言两语把它带过去，RNN 跟 Self-attention 一样，都是要处理 input 是一个 sequence 的状况</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161002154.png" alt="image-20210413093042847" /></div>

<p>在 RNN 里面呢</p>
<ul>
<li>左边是你的 input sequence，你有一个 memory 的 vector</li>
<li>然后你有一个 RNN 的 block，这个 RNN 的 block 呢，它吃 memory 的 vector，吃第一个 input 的 vector</li>
<li>然后 output 一个东西，然后根据这个 output 的东西，我们通常叫做这个 hidden，这个 hidden 的 layer 的 output</li>
<li>然后通过这个 fully connected network，然后再去做你想要的 prediction</li>
</ul>
<p>接下来当sequence 里面，第二个 vector 作为 input 的时候，也会把前一个时间点吐出来的东西，当做下一个时间点的输入，再丢进 RNN 里面，然后再产生新的 vector，再拿去给 fully connected network</p>
<p>然后第三个 vector 进来的时候，你把第三个 vector 跟前一个时间点的输出，一起丢进 RNN，再产生新的输出，然后在第四个时间点</p>
<p>第四个 vector 输入的时候，把第四个 vector 跟前一个时间点，产生出来的输出，再一起做处理，得到新的输出，再通过 fully connected network 的 layer，这个就是 RNN</p>
<p>Recurrent Neural Network跟 Self-attention 做的事情其实也非常像，它们的 <strong>input 都是一个 vector sequence</strong></p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161002535.png" alt="image-20210413152809037" /></div>

<p>Self-attention output 是另外一个 vector sequence，这里面的每一个 vector，都<strong>考虑了整个 input sequence 以后</strong>，再给 fully connected network 去做处理</p>
<p>那 RNN 呢，它也会 output 另外一群 vector，这另外一排 vector 也会给，fully connected network 做进一步的处理，那 Self-attention 跟 RNN 有什么不同呢</p>
<p>当然一个非常显而易见的不同，你可能会说，这边的每一个 vector，它都考虑了整个 input 的 sequence，而 RNN 每一个 vector，只考虑了左边已经输入的 vector，它没有考虑右边的 vector，那这是一个很好的观察</p>
<p>但是 <strong>RNN 其实也可以是双向的</strong>，所以如果你 RNN 用双向的 RNN 的话，其实这边的每一个 hidden 的 output，每一个 memory 的 output，其实也可以看作是考虑了整个 input 的 sequence</p>
<p>但是假设我们把 RNN 的 output，跟 Self-attention 的 output 拿来做对比的话，就算你用 bidirectional 的 RNN，还是有一些差别的</p>
<ul>
<li>
<p>对 RNN 来说，假设最右边这个黄色的 vector，要考虑最左边的这个输入，那它必须要把最左边的输入存在 memory 里面，然后接下来都不能够忘掉，一路带到最右边，才能够在最后一个时间点被考虑</p>
</li>
<li>
<p>但对 Self-attention 来说没有这个问题，它只要这边输出一个 query，这边输出一个 key，只要它们 match 得起来，天涯若比邻，你可以从非常远的 vector，在整个 sequence 上非常远的 vector，轻易地抽取资讯，所以这是 RNN 跟 Self-attention，一个不一样的地方</p>
</li>
</ul>
<p>还有另外一个更主要的不同是，RNN 今天在处理的时候， input 一排 sequence，output 一排 sequence 的时候，<strong>RNN 是没有办法平行化的</strong></p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161002089.png" alt="image-20210413153504431" /></div>

<p>RNN 它今天 input 一排是 vector，output 另外一排 vector 的时候，它没有办法一次处理，没有办法平行处理所有的 output</p>
<p>但 Self-attention 有一个优势，是它可以平行处理所有的输出，你今天 input 一排 vector，再 output 这四个 vector 的时候，<strong>这四个 vector 是平行产生的，并不需要等谁先运算完才把其他运算出来</strong>，output 的这个 vector，里面的 output 这个 vector sequence 里面，每一个 vector 都是同时产生出来的</p>
<p>所以在运算速度上，Self-attention 会比 RNN 更有效率</p>
<p>那你今天发现说，<strong>很多的应用都往往把 RNN 的架构，逐渐改成 Self-attention 的架构了</strong>，如果你想要更进一步了解，RNN 跟 Self-attention 的关系的话，你可以看下面这篇文章，Transformers are RNNs，里面会告诉你说，Self-attention 你加上了什么东西以后，其实它就变成了 RNN，发现说这也不是很旧的 paper，这个是去年的六月放到 arXiv 上</p>
<p>所以今天讲的都是一些很新的研究成果，那 RNN 的部分呢，我们这门课就不会提到，假设你对 RNN 有兴趣的话，以下是这一门课之前的上课录影，那 RNN 的部分，因为这一次不会讲到，所以特别有做了英文的版本，RNN 呢 是中文英文版本，都同时有放在 YouTube 上面</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161002509.png" alt="image-20210413153721866" /></div>

<h3 id="self-attention-for-graph">Self-attention for Graph<a class="anchor-link" href="#self-attention-for-graph" title="Permanent link">&para;</a></h3>
<p>Graph 也可以看作是一堆 vector，那如果是一堆 vector，就可以用 Self-attention 来处理，所以 Self-attention 也可以用在 Graph 上面，但是当我们把 Self-attention，用在Graph 上面的时候，有什么样特别的地方呢？</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161002957.png" alt="image-20210413154403199"/></div>

<p>在 Graph 上面，每一个 node 可以表示成一个向量，但<strong>不只有 node 的资讯，还有 edge 的资讯</strong>，我们知道哪些 node 之间是有相连的，也就是哪些 node 是有关联的，我们知道哪些向量间是有关联，那之前我们在做 Self-attention 的时候，所谓的关联性是 network 自己找出来的，但是现在既然有了 Graph 的资讯，<strong>有了 edge 的资讯，那关联性也许就不需要透过机器自动找出来，这个图上面的 edge 已经暗示了我们，node 跟 node 之间的关联性</strong>。所以今天当你把 Self-attention，用在 Graph 上面的时候，你有一个选择是你在做这个，Attention Matrix 计算的时候，你可以<strong>只计算有 edge 相连的 node 就好</strong>。</p>
<p>举例来说在这个图上，node 1 跟 node 8 有相连，那我们只需要计算 node 1 跟 node 8，这两个向量之间的 attention 的分数，那 1 跟 6 相连，所以只有 1 跟 6 之间，需要计算 attention 的分数，1 跟 5 有相连，所以只有 1 跟 5 需要计算 attention 的分数，2 跟 3 有相连，所以只有 2 跟 3 需要计算 attention 的分数，以此类推……</p>
<p>那如果两个 node 之间没有相连，那其实很有可能就暗示我们，这两个 node 之间没有关系，<strong>既然没有关系，我们就不需要再去计算它的 attention score，直接把它设为 0 就好了</strong>。</p>
<p>因为这个 <strong>Graph 往往是人为根据某些 domain knowledge 建出来的</strong>，那 domain knowledge 告诉我们说，这两个向量彼此之间没有关联，我们就没有必要再用机器去学习这件事情。</p>
<p>其实当我们把 Self-attention，按照我们这边讲的这种限制，用在 Graph 上面的时候，其实就是一种 Graph Neural Network，也就是一种 GNN</p>
<p>那我知道 GNN，现在也是一个很 fancy 的题目，那我不会说 Self-attention 就要囊括了，所有 GNN 的各种变形了，但把 Self-attention 用在 Graph 上面，是某一种类型的 Graph Neural Network，那这边呢，一样我们也没有办法细讲了，GNN 这边坑也是很深啊，这边水是很深，那就放一下助教之前上课的连结</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161002451.png" alt="image-20210413154823956" /></div>

<p>大概花了快三个小时，在讲 Graph Neural Network，而且其实还没有讲完，就告诉你说这个 Graph Neural Network，也是有非常深的技术，这边水也是很深，那这不是我们今天这一堂课可以讲的内容，好 </p>
<h2 id="more">More<a class="anchor-link" href="#more" title="Permanent link">&para;</a></h2>
<p>其实Self-attention 有非常非常多的变形，你可以看一篇 paper 叫做，Long Range Arena，里面比较了各种不同的 Self-attention 的变形</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161002028.png" alt="image-20210413155347467" /></div>

<p>因为 Self-attention 它最大的问题就是，<strong>它的运算量非常地大</strong>，所以怎么样减少 Self-attention 的运算量，是一个未来的重点，可以看到这边有，各种各式各样 Self-attention 的变形。</p>
<p>Self-attention 最早是，用在 Transformer 上面，所以很多人讲 Transformer 的时候，其实它指的就是这个 Self-attention，有人说广义的 Transformer，指的就是 Self-attention，那所以后来各式各样的，Self-attention 的变形都这样做，都叫做是什么 former，比如说 Linformer Performer Reformer 等等，所以 Self-attention 的变形，现在都叫做 xxformer</p>
<p>那可以看到，往右代表它运算的速度，所以有很多各式各样新的 xxformer，它们的速度会比原来的 Transformer 快，但是快的速度带来的就是 performance 变差</p>
<p>这个纵轴代表是 performance，所以它们往往比原来的 Transformer，performance 差一点，但是速度会比较快</p>
<p>那到底什么样的 Self-attention，才能够真的又快又好，这仍然是一个尚待研究的问题，如果你对 Self-attention，想要进一步研究的话，你还可以看一下，Efficient Transformers: A Survey 这篇 paper，里面会跟你介绍，各式各样 Self-attention 的变形。</p>
<h2 id="references">References<a class="anchor-link" href="#references" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://github.com/unclestrong/DeepLearning_LHY21_Notes">DeepLearning_LHY21_Notes</a></li>
<li>[Slides-Self-Attention](</li>
</ul>
                </div>

                <!-- Comments Section (Giscus) -->
                <section class="comments-section">
                    <h3><i class="fas fa-comments"></i> 评论</h3>
                    <script src="https://giscus.app/client.js"
                        data-repo="Geeks-Z/Geeks-Z.github.io"
                        data-repo-id=""
                        data-category="Announcements"
                        data-category-id=""
                        data-mapping="pathname"
                        data-strict="0"
                        data-reactions-enabled="1"
                        data-emit-metadata="0"
                        data-input-position="bottom"
                        data-theme="preferred_color_scheme"
                        data-lang="zh-CN"
                        crossorigin="anonymous"
                        async>
                    </script>
                </section>
            </article>

            <footer class="post-footer">
                <p>© 2025 Hongwei Zhao. Built with ❤️</p>
            </footer>
        </main>
    </div>

    <!-- Theme Toggle Button -->
    <button class="theme-toggle" id="themeToggle" aria-label="切换主题">
        <i class="fas fa-moon"></i>
    </button>

    <script>
        // Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        const html = document.documentElement;
        const icon = themeToggle.querySelector('i');
        const hljsDark = document.getElementById('hljs-theme-dark');
        const hljsLight = document.getElementById('hljs-theme-light');
        
        // Check saved theme or system preference
        const savedTheme = localStorage.getItem('theme');
        const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
        
        if (savedTheme === 'dark' || (!savedTheme && prefersDark)) {
            html.setAttribute('data-theme', 'dark');
            icon.className = 'fas fa-sun';
            hljsDark.disabled = false;
            hljsLight.disabled = true;
        }
        
        themeToggle.addEventListener('click', () => {
            const isDark = html.getAttribute('data-theme') === 'dark';
            if (isDark) {
                html.removeAttribute('data-theme');
                icon.className = 'fas fa-moon';
                localStorage.setItem('theme', 'light');
                hljsDark.disabled = true;
                hljsLight.disabled = false;
            } else {
                html.setAttribute('data-theme', 'dark');
                icon.className = 'fas fa-sun';
                localStorage.setItem('theme', 'dark');
                hljsDark.disabled = false;
                hljsLight.disabled = true;
            }
            
            // Update Giscus theme
            const giscusFrame = document.querySelector('iframe.giscus-frame');
            if (giscusFrame) {
                giscusFrame.contentWindow.postMessage({
                    giscus: {
                        setConfig: {
                            theme: isDark ? 'light' : 'dark'
                        }
                    }
                }, 'https://giscus.app');
            }
        });
        
        // Highlight.js
        document.addEventListener('DOMContentLoaded', () => {
            hljs.highlightAll();
        });
        
        // KaTeX auto-render
        document.addEventListener('DOMContentLoaded', () => {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
        
        // TOC active state
        const tocLinks = document.querySelectorAll('.toc-container a');
        const headings = document.querySelectorAll('.post-content h2, .post-content h3, .post-content h4');
        
        function updateTocActive() {
            let current = '';
            headings.forEach(heading => {
                const rect = heading.getBoundingClientRect();
                if (rect.top <= 100) {
                    current = heading.getAttribute('id');
                }
            });
            
            tocLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === '#' + current) {
                    link.classList.add('active');
                }
            });
        }
        
        window.addEventListener('scroll', updateTocActive);
        updateTocActive();
    </script>
</body>
</html>
