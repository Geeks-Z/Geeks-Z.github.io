<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Untitled</title>
    <meta name="description" content="Untitled - Hongwei Zhao's Blog">
    <meta name="author" content="Hongwei Zhao">
    
    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <!-- Code Highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css" id="hljs-theme-dark">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css" id="hljs-theme-light" disabled>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    
    <style>
        :root {
            /* Light Theme - 明亮清新配色 */
            --primary-color: #4A90D9;
            --primary-hover: #3678C2;
            --link-color: #E86B5F;
            --text-color: #2D2D2D;
            --text-light: #5A5A5A;
            --text-muted: #8A8A8A;
            --bg-color: #FFFFFF;
            --bg-secondary: #F5F7FA;
            --bg-code: #F8F9FC;
            --border-color: #E8ECF0;
            --shadow: 0 2px 8px rgba(0,0,0,0.06);
            --shadow-lg: 0 8px 24px rgba(0,0,0,0.08);
        }

        [data-theme="dark"] {
            --primary-color: #5dade2;
            --primary-hover: #85c1e9;
            --link-color: #e74c3c;
            --text-color: #e5e7eb;
            --text-light: #9ca3af;
            --text-muted: #6b7280;
            --bg-color: #1a1a2e;
            --bg-secondary: #16213e;
            --bg-code: #0f0f23;
            --border-color: #374151;
            --shadow: 0 1px 3px rgba(0,0,0,0.3);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.4);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 16px;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            transition: background-color 0.3s, color 0.3s;
        }

        a {
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--primary-hover);
            text-decoration: underline;
        }

        /* Layout */
        .page-wrapper {
            display: flex;
            max-width: 1400px;
            margin: 0 auto;
            padding: 2rem;
            gap: 3rem;
        }

        /* Sidebar TOC */
        .toc-sidebar {
            width: 260px;
            flex-shrink: 0;
            position: sticky;
            top: 2rem;
            height: fit-content;
            max-height: calc(100vh - 4rem);
            overflow-y: auto;
        }

        .toc-container {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 1.5rem;
            border: 1px solid var(--border-color);
        }

        .toc-container h3 {
            font-size: 0.85rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-muted);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .toc-container ul {
            list-style: none;
        }

        .toc-container li {
            margin-bottom: 0.5rem;
        }

        .toc-container a {
            font-size: 0.9rem;
            color: var(--text-light);
            display: block;
            padding: 0.25rem 0;
            border-left: 2px solid transparent;
            padding-left: 0.75rem;
            transition: all 0.2s;
        }

        .toc-container a:hover,
        .toc-container a.active {
            color: var(--primary-color);
            border-left-color: var(--primary-color);
            text-decoration: none;
        }

        .toc-container ul ul {
            margin-left: 1rem;
        }

        /* Main Content */
        .main-content {
            flex: 1;
            min-width: 0;
            max-width: 800px;
        }

        /* Header */
        .post-header {
            margin-bottom: 2rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
            margin-bottom: 1.5rem;
        }

        .back-link:hover {
            color: var(--primary-color);
            text-decoration: none;
        }

        .post-header h1 {
            font-size: 2.25rem;
            font-weight: 700;
            line-height: 1.3;
            margin-bottom: 1rem;
            color: var(--text-color);
        }

        .post-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
        }

        .post-meta span {
            display: flex;
            align-items: center;
            gap: 0.4rem;
        }

        .post-meta i {
            color: var(--text-muted);
        }

        .tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 1rem;
        }

        .tag {
            display: inline-block;
            font-size: 0.8rem;
            background: var(--bg-secondary);
            color: var(--text-light);
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            border: 1px solid var(--border-color);
            transition: all 0.2s;
        }

        .tag:hover {
            background: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }

        /* Article Content */
        .post-content {
            font-size: 1rem;
            line-height: 1.9;
        }

        .post-content h1,
        .post-content h2,
        .post-content h3,
        .post-content h4,
        .post-content h5,
        .post-content h6 {
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-weight: 600;
            line-height: 1.4;
            color: var(--text-color);
        }

        .post-content h1 { font-size: 1.875rem; }
        .post-content h2 { 
            font-size: 1.5rem; 
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }
        .post-content h3 { font-size: 1.25rem; }
        .post-content h4 { font-size: 1.125rem; }

        .post-content p {
            margin-bottom: 1.25rem;
        }

        .post-content ul,
        .post-content ol {
            margin: 1.25rem 0;
            padding-left: 1.5rem;
        }

        .post-content li {
            margin-bottom: 0.5rem;
        }

        .post-content blockquote {
            margin: 1.5rem 0;
            padding: 1rem 1.5rem;
            background: var(--bg-secondary);
            border-left: 4px solid var(--primary-color);
            border-radius: 0 8px 8px 0;
            color: var(--text-light);
            font-style: italic;
        }

        .post-content blockquote p:last-child {
            margin-bottom: 0;
        }

        /* Code */
        .post-content code {
            font-family: 'JetBrains Mono', 'Fira Code', Consolas, monospace;
            font-size: 0.9em;
            background: var(--bg-code);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            color: var(--link-color);
        }

        .post-content pre {
            margin: 1.5rem 0;
            padding: 1.25rem;
            background: var(--bg-code);
            border-radius: 10px;
            overflow-x: auto;
            border: 1px solid var(--border-color);
        }

        .post-content pre code {
            background: none;
            padding: 0;
            color: inherit;
            font-size: 0.875rem;
            line-height: 1.6;
        }

        /* Images */
        .post-content img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            margin: 1.5rem 0;
            box-shadow: var(--shadow-lg);
        }

        /* Tables */
        .post-content table {
            width: 100%;
            margin: 1.5rem 0;
            border-collapse: collapse;
            font-size: 0.95rem;
        }

        .post-content th,
        .post-content td {
            padding: 0.75rem 1rem;
            border: 1px solid var(--border-color);
            text-align: left;
        }

        .post-content th {
            background: var(--bg-secondary);
            font-weight: 600;
        }

        .post-content tr:nth-child(even) {
            background: var(--bg-secondary);
        }

        /* Math */
        .math-display {
            margin: 1.5rem 0;
            overflow-x: auto;
            padding: 1rem;
            background: var(--bg-secondary);
            border-radius: 8px;
        }

        /* Anchor links */
        .anchor-link {
            opacity: 0;
            margin-left: 0.5rem;
            color: var(--text-muted);
            font-weight: 400;
            transition: opacity 0.2s;
        }

        .post-content h2:hover .anchor-link,
        .post-content h3:hover .anchor-link,
        .post-content h4:hover .anchor-link {
            opacity: 1;
        }

        /* Theme Toggle */
        .theme-toggle {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            background: var(--primary-color);
            color: white;
            border: none;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.25rem;
            box-shadow: var(--shadow-lg);
            transition: transform 0.2s, background 0.2s;
            z-index: 1000;
        }

        .theme-toggle:hover {
            transform: scale(1.1);
            background: var(--primary-hover);
        }

        /* Comments Section */
        .comments-section {
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-color);
        }

        .comments-section h3 {
            font-size: 1.25rem;
            margin-bottom: 1.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        /* Footer */
        .post-footer {
            margin-top: 3rem;
            padding-top: 1.5rem;
            border-top: 1px solid var(--border-color);
            text-align: center;
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        /* Responsive */
        @media (max-width: 1024px) {
            .toc-sidebar {
                display: none;
            }
            
            .page-wrapper {
                padding: 1.5rem;
            }
        }

        @media (max-width: 768px) {
            .post-header h1 {
                font-size: 1.75rem;
            }
            
            .post-meta {
                flex-direction: column;
                gap: 0.5rem;
            }
            
            .theme-toggle {
                bottom: 1rem;
                right: 1rem;
                width: 44px;
                height: 44px;
            }
        }

        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-secondary);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--border-color);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--text-muted);
        }
    </style>
</head>
<body>
    <div class="page-wrapper">
        <!-- TOC Sidebar -->
        <aside class="toc-sidebar">
            <div class="toc-container">
                <h3><i class="fas fa-list"></i> 目录</h3>
                <div class="toc">
<ul>
<li><a href="#transformer">Transformer</a></li>
<li><a href="#transformer中的self-attention">Transformer中的Self-Attention</a><ul>
<li><a href="#self-attention-in-detail">Self-Attention in Detail</a></li>
<li><a href="#matrix-calculation-of-self-attention">Matrix Calculation of Self-Attention</a></li>
<li><a href="#many-heads">Many Heads</a></li>
<li><a href="#positional-encoding">Positional Encoding</a></li>
<li><a href="#the-residuals">The Residuals</a></li>
</ul>
</li>
<li><a href="#encoder">Encoder</a></li>
<li><a href="#decoder">Decoder</a><ul>
<li><a href="#decoder--autoregressive-at">Decoder – Autoregressive (AT)</a></li>
<li><a href="#decoder--non-autoregressive-nat">Decoder – Non-autoregressive (NAT)</a></li>
<li><a href="#at-vs-nat">AT v.s. NAT</a></li>
</ul>
</li>
<li><a href="#encoder-decoder">Encoder-Decoder</a></li>
<li><a href="#training">Training</a></li>
<li><a href="#answer">Answer</a><ul>
<li><a href="#1为什么除以sqrtd_k">1、为什么除以$\sqrt{d_k}$</a></li>
<li><a href="#2为什么多头">2、为什么多头</a></li>
<li><a href="#3原始qkv来源">3、原始Q、K、V​来源</a></li>
</ul>
</li>
<li><a href="#reference">Reference</a></li>
</ul>
</div>

            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <article>
                <header class="post-header">
                    <a href="../../../index.html" class="back-link">
                        <i class="fas fa-arrow-left"></i> 返回博客列表
                    </a>
                    <h1>Untitled</h1>
                    <div class="post-meta">
                        <span><i class="fas fa-calendar-alt"></i> 2026-02-04</span>
                        <span><i class="fas fa-folder"></i> AINotes/04.Transformer</span>
                        <span><i class="fas fa-user"></i> Hongwei Zhao</span>
                    </div>
                    <div class="tags">
                        
                    </div>
                </header>

                <div class="post-content">
                    <h2 id="transformer">Transformer<a class="anchor-link" href="#transformer" title="Permanent link">&para;</a></h2>
<blockquote>
<p>paper: <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></p>
</blockquote>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003983.png" alt="image-20211206104820156"  style="zoom:50%;" /></div>

<h2 id="transformer中的self-attention">Transformer中的Self-Attention<a class="anchor-link" href="#transformer中的self-attention" title="Permanent link">&para;</a></h2>
<h3 id="self-attention-in-detail">Self-Attention in Detail<a class="anchor-link" href="#self-attention-in-detail" title="Permanent link">&para;</a></h3>
<p>计算自注意力的第一步就是从每个编码器的输入向量（每个单词的词向量）中生成三个向量。也就是说对于每个单词，我们创造一个查询向量、一个键向量和一个值向量。这三个向量是通过词嵌入与三个权重矩阵后相乘创建的。</p>
<p>可以发现这些新向量在维度上比词嵌入向量更低。他们的维度是64，而词嵌入和编码器的输入/输出向量的维度是512. 但实际上不强求维度更小，这只是一种基于架构上的选择，它可以使多头注意力（multiheaded attention）的大部分计算保持不变。</p>
<p><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003274.png" alt="image-20211204131350691" style="zoom:67%;" /></p>
<p><span class="math-inline">X_1</span> <span class="math-inline">W^Q</span> 重矩阵相乘得到<span class="math-inline">q_1</span>， 就是与这个单词相关的查询向量。最终使得输入序列的每个单词的创建一个查询向量、一个键向量和一个值向量。</p>
<p>计算自注意力的第二步是计算得分。假设我们在为这个例子中的第一个词“Thinking”计算自注意力向量，我们需要拿输入句子中的每个单词对“Thinking”打分。这些分数决定了在编码单词“Thinking”的过程中有多重视句子的其它部分。</p>
<p>这些分数是通过打分单词（所有输入句子的单词）的键向量与“Thinking”的查询向量相点积来计算的。所以如果我们是处理位置最靠前的词的自注意力的话，第一个分数是<span class="math-inline">q_1</span> <span class="math-inline">k_1</span> 点积，第二个分数是<span class="math-inline">q_1</span> <span class="math-inline">k_2</span> 点积。</p>
<p><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003859.png" alt="image-20211204131957766" style="zoom:70%;" /></p>
<p>第三步和第四步是将分数除以8(8是论文中使用的键向量的维数64的平方根，这会让梯度更稳定。这里也可以使用其它值，8只是默认值)，然后通过softmax传递结果。softmax的作用是使所有单词的分数归一化，得到的分数都是正值且和为1。</p>
<p><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003398.png" alt="image-20211204132018172" style="zoom:70%;" /></p>
<p>这个softmax分数决定了每个单词对编码当下位置（“Thinking”）的贡献。显然，已经在这个位置上的单词将获得最高的softmax分数，但有时关注另一个与当前单词相关的单词也会有帮助。</p>
<p>第五步是将每个值向量乘以softmax分数(这是为了准备之后将它们求和)。这里的直觉是希望关注语义上相关的单词，并弱化不相关的单词(例如，让它们乘以0.001这样的小数)。</p>
<p>第六步是对加权值向量求和（译注：自注意力的另一种解释就是在编码某个单词时，就是将所有单词的表示（值向量）进行加权求和，而权重是通过该词的表示（键向量）与被编码词表示（查询向量）的点积并通过softmax得到。），然后即得到自注意力层在该位置的输出(在我们的例子中是对于第一个单词)。</p>
<p><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003984.png" alt="image-20211204132039614" style="zoom:67%;" /></p>
<p>这样自注意力的计算就完成了。得到的向量就可以传给前馈神经网络。然而实际中，这些计算是以矩阵形式完成的，以便算得更快。那我们接下来就看看如何用矩阵实现的。</p>
<h3 id="matrix-calculation-of-self-attention">Matrix Calculation of Self-Attention<a class="anchor-link" href="#matrix-calculation-of-self-attention" title="Permanent link">&para;</a></h3>
<p>第一步是计算查询矩阵、键矩阵和值矩阵。为此，我们将将输入句子的词嵌入装进矩阵X中，将其乘以我们训练的权重矩阵(<span class="math-inline">W^Q</span>，<span class="math-inline">W^K</span>，<span class="math-inline">W^V</span>)。</p>
<p><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003340.png" alt="image-20211204132247579" style="zoom:67%;" /></p>
<p><span class="math-inline">x</span> 阵中的每一行对应于输入句子中的一个单词。我们再次看到词嵌入向量 (512，或图中的4个格子)和<span class="math-inline">q/k/v</span> 量(64，或图中的3个格子)的大小差异。</p>
<p><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003762.png" alt="image-20211204132302809" style="zoom:67%;" /></p>
<blockquote>
<p>除以<span class="math-inline">\sqrt{d_k}</span> 过程就是Scale过程：如果<span class="math-inline">QK^T</span> 大，将导致在经过sofrmax操作后产生非常小的梯度，不利于网络的训练。</p>
</blockquote>
<p>最后，由于我们处理的是矩阵，我们可以将步骤2到步骤6合并为一个公式来计算自注意力层的输出。这种通过 query 和 key 的相似性程度来确定 value 的权重分布的方法被称为scaled dot-product attention。其实scaled dot-Product attention就是我们常用的使用点积进行相似度计算的attention，只是多除了一个（为K的维度）起到调节作用，使得内积不至于太大。</p>
<p><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003144.png" alt="image-20211205172551524" style="zoom:50%;" /></p>
<p><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003625.png" alt="image-20211205145718770" style="zoom:67%;" /></p>
<h3 id="many-heads">Many Heads<a class="anchor-link" href="#many-heads" title="Permanent link">&para;</a></h3>
<p>通过增加一种叫做“多头”注意力（“multi-headed” attention）的机制，论文进一步完善了自注意力层，并在两方面提高了注意力层的性能：</p>
<p>1.它扩展了模型专注于不同位置的能力。在上面的例子中，虽然每个编码都在<span class="math-inline">z_1</span> 有或多或少的体现，但是它可能被实际的单词本身所支配。如果我们翻译一个句子，比如“The animal didn’t cross the street because it was too tired”，我们会想知道“it”指的是哪个词，这时模型的“多头”注意机制会起到作用。</p>
<p>2.它给出了注意力层的多个“表示子空间”（representation subspaces）。接下来我们将看到，对于“多头”注意机制，我们有多个查询/键/值权重矩阵集(Transformer使用八个注意力头，因此我们对于每个编码器/解码器有八个矩阵集合)。这些集合中的每一个都是随机初始化的，在训练之后，每个集合都被用来将输入词嵌入(或来自较低编码器/解码器的向量)投影到不同的表示子空间中。</p>
<p><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003104.png" alt="image-20211204132404792" style="zoom:67%;" /></p>
<p>在“多头”注意机制下，我们为每个头保持独立的查询/键/值权重矩阵，从而产生不同的查询/键/值矩阵。和之前一样，我们拿<span class="math-inline">X</span> 以<span class="math-inline">W_Q/W_K/W_V</span> 阵来产生查询/键/值矩阵。</p>
<p>如果我们做与上述相同的自注意力计算，只需八次不同的权重矩阵运算，我们就会得到八个不同的<span class="math-inline">Z</span> 阵。</p>
<p><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003471.png" alt="image-20211204132459193" style="zoom:75%;" /></p>
<p>这给我们带来了一点挑战。前馈层不需要8个矩阵，它只需要一个矩阵(由每一个单词的表示向量组成)。所以我们需要一种方法把这八个矩阵压缩成一个矩阵。那该怎么做？其实可以直接把这些矩阵拼接在一起，然后用一个附加的权重矩阵<span class="math-inline">W_O</span> 它们相乘。</p>
<p><img alt="image-20211204132529859" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003993.png" /></p>
<p>这几乎就是多头自注意力的全部。这确实有好多矩阵，我们试着把它们集中在一个图片中，这样可以一眼看清。</p>
<p><img alt="image-20211204132621182" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003635.png" /></p>
<p>那么在整个模型中，是如何使用attention的呢？如下图，首先在编码器到解码器的地方使用了多头attention进行连接，K，V，Q分别是编码器的层输出（这里K=V）和解码器中多头attention的输入。其实就和主流的机器翻译模型中的attention一样，利用解码器和编码器attention来进行翻译对齐。然后在编码器和解码器中都使用了多头自注意力self-attention来学习文本的表示。Self-attention即K=V=Q，例如输入一个句子，那么里面的每个词都要和该句子中的所有词进行attention计算。目的是学习句子内部的词依赖关系，捕获句子的内部结构。</p>
<p><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003137.png" alt="image-20211205172419025" style="zoom:50%;" /></p>
<p><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003578.png" alt="在这里插入图片描述" style="zoom:120%;" /></p>
<p>对于使用自注意力机制的原因，论文中提到主要从三个方面考虑（每一层的复杂度，是否可以并行，长距离依赖学习），并给出了和RNN，CNN计算复杂度的比较。可以看到，如果输入序列n小于表示维度d的话，每一层的时间复杂度self-attention是比较有优势的。当n比较大时，作者也给出了一种解决方案self-attention（restricted）即每个词不是和所有词计算attention，而是只与限制的r个词去计算attention。在并行方面，多头attention和CNN一样不依赖于前一时刻的计算，可以很好的并行，优于RNN。在长距离依赖上，由于self-attention是每个词和所有词都要计算attention，所以不管他们中间有多长距离，最大的路径长度也都只是1。可以捕获长距离依赖关系。</p>
<p><img alt="在这里插入图片描述" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003039.png" /></p>
<p>既然我们已经摸到了注意力机制的这么多“头”，那么让我们重温之前的例子，看看我们在例句中编码“it”一词时，不同的注意力“头”集中在哪里：</p>
<p><img alt="image-20211204132927172" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003644.png" /></p>
<p>当我们编码“it”一词时，一个注意力头集中在“animal”上，而另一个则集中在“tired”上，从某种意义上说，模型对“it”一词的表达在某种程度上是“animal”和“tired”的代表。</p>
<p>然而，如果我们把所有的attention都加到图示里，事情就更难解释了：</p>
<p><img alt="image-20211204132941876" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003183.png" /></p>
<h3 id="positional-encoding">Positional Encoding<a class="anchor-link" href="#positional-encoding" title="Permanent link">&para;</a></h3>
<p>根据自注意力机制原理的介绍我们知道，自注意力机制在实际运算过程中不过就是几个矩阵来回相乘进行线性变换而已。因此，这就导致即使是打乱各个词的顺序，那么最终计算得到的结果本质上却没有发生任何变换，换句话说仅仅只使用自注意力机制会丢失文本原有的序列信息。</p>
<p><img alt="图片" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202311132225247.jpeg" /></p>
<p>如上图所示，在经过词嵌入表示后，序列“我 在 看 书”经过了一次线性变换。现在，我们将序列变成“书 在 看 我”，然后同样以中间这个权重矩阵来进行线性变换，过程如下图所示。</p>
<p><img alt="图片" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202311132225599.jpeg" /></p>
<p>根据上图的计算结果来看，序列在交换位置前和交换位置后计算得到的结果在本质上并没有任何区别，仅仅只是交换了对应的位置。因此，基于这样的原因，Transformer在原始输入文本进行Token Embedding后，又额外的加入了一个Positional Embedding来刻画数据在时序上的特征。</p>
<p>首先通过一幅图直观看看经过Positional Embedding处理后到底产生了什么样的变化。</p>
<p><img alt="image-20231114095833445" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202311140958504.png" /></p>
<p>如上图所示，横坐标表示输入序列中的每一个Token，每一条曲线或者直线表示对应Token在每个维度上对应的位置信息。在左图中，每个维度所对应的位置信息都是一个不变的常数；而在右图中，每个维度所对应的位置信息都是基于某种公式变换所得到。换句话说就是，左图中任意两个Token上的向量都可以进行位置交换而模型却不能捕捉到这一差异，但是加入右图这样的位置信息模型却能够感知到。例如位置20这一处的向量，在左图中无论你将它换到哪个位置，都和原来一模一样；但在右图中，你却再也找不到与位置20处位置信息相同的位置。</p>
<p><img alt="image-20231114100310838" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202311141003900.png" /></p>
<p>如上图所示，原始输入在经过Token Embedding后，又加入了一个常数位置信息的的Positional Embedding。在经过一次线性变换后便得到了图右边所示的结果。接下来，我们再交换序列的位置，并同时进行Positional Embedding观察其结果。</p>
<p><img alt="image-20231114100341367" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202311141003429.png" /></p>
<p>如上图所示，在交换序列位置后，采用同样的Positional Embedding进行处理，并且进行线性变换。可以发现，其计算结果同上图（我在看书）中的计算结果本质上也没有发生变换。因此，这就再次证明，如果Positional Embedding中位置信息是以常数形式进行变换，那么这样的Positional Embedding是无效的。</p>
<p>在Transformer中，作者采用了如公式所示的规则来生成各个维度的位置信息：</p>
<p><span class="math-inline">PE_{pos,2i}=sin(pos/10000^{2i/d_{model}}) </span> </p>
<p><span class="math-inline">PE_{pos,2i+1}=cos(pos/10000^{2i/d_{model}}) </span> </p>
<p>其中<span class="math-inline">PE</span> 是这个Positional Embedding矩阵，<span class="math-inline">pos\in[0,max_len]</span> 示具体的某一个位置，<span class="math-inline">i\in[0,d_{model}/2]</span> 示具体的某一维度。融入Positional Embedding的方式如下图所示：</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161002414.png" alt="image-20210412201736447" /></div>

<p>最终，在融入这种非常数的Positional Embedding位置信息后，便可以得到如下图所示的对比结果。</p>
<p><img alt="image-20231114101138004" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202311141011098.png" /></p>
<h3 id="the-residuals">The Residuals<a class="anchor-link" href="#the-residuals" title="Permanent link">&para;</a></h3>
<p>在继续进行下去之前，我们需要提到一个编码器架构中的细节：在每个编码器中的每个子层（自注意力、前馈网络）的周围都有一个残差连接，并且都跟随着一个“层-归一化”步骤。</p>
<p>层-归一化步骤：https://arxiv.org/abs/1607.06450</p>
<p><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003143.png" alt="image-20211204133257470" style="zoom:67%;" /></p>
<p>如果我们去可视化这些向量以及这个和自注意力相关联的层-归一化操作，那么看起来就像下面这张图描述一样：</p>
<p><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003688.png" alt="image-20211204133337139" style="zoom:67%;" /></p>
<p>解码器的子层也是这样样的。如果我们想象一个2 层编码-解码结构的transformer，它看起来会像下面这张图一样：</p>
<p><img alt="image-20211204133411389" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003324.png" /></p>
<h2 id="encoder">Encoder<a class="anchor-link" href="#encoder" title="Permanent link">&para;</a></h2>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003400.png" alt="image-20210429205911444"  /></div>

<p>在transformer的Encoder里面，会<strong>分成很多的block</strong></p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003863.png" alt="image-20210429210126607"/></div>

<p>每一个block都是输入一排向量，输出一排向量，你输入一排向量，第一个block输出另外一排向量，再输给另外一个block，到最后一个block，会输出最终的vector sequence，<strong>每一个block 其实，并不是neural network的一层</strong>，<strong>每一个block里面做的事情，是好几个layer在做的事情</strong>，在transformer的Encoder里面，每一个block做的事情，大概是这样子的</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003353.png" alt="image-20210429210257652" /></div>

<ul>
<li>input一排vector以后，做self-attention，考虑整个sequence的资讯，Output另外一排vector.</li>
<li>接下来这一排vector，会再丢到fully connected的feed forward network里面，再output另外一排vector，这一排vector就是block的输出</li>
</ul>
<p>事实上在原来的<strong>transformer里面，它做的事情是更复杂的</strong></p>
<p>在transformer里面，它加入了一个设计，我们<strong>不只是输出这个vector</strong>，我们还要<strong>把这个vector加上它的input</strong>，它要把input拉过来，直接加给输出，得到新的output 。也就是说，这边假设这个vector叫做<span class="math-inline">a</span>，这个vector叫做<span class="math-inline">b</span>，你要把<span class="math-inline">a+b</span> 作是新的输出。这样子的network架构，叫做<strong>residual connection</strong>。</p>
<p>得到residual的结果以后，再把它做一件事情叫做normalization，这边用的不是batch normalization，这边用的叫做<strong>layer normalization</strong></p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003501.png" alt="image-20210429211313025" style="zoom:60%;"  /></div>

<p>layer normalization做的事情，比bacth normalization更简单一点</p>
<p>输入一个向量，输出另外一个向量，不需要考虑batch，它会<strong>把输入的这个向量，计算它的mean跟standard deviation</strong></p>
<p>但是要注意一下，<strong>batch normalization是对不同example，不同feature的同一个dimension，去计算mean跟standard deviation</strong>（batch的每个特征归一化）</p>
<p>但<strong>layer normalization，它是对同一个feature，同一个example里面，不同的dimension，去计算mean跟standard deviation</strong>（每个example归一化）</p>
<p>计算出mean和standard deviation以后，就可以做一个normalize，我们把input 这个vector里面每一个，dimension减掉mean，再除以standard deviation以后得到<span class="math-inline">x'_i</span>，就是layer normalization的输出<br />
<div class="math-display"><br />
x'_i=\frac{x_i-m}{\sigma}<br />
</div><br />
得到layer normalization的输出以后，它的这个输出才是FC network的输入。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003096.png" alt="image-20210429211858981" style="zoom:60%;"  /></div>

<p>而<strong>FC network这边，也有residual的架构</strong>，所以我们会把FC network的input，跟它的output加起来做一下residual，得到新的输出。这个FC network做完residual以后，还不是结束，你要把residual的结果，<strong>再做一次layer normalization</strong>，得到的输出，才是residual network里面，一个block的输出，所以这个是挺复杂的。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003678.png" alt="image-20210429212721750" style="zoom:60%;"  /></div>

<ul>
<li>首先 你有self-attention，其实在input的地方，还有加上positional encoding，我们之前已经有讲过，如果你只用self-attention，你没有位置的资讯，所以你需要加上positional的information，然后在这个图上，有特别画出positional的information。</li>
<li>Multi-Head Attention，这个就是self-attention的block，这边有特别强调说，它是Multi-Head的self-attention。</li>
<li>Add&amp;norm，就是residual加layer normalization，我们刚才有说self-attention，有加上residual的connection，加下来还要过layer normalization，这边这个图上的Add&amp;norm，就是residual加layer norm的意思。</li>
<li>接下来，要过feed forward network。</li>
<li>fc的feed forward network以后再做一次Add&amp;norm，再做一次residual加layer norm，才是一个block的输出。</li>
<li>然后这个block会重复n次，这个复杂的block，其实在之后会讲到的，一个非常重要的模型BERT里面，会再用到 ，它其实就是transformer的encoder。</li>
</ul>
<hr />
<h2 id="decoder">Decoder<a class="anchor-link" href="#decoder" title="Permanent link">&para;</a></h2>
<h3 id="decoder--autoregressive-at">Decoder – Autoregressive (AT)<a class="anchor-link" href="#decoder--autoregressive-at" title="Permanent link">&para;</a></h3>
<p><strong>Decoder 要做的事情就是产生输出</strong>，也就是<strong>产生语音辨识的结果</strong>， Decoder 怎么产生这个语音辨识的结果</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003542.png" alt="image-20210505193416391" /></div>

<p>首先，你要先给它一个特殊的符号，这个特殊的符号代表开始，在助教的投影片里面，是写 Begin Of Sentence，缩写是 BOS</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003126.png" alt="image-20210505193652691" style="zoom:60%;" /></div>

<p>就是 Begin 的意思，这个是一个 Special 的 Token，代表了开始这个事情。在这个机器学习里面，假设你要处理 NLP 的问题，<strong>每一个 Token，你都可以把它用一个 One-Hot 的 Vector 来表示</strong>，One-Hot Vector 就其中一维是 1，其他都是 0，所以 <strong>BEGIN 也是用 One-Hot Vector 来表示</strong>，其中一维是 1，其他是 0</p>
<p>接下来Decoder 会<strong>吐出一个向量，这个 Vector 的长度很长，跟你的 Vocabulary 的 Size 是一样的</strong></p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003655.png" alt="image-20210505194403395"  style="zoom:60%;" /></div>

<p>每一个中文的字，都会对应到一个数值，因为在<strong>产生这个向量之前，你通常会先跑一个 Softmax</strong>，就跟做分类一样，所以这一个向量里面的分数，它是一个 Distribution，也就是它这个向量里面的值全部加起来总和会是 1。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003125.png" alt="image-20210505195003941" style="zoom: 70%;" /></div>

<p><strong>分数最高的一个中文字，它就是最终的输出</strong>。在这个例子里面，机的分数最高，所以机，就当做是这个 Decoder 第一个输出。</p>
<p>然后接下来，你<strong>把“机”当做是 Decoder 新的 Input</strong>，原来 Decoder 的 Input，只有 BEGIN 这个特别的符号，现在它<strong>除了 BEGIN 以外，它还有“机”</strong>作为它的 Input。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003679.png" alt="image-20210505195340257"   /></div>

<p>所以 Decoder <strong>现在它有两个输入</strong></p>
<ul>
<li>一个是 <strong>BEGIN</strong> 这个符号</li>
<li>一个是“机”</li>
</ul>
<p>根据这两个输入，它输出一个蓝色的向量，根据这个蓝色的向量里面，给每一个中文的字的分数，我们会决定第二个输出，哪一个字的分数最高，它就是输出，假设"器"的分数最高，<strong>"器"就是输出</strong>。</p>
<p>然后现在 Decoder </p>
<ul>
<li>看到了 BEGIN</li>
<li>看到了"机" </li>
<li>看到了"器"</li>
</ul>
<p>它接下来，还要再决定接下来要输出什么，它可能，就输出"学"，这一个过程就反覆的持续下去</p>
<p>所以现在 Decoder </p>
<ul>
<li>
<p>看到了 BEGIN </p>
</li>
<li>
<p>看到了"机" </p>
</li>
<li>
<p>看到了"器"</p>
</li>
<li>
<p>还有"学"</p>
</li>
</ul>
<p><strong>Encoder 这边其实也有输入</strong>，等一下再讲 Encoder 的输入，Decoder 是怎么处理的。</p>
<p>所以 Decoder 看到 Encoder 这边的输入，看到"机"  ，看到"器" ，看到"学"，决定接下来输出一个向量，这个向量里面，"习"这个中文字的分数最高的，所以它就输出"习"。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003241.png" alt="image-20210505203022207"   /></div>

<p><strong>然后这个 Process ，就反覆持续下去</strong>，这边有一个关键的地方，我们特别用红色的虚线把它标出来</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003665.png" alt="image-20210505203437172" style="zoom:80%;" /></div>

<p>也就是说 Decoder 看到的输入，其实是它在前一个时间点，自己的输出，<strong>Decoder 会把自己的输出，当做接下来的输入</strong>。</p>
<p>我们来看一下这个 <strong>Decoder内部的结构</strong>长什么样子？</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003234.png" alt="image-20210505203958558"  style="zoom:80%;" /></div>

<p>这里先<strong>把 Encoder 的部分先暂时省略掉</strong>，那在 Transformer 里面，Decoder 的结构，长得是这个样子的，看起来有点复杂，比 Encoder 还稍微复杂一点，</p>
<p>那我们现在先把 Encoder 跟 Decoder 放在一起</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003647.png" alt="image-20210505204215642" style="zoom:80%;"  /></div>

<p>稍微比较一下它们之间的差异，那你会发现说，如果我们把 Decoder 中间这一块盖起来，其实 Encoder 跟 Decoder，并没有那么大的差别。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003154.png" alt="image-20210505204300084"  style="zoom:80%;" /></div>

<p>在 Decoder 这边，Multi-Head Attention 这一个 Block 上面，还<strong>加了一个  Masked</strong>，这个 Masked 的意思是这样子的，这是我们原来的 Self-Attention</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003778.png" alt="image-20210505204547594" style="zoom: 67%;" /></div>

<p>Input 一排 Vector，Output 另外一排 Vector，这一排 Vector <strong>每一个输出</strong>，都要看过完整的 Input 以后，才做决定，所以输出 <span class="math-inline">b^1</span> 的时候，其实是根据 <span class="math-inline">a^1</span> 到 <span class="math-inline">a^4</span> 所有的资讯，去输出 <span class="math-inline">b^1</span>。</p>
<p>当我们把 Self-Attention，转成 Masked Attention 的时候，它的不同点是，现在我们<strong>不能再看右边的部</strong>分，也就是产生 <span class="math-inline">b^1</span> 的时候，我们只能考虑 <span class="math-inline">a^1</span> 的资讯，你不能够再考虑 <span class="math-inline">a^2</span> <span class="math-inline">a^3</span> <span class="math-inline">a^4</span>。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003440.png" alt="image-20210505205155734"   /></div>

<p>产生 <span class="math-inline">b^2</span> 的时候，你只能考虑 <span class="math-inline">a^1</span> <span class="math-inline">a^2</span> 的资讯，不能再考虑 <span class="math-inline">a^3</span> <span class="math-inline">a^4</span> 的资讯。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003962.png" alt="image-20210505205223197"   /></div>

<p>产生 <span class="math-inline">b^3</span> 的时候，你就不能考虑 <span class="math-inline">a^4</span> 的资讯，</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003465.png" alt="image-20210505205307428"   /></div>

<p>产生 <span class="math-inline">b^4</span> 的时候，你可以用整个 Input Sequence 的资讯，这个就是 Masked 的 Self-Attention，</p>
<p>讲得更具体一点，你做的事情是，当我们要产生 <span class="math-inline">b^2</span> 的时候，我们只拿第二个位置的 Query <span class="math-inline">b^2</span>，去跟第一个位置的 Key，和第二个位置的 Key，去计算 Attention，第三个位置跟第四个位置，就不管它，不去计算 Attention。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161003771.png" alt="image-20210505205618457"   /></div>

<p>我们这样子不去管这个 <span class="math-inline">a^2</span> 右边的地方，只考虑 <span class="math-inline">a^1</span> 跟 <span class="math-inline">a^2</span>，只考虑 <span class="math-inline">q^1</span> <span class="math-inline">q^2</span>，只考虑 <span class="math-inline">k^1</span> <span class="math-inline">k^2</span>，<span class="math-inline">q^2</span> 只跟 <span class="math-inline">k^1</span> 跟 <span class="math-inline">k^2</span> 去计算 Attention，然后最后只计算 <span class="math-inline">b^1</span> 跟 <span class="math-inline">b^2</span> 的 Weighted Sum。然后当我们输出这个 <span class="math-inline">b^2</span> 的时候，<span class="math-inline">b^2</span> 就只考虑了 <span class="math-inline">a^1</span> 跟 <span class="math-inline">a^2</span>，就没有考虑到 <span class="math-inline">a^3</span> 跟 <span class="math-inline">a^4</span>。</p>
<p>那为什么会这样，为什么需要加 Masked？</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161004095.png" alt="image-20210505205746128"   /></div>

<p>这件事情其实非常地直觉：我们一开始 Decoder 的运作方式，它是<strong>一个一个输出</strong>，所以是先有 <span class="math-inline">a^1</span> 再有 <span class="math-inline">a^2</span>，再有 <span class="math-inline">a^3</span> 再有 <span class="math-inline">a^4</span>。</p>
<p><strong>这跟原来的 Self-Attention 不一样</strong>，原来的 Self-Attention，<span class="math-inline">a^1</span> 跟 <span class="math-inline">a^4</span> 是一次整个输进去你的 Model 里面的，在我们讲 Encoder 的时候，Encoder 是一次把 <span class="math-inline">a^1</span> 跟 <span class="math-inline">a^4</span>，都整个都读进去。但是对 Decoder 而言，先有 <span class="math-inline">a^1</span> 才有 <span class="math-inline">a^2</span>，才有 <span class="math-inline">a^3</span> 才有 <span class="math-inline">a^4</span>，所以实际上，当你有 <span class="math-inline">a^2</span>，你要计算 <span class="math-inline">b^2</span> 的时候，你是没有 <span class="math-inline">a^3</span> 跟 <span class="math-inline">a^4</span> 的，所以你根本就没有办法把 <span class="math-inline">a^3</span> <span class="math-inline">a^4</span> 考虑进来。</p>
<p>所以这就是为什么，在那个 Decoder 的那个图上面，Transformer 原始的 Paper 特别跟你强调说，<strong>那不是一个一般的 Attention，这是一个 Masked 的 Self-Attention</strong>，意思只是想要告诉你说，Decoder 它的 Token，<strong>它输出的东西是一个一个产生的，所以它只能考虑它左边的东西，它没有办法考虑它右边的东西</strong>。</p>
<p>讲了 Decoder 的运作方式，但是这边，还有一个非常关键的问题，<strong>Decoder 必须自己决定输出的 Sequence 的长度</strong>。</p>
<p>所以我们要让 Decoder 做的事情，也是一样，要让它可以输出一个断，所以你要<strong>特别准备一个特别的符号</strong>，这个符号，就叫做断，我们这边用 END 来表示这个特殊的符号。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161004714.png" alt="image-20210505210756634"   /></div>

<p>BEGIN 只会在输入的时候出现，断只会在输出的时候出现，所以在助教的程式里面，如果你仔细研究一下的话，会发现说 END 跟 BEGIN，用的其实是同一个符号，但你用不同的符号，也是完全可以的，也完全没有问题。</p>
<p>这个就是  Autoregressive  Decoder ，它运作的方式。</p>
<p>在真实预测时解码器需要将上一个时刻的输出作为下一个时刻解码的输入，然后一个时刻一个时刻的进行解码操作。显然，如果训练时也采用同样的方法那将是十分费时的。因此，在训练过程中，解码器也同编码器一样，一次接收解码时所有时刻的输入进行计算。这样做的好处，一是通过多样本并行计算能够加快网络的训练速度；二是在训练过程中直接喂入解码器正确的结果而不是上一时刻的预测值（因为训练时上一时刻的预测值可能是错误的）能够更好的训练网络。</p>
<p>例如在用平行语料<code>"我 是 谁"</code>&lt;==&gt;<code>"who am i"</code>对网络进行训练时，编码器的输入便是<code>"我 是 谁"</code>，而解码器的输入则是<code>"&lt;s&gt; who am i"</code>，对应的正确标签则是<code>"who am i &lt;e&gt;"</code>。</p>
<p>假设现在解码器的输入<code>"&lt;s&gt; who am i"</code>在分别乘上一个矩阵进行线性变换后得到了<span class="math-inline">Q、K、V，</span>  <span class="math-inline">Q</span> 与 <span class="math-inline">K</span> 作用后得到了注意力权重矩阵（此时还未进行softmax操作），如图所示。</p>
<p><img alt="图片" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202311192052656.jpeg" /></p>
<p>从上图可以看出，此时已经计算得到了注意力权重矩阵。由第1行的权重向量可知，在解码第1个时刻时应该将20%（严格来说应该是经过softmax后的值）的注意力放到<code>"&lt;s&gt;"</code>上，30%的注意力放到<code>"who"</code>上等等。不过此时有一个问题就是，模型在实际的预测过程中只是将当前时刻之前（包括当前时刻）的所有时刻作为输入来预测下一个时刻，也就是说模型在预测时是看不到当前时刻之后的信息。因此，Transformer中的Decoder通过加入注意力掩码机制来解决了这一问题。</p>
<blockquote>
<p>self-attention layers in the decoder <strong>allow each position in the decoder to attend to all positions in the decoder up to and including that position</strong>. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections.</p>
</blockquote>
<p>如下图所示，左边依旧是通过<span class="math-inline">Q</span> <span class="math-inline">K</span> 算得到了注意力权重矩阵（此时还未进行softmax操作），而中间的就是所谓的注意力掩码矩阵，两者在相加之后再乘上矩阵<span class="math-inline">V</span> 得到了整个自注意力机制的输出，也就是Masked Multi-Head Attention。</p>
<p><img alt="图片" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202311192052960.jpeg" /></p>
<p>那为什么注意力权重矩阵加上这个注意力掩码矩阵就能够达到这样的效果呢？以上图第1行权重为例，当解码器对第1个时刻进行解码时其对应的输入只有<code>"&lt;s&gt;"</code>，因此这就意味着此时应该将所有的注意力放在第1个位置上（尽管在训练时解码器一次喂入了所有的输入），换句话说也就是第1个位置上的权重应该是1，而其它位置则是0。从上上图可以看出，第1行注意力向量在加上第1行注意力掩码，再经过softmax操作后便得到了一个类似的向量。那么，通过这个向量就能够保证在解码第1个时刻时只能将注意力放在第1个位置上的特性。同理，在解码后续的时刻也是类似的过程。</p>
<h3 id="decoder--non-autoregressive-nat">Decoder – Non-autoregressive (NAT)<a class="anchor-link" href="#decoder--non-autoregressive-nat" title="Permanent link">&para;</a></h3>
<p>用两页投影片，非常简短地讲一下，Non-Autoregressive 的 Model</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161004581.png" alt="image-20210505213222908"   /></div>

<p>Non-Autoregressive ，通常缩写成 NAT，所以有时候 Autoregressive 的 Model，也缩写成 AT，Non-Autoregressive 的 Model 是怎么运作的</p>
<h3 id="at-vs-nat">AT v.s. NAT<a class="anchor-link" href="#at-vs-nat" title="Permanent link">&para;</a></h3>
<p>这个  Autoregressive  的 Model 是</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161004999.png" alt="image-20210505213259792" /></div>

<p><strong>先输入 BEGIN</strong>，<strong>然后</strong>出现 w1，然后再<strong>把 w1 当做输入，</strong>再<strong>输出 w2，</strong>直到输出 END 为止**</p>
<p>那  NAT  是这样，它<strong>不是依次产生</strong></p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161004044.png" alt="image-20210505213441943" style="zoom:67%;" /></div>

<p>就假设我们现在产生是中文的句子，它不是依次产生一个字，它是<strong>一次把整个句子都产生出来</strong></p>
<p>NAT 的 Decoder<strong>可能吃的是一整排的 BEGIN 的 Token</strong>，你就把一堆一排 BEGIN 的 Token 都丢给它，让它一次产生一排 Token 就结束了</p>
<p>举例来说，如果你丢给它 4 个 BEGIN 的 Token，它就产生 4 个中文的字，变成一个句子，就结束了，所以它只要一个步骤，就可以完成句子的生成。</p>
<p>这边你可能会问一个问题：刚才不是说不知道输出的长度应该是多少吗，那我们这边<strong>怎么知道 BEGIN 要放多少个</strong>，当做 NAT Decoder 的输入？</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161004327.png" alt="image-20210505214100885"   /></div>

<p>没错 这件事没有办法很自然的知道，没有办法很直接的知道，所以有几个做法</p>
<ul>
<li>一个做法是，你<strong>另外learn一个 Classifier</strong>，这个 Classifier ，它吃 Encoder 的 Input，然后输出是一个数字，这个数字代表 Decoder 应该要输出的长度，这是一种可能的做法</li>
<li>另一种可能做法就是，你就不管三七二十一，<strong>给它一堆 BEGIN 的 Token</strong>，你就假设说，你现在输出的句子的长度，绝对不会超过 300 个字，你就假设一个句子长度的上限，然后 BEGIN ，你就给它 300 个 BEGIN，然后就会输出 300 个字嘛，然后，你再看看<strong>什么地方输出 END</strong>，输出 END 右边的，就当做它没有输出，就结束了，这是另外一种处理 NAT 的这个 Decoder，它应该输出的长度的方法</li>
</ul>
<p>那 NAT 的 Decoder，它有什么样的<strong>好处</strong>，</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161004634.png" alt="image-20210505214412075" style="zoom:67%;" /></div>

<ul>
<li>它第一个好处是，<strong>并行化</strong>，这个 AT 的 Decoder，它在输出它的句子的时候，是一个一个一个字产生的，所以你有你的，假设要输出长度一百个字的句子，那你就需要做一百次的 Decode</li>
</ul>
<p>但是 NAT 的 Decoder 不是这样，不管句子的长度如何，都是<strong>一个步骤就产生出完整的句子</strong>，所以在速度上，NAT 的 Decoder 它会跑得比，AT 的 Decoder 要快，那你可以想像说，这个 NAT Decoder 的想法显然是在，由这个 Transformer 以后，有这种 Self-Attention 的 Decoder 以后才有的</p>
<p>因为以前如果你是用那个  LSTM ，用  RNN  的话，那你就算给它一排 BEGIN，它也没有办法同时产生全部的输出，它的输出还是一个一个产生的，所以在没有这个 Self-Attention 之前，只有 RNN，只有 LSTM 的时候，根本就不会有人想要做什么 NAT 的 Decoder，不过自从有了 Self-Attention 以后，那 NAT 的 Decoder，现在就算是一个热门的研究的主题了</p>
<ul>
<li>那 NAT 的 Decoder 还有另外一个好处就是，你比较能够<strong>控制它输出的长度</strong>，举语音合成为例，其实在语音合成里面，NAT 的 Decoder 算是非常常用的，它并不是一个什么罕见的招数。</li>
</ul>
<p>比如说有，所以语音合成今天你都可以用，Sequence To Sequence 的模型来做，那最知名的，是一个叫做  Tacotron  的模型，那它是 AT 的 Decoder</p>
<p>那有另外一个模型叫  FastSpeech ，那它是 NAT 的 Decoder，那 NAT 的 Decoder 有一个好处，就是你可以控制你输出的长度，那我们刚才说怎么决定，NAT 的 Decoder 输出多长</p>
<p>你可能有一个 Classifier，决定 NAT 的 Decoder 应该输出的长度，那如果在做语音合成的时候，假设你现在突然想要让你的系统讲快一点，加速，那你就把那个 Classifier 的 Output 除以二，它讲话速度就变两倍快，然后如果你想要这个讲话放慢速度，那你就把那个 Classifier 输出的那个长度，它 Predict 出来的长度乘两倍，那你的这个 Decoder ，说话的速度就变两倍慢</p>
<p>所以你可以如果有这种 NAT 的 Decoder，那你有 Explicit 去 Model，Output  长度应该是多少的话，你就比较有机会去控制，你的 Decoder 输出的长度应该是多少，你就可以做种种的变化</p>
<p>NAT 的 Decoder，最近它之所以是一个热门研究主题，就是它虽然表面上看起来有种种的厉害之处，尤其是平行化是它最大的优势，但是 <strong>NAT 的 Decoder ，它的 Performance，往往都不如 AT 的 Decoder</strong></p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161004425.png" alt="image-20210505214728437"   /></div>

<p>所以发现有很多很多的研究试图让，NAT 的 Decoder 的 Performance 越来越好，试图去逼近 AT 的 Decoder，不过今天你要让 NAT 的 Decoder，跟 AT 的 Decoder Performance 一样好，你<strong>必须要用非常多的 Trick</strong> 才能够办到，就 AT 的 Decoder 随便 Train 一下，NAT 的 Decoder 你要花很多力气，才有可能跟 AT 的 Performance 差不多</p>
<hr />
<h2 id="encoder-decoder">Encoder-Decoder<a class="anchor-link" href="#encoder-decoder" title="Permanent link">&para;</a></h2>
<p>接下来就要讲<strong>Encoder 跟 Decoder它们中间是怎么传递资讯</strong>的了，也就是我们要讲，刚才我们刻意把它遮起来的那一块</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161004360.png" alt="image-20210506103314101" /></div>

<p>这块叫做  <strong>Cross Attention</strong> ，它是连接 Encoder 跟 Decoder 之间的桥樑，那这一块里面啊，会发现有<strong>两个输入来自于 Encoder</strong>，Encoder 提供两个箭头，然后 <strong>Decoder 提供了一个箭头</strong>，所以从左边这两个箭头，Decoder 可以读到 Encoder 的输出。</p>
<p>Encoder输入一排向量，输出一排向量，我们叫它 <span class="math-inline">a^1 a^2 a^3</span></p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161004201.png" alt="image-20210506104141482" /></div>

<p>Decoder 会先吃 BEGIN ，那 BEGIN 这个 Special 的 Token 读进来以后，可能会经过 Self-Attention，这个 Self-Attention 是有做 Mask 的，然后得到一个向量，然后接下来把这个向量乘上一个矩阵做一个 Transform，得到一个 Query 叫做 <span class="math-inline">q</span></p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161004882.png" alt="image-20210506105155338" style="zoom: 50%;" /></div>

<p>然后这边的 <span class="math-inline">a^1 a^2 a^3</span> 呢，也都产生 Key，Key1 Key2 Key3，那把这个 <span class="math-inline">q</span> 跟 <span class="math-inline">k^1 k^2 k^3</span>，去计算 Attention 的分数，得到 <span class="math-inline">α_1 α_2 α_3</span>，当然你可能一样会做 Softmax，把它稍微做一下 Normalization，所以我这边加一个 '，代表它可能是做过 Normalization。</p>
<p>接下来再把 <span class="math-inline">α_1^{'}</span> <span class="math-inline">α_2^{'}</span> <span class="math-inline">α_3^{'}</span>，就乘上 <span class="math-inline">v^1 v^2 v^3</span>，再把它 Weighted Sum 加起来会得到 <span class="math-inline">v</span></p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161004535.png" alt="image-20210506105352521"   /></div>

<p>那这一个 <span class="math-inline">v</span>，就是接下来会丢到 Fully-Connected 的Network 做接下来的处理，那这个步骤就是 <span class="math-inline">q</span> 来自于 Decoder，<span class="math-inline">k</span> 跟 <span class="math-inline">v </span> 自于 Encoder，这个步骤就叫做 <strong>Cross Attention</strong>。</p>
<p>所以 <strong>Decoder 就是凭借着产生一个 <span class="math-inline">q</span>，去 Encoder 这边抽取资讯出来，当做接下来的 Decoder 的Fully-Connected 的 Network 的 Input</strong>。</p>
<p>现在假设产生第二个，第一个这个中文的字产生一个“机”，接下来的运作也是一模一样的。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161004099.png" alt="image-20210506105747730" style="zoom: 50%;" /></div>

<p>输入 BEGIN 输入“机”，产生一个向量，这个向量一样乘上一个 Linear 的 Transform，得到 <span class="math-inline">q^{'}</span>，得到一个 Query，这个 Query 一样跟 <span class="math-inline">k^1 k^2 k^3</span>，去计算 Attention 的分数，一样跟 <span class="math-inline">v^1 v^2 v^3</span> 做 Weighted Sum 做加权，然后加起来得到 <span class="math-inline">v'</span>，交给接下来 Fully-Connected Network 做处理。</p>
<p>Decoder预测解码过程如下图所示</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202311161043123.gif" /></p>
<p>Decoder在对当前时刻进行解码输出时，都会将当前时刻之前所有的预测结果作为输入来对下一个时刻的输出进行预测</p>
<hr />
<h2 id="training">Training<a class="anchor-link" href="#training" title="Permanent link">&para;</a></h2>
<p>已经清楚说 Input 一个 Sequence，是怎么得到最终的输出，那接下来就进入训练的部分。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161004948.png" alt="image-20210506121209178"   /></div>

<p>假设是做语音辨识，那你要有<strong>训练资料</strong>，你要收集一大堆的声音讯号，每一句声音讯号都要有工读生来听打一下，打出说它的这个对应的词汇是什么</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161004162.png" alt="image-20210506121209178"  style="zoom:150%;" /></div>

<p>工读生听这段是机器学习，他就把机器学习四个字打出来，所以就知道说你的这个 Transformer，应该要学到听到这段声音讯号，它的输出就是机器学习这四个中文字。</p>
<p>我们已经知道说输入这段声音讯号，第一个应该要输出的中文字是“机”，所以今天当我们把 BEGIN丢给这个 Encoder 的时候，它第一个输出应该要跟“机”越接近越好</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161004978.png" alt="image-20210506122929142"   /></div>

<p><strong>“机”这个字会被表示成一个 One-Hot 的 Vector</strong>，在这个 Vector 里面，只有机对应的那个维度是 1，其他都是 0，这是正确答案，那我们的 Decoder，它的输出是一个 Distribution，是一个机率的分布，我们会希望这一个机率的分布，跟这个 One-Hot 的 Vector 越接近越好。</p>
<p>所以你会去计算这个 Ground Truth，跟这个 Distribution 它们之间的 Cross Entropy，然后我们希望这个  Cross Entropy  的值，越小越好。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161004609.png" alt="image-20210506123013981"   /></div>

<p>它就<strong>跟分类很像</strong>，每一次 Decoder 在产生一个中文字的时候，其实就是做了一次分类的问题，中文字假设有四千个，那就是<strong>做有四千个类别的分类的问题</strong>。</p>
<p>所以实际上训练的时候这个样子，我们已经知道输出应该是“机器学习”这四个字，就告诉你的 Decoder ，现在你第一次的输出 第二次的输出，第三次的输出 第四次输出，应该分别就是“机” “器” “学”跟“习”，这四个中文字的 One-Hot Vector，我们<strong>希望我们的输出，跟这四个字的 One-Hot Vector 越接近越好</strong></p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161004191.png" alt="image-20210506124154943"   /></div>

<p>在训练的时候，每一个输出都会有一个 Cross Entropy，每一个输出跟 One-Hot Vector，跟它对应的正确答案都有一个 Cross Entropy，我们要希望所有的 Cross Entropy 的总和最小，越小越好</p>
<p>所以这边做了四次分类的问题，我们希望这些分类的问题，它总合起来的 Cross Entropy 越小越好，<strong>还有 END 这个符号</strong>。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202208161004743.png" alt="image-20210506150925655"   /></div>

<p>那这个就是 Decoder 的训练<strong>：把 Ground Truth ，正确答案给它，希望 Decoder 的输出跟正确答案越接近越好</strong></p>
<p>那这边有一件值得我们注意的事情，在<strong>训练</strong>的时候我们会给 Decoder 看<strong>正确答案</strong>，也就是我们会告诉它说</p>
<ul>
<li>在已经有 "BEGIN"，在有"机"的情况下你就要输出"器"</li>
<li>有 "BEGIN" 有"机" 有"器"的情况下输出"学"</li>
<li>有 "BEGIN" 有"机" 有"器" 有"学"的情况下输出"习"</li>
<li>有 "BEGIN" 有"机" 有"器" 有"学" 有"习"的情况下，你就要输出"断"</li>
</ul>
<p>在 Decoder 训练的时候，我们会在输入的时候给它<strong>正确的答案</strong>，那这件事情叫做  Teacher Forcing </p>
<p>那这个时候你马上就会有一个问题了</p>
<ul>
<li>训练的时候，Decoder 有偷看到正确答案了</li>
<li>但是测试的时候，显然没有正确答案可以给 Decoder 看</li>
</ul>
<p>刚才也有强调说在真正使用这个模型，在 Inference 的时候，Decoder 看到的是自己的输入，这<strong>中间显然有一个  Mismatch </strong>，那等一下我们会有一页投影片的说明，有什么样可能的解决方式</p>
<h2 id="answer">Answer<a class="anchor-link" href="#answer" title="Permanent link">&para;</a></h2>
<h3 id="1为什么除以sqrtd_k">1、为什么除以<span class="math-inline">\sqrt{d_k}</span><a class="anchor-link" href="#1为什么除以sqrtd_k" title="Permanent link">&para;</a></h3>
<p>除以<span class="math-inline">\sqrt{d_k}</span> 过程就是Scale过程，对于较大的<span class="math-inline">d_k</span> 说，在完成<span class="math-inline">QK^T</span> 乘积后将会得到很大的值，而这将导致在经过softmax操作后产生非常小的梯度，不利于网络的训练。</p>
<h3 id="2为什么多头">2、为什么多头<a class="anchor-link" href="#2为什么多头" title="Permanent link">&para;</a></h3>
<ul>
<li>模型在对当前位置的信息进行编码时，会过度的将注意力集中于自身的位置</li>
<li>使用多头注意力机制还能够给予注意力层的输出包含有不同子空间中的编码表示信息，从而增强模型的表达能力</li>
</ul>
<h3 id="3原始qkv来源">3、原始Q、K、V​来源<a class="anchor-link" href="#3原始qkv来源" title="Permanent link">&para;</a></h3>
<p>根据Transformer结构图可知，在整个Transformer中涉及到自注意力机制的一共有3个部分：Encoder中的Multi-Head Attention；Decoder中的Masked Multi-Head Attention；Encoder和Decoder交互部分的Multi-Head Attention。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202311192105358.png" alt="image-20231119210214371" style="zoom:50%;" /></div>

<ol>
<li>对于Encoder中的Multi-Head Attention来说，其原始<span class="math-inline">q、k、v</span> 是Encoder的Token输入经过Embedding后的结果。<span class="math-inline">q、k、v</span> 别经过一次线性变换（各自乘以一个权重矩阵）后得到了<span class="math-inline">Q、K、V</span>，然后再进行自注意力运算得到Encoder部分的输出结果Memory。</li>
<li>对于Decoder中的Masked Multi-Head Attention来说，其原始<span class="math-inline">q、k、v</span> 是Decoder的Token输入经过Embedding后的结果。<span class="math-inline">q、k、v</span> 别经过一次线性变换后得到了<span class="math-inline">Q、K、V</span>，然后再进行自注意力运算得到Masked Multi-Head Attention部分的输出结果，即待解码向量。</li>
<li>对于Encoder和Decoder交互部分的Multi-Head Attention，其原始<span class="math-inline">q、k、v</span> 别是上面的带解码向量和Memory。<span class="math-inline">q、k、v</span> 别经过一次线性变换后得到了<span class="math-inline">Q、K、V</span>，然后再进行自注意力运算得到Decoder部分的输出结果。之所以这样设计也是在模仿传统Encoder-Decoder网络模型的解码过程。</li>
</ol>
<h2 id="reference">Reference<a class="anchor-link" href="#reference" title="Permanent link">&para;</a></h2>
<ol>
<li><a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a> </li>
<li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
<li><a href="https://blog.csdn.net/longxinchen_ml/article/details/86533005">图解Transformer（完整版）</a></li>
<li><a href="https://blog.csdn.net/jiaowoshouzi/article/details/89073944?ops_request_misc=%7B%22request%5Fid%22%3A%22163845966116780265483857%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&amp;request_id=163845966116780265483857&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~top_positive~default-1-89073944.pc_v2_rank_blog_default&amp;utm_term=BERT&amp;spm=1018.2226.3001.4450">一文读懂BERT(原理篇)_程序猿废柴的博客-CSDN博客_bert</a></li>
<li><a href="https://mp.weixin.qq.com/s/jWFktOz_FPKvQ53ldDp5DA">4万字+50幅图一网打尽Transformer</a></li>
</ol>
                </div>

                <!-- Comments Section (Giscus) -->
                <section class="comments-section">
                    <h3><i class="fas fa-comments"></i> 评论</h3>
                    <script src="https://giscus.app/client.js"
                        data-repo="Geeks-Z/Geeks-Z.github.io"
                        data-repo-id=""
                        data-category="Announcements"
                        data-category-id=""
                        data-mapping="pathname"
                        data-strict="0"
                        data-reactions-enabled="1"
                        data-emit-metadata="0"
                        data-input-position="bottom"
                        data-theme="preferred_color_scheme"
                        data-lang="zh-CN"
                        crossorigin="anonymous"
                        async>
                    </script>
                </section>
            </article>

            <footer class="post-footer">
                <p>© 2025 Hongwei Zhao. Built with ❤️</p>
            </footer>
        </main>
    </div>

    <!-- Theme Toggle Button -->
    <button class="theme-toggle" id="themeToggle" aria-label="切换主题">
        <i class="fas fa-moon"></i>
    </button>

    <script>
        // Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        const html = document.documentElement;
        const icon = themeToggle.querySelector('i');
        const hljsDark = document.getElementById('hljs-theme-dark');
        const hljsLight = document.getElementById('hljs-theme-light');
        
        // Check saved theme or system preference
        const savedTheme = localStorage.getItem('theme');
        const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
        
        if (savedTheme === 'dark' || (!savedTheme && prefersDark)) {
            html.setAttribute('data-theme', 'dark');
            icon.className = 'fas fa-sun';
            hljsDark.disabled = false;
            hljsLight.disabled = true;
        }
        
        themeToggle.addEventListener('click', () => {
            const isDark = html.getAttribute('data-theme') === 'dark';
            if (isDark) {
                html.removeAttribute('data-theme');
                icon.className = 'fas fa-moon';
                localStorage.setItem('theme', 'light');
                hljsDark.disabled = true;
                hljsLight.disabled = false;
            } else {
                html.setAttribute('data-theme', 'dark');
                icon.className = 'fas fa-sun';
                localStorage.setItem('theme', 'dark');
                hljsDark.disabled = false;
                hljsLight.disabled = true;
            }
            
            // Update Giscus theme
            const giscusFrame = document.querySelector('iframe.giscus-frame');
            if (giscusFrame) {
                giscusFrame.contentWindow.postMessage({
                    giscus: {
                        setConfig: {
                            theme: isDark ? 'light' : 'dark'
                        }
                    }
                }, 'https://giscus.app');
            }
        });
        
        // Highlight.js
        document.addEventListener('DOMContentLoaded', () => {
            hljs.highlightAll();
        });
        
        // KaTeX auto-render
        document.addEventListener('DOMContentLoaded', () => {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
        
        // TOC active state
        const tocLinks = document.querySelectorAll('.toc-container a');
        const headings = document.querySelectorAll('.post-content h2, .post-content h3, .post-content h4');
        
        function updateTocActive() {
            let current = '';
            headings.forEach(heading => {
                const rect = heading.getBoundingClientRect();
                if (rect.top <= 100) {
                    current = heading.getAttribute('id');
                }
            });
            
            tocLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === '#' + current) {
                    link.classList.add('active');
                }
            });
        }
        
        window.addEventListener('scroll', updateTocActive);
        updateTocActive();
    </script>
</body>
</html>
