<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>------------------------------------</title>
    <meta name="description" content="------------------------------------ - Hongwei Zhao's Blog">
    <meta name="author" content="Hongwei Zhao">
    
    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <!-- Code Highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css" id="hljs-theme-dark">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css" id="hljs-theme-light" disabled>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    
    <style>
        :root {
            /* Light Theme */
            --primary-color: #2980b9;
            --primary-hover: #1a5276;
            --link-color: #c0392b;
            --text-color: #333;
            --text-light: #666;
            --text-muted: #999;
            --bg-color: #fff;
            --bg-secondary: #f8f9fa;
            --bg-code: #f5f5f5;
            --border-color: #e5e7eb;
            --shadow: 0 1px 3px rgba(0,0,0,0.1);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.1);
        }

        [data-theme="dark"] {
            --primary-color: #5dade2;
            --primary-hover: #85c1e9;
            --link-color: #e74c3c;
            --text-color: #e5e7eb;
            --text-light: #9ca3af;
            --text-muted: #6b7280;
            --bg-color: #1a1a2e;
            --bg-secondary: #16213e;
            --bg-code: #0f0f23;
            --border-color: #374151;
            --shadow: 0 1px 3px rgba(0,0,0,0.3);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.4);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 16px;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            transition: background-color 0.3s, color 0.3s;
        }

        a {
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--primary-hover);
            text-decoration: underline;
        }

        /* Layout */
        .page-wrapper {
            display: flex;
            max-width: 1400px;
            margin: 0 auto;
            padding: 2rem;
            gap: 3rem;
        }

        /* Sidebar TOC */
        .toc-sidebar {
            width: 260px;
            flex-shrink: 0;
            position: sticky;
            top: 2rem;
            height: fit-content;
            max-height: calc(100vh - 4rem);
            overflow-y: auto;
        }

        .toc-container {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 1.5rem;
            border: 1px solid var(--border-color);
        }

        .toc-container h3 {
            font-size: 0.85rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-muted);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .toc-container ul {
            list-style: none;
        }

        .toc-container li {
            margin-bottom: 0.5rem;
        }

        .toc-container a {
            font-size: 0.9rem;
            color: var(--text-light);
            display: block;
            padding: 0.25rem 0;
            border-left: 2px solid transparent;
            padding-left: 0.75rem;
            transition: all 0.2s;
        }

        .toc-container a:hover,
        .toc-container a.active {
            color: var(--primary-color);
            border-left-color: var(--primary-color);
            text-decoration: none;
        }

        .toc-container ul ul {
            margin-left: 1rem;
        }

        /* Main Content */
        .main-content {
            flex: 1;
            min-width: 0;
            max-width: 800px;
        }

        /* Header */
        .post-header {
            margin-bottom: 2rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
            margin-bottom: 1.5rem;
        }

        .back-link:hover {
            color: var(--primary-color);
            text-decoration: none;
        }

        .post-header h1 {
            font-size: 2.25rem;
            font-weight: 700;
            line-height: 1.3;
            margin-bottom: 1rem;
            color: var(--text-color);
        }

        .post-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
        }

        .post-meta span {
            display: flex;
            align-items: center;
            gap: 0.4rem;
        }

        .post-meta i {
            color: var(--text-muted);
        }

        .tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 1rem;
        }

        .tag {
            display: inline-block;
            font-size: 0.8rem;
            background: var(--bg-secondary);
            color: var(--text-light);
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            border: 1px solid var(--border-color);
            transition: all 0.2s;
        }

        .tag:hover {
            background: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }

        /* Article Content */
        .post-content {
            font-size: 1rem;
            line-height: 1.9;
        }

        .post-content h1,
        .post-content h2,
        .post-content h3,
        .post-content h4,
        .post-content h5,
        .post-content h6 {
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-weight: 600;
            line-height: 1.4;
            color: var(--text-color);
        }

        .post-content h1 { font-size: 1.875rem; }
        .post-content h2 { 
            font-size: 1.5rem; 
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }
        .post-content h3 { font-size: 1.25rem; }
        .post-content h4 { font-size: 1.125rem; }

        .post-content p {
            margin-bottom: 1.25rem;
        }

        .post-content ul,
        .post-content ol {
            margin: 1.25rem 0;
            padding-left: 1.5rem;
        }

        .post-content li {
            margin-bottom: 0.5rem;
        }

        .post-content blockquote {
            margin: 1.5rem 0;
            padding: 1rem 1.5rem;
            background: var(--bg-secondary);
            border-left: 4px solid var(--primary-color);
            border-radius: 0 8px 8px 0;
            color: var(--text-light);
            font-style: italic;
        }

        .post-content blockquote p:last-child {
            margin-bottom: 0;
        }

        /* Code */
        .post-content code {
            font-family: 'JetBrains Mono', 'Fira Code', Consolas, monospace;
            font-size: 0.9em;
            background: var(--bg-code);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            color: var(--link-color);
        }

        .post-content pre {
            margin: 1.5rem 0;
            padding: 1.25rem;
            background: var(--bg-code);
            border-radius: 10px;
            overflow-x: auto;
            border: 1px solid var(--border-color);
        }

        .post-content pre code {
            background: none;
            padding: 0;
            color: inherit;
            font-size: 0.875rem;
            line-height: 1.6;
        }

        /* Images */
        .post-content img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            margin: 1.5rem 0;
            box-shadow: var(--shadow-lg);
        }

        /* Tables */
        .post-content table {
            width: 100%;
            margin: 1.5rem 0;
            border-collapse: collapse;
            font-size: 0.95rem;
        }

        .post-content th,
        .post-content td {
            padding: 0.75rem 1rem;
            border: 1px solid var(--border-color);
            text-align: left;
        }

        .post-content th {
            background: var(--bg-secondary);
            font-weight: 600;
        }

        .post-content tr:nth-child(even) {
            background: var(--bg-secondary);
        }

        /* Math */
        .math-display {
            margin: 1.5rem 0;
            overflow-x: auto;
            padding: 1rem;
            background: var(--bg-secondary);
            border-radius: 8px;
        }

        /* Anchor links */
        .anchor-link {
            opacity: 0;
            margin-left: 0.5rem;
            color: var(--text-muted);
            font-weight: 400;
            transition: opacity 0.2s;
        }

        .post-content h2:hover .anchor-link,
        .post-content h3:hover .anchor-link,
        .post-content h4:hover .anchor-link {
            opacity: 1;
        }

        /* Theme Toggle */
        .theme-toggle {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            background: var(--primary-color);
            color: white;
            border: none;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.25rem;
            box-shadow: var(--shadow-lg);
            transition: transform 0.2s, background 0.2s;
            z-index: 1000;
        }

        .theme-toggle:hover {
            transform: scale(1.1);
            background: var(--primary-hover);
        }

        /* Comments Section */
        .comments-section {
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-color);
        }

        .comments-section h3 {
            font-size: 1.25rem;
            margin-bottom: 1.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        /* Footer */
        .post-footer {
            margin-top: 3rem;
            padding-top: 1.5rem;
            border-top: 1px solid var(--border-color);
            text-align: center;
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        /* Responsive */
        @media (max-width: 1024px) {
            .toc-sidebar {
                display: none;
            }
            
            .page-wrapper {
                padding: 1.5rem;
            }
        }

        @media (max-width: 768px) {
            .post-header h1 {
                font-size: 1.75rem;
            }
            
            .post-meta {
                flex-direction: column;
                gap: 0.5rem;
            }
            
            .theme-toggle {
                bottom: 1rem;
                right: 1rem;
                width: 44px;
                height: 44px;
            }
        }

        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-secondary);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--border-color);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--text-muted);
        }
    </style>
</head>
<body>
    <div class="page-wrapper">
        <!-- TOC Sidebar -->
        <aside class="toc-sidebar">
            <div class="toc-container">
                <h3><i class="fas fa-list"></i> 目录</h3>
                <div class="toc">
<ul>
<li><a href="#一adalora在做一件什么事">一、AdaLoRA在做一件什么事</a><ul>
<li><a href="#11-lora是怎么做微调的">1.1 LoRA是怎么做微调的</a></li>
<li><a href="#12-对所有模块都采用同一个秩是否合理">1.2 对所有模块都采用同一个秩，是否合理</a></li>
<li><a href="#13-微调过程中一直保持秩不变是否合理">1.3 微调过程中一直保持秩不变，是否合理</a></li>
<li><a href="#14-adalora的总体改进目标">1.4 AdaLoRA的总体改进目标</a></li>
</ul>
</li>
<li><a href="#二adalora的原理">二、AdaLoRA的原理</a><ul>
<li><a href="#21-svd分解">2.1 SVD分解</a></li>
<li><a href="#22-让模型学习svd分解的近似">2.2 让模型学习SVD分解的近似</a></li>
<li><a href="#23-adalora动态更新秩的过程">2.3 AdaLoRA动态更新秩的过程</a></li>
</ul>
</li>
<li><a href="#三adalora的loss设计">三、AdaLoRA的Loss设计</a></li>
<li><a href="#四adalora中的重要性分数">四、AdaLoRA中的重要性分数</a><ul>
<li><a href="#41-单参数重要性分数">4.1 单参数重要性分数</a></li>
<li><a href="#42-改进单参数重要性分数">4.2 改进：单参数重要性分数</a></li>
<li><a href="#43-三元组重要性分数">4.3 三元组重要性分数</a></li>
</ul>
</li>
<li><a href="#五动态调整矩阵的秩">五、动态调整矩阵的秩</a><ul>
<li><a href="#51-调整函数">5.1 调整函数</a></li>
<li><a href="#52-top_b策略">5.2 top_b策略</a></li>
</ul>
</li>
<li><a href="#六adalora训练流程总结必看">六、AdaLoRA训练流程总结（必看）</a></li>
<li><a href="#七参考">七、参考</a></li>
</ul>
</div>

            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <article>
                <header class="post-header">
                    <a href="../../../../index.html" class="back-link">
                        <i class="fas fa-arrow-left"></i> 返回博客列表
                    </a>
                    <h1>------------------------------------</h1>
                    <div class="post-meta">
                        <span><i class="fas fa-calendar-alt"></i> 2026-01-28</span>
                        <span><i class="fas fa-folder"></i> AINotes/14.PEFT/01.LoRA</span>
                        <span><i class="fas fa-user"></i> Hongwei Zhao</span>
                    </div>
                    <div class="tags">
                        
                    </div>
                </header>

                <div class="post-content">
                    <h2 id="一adalora在做一件什么事">一、AdaLoRA在做一件什么事<a class="anchor-link" href="#一adalora在做一件什么事" title="Permanent link">&para;</a></h2>
<h3 id="11-lora是怎么做微调的">1.1 LoRA是怎么做微调的<a class="anchor-link" href="#11-lora是怎么做微调的" title="Permanent link">&para;</a></h3>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202510271512987.jpeg" /></p>
<p>假设原始参数为 <span class="math-inline">W</span> ，全参数微调后对原始参数的改变量为 <span class="math-inline">\Delta W</span> 。在LoRA中，我们用两个矩阵的乘积 <span class="math-inline">BA</span> 来近似<span class="math-inline">\Delta W</span>做SVD分解后的结果，则模型的输出结果满足： <br />
<div class="math-display"><br />
h = Wx + \Delta W x=Wx + \frac{\alpha}{r}BAx<br />
</div></p>
<p>其中，<span class="math-inline">r</span> 就是我们设定的矩阵的秩<strong>，在微调过程中，所有做LoRA适配器的module，它们的 <span class="math-inline">r</span> 都是一致的，且在训练过程中不会改变</strong>（对秩的含义不理解的，可参见<a href="https://zhuanlan.zhihu.com/p/646831196">这篇文章</a>的4.1。对 <span class="math-inline">\frac{\alpha}{r}</span> 不了解的，可参见<a href="https://zhuanlan.zhihu.com/p/654897296">这篇文章</a>的3.2）。<strong>在LoRA原始论文中，作者最终选择对attention模块的</strong> <span class="math-inline">W_{q}, W_{v}</span> <strong>做低秩适配</strong>。  </p>
<p>好，那么现在，问题来了：  </p>
<ul>
<li><strong>对所有模块都采用同一个秩 <span class="math-inline">r</span>，这是合理的吗？</strong></li>
<li><strong>在微调过程中，一直保持秩 <span class="math-inline">r</span> 不变，这是合理的吗？</strong></li>
</ul>
<p>我们来分别细看这两个问题。</p>
<h3 id="12-对所有模块都采用同一个秩是否合理">1.2 对所有模块都采用同一个秩，是否合理<a class="anchor-link" href="#12-对所有模块都采用同一个秩是否合理" title="Permanent link">&para;</a></h3>
<p>这个问题，早在AdaLoRA之前，LoRA的作者就意识到了，并做了如下实验：  </p>
<p><img alt="" src="https://pic2.zhimg.com/v2-219d50b80d95590fc77782cd3f7d5f6d_1440w.jpg" /></p>
<p>图中亮色表示对应模块的intrinsic rank（本征/内在秩），从中可以发现， <span class="math-inline">W_{q}</span> 的intrinsic rank要大于 <span class="math-inline">W_{v}</span> ，<strong>因此LoRA的作者在这里就挖了个可以让后人来填的坑：不同的模块可以用不同的秩</strong>。（对LoRA实验结果有疑惑的，可以看<a href="https://zhuanlan.zhihu.com/p/646831196">这篇文章</a>第五部分）。  </p>
<p>所以到了AdaLoRA这里，它的作者也做了两个实验：  </p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20251104152548.png" style="zoom: 80%;" /></div>

<p>实验（a）对应着左图，<strong>作者使用LoRA，在模型的每个layer中微调特定的某个模块</strong>，然后去评估模型的效果。可以直观发现，微调不同模块时效果差异显著。  </p>
<p>实验（b）对应着右图，<strong>作者使用LoRA，对模型不同layer的所有模块做微调</strong>。例如1，2，3表示对模型前三层的所有模块做lora微调。可以发现，微调不同layer时模型效果差异显著，微调最后三层的效果最好。  </p>
<p>这些实验都证明了一件事：<strong>在使用LoRA微调时，对模型的不同模块使用相同的秩，显然是不合理的</strong>。</p>
<h3 id="13-微调过程中一直保持秩不变是否合理">1.3 微调过程中一直保持秩不变，是否合理<a class="anchor-link" href="#13-微调过程中一直保持秩不变是否合理" title="Permanent link">&para;</a></h3>
<p>对于这点，作者并没有做实验来说明，但我们可以从1.2的分析中得到一些思路。  </p>
<p>在1.2中，我们通过实验证明“对不同模块采用不同秩”的必要性，那么接下来势必就要解决“每个模块的秩到底要设成多少”的问题。解答这个问题最直接的办法，就是交给模型去学。<strong>那模型学习的过程，肯定是个探索性的过程呀，理想情况下，你给模型一个初始化的秩，然后让它在训练过程中，学会慢慢调整这个秩，直到最优</strong>。所以，在微调step的更新过程中，秩也不会保持不变。</p>
<h3 id="14-adalora的总体改进目标">1.4 AdaLoRA的总体改进目标<a class="anchor-link" href="#14-adalora的总体改进目标" title="Permanent link">&para;</a></h3>
<p>好，到此为止，<strong>AdaLoRA的总体改进目标</strong>就出来了： <br />
<strong>找到一种办法，让模型在微调过程中，去学习每个模块参数对训练结果（以loss衡量）的重要性。然后，根据重要性，动态地调整不同模块的秩。</strong></p>
<p>作者管这样的策略叫<strong>[参数预算]（parameter budget）</strong>，作为一个学财会出身的人，这真是非常形象了。待训练的参数越多，训练代价越大，因此做好参数预算方案，集中训练最重要的那些参数，ROI才会越高。</p>
<h2 id="二adalora的原理">二、AdaLoRA的原理<a class="anchor-link" href="#二adalora的原理" title="Permanent link">&para;</a></h2>
<h3 id="21-svd分解">2.1 SVD分解<a class="anchor-link" href="#21-svd分解" title="Permanent link">&para;</a></h3>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20251026101856.png" style="zoom: 80%;" /></div>

<p>如图，矩阵 <span class="math-inline">M</span> 是我们需要做信息量检查的矩阵。假设在输入数据的特征空间中，<strong>存在一组正交的单位向量</strong> <span class="math-inline">\vec{v_1}, \vec{v_2}</span> ，经过 <span class="math-inline">M</span> 的变换后，它们变成另一组正交向量 <span class="math-inline">\sigma_1 \vec{u_1}, \sigma_2 \vec{u_2}</span> ，其中 <span class="math-inline">\vec{u_1}, \vec{u_2}</span> <strong>也是一组正交的单位向量</strong>， <span class="math-inline">\sigma_1, \sigma_2</span> 分别表示对应方向上的模。上面这一顿变幻，可以写成：<br />
<div class="math-display"><br />
M[\vec{v_1}, \vec{v_2}] = [\sigma_1 \vec{u_1}, \sigma_2 \vec{u_2}]<br />
</div><br />
稍加改写，就有：<br />
<div class="math-display"><br />
M = \begin{bmatrix}\vec{u_1}&amp;\vec{u_2}\end{bmatrix}\begin{bmatrix}\sigma_1&amp;0 \0&amp;\sigma_2\end{bmatrix}\begin{bmatrix}\vec{v_1}\\vec{v_2}\end{bmatrix}<br />
</div></p>
<p>不难发现， <span class="math-inline">\sigma_{1}, \sigma_{2}</span> <strong>中隐含了对“信息量”的提示</strong>。在本例中 <span class="math-inline">v</span> 经过 <span class="math-inline">M</span> 的转换投射到 <span class="math-inline">u</span> 上时， <span class="math-inline">M</span> 强调了在1方向上蕴含的信息。  </p>
<p>现在再宽泛一些，如果我们能找到这样的一组 <span class="math-inline">v</span> 和 <span class="math-inline">u</span> ，并令 <span class="math-inline">\sigma</span> 矩阵的值从大到小进行排列，那么我们不就能对 <span class="math-inline">M</span> 进行拆解，同时在拆解过程中，找出 <span class="math-inline">M</span> 所强调的那些特征方向了吗？也就是说： <br />
<div class="math-display"><br />
M = U\Sigma V^{T}<br />
</div><br />
<strong>当我们找到这样的</strong> <span class="math-inline">U, \Sigma, V</span> <strong>矩阵后，我们再从这三者中取出对应的<code>top r</code> 行（或列），不就相当于关注到了</strong> <span class="math-inline">M</span> <strong>最强调的那几维特征，进而就能用更低维的矩阵，来近似表达</strong> <span class="math-inline">M</span> <strong>了？</strong>按这种思维拆解 <span class="math-inline">M</span> 的方法，我们称为<strong>SVD分解（奇异值分解）</strong>。</p>
<p>我们再通过一个代码例子，更直观地感受一下这种近似，大家注意看下注释（例子改编自：<a href="https://medium.com/@Shrishml/lora-low-rank-adaptation-from-the-first-principle-7e1adec71541">LoRA: Low-Rank Adaptation from the first principle</a></p>
<pre><code class="language-python">import torch
import numpy as np
torch.manual_seed(0)

# ------------------------------------
# n：输入数据维度
# m：输出数据维度
# ------------------------------------
n = 10
m = 10

# ------------------------------------
# 随机初始化权重W
# 之所以这样初始化，是为了让W不要满秩，
# 这样才有低秩分解的意义
# ------------------------------------
nr = 10
mr = 2
W = torch.randn(nr,mr)@torch.randn(mr,nr)

# ------------------------------------
# 随机初始化输入数据x
# ------------------------------------
x = torch.randn(n)

# ------------------------------------
# 计算Wx
# ------------------------------------
y = W@x
print(&quot;原始权重W计算出的y值为:\n&quot;, y)

# ------------------------------------
# 计算W的秩
# ------------------------------------
r= np.linalg.matrix_rank(W)
print(&quot;W的秩为: &quot;, r)

# ------------------------------------
# 对W做SVD分解
# ------------------------------------
U, S, V = torch.svd(W)

# ------------------------------------
# 根据SVD分解结果，
# 计算低秩矩阵A和B
# ------------------------------------
U_r = U[:, :r]
S_r = torch.diag(S[:r])
V_r = V[:,:r].t()

B = U_r@S_r # shape = (d, r)
A = V_r     # shape = (r, d)

# ------------------------------------
# 计算y_prime = BAx
# ------------------------------------
y_prime = B@A@x

print(&quot;SVD分解W后计算出的y值为:\n&quot;, y_prime)

print(&quot;原始权重W的参数量为: &quot;, W.shape[0]*W.shape[1])
print(&quot;低秩适配后权重B和A的参数量为: &quot;, A.shape[0]*A.shape[1] + B.shape[0]*B.shape[1])
</code></pre>
<p>输出结果为：</p>
<pre><code class="language-python">原始权重W计算出的y值为:
 tensor([ 3.3896,  1.0296,  1.5606, -2.3891, -0.4213, -2.4668, -4.4379, -0.0375,
        -3.2790, -2.9361])
W的秩为:  2
SVD分解W后计算出的y值为:
 tensor([ 3.3896,  1.0296,  1.5606, -2.3891, -0.4213, -2.4668, -4.4379, -0.0375,
        -3.2790, -2.9361])
原始权重W的参数量为:  100
低秩适配后权重B和A的参数量为:  40
</code></pre>
<p><strong>参数量变少了，但并不影响最终输出的结果</strong>。</p>
<h3 id="22-让模型学习svd分解的近似">2.2 让模型学习SVD分解的近似<a class="anchor-link" href="#22-让模型学习svd分解的近似" title="Permanent link">&para;</a></h3>
<p>从2.1中，我们知道SVD分解后，会将原矩阵 <span class="math-inline">M</span> 拆成 <span class="math-inline">M = U\Sigma V^{T}</span> 这样的形式，<strong>其中 <span class="math-inline">M</span> 和 <span class="math-inline">V</span> 都是正交矩阵</strong>，即满足 <span class="math-inline">M^{T}M= I, V^{T}V = I</span> 。以 <span class="math-inline">M</span> 为二维矩阵为例，这个SVD分解的式子长成下面这样：<br />
<div class="math-display"><br />
M = \begin{bmatrix}\vec{u_1}&amp;\vec{u_2}\end{bmatrix}\begin{bmatrix}\sigma_1&amp;0 \0&amp;\sigma_2\end{bmatrix}\begin{bmatrix}\vec{v_1}\\vec{v_2}\end{bmatrix}<br />
</div></p>
<p><strong>LoRA的总体设计思想</strong>，是 <span class="math-inline">\Delta W = U \Sigma V^{T} \approx BA</span> ，也就是让模型去学习两个矩阵A和B，用来近似SVD分解的结果，同时将A和B的秩都统一设成 <span class="math-inline">r</span>。  </p>
<p><strong>到了AdaLoRA这里，就有新方法了，我让模型去学习三个权重矩阵：</strong> <span class="math-inline">P, \Lambda, Q</span> ，直接去近似 <span class="math-inline">\Delta W</span> 真实的SVD分解结果，也就是 <span class="math-inline">\Delta W = U \Sigma V^{T} \approx P\Lambda Q</span> ，这样也能达到目的。</p>
<h3 id="23-adalora动态更新秩的过程">2.3 AdaLoRA动态更新秩的过程<a class="anchor-link" href="#23-adalora动态更新秩的过程" title="Permanent link">&para;</a></h3>
<p>接下来，我们来仔细端详一下 <span class="math-inline">\Delta W = P \Lambda Q</span> 。下图中描绘了AdaLoRA动态变秩的过程：  </p>
<p><img alt="" src="https://pica.zhimg.com/v2-d1c8c18c6e14fc320c9da6bd476e898a_1440w.jpg" /></p>
<p><strong>AdaLoRA变秩的整体流程如下：</strong>  </p>
<p>（1）<strong>首先，我们初始化三个矩阵</strong> <span class="math-inline">P, \Lambda, Q</span> 。其中， <span class="math-inline">\Lambda</span> 矩阵比较特殊，其大部分元素为0，只有对角线上的 <span class="math-inline">r</span> 个元素有值。所以实操中我们可将其视为长度为 <span class="math-inline">r</span> 的向量，即 <span class="math-inline">\Lambda \in \mathbb{R^{r}}</span> 。初始化时，<strong>我们将</strong> <span class="math-inline">\Lambda</span> <strong>初始化为0，</strong> <span class="math-inline">P, Q</span> <strong>初始化为高斯随机矩阵</strong>。这样做的目的和LoRA一样，都是为了在训练开始保证 <span class="math-inline">\Delta W</span> 是0，以此避免引入噪声。  </p>
<p>（2）然后，<strong>我们正常做forward和backward，得到Loss和参数的梯度</strong>。（Loss的设计我们在后文细说）  </p>
<p>（3）接着，<strong>根据Loss和参数梯度</strong>，我们可以对图中所示的每个三元组(triplets) <span class="math-inline">\mathcal{G_{i}}{P_{<em>i}, \sigma_{i}, Q_{i</em>}}</span> <strong>计算重要性分数</strong>(importance scoring)，其中， <span class="math-inline">P_{<em>i}, Q_{i</em>}</span> 分别表示“第i列”和“第i行”。（重要性分数的计算方法我们在后文细说）  </p>
<p>（4）接着，<strong>根据计算出来的重要性分数，我们将不重要的三元组挑选出来</strong>。（根据重要性分数筛选三元组的方法，我们在后文细说）。  </p>
<p>（5）接着，<strong>对于不重要的三元组，我们将其</strong> <span class="math-inline">\sigma</span> <strong>值置0</strong>。这样，在下一次做forward时，这个三元组里对应的 <span class="math-inline">P</span> 向量和 <span class="math-inline">Q</span> 向量<strong>相当于</strong>被mask掉了，对Loss没有贡献。也就起到了<strong>变秩</strong>的效果。  </p>
<p>（6）接着，使用（2）中计算出来的梯度，更新 <span class="math-inline">P</span> 和 <span class="math-inline">Q</span> 的参数。  </p>
<p>（7）然后，使用更新完毕的 <span class="math-inline">P, \Lambda, Q</span> ，开启新一轮forward和backward，重复上面步骤，随时动态更新参数的秩。  </p>
<p><strong>需要特别说明的是，为什么在（5）中，我们只是将</strong> <span class="math-inline">\sigma</span> <strong>置0，而不是把整个三元组删掉呢？</strong>因为前面说过，模型的学习是一个探索的过程，<strong>在一开始模型认为不重要的三元组，在后续过程中模型可能会慢慢学到它的重要性。因此，mask是比删掉更合理的方法</strong>。也正是这个原因，我们在（6）中，不管三元组有没有被mask掉，我们都会正常用梯度更新 <span class="math-inline">P</span> 和 <span class="math-inline">Q</span> 。  </p>
<p>好，理清了整体流程后，我们就可以来看细节了，在上述过程里，我们遗留了3个细节问题有待探讨：  </p>
<ul>
<li><strong>AdaLoRA中，Loss要怎么设计？</strong></li>
<li><strong>AdaLoRA中，重要性分数要怎么算？</strong></li>
<li><strong>AdaLoRA中，如何根据重要性分数筛选不重要的三元组，进而动态调整矩阵的秩？</strong></li>
</ul>
<p>我们来分别细看这三个问题。</p>
<h2 id="三adalora的loss设计">三、AdaLoRA的Loss设计<a class="anchor-link" href="#三adalora的loss设计" title="Permanent link">&para;</a></h2>
<p>在第二部分中，我们以某一模块的更新做了举例，但是在实际应用中，需要做AdaLoRA适配的模块肯定不止一个( <span class="math-inline">W_{q}, W_{k}, W_{v}, W_{o}, W_{f_{1}}...</span> )，<strong>假设我们有n个模块需要做AdaLoRA适配</strong>，<strong>现在我们来严谨定义一些数学表达符号</strong>：  </p>
<ul>
<li>
<p><span class="math-inline">\Delta_{k} = P_{k}\Lambda_{k}Q_{k}</span> ，表示第k个需要做AdaLoRA适配的模块。其中 <span class="math-inline">k = 1, ..., n</span> ，n表示共有n个模块需要做AdaLoRA适配</p>
</li>
<li>
<p><span class="math-inline">\mathcal{G_{k, i}} = {P_{k, <em>i}, \lambda_{k, i}, Q_{k, i</em>}}</span> ，表示第k个模块的第i个<strong>三元组</strong>。注意上文中的 <span class="math-inline">\sigma</span> 用 <span class="math-inline">\lambda</span> 表示。（怪我，画完图才发现这个gap，但是不想改图了，大家能理解就行）</p>
</li>
<li>
<p><span class="math-inline">S_{k, i}</span> ：表示第k个模块的第i个三元组的<strong>重要性分数</strong>。</p>
</li>
<li>
<p><span class="math-inline">\mathcal{P} = {{P_{k}}^{n}_{k=1}}</span> ：表示所有n个模块的P矩阵的集合</p>
</li>
<li>
<p><span class="math-inline">\mathcal{E} ={{\lambda_{k}}^{n}_{k=1}}</span> ：表示所有n个模块的 <span class="math-inline">\Lambda</span> 矩阵的集合（实操中不是矩阵，是r维向量，前文说过）</p>
</li>
<li>
<p><span class="math-inline">\mathcal{Q} = {{Q_{k}}^{n}_{k=1}}</span> ：表示所有n个模块的Q矩阵的集合</p>
</li>
<li>
<p><span class="math-inline">\mathcal{C}(\mathcal{P}, \mathcal{E}, \mathcal{Q})</span> ：表示模型<strong>在训练集上的损失函数</strong></p>
</li>
<li>
<p><span class="math-inline">\mathcal{L}(\mathcal{P}, \mathcal{E}, \mathcal{Q})</span> ：表示<strong>模型最终的损失函数</strong></p>
</li>
</ul>
<p>细心的你可能已经发现了，这怎么有两个损失函数呢？那我们就直接来看<strong>最终损失函数的定义</strong>： <br />
<div class="math-display"><br />
\mathcal{L}(\mathcal{P}, \mathcal{E}, \mathcal{Q}) = \mathcal{C}(\mathcal{P}, \mathcal{E}, \mathcal{Q}) + \gamma \sum_{k=1}^{n} R(P_{k}, Q_{k}), \gamma &gt; 0<br />
</div><br />
其中， <span class="math-inline">R(P, Q) = || P^{T}P- I||<em>{F}^{2} + || Q^{T}Q - I||</em>{F}^{2}</span>  </p>
<p><span class="math-inline">\mathcal{C}(\mathcal{P}, \mathcal{E}, \mathcal{Q})</span> 好理解，<strong>它表示预测结果和真实标签间的差异</strong>。那后面那项又是什么呢？还记得我们在2.1中提过<strong>SVD分解相关的定义吗：<span class="math-inline">P</span> 和 <span class="math-inline">Q</span> 都必须是正交矩阵</strong>，即满足 <span class="math-inline">P^{T}P = I, Q^{T}Q = I</span> 。  </p>
<p>而在AdaLoRA中，我们是寄希望于模型学出的 <span class="math-inline">P,Q</span> 能满足这个性质，所以在设计Loss时我们当然也要考虑它：<strong>当 <span class="math-inline">P</span> 和 <span class="math-inline">Q</span> 偏离这个性质太远时，我们在Loss中给予相应的惩罚，</strong> <span class="math-inline">\gamma</span> <strong>就是惩罚力度，也被称为正则项</strong>。  </p>
<p><strong>其实，这也是AdaLoRA相比于LoRA效果能更好的原因之一</strong>：LoRA中是让模型学习BA，去近似SVD分解的结果，但是在训练过程中，没有引入任何SVD分解相关的性质做约束，所以模型就可能学歪了（因此LoRA作者在文章中写了很多实验，证明学出来的BA在一定程度上能近似SVD分解，能取得较好的效果）。而AdaLoRA则是直接将这一束缚考虑到了Loss中。</p>
<h2 id="四adalora中的重要性分数">四、AdaLoRA中的重要性分数<a class="anchor-link" href="#四adalora中的重要性分数" title="Permanent link">&para;</a></h2>
<p>我们依然先明确几个数学表达：  </p>
<ul>
<li><span class="math-inline">S_{k, i}</span> ：表示第k个模块的第i个三元组的<strong>重要性分数</strong>。</li>
<li><span class="math-inline">\lambda_{k, i}</span> ：表示第k个模块的 <span class="math-inline">\Lambda</span> 矩阵（再强调一下，实操中不是矩阵是向量）的第i个元素。</li>
<li><span class="math-inline">P_{k, ji}</span> ：表示第k个模块的P矩阵的第j行第i列个元素。</li>
<li><span class="math-inline">Q_{k, ij}</span> ：表示第k个模块的Q矩阵的第i行第j列个元素。</li>
<li><span class="math-inline">s(.)</span> ：表示计算单元素重要性的函数，我们在下文会细说。</li>
<li><span class="math-inline">\mathcal {L}</span> ：表示模型的损失函数。</li>
</ul>
<h3 id="41-单参数重要性分数">4.1 单参数重要性分数<a class="anchor-link" href="#41-单参数重要性分数" title="Permanent link">&para;</a></h3>
<p><strong>在开始正式讲重要性分数怎么算之前，我们先来看一个问题：到底什么是重要性分数？</strong>  </p>
<p>我们在1.4中说过，AdaLoRA的整体目标是要做<strong>参数预算（budget）</strong>，也就是忽略不重要的参数，然后把训练资源给重要的参数，在AdaLoRA中，我们是通过“<strong>变秩</strong>”来实现这种预算的动态分配的<strong>。所以现在问题就变成：如何判断一个矩阵中的一个参数（我们将其表示为</strong> <span class="math-inline">w_{ij}</span> <strong>）对模型训练是否重要？</strong>  </p>
<p><strong>一个最直观的想法就是：去看看这个参数对Loss的影响</strong>。所以，在前人的研究中，就提出过<strong>“梯度\参数”（gradient-weight product）</strong>这种算法： <br />
<div class="math-display"><br />
I(w_{ij}) = |w_{ij}\bigtriangledown_{w_{ij}}\mathcal{L}|<br />
</div><br />
这种算法的设计思想非常直接：我去看看Loss在这个参数上的梯度，同时也考虑一下这个参数本身的大小，不就能综合判断出这个参数对模型的影响程度了吗？</p>
<h3 id="42-改进单参数重要性分数">4.2 改进：单参数重要性分数<a class="anchor-link" href="#42-改进单参数重要性分数" title="Permanent link">&para;</a></h3>
<p>4.1中的这个直观有效的想法，被广泛运用在前人做的参数剪枝的优化中，<strong>但它有一个显著的问题：我是在mini-batch上计算重要性分数的，不同step间重要性分数可能会受到mini-batch客观波动的影响</strong>，有啥办法能消除这种影响吗？  </p>
<p>当然有啦，遇到这种消除波动的问题，我们肯定要祭出momentum 。  </p>
<p>所以，<strong>AdaLoRA作者就提出了这样一种计算 <span class="math-inline">t</span> 时刻，单个模型参数重要性的方法</strong>：  </p>
<p><img alt="" src="https://pic4.zhimg.com/v2-57b9f17c14df237090188659b85bcc13_1440w.jpg" /></p>
<p>其中：  </p>
<p><img alt="" src="https://picx.zhimg.com/v2-53154675b0d77d5886b3f454df6c116f_1440w.jpg" /></p>
<ul>
<li>
<p><span class="math-inline"> I^{(t)}(w_{ij})</span> 表示根据4.1中的公式，计算出的t时刻的单参数重要性</p>
</li>
<li>
<p><span class="math-inline"> \bar I^{(t)}(w_{ij}) </span> 表示前t-1个时刻该单参数重要性的平滑值</p>
</li>
<li>
<p><span class="math-inline">|I^{(t)}(w_{ij}) -\bar I^{(t)}(w_{ij}) |</span> 表示当前值与平滑值之间的差异。这一项的意义是，你也不能一股脑地去平滑，也要考虑到重要性分数的真实波动情况</p>
</li>
</ul>
<p><strong>好，到这一步，我们就把计算单参数重要性分数的函数</strong> <span class="math-inline">s^{(t)}(w_{ij})</span> <strong>讲清楚了！</strong>知道了单参数的重要性分数，自然就可以知道整个三元组的重要性分数啦！</p>
<h3 id="43-三元组重要性分数">4.3 三元组重要性分数<a class="anchor-link" href="#43-三元组重要性分数" title="Permanent link">&para;</a></h3>
<p>在AdaLoRA中，三元组重要性分数计算方式如下：  </p>
<p><img alt="" src="https://pic3.zhimg.com/v2-e5d0a345f6c6294ea0f79630b600f5de_1440w.jpg" /></p>
<p>其中，小写的s就是我们在4.2中定义的单参数计算函数。 <br />
现在看这个公式，是不是很好理解呢：<strong>三元组的重要性分数 =</strong> <span class="math-inline">\lambda</span> <strong>的重要性分数 + P矩阵中所有元素重要性分数的均值 + Q矩阵中所有元素重要性分数的均值。</strong>取均值的原因，是不希望参数量影响到重要性分数。  </p>
<p>太好啦，到这一步为止，我们已经把AdaLoRA中难啃的Loss和三元组重要性分数讲完了，是不是比想象得简单很多？<strong>接下来我们来啃最后一块硬骨头：知道了三元组的重要性分数后，怎么动态调整矩阵的秩？</strong></p>
<h2 id="五动态调整矩阵的秩">五、动态调整矩阵的秩<a class="anchor-link" href="#五动态调整矩阵的秩" title="Permanent link">&para;</a></h2>
<p>老规矩，在开始讲解前，我们再来明确几个数学符号：  </p>
<ul>
<li><span class="math-inline">\bigtriangledown_{P_{k}}\mathcal{L}(\mathcal{P}^{(t)}, \mathcal{E}^{(t)}, \mathcal{Q}^{(t)})</span> ：表示第 <span class="math-inline">k</span> 个模块的P矩阵在 <span class="math-inline">t</span> 时刻的梯度</li>
<li><span class="math-inline"> \bigtriangledown_{Q_{k}}\mathcal{L}(\mathcal{P}^{(t)}, \mathcal{E}^{(t)}, \mathcal{Q}^{(t)})</span> ：表示第 <span class="math-inline">k</span> 个模块的Q矩阵在 <span class="math-inline">t</span> 时刻的梯度</li>
<li><span class="math-inline">\bigtriangledown_{\Lambda_{k}}\mathcal{L}(\mathcal{P}^{(t)}, \mathcal{E}^{(t)}, \mathcal{Q}^{(t)})</span> ：表示第 <span class="math-inline">k</span> 个模块的 <span class="math-inline">\Lambda</span> 矩阵在 <span class="math-inline">t</span> 时刻的梯度</li>
</ul>
<h3 id="51-调整函数">5.1 调整函数<a class="anchor-link" href="#51-调整函数" title="Permanent link">&para;</a></h3>
<p>前文说过，<strong>动态调整矩阵秩的核心，就是根据三元组重要性分数，对</strong> <span class="math-inline">\Lambda</span> <strong>矩阵中相应的</strong> <span class="math-inline">\lambda</span> <strong>做置0处理。所以，我们就来看看</strong> <span class="math-inline">\lambda</span> <strong>的置0策略。</strong>  </p>
<p>（1）首先，我们拿 <span class="math-inline"> \bigtriangledown_{\Lambda_{k}}\mathcal{L}(\mathcal{P}^{(t)}, \mathcal{E}^{(t)}, \mathcal{Q}^{(t)})</span> ，先更新一波 <span class="math-inline">\Lambda_{k}</span> ，即我们有： <br />
<div class="math-display"><br />
\widetilde{\Lambda}<em>{k}^{t} = \Lambda</em>{k}^{t} - \eta \bigtriangledown_{\Lambda_{k}}\mathcal{L}(\mathcal{P}^{(t)}, \mathcal{E}^{(t)}, \mathcal{Q}^{(t)})<br />
</div><br />
其中， <span class="math-inline">\eta</span> 是我们的学习率（learning_rate） <br />
<strong>注意，这里</strong> <span class="math-inline">\widetilde{\Lambda}_{k}^{t}</span> <strong>头上还顶着t时刻的标志，而不是t+1</strong>，也就是说，我<strong>们对</strong> <span class="math-inline">\Lambda</span> <strong>做完梯度更新后的结果，并不是t+1时刻的结果。我们做完置0后，才是t+1时刻的结果。</strong>  </p>
<p>（2）接着，我们按以下方式判断 <span class="math-inline">\Lambda</span> 矩阵中哪些元素应该置0，哪些元素应该保持为梯度更新后的结果：  </p>
<p><img alt="" src="https://pic4.zhimg.com/v2-bbf33df9863bad423427f55dfc6b79c9_1440w.jpg" /></p>
<p>这个看起来很复杂的公式，表达的意思很简单：  </p>
<ul>
<li>t+1时刻 <span class="math-inline">\Lambda</span> 矩阵，是由 <span class="math-inline">\mathcal{T}</span> 这个函数决定的，这个函数的输入是梯度更新后的 <span class="math-inline">\widetilde{\Lambda}<em>{k}^{t}</span> <strong>，</strong>以及第k个模块所有三元组的重要性分数 <span class="math-inline">S^{(t)}</em>{k}</span> .</li>
<li>那 <span class="math-inline">\mathcal{T}</span> 这个函数具体就长成后面带大括号的那个样子。也就是重要性分数排在top_b的三元组，它们的 <span class="math-inline">\lambda</span> 保持原样，其余的则置0</li>
<li>最后，这里的top_b也涉及一种策略，那就是它的值是随着t的变动而变化的（例如t=1时，我取的是top 2； t=2时，我取的是top b之类）。我们在下一小节细说这个策略</li>
</ul>
<h3 id="52-top_b策略">5.2 top_b策略<a class="anchor-link" href="#52-top_b策略" title="Permanent link">&para;</a></h3>
<p>在开始讲top_b策略前，我们先来思考一个问题：<strong>为什么每次选出的重要三元组的个数，要随着时刻t的变动而变动？</strong>  </p>
<p>这个问题的答案还是：<strong>模型的学习是探索性的过程</strong>。  </p>
<p><strong>在训练刚开始，我们逐渐增加top_b，也就是逐渐加秩，让模型尽可能多探索。到后期再慢慢把top_b降下来，直到最后以稳定的top_b进行训练，达到AdaLoRA的总目的：把训练资源留给最重要的参数。这个过程就和warm-up非常相似。</strong>  </p>
<p>具体的策略在原始论文3.3节中有讲解，不难，所以我就不另外分析啦，大家可以自行阅读。以及本文的实验部分，我也不在这边写了，实验部分一句话总结就是相比LoRA确实有了不错的提升，大家可以自己去看细节。</p>
<h2 id="六adalora训练流程总结必看">六、AdaLoRA训练流程总结（必看）<a class="anchor-link" href="#六adalora训练流程总结必看" title="Permanent link">&para;</a></h2>
<p>最后，我们把AdaLoRA的整体训练流程总结一下：  </p>
<p><img alt="" src="https://pic3.zhimg.com/v2-46c39e1bda7a369b65489f1f6036d634_1440w.jpg" /></p>
<p>（1）拿到训练数据集，确定好总训练步长T。根据总步长T设计好top_b的warm-up策略，并设定好一系列超参，同时也把 <span class="math-inline">P, \Lambda, Q</span> 的初始化做好 <br />
（2）进入某个step的迭代 <br />
（3）给模型喂一份mini-batch，正常做forward和backward，计算loss和梯度 <br />
（4）（5）对某一个三元组，我们先计算其中每个参数的重要性（单参数重要性） <br />
（6）根据单参数重要性，计算出整个三元组的重要性分数 <br />
（7）使用（3）中计算好的梯度，正常更新矩阵P和Q <br />
（8）根据三元组重要性分数、动态调秩策略、top_b来判断要给哪些 <span class="math-inline">\lambda</span> 置0，其对应的三元组中的P和Q向量相当于被mask掉，以此来实现动态调秩的目的。这番操作后，我们得到更新的矩阵 <span class="math-inline">\Lambda</span> 然后将 <span class="math-inline">P, \Lambda, Q</span> 送入下一轮训练。  </p>
<p>以此类推。  </p>
<h2 id="七参考">七、参考<a class="anchor-link" href="#七参考" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/657130029">图解大模型微调系列之：AdaLoRA，能做“财务”预算的低秩适配器</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2303.10512">https://arxiv.org/abs/2303.10512</a>  </li>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/QingruZhang/AdaLoRA">https://github.com/QingruZhang/AdaLoRA</a></li>
</ul>
                </div>

                <!-- Comments Section (Giscus) -->
                <section class="comments-section">
                    <h3><i class="fas fa-comments"></i> 评论</h3>
                    <script src="https://giscus.app/client.js"
                        data-repo="Geeks-Z/Geeks-Z.github.io"
                        data-repo-id=""
                        data-category="Announcements"
                        data-category-id=""
                        data-mapping="pathname"
                        data-strict="0"
                        data-reactions-enabled="1"
                        data-emit-metadata="0"
                        data-input-position="bottom"
                        data-theme="preferred_color_scheme"
                        data-lang="zh-CN"
                        crossorigin="anonymous"
                        async>
                    </script>
                </section>
            </article>

            <footer class="post-footer">
                <p>© 2025 Hongwei Zhao. Built with ❤️</p>
            </footer>
        </main>
    </div>

    <!-- Theme Toggle Button -->
    <button class="theme-toggle" id="themeToggle" aria-label="切换主题">
        <i class="fas fa-moon"></i>
    </button>

    <script>
        // Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        const html = document.documentElement;
        const icon = themeToggle.querySelector('i');
        const hljsDark = document.getElementById('hljs-theme-dark');
        const hljsLight = document.getElementById('hljs-theme-light');
        
        // Check saved theme or system preference
        const savedTheme = localStorage.getItem('theme');
        const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
        
        if (savedTheme === 'dark' || (!savedTheme && prefersDark)) {
            html.setAttribute('data-theme', 'dark');
            icon.className = 'fas fa-sun';
            hljsDark.disabled = false;
            hljsLight.disabled = true;
        }
        
        themeToggle.addEventListener('click', () => {
            const isDark = html.getAttribute('data-theme') === 'dark';
            if (isDark) {
                html.removeAttribute('data-theme');
                icon.className = 'fas fa-moon';
                localStorage.setItem('theme', 'light');
                hljsDark.disabled = true;
                hljsLight.disabled = false;
            } else {
                html.setAttribute('data-theme', 'dark');
                icon.className = 'fas fa-sun';
                localStorage.setItem('theme', 'dark');
                hljsDark.disabled = false;
                hljsLight.disabled = true;
            }
            
            // Update Giscus theme
            const giscusFrame = document.querySelector('iframe.giscus-frame');
            if (giscusFrame) {
                giscusFrame.contentWindow.postMessage({
                    giscus: {
                        setConfig: {
                            theme: isDark ? 'light' : 'dark'
                        }
                    }
                }, 'https://giscus.app');
            }
        });
        
        // Highlight.js
        document.addEventListener('DOMContentLoaded', () => {
            hljs.highlightAll();
        });
        
        // KaTeX auto-render
        document.addEventListener('DOMContentLoaded', () => {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
        
        // TOC active state
        const tocLinks = document.querySelectorAll('.toc-container a');
        const headings = document.querySelectorAll('.post-content h2, .post-content h3, .post-content h4');
        
        function updateTocActive() {
            let current = '';
            headings.forEach(heading => {
                const rect = heading.getBoundingClientRect();
                if (rect.top <= 100) {
                    current = heading.getAttribute('id');
                }
            });
            
            tocLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === '#' + current) {
                    link.classList.add('active');
                }
            });
        }
        
        window.addEventListener('scroll', updateTocActive);
        updateTocActive();
    </script>
</body>
</html>
