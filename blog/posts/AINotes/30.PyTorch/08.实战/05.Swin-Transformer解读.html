<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Swin Transformer 解读</title>
    <meta name="description" content="Swin Transformer 解读 - Hongwei Zhao's Blog">
    <meta name="author" content="Hongwei Zhao">
    
    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <!-- Code Highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css" id="hljs-theme-dark">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css" id="hljs-theme-light" disabled>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    
    <style>
        :root {
            /* Light Theme - 明亮清新配色 */
            --primary-color: #4A90D9;
            --primary-hover: #3678C2;
            --link-color: #E86B5F;
            --text-color: #2D2D2D;
            --text-light: #5A5A5A;
            --text-muted: #8A8A8A;
            --bg-color: #FFFFFF;
            --bg-secondary: #F5F7FA;
            --bg-code: #F8F9FC;
            --border-color: #E8ECF0;
            --shadow: 0 2px 8px rgba(0,0,0,0.06);
            --shadow-lg: 0 8px 24px rgba(0,0,0,0.08);
        }

        [data-theme="dark"] {
            --primary-color: #5dade2;
            --primary-hover: #85c1e9;
            --link-color: #e74c3c;
            --text-color: #e5e7eb;
            --text-light: #9ca3af;
            --text-muted: #6b7280;
            --bg-color: #1a1a2e;
            --bg-secondary: #16213e;
            --bg-code: #0f0f23;
            --border-color: #374151;
            --shadow: 0 1px 3px rgba(0,0,0,0.3);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.4);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 16px;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            transition: background-color 0.3s, color 0.3s;
        }

        a {
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--primary-hover);
            text-decoration: underline;
        }

        /* Layout */
        .page-wrapper {
            display: flex;
            max-width: 1400px;
            margin: 0 auto;
            padding: 2rem;
            gap: 3rem;
        }

        /* Sidebar TOC */
        .toc-sidebar {
            width: 260px;
            flex-shrink: 0;
            position: sticky;
            top: 2rem;
            height: fit-content;
            max-height: calc(100vh - 4rem);
            overflow-y: auto;
        }

        .toc-container {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 1.5rem;
            border: 1px solid var(--border-color);
        }

        .toc-container h3 {
            font-size: 0.85rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-muted);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .toc-container ul {
            list-style: none;
        }

        .toc-container li {
            margin-bottom: 0.5rem;
        }

        .toc-container a {
            font-size: 0.9rem;
            color: var(--text-light);
            display: block;
            padding: 0.25rem 0;
            border-left: 2px solid transparent;
            padding-left: 0.75rem;
            transition: all 0.2s;
        }

        .toc-container a:hover,
        .toc-container a.active {
            color: var(--primary-color);
            border-left-color: var(--primary-color);
            text-decoration: none;
        }

        .toc-container ul ul {
            margin-left: 1rem;
        }

        /* Main Content */
        .main-content {
            flex: 1;
            min-width: 0;
            max-width: 800px;
        }

        /* Header */
        .post-header {
            margin-bottom: 2rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
            margin-bottom: 1.5rem;
        }

        .back-link:hover {
            color: var(--primary-color);
            text-decoration: none;
        }

        .post-header h1 {
            font-size: 2.25rem;
            font-weight: 700;
            line-height: 1.3;
            margin-bottom: 1rem;
            color: var(--text-color);
        }

        .post-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
        }

        .post-meta span {
            display: flex;
            align-items: center;
            gap: 0.4rem;
        }

        .post-meta i {
            color: var(--text-muted);
        }

        .tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 1rem;
        }

        .tag {
            display: inline-block;
            font-size: 0.8rem;
            background: var(--bg-secondary);
            color: var(--text-light);
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            border: 1px solid var(--border-color);
            transition: all 0.2s;
        }

        .tag:hover {
            background: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }

        /* Article Content */
        .post-content {
            font-size: 1rem;
            line-height: 1.9;
        }

        .post-content h1,
        .post-content h2,
        .post-content h3,
        .post-content h4,
        .post-content h5,
        .post-content h6 {
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-weight: 600;
            line-height: 1.4;
            color: var(--text-color);
        }

        .post-content h1 { font-size: 1.875rem; }
        .post-content h2 { 
            font-size: 1.5rem; 
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }
        .post-content h3 { font-size: 1.25rem; }
        .post-content h4 { font-size: 1.125rem; }

        .post-content p {
            margin-bottom: 1.25rem;
        }

        .post-content ul,
        .post-content ol {
            margin: 1.25rem 0;
            padding-left: 1.5rem;
        }

        .post-content li {
            margin-bottom: 0.5rem;
        }

        .post-content blockquote {
            margin: 1.5rem 0;
            padding: 1rem 1.5rem;
            background: var(--bg-secondary);
            border-left: 4px solid var(--primary-color);
            border-radius: 0 8px 8px 0;
            color: var(--text-light);
            font-style: italic;
        }

        .post-content blockquote p:last-child {
            margin-bottom: 0;
        }

        /* Code */
        .post-content code {
            font-family: 'JetBrains Mono', 'Fira Code', Consolas, monospace;
            font-size: 0.9em;
            background: var(--bg-code);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            color: var(--link-color);
        }

        .post-content pre {
            margin: 1.5rem 0;
            padding: 1.25rem;
            background: var(--bg-code);
            border-radius: 10px;
            overflow-x: auto;
            border: 1px solid var(--border-color);
        }

        .post-content pre code {
            background: none;
            padding: 0;
            color: inherit;
            font-size: 0.875rem;
            line-height: 1.6;
        }

        /* Images */
        .post-content img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            margin: 1.5rem 0;
            box-shadow: var(--shadow-lg);
        }

        /* Tables */
        .post-content table {
            width: 100%;
            margin: 1.5rem 0;
            border-collapse: collapse;
            font-size: 0.95rem;
        }

        .post-content th,
        .post-content td {
            padding: 0.75rem 1rem;
            border: 1px solid var(--border-color);
            text-align: left;
        }

        .post-content th {
            background: var(--bg-secondary);
            font-weight: 600;
        }

        .post-content tr:nth-child(even) {
            background: var(--bg-secondary);
        }

        /* Math */
        .math-display {
            margin: 1.5rem 0;
            overflow-x: auto;
            padding: 1rem;
            background: var(--bg-secondary);
            border-radius: 8px;
        }

        /* Anchor links */
        .anchor-link {
            opacity: 0;
            margin-left: 0.5rem;
            color: var(--text-muted);
            font-weight: 400;
            transition: opacity 0.2s;
        }

        .post-content h2:hover .anchor-link,
        .post-content h3:hover .anchor-link,
        .post-content h4:hover .anchor-link {
            opacity: 1;
        }

        /* Theme Toggle */
        .theme-toggle {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            background: var(--primary-color);
            color: white;
            border: none;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.25rem;
            box-shadow: var(--shadow-lg);
            transition: transform 0.2s, background 0.2s;
            z-index: 1000;
        }

        .theme-toggle:hover {
            transform: scale(1.1);
            background: var(--primary-hover);
        }

        /* Comments Section */
        .comments-section {
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-color);
        }

        .comments-section h3 {
            font-size: 1.25rem;
            margin-bottom: 1.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        /* Footer */
        .post-footer {
            margin-top: 3rem;
            padding-top: 1.5rem;
            border-top: 1px solid var(--border-color);
            text-align: center;
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        /* Responsive */
        @media (max-width: 1024px) {
            .toc-sidebar {
                display: none;
            }
            
            .page-wrapper {
                padding: 1.5rem;
            }
        }

        @media (max-width: 768px) {
            .post-header h1 {
                font-size: 1.75rem;
            }
            
            .post-meta {
                flex-direction: column;
                gap: 0.5rem;
            }
            
            .theme-toggle {
                bottom: 1rem;
                right: 1rem;
                width: 44px;
                height: 44px;
            }
        }

        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-secondary);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--border-color);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--text-muted);
        }
    </style>
</head>
<body>
    <div class="page-wrapper">
        <!-- TOC Sidebar -->
        <aside class="toc-sidebar">
            <div class="toc-container">
                <h3><i class="fas fa-list"></i> 目录</h3>
                <div class="toc">
<ul>
<li><a href="#前言">前言</a></li>
<li><a href="#模型结构">模型结构</a></li>
<li><a href="#patch-embedding">Patch Embedding</a></li>
<li><a href="#patch-merging">Patch Merging</a></li>
<li><a href="#window-partitionreverse">Window Partition/Reverse</a></li>
<li><a href="#window-attention">Window Attention</a><ul>
<li><a href="#相关位置编码的直观理解">相关位置编码的直观理解</a></li>
<li><a href="#相关位置编码的代码详解">相关位置编码的代码详解</a></li>
</ul>
</li>
<li><a href="#shifted-window-attention">Shifted Window Attention</a><ul>
<li><a href="#特征图移位操作">特征图移位操作</a></li>
<li><a href="#attention-mask">Attention Mask</a></li>
</ul>
</li>
<li><a href="#w-msa-和-msa-的复杂度对比">W-MSA 和 MSA 的复杂度对比</a><ul>
<li><a href="#msa-模块的计算量">MSA 模块的计算量</a></li>
<li><a href="#w-msa-模块的计算量">W-MSA 模块的计算量</a></li>
</ul>
</li>
<li><a href="#整体流程图">整体流程图</a></li>
</ul>
</div>

            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <article>
                <header class="post-header">
                    <a href="../../../../index.html" class="back-link">
                        <i class="fas fa-arrow-left"></i> 返回博客列表
                    </a>
                    <h1>Swin Transformer 解读</h1>
                    <div class="post-meta">
                        <span><i class="fas fa-calendar-alt"></i> 2026-02-04</span>
                        <span><i class="fas fa-folder"></i> AINotes/30.PyTorch/08.实战</span>
                        <span><i class="fas fa-user"></i> Hongwei Zhao</span>
                    </div>
                    <div class="tags">
                        
                    </div>
                </header>

                <div class="post-content">
                    <h1 id="swin-transformer-解读">Swin Transformer 解读<a class="anchor-link" href="#swin-transformer-解读" title="Permanent link">&para;</a></h1>
<p>
<font size=3><b>[Swin-T] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</b></font>
<br>
<font size=2>Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.</font>
<br>
<font size=2>ICCV 2021.</font>
<a href='https://arxiv.org/pdf/2103.14030.pdf'>[paper]</a> <a href='https://github.com/microsoft/Swin-Transformer'>[code]</a> 
<br>
<font size=3>解读者：沈豪，复旦大学博士，Datawhale成员</font>
<br>
</p>

<h2 id="前言">前言<a class="anchor-link" href="#前言" title="Permanent link">&para;</a></h2>
<p>《<a href="https://arxiv.org/abs/2103.14030">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a>》作为 2021 ICCV 最佳论文，屠榜了各大 CV 任务，性能优于 DeiT、ViT 和 EfficientNet 等主干网络，已经替代经典的 CNN 架构，成为了<strong>计算机视觉领域通用的 backbone</strong>。它基于了 ViT 模型的思想，创新性的引入了<strong>滑动窗口机制</strong>，让模型能够学习到跨窗口的信息，同时也。同时通过<strong>下采样层</strong>，使得模型能够处理超分辨率的图片，节省计算量以及能够关注全局和局部的信息。而本文将从原理和代码角度详细解析 Swin Transformer 的架构。</p>
<p>目前将 Transformer 从自然语言处理领域应用到计算机视觉领域主要有两大挑战：</p>
<ul>
<li>视觉实体的方差较大，例如同一个物体，拍摄角度不同，转化为二进制后的图片就会具有很大的差异。同时在不同场景下视觉 Transformer 性能未必很好。</li>
<li>图像分辨率高，像素点多，如果采用 ViT 模型，自注意力的计算量会与像素的平方成正比。</li>
</ul>
<p>针对上述两个问题，论文中提出了一种基于<strong>滑动窗口机制，具有层级设计（下采样层）</strong> 的 Swin Transformer。</p>
<p>其中<strong>滑窗操作</strong>包括<strong>不重叠的 local window，和重叠的 cross-window</strong>。将注意力计算限制在一个窗口（window size 固定）中，<strong>一方面能引入 CNN 卷积操作的局部性，另一方面能大幅度节省计算量</strong>，它只和窗口数量成线性关系。通过<strong>下采样</strong>的层级设计，能够逐渐增大感受野，从而使得注意力机制也能够注意到<strong>全局</strong>的特征。</p>
<p><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202403231022979.png" alt="Swin-T&ViT" style="zoom:50%;" /></p>
<p>在论文的最后，作者也通过大量的实验证明 Swin Transformer 相较于以前的 SOTA 模型均有提高，尤其是在 ADE20K 数据和 COCO 数据集上的表现。也证明了 Swin Transformer 可以作为一种通用骨干网络被使用。</p>
<h2 id="模型结构">模型结构<a class="anchor-link" href="#模型结构" title="Permanent link">&para;</a></h2>
<p><img alt="Architecture" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202403231022980.png" /></p>
<p>整个模型采取层次化的设计，一共包含 4 个 Stage，除第一个 stage 外，每个 stage 都会先通过 <strong>Patch Merging</strong> 层缩小输入特征图的分辨率，进行<strong>下采样操作</strong>，像 CNN 一样逐层扩大感受野，以便获取到全局的信息。</p>
<p>以论文的角度：</p>
<ul>
<li>在输入开始的时候，做了一个<code>Patch Partition</code>，即 ViT 中<code>Patch Embedding</code>操作，通过 <strong>Patch_size</strong> 为 4 的卷积层将图片切成一个个 <strong>Patch</strong> ，并嵌入到<code>Embedding</code>，将 <strong>embedding_size</strong> 转变为 48（可以将 CV 中图片的<strong>通道数</strong>理解为 NLP 中 token 的<strong>词嵌入长度</strong>）。</li>
<li>随后在第一个 Stage 中，通过<code>Linear Embedding</code>调整通道数为 C。</li>
<li>在每个 Stage 里（除第一个 Stage ），均由<code>Patch Merging</code>和多个<code>Swin Transformer Block</code>组成。</li>
<li>其中<code>Patch Merging</code>模块主要在每个 Stage 一开始降低图片分辨率，进行下采样的操作。</li>
<li>而<code>Swin Transformer Block</code>具体结构如右图所示，主要是<code>LayerNorm</code>，<code>Window Attention</code> ，<code>Shifted Window Attention</code>和<code>MLP</code>组成 。</li>
</ul>
<p>从代码的角度：</p>
<p>在微软亚洲研究院提供的代码中，是将<code>Patch Merging</code>作为每个 Stage 最后结束的操作，输入先进行<code>Swin Transformer Block</code>操作，再下采样。而<strong>最后一个 Stage 不需要进行下采样操作</strong>，之间通过后续的全连接层与 <strong>target label</strong> 计算损失。</p>
<p><img alt="code_Architecture" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202403231022981.png" /></p>
<pre><code class="language-python"># window_size=7
# input_batch_image.shape=[128,3,224,224]
class SwinTransformer(nn.Module):
    def __init__(...):
        super().__init__()
        ...
        # absolute position embedding
        if self.ape:
            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))

        self.pos_drop = nn.Dropout(p=drop_rate)

        # build layers
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            layer = BasicLayer(...)
            self.layers.append(layer)

        self.norm = norm_layer(self.num_features)
        self.avgpool = nn.AdaptiveAvgPool1d(1)
        self.head = nn.Linear(self.num_features, num_classes) if num_classes &gt; 0 else nn.Identity()

    def forward_features(self, x):
        x = self.patch_embed(x) # Patch Partition
        if self.ape:
            x = x + self.absolute_pos_embed
        x = self.pos_drop(x)

        for layer in self.layers:
            x = layer(x)

        x = self.norm(x)  # Batch_size Windows_num Channels
        x = self.avgpool(x.transpose(1, 2))  # Batch_size Channels 1
        x = torch.flatten(x, 1)
        return x

    def forward(self, x):
        x = self.forward_features(x)
        x = self.head(x) # self.head =&gt; Linear(in=Channels,out=Classification_num)
        return x
</code></pre>
<p>其中有几个地方处理方法与 ViT 不同：</p>
<ul>
<li>ViT 在输入会给 embedding 进行位置编码。而 Swin-T 这里则是作为一个<strong>可选项</strong>（<code>self.ape</code>），Swin-T 是在计算 Attention 的时候做了一个<strong>相对位置编码</strong>，我认为这是这篇论文设计最巧妙的地方。</li>
<li>ViT 会单独加上一个可学习参数，作为分类的 token。而 Swin-T 则是<strong>直接做平均</strong>（avgpool），输出分类，有点类似 CNN 最后的全局平均池化层。</li>
</ul>
<h2 id="patch-embedding">Patch Embedding<a class="anchor-link" href="#patch-embedding" title="Permanent link">&para;</a></h2>
<p>在输入进 Block 前，我们需要将图片切成一个个 patch，然后嵌入向量。</p>
<p>具体做法是对原始图片裁成一个个 <code>window_size * window_size</code> 的窗口大小，然后进行嵌入。</p>
<p>这里可以通过二维卷积层，<strong>将 stride，kernel_size 设置为 window_size 大小</strong>。设定输出通道来确定嵌入向量的大小。最后将 H,W 维度展开，并移动到第一维度。</p>
<blockquote>
<p>论文中输出通道设置为 48，但是代码中为 96，以下我们均以代码为准。</p>
<p>Batch_size=128</p>
</blockquote>
<p><img alt="Patch_embedding" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202403231022982.png" /></p>
<pre><code class="language-python">import torch
import torch.nn as nn

class PatchEmbed(nn.Module):
    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
        super().__init__()
        img_size = to_2tuple(img_size) # -&gt; (img_size, img_size)
        patch_size = to_2tuple(patch_size) # -&gt; (patch_size, patch_size)
        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]
        self.img_size = img_size
        self.patch_size = patch_size
        self.patches_resolution = patches_resolution
        self.num_patches = patches_resolution[0] * patches_resolution[1]

        self.in_chans = in_chans
        self.embed_dim = embed_dim

        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

    def forward(self, x):
        # 假设采取默认参数，论文中embedding_size是96，但是代码中为48.我们以代码为准
        x = self.proj(x) # 出来的是(N, 96, 224/4, 224/4)
        x = torch.flatten(x, 2) # 把HW维展开，(N, 96, 56*56)
        x = torch.transpose(x, 1, 2)  # 把通道维放到最后 (N, 56*56, 96)
        if self.norm is not None:
            x = self.norm(x)
        return x
</code></pre>
<h2 id="patch-merging">Patch Merging<a class="anchor-link" href="#patch-merging" title="Permanent link">&para;</a></h2>
<p>该模块的作用是在每个 Stage 开始前做降采样，用于缩小分辨率，调整通道数进而形成层次化的设计，同时也能节省一定运算量。</p>
<blockquote>
<p>在 CNN 中，则是在每个 Stage 开始前用<code>stride=2</code>的卷积/池化层来降低分辨率。</p>
</blockquote>
<p>每次降采样是两倍，因此<strong>在行方向和列方向上，间隔 2 选取元素</strong>。</p>
<p>然后拼接在一起作为一整个张量，最后展开。<strong>此时通道维度会变成原先的 4 倍</strong>（因为 H,W 各缩小 2 倍），此时再通过一个<strong>全连接层再调整通道维度为原来的两倍</strong>。</p>
<p>下面是一个示意图（输入张量 N=1, H=W=8, C=1，不包含最后的全连接层调整）</p>
<p><img alt="Patch_Merging" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202403231022983.png" /></p>
<p><img alt="Patch_Merging_dim" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202403231022984.png" /></p>
<pre><code class="language-python">class PatchMerging(nn.Module):
    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):
        super().__init__()
        self.input_resolution = input_resolution
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = norm_layer(4 * dim)

    def forward(self, x):
        &quot;&quot;&quot;
        x: B, H*W, C
        &quot;&quot;&quot;
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, &quot;input feature has wrong size&quot;
        assert H % 2 == 0 and W % 2 == 0, f&quot;x size ({H}*{W}) are not even.&quot;

        x = x.view(B, H, W, C)

        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C
        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C
        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C
        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C
        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C
        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C

        x = self.norm(x)
        x = self.reduction(x)

        return x
</code></pre>
<h2 id="window-partitionreverse">Window Partition/Reverse<a class="anchor-link" href="#window-partitionreverse" title="Permanent link">&para;</a></h2>
<p><code>window partition</code>函数是用于对张量划分窗口，指定窗口大小。将原本的张量从 <code>N H W C</code>, 划分成 <code>num_windows*B, window_size, window_size, C</code>，其中 <code>num_windows = H*W / window_size*window_size</code>，即窗口的个数。而<code>window reverse</code>函数则是对应的逆过程。这两个函数会在后面的<code>Window Attention</code>用到。</p>
<p><img alt="Window_Partition_Reverse" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202403231022985.png" /></p>
<pre><code class="language-python">def window_partition(x, window_size):
    B, H, W, C = x.shape
    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
    return windows

def window_reverse(windows, window_size, H, W):
    B = int(windows.shape[0] / (H * W / window_size / window_size))
    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
    return x
</code></pre>
<h2 id="window-attention">Window Attention<a class="anchor-link" href="#window-attention" title="Permanent link">&para;</a></h2>
<p>传统的 Transformer 都是<strong>基于全局来计算注意力的</strong>，因此计算复杂度十分高。而 Swin Transformer 则将<strong>注意力的计算限制在每个窗口内</strong>，进而减少了计算量。我们先简单看下公式</p>
<p><div class="math-display"><br />
Attention(Q,K,V)=Softmax(\frac{{QK}^T}{\sqrt d}+B)V<br />
</div></p>
<p>主要区别是在原始计算 Attention 的公式中的 Q,K 时<strong>加入了相对位置编码</strong>。</p>
<p><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202403231022986.png" alt="Swin-T_block" style="zoom:50%;" /></p>
<pre><code class="language-python">class WindowAttention(nn.Module):
    r&quot;&quot;&quot; Window based multi-head self attention (W-MSA) module with relative position bias.
    It supports both of shifted and non-shifted window.

    Args:
        dim (int): Number of input channels.
        window_size (tuple[int]): The height and width of the window.
        num_heads (int): Number of attention heads.
        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set
        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
    &quot;&quot;&quot;

    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):

        super().__init__()
        self.dim = dim
        self.window_size = window_size  # Wh, Ww
        self.num_heads = num_heads # nH
        head_dim = dim // num_heads # 每个注意力头对应的通道数
        self.scale = qk_scale or head_dim ** -0.5

        # define a parameter table of relative position bias
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 设置一个形状为（2*(Wh-1) * 2*(Ww-1), nH）的可学习变量，用于后续的位置编码

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

        trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)
        # 相关位置编码...
</code></pre>
<h3 id="相关位置编码的直观理解">相关位置编码的直观理解<a class="anchor-link" href="#相关位置编码的直观理解" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Q,K,V.shape=[numWindwos*B, num_heads, window_size*window_size, head_dim]</p>
<ul>
<li>window_size*window_size 即 NLP 中<code>token</code>的个数</li>
<li><span class="math-inline">head_dim=\frac{Embedding_dim}{num_heads}</span> 即 NLP 中<code>token</code>的词嵌入向量的维度</li>
</ul>
<p><span class="math-inline">{QK}^T</span> 算出来的<code>Attention</code>张量的形状为<code>[numWindows*B, num_heads, Q_tokens, K_tokens]</code></p>
<ul>
<li>其中 Q_tokens=K_tokens=window_size*window_size</li>
</ul>
</blockquote>
<p>以<code>window_size=2</code>为例：</p>
<p><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202403231022987.png" alt="绝对位置索引" style="zoom:33%;" /></p>
<p>因此：<span class="math-inline">{QK}^T=\left[\begin{array}{cccc}a_{11} &amp; a_{12} &amp; a_{13} &amp; a_{14} \ a_{21} &amp; a_{22} &amp; a_{23} &amp; a_{24} \ a_{31} &amp; a_{32} &amp; a_{33} &amp; a_{34} \ a_{41} &amp; a_{42} &amp; a_{43} &amp; a_{44}\end{array}\right]</span></p>
<ul>
<li><strong>第 <span class="math-inline">i</span> 行表示第 <span class="math-inline">i</span> 个 token 的<code>query</code>对所有 token 的<code>key</code>的 attention。</strong></li>
<li>对于 Attention 张量来说，<strong>以不同元素为原点，其他元素的坐标也是不同的</strong>，</li>
</ul>
<p><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202403231022988.png" alt="相对位置索引" style="zoom:50%;" /></p>
<p>所以<span class="math-inline">{QK}^T的相对位置索引=\left[\begin{array}{cccc}(0,0) &amp; (0,-1) &amp; (-1,0) &amp; (-1,-1) \ (0,1) &amp; (0,0) &amp; (-1,1) &amp; (-1,0) \ (1,0) &amp; (1,-1) &amp; (0,0) &amp; (0,-1) \ (1,1) &amp; (1,0) &amp; (0,1) &amp; (0,0)\end{array}\right]</span></p>
<p>由于最终我们希望使用一维的位置坐标 <code>x+y</code> 代替二维的位置坐标<code>(x,y)</code>，为了避免 (1,2) (2,1) 两个坐标转为一维时均为 3，我们之后对相对位置索引进行了一些<strong>线性变换</strong>，使得能通过<strong>一维</strong>的位置坐标<strong>唯一映射</strong>到一个<strong>二维</strong>的位置坐标，详细可以通过代码部分进行理解。</p>
<h3 id="相关位置编码的代码详解">相关位置编码的代码详解<a class="anchor-link" href="#相关位置编码的代码详解" title="Permanent link">&para;</a></h3>
<p>首先我们利用<code>torch.arange</code>和<code>torch.meshgrid</code>函数生成对应的坐标，这里我们以<code>windowsize=2</code>为例子</p>
<pre><code class="language-python">coords_h = torch.arange(self.window_size[0])
coords_w = torch.arange(self.window_size[1])
coords = torch.meshgrid([coords_h, coords_w]) # -&gt; 2*(wh, ww)
&quot;&quot;&quot;
  (tensor([[0, 0],
           [1, 1]]),
   tensor([[0, 1],
           [0, 1]]))
&quot;&quot;&quot;
</code></pre>
<p>然后堆叠起来，展开为一个二维向量</p>
<pre><code class="language-python">coords = torch.stack(coords)  # 2, Wh, Ww
coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
&quot;&quot;&quot;
tensor([[0, 0, 1, 1],
        [0, 1, 0, 1]])
&quot;&quot;&quot;
</code></pre>
<p>利用广播机制，分别在第一维，第二维，插入一个维度，进行广播相减，得到 <code>2, wh*ww, wh*ww</code>的张量</p>
<pre><code class="language-python">relative_coords_first = coords_flatten[:, :, None]  # 2, wh*ww, 1
relative_coords_second = coords_flatten[:, None, :] # 2, 1, wh*ww
relative_coords = relative_coords_first - relative_coords_second # 最终得到 2, wh*ww, wh*ww 形状的张量
</code></pre>
<p><img alt="relative_pos_code" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202403231022989.png" /></p>
<p>因为采取的是相减，所以得到的索引是从负数开始的，<strong>我们加上偏移量，让其从 0 开始</strong>。</p>
<pre><code class="language-python">relative_coords = relative_coords.permute(1, 2, 0).contiguous() # Wh*Ww, Wh*Ww, 2
relative_coords[:, :, 0] += self.window_size[0] - 1
relative_coords[:, :, 1] += self.window_size[1] - 1
</code></pre>
<p>后续我们需要将其展开成一维偏移量。而对于 (1，2）和（2，1）这两个坐标。在二维上是不同的，<strong>但是通过将 x,y 坐标相加转换为一维偏移的时候，他的偏移量是相等的</strong>。</p>
<p><img alt="bias0" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202403231022990.png" /></p>
<p>所以最后我们对其中做了个乘法操作，以进行区分</p>
<pre><code class="language-python">relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
</code></pre>
<p><img alt="offset multiply" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202403231022991.png" /></p>
<p>然后再最后一维上进行求和，展开成一个一维坐标，并注册为一个不参与网络学习的变量</p>
<pre><code class="language-python">relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww
self.register_buffer(&quot;relative_position_index&quot;, relative_position_index)
</code></pre>
<p>之前计算的是相对位置索引，并不是相对位置偏置参数。真正使用到的可训练参数<span class="math-inline">\hat B</span> 保存在<code>relative position bias table</code>表里的，这个表的长度是等于 <strong>(2M−1) × (2M−1)</strong> (在二维位置坐标中线性变化乘以 2M-1 导致)的。那么上述公式中的相对位置偏执参数 B 是根据上面的相对位置索引表根据查<code>relative position bias table</code>表得到的。</p>
<p><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202403231022992.png" alt="relative_pos_bias_table" style="zoom:50%;" /></p>
<p>接着我们看前向代码</p>
<pre><code class="language-python">def forward(self, x, mask=None):
    &quot;&quot;&quot;
    Args:
        x: input features with shape of (num_windows*B, N, C)
        mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
    &quot;&quot;&quot;
    B_, N, C = x.shape

    qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
    q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)

    q = q * self.scale
    attn = (q @ k.transpose(-2, -1))

    relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH
    relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww
    attn = attn + relative_position_bias.unsqueeze(0) # (1, num_heads, windowsize, windowsize)

    if mask is not None: # 下文会分析到
        ...
    else:
        attn = self.softmax(attn)

    attn = self.attn_drop(attn)

    x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
    x = self.proj(x)
    x = self.proj_drop(x)
    return x
</code></pre>
<ul>
<li>首先输入张量形状为 <code>[numWindows*B, window_size * window_size, C]</code></li>
<li>然后经过<code>self.qkv</code>这个全连接层后，进行 reshape，调整轴的顺序，得到形状为<code>[3, numWindows*B, num_heads, window_size*window_size, c//num_heads]</code>，并分配给<code>q,k,v</code>。</li>
<li>根据公式，我们对<code>q</code>乘以一个<code>scale</code>缩放系数，然后与<code>k</code>（为了满足矩阵乘要求，需要将最后两个维度调换）进行相乘。得到形状为<code>[numWindows*B, num_heads, window_size*window_size, window_size*window_size]</code>的<code>attn</code>张量</li>
<li>之前我们针对位置编码设置了个形状为<code>(2*window_size-1*2*window_size-1, numHeads)</code>的可学习变量。我们用计算得到的相对编码位置索引<code>self.relative_position_index.vew(-1)</code>选取，得到形状为<code>(window_size*window_size, window_size*window_size, numHeads)</code>的编码，再 permute(2,0,1)后加到<code>attn</code>张量上</li>
<li>暂不考虑 mask 的情况，剩下就是跟 transformer 一样的 softmax，dropout，与<code>V</code>矩阵乘，再经过一层全连接层和 dropout</li>
</ul>
<h2 id="shifted-window-attention">Shifted Window Attention<a class="anchor-link" href="#shifted-window-attention" title="Permanent link">&para;</a></h2>
<p>前面的 Window Attention 是在每个窗口下计算注意力的，为了更好的和其他 window 进行信息交互，Swin Transformer 还引入了 shifted window 操作。</p>
<p><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202403231022993.png" alt="Shifted_Window" style="zoom:67%;" /></p>
<p>左边是没有重叠的 Window Attention，而右边则是将窗口进行移位的 Shift Window Attention。可以看到移位后的窗口包含了原本相邻窗口的元素。但这也引入了一个新问题，即 <strong>window 的个数翻倍了</strong>，由原本四个窗口变成了 9 个窗口。在实际代码里，我们是<strong>通过对特征图移位，并给 Attention 设置 mask 来间接实现的</strong>。能在<strong>保持原有的 window 个数下</strong>，最后的计算结果等价。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20240328212134.png" style="zoom: 60%;" /></div>

<h3 id="特征图移位操作">特征图移位操作<a class="anchor-link" href="#特征图移位操作" title="Permanent link">&para;</a></h3>
<p>代码里对特征图移位是通过<code>torch.roll</code>来实现的，下面是示意图</p>
<p><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202403231022994.png" alt="torch_roll" style="zoom:67%;" /></p>
<blockquote>
<p>如果需要<code>reverse cyclic shift</code>的话只需把参数<code>shifts</code>设置为对应的正数值。</p>
</blockquote>
<h3 id="attention-mask">Attention Mask<a class="anchor-link" href="#attention-mask" title="Permanent link">&para;</a></h3>
<p>这是 Swin Transformer 的精华，通过设置合理的 mask，让<code>Shifted Window Attention</code>在与<code>Window Attention</code>相同的窗口个数下，达到等价的计算结果。</p>
<p>首先我们对 Shift Window 后的每个窗口都给上 index，并且做一个<code>roll</code>操作（window_size=2, shift_size=1）</p>
<p><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202403231022995.png" alt="Shift window index" style="zoom:67%;" /></p>
<p>我们希望在计算 Attention 的时候，<strong>让具有相同 index QK 进行计算，而忽略不同 index QK 计算结果</strong>。最后正确的结果如下图所示</p>
<p><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202403231022996.png" alt="Mask" style="zoom:50%;" /></p>
<p>而要想在原始四个窗口下得到正确的结果，我们就必须给 Attention 的结果加入一个 mask（如上图最右边所示）相关代码如下：</p>
<pre><code class="language-python">if self.shift_size &gt; 0:
    # calculate attention mask for SW-MSA
    H, W = self.input_resolution
    img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1
    h_slices = (slice(0, -self.window_size),
                slice(-self.window_size, -self.shift_size),
                slice(-self.shift_size, None))
    w_slices = (slice(0, -self.window_size),
                slice(-self.window_size, -self.shift_size),
                slice(-self.shift_size, None))
    cnt = 0
    for h in h_slices:
        for w in w_slices:
            img_mask[:, h, w, :] = cnt
            cnt += 1

    mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1
    mask_windows = mask_windows.view(-1, self.window_size * self.window_size)
    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))
else:
    attn_mask = None
</code></pre>
<p>以上图的设置，我们用这段代码会得到这样的一个 mask</p>
<pre><code>tensor([[[[[   0.,    0.,    0.,    0.],
           [   0.,    0.,    0.,    0.],
           [   0.,    0.,    0.,    0.],
           [   0.,    0.,    0.,    0.]]],


         [[[   0., -100.,    0., -100.],
           [-100.,    0., -100.,    0.],
           [   0., -100.,    0., -100.],
           [-100.,    0., -100.,    0.]]],


         [[[   0.,    0., -100., -100.],
           [   0.,    0., -100., -100.],
           [-100., -100.,    0.,    0.],
           [-100., -100.,    0.,    0.]]],


         [[[   0., -100., -100., -100.],
           [-100.,    0., -100., -100.],
           [-100., -100.,    0., -100.],
           [-100., -100., -100.,    0.]]]]])
</code></pre>
<p>在之前的 window attention 模块的前向代码里，包含这么一段</p>
<pre><code class="language-python">if mask is not None:
    nW = mask.shape[0] # 一张图被分为多少个windows eg:[4,49,49]
    attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0) # torch.Size([128, 4, 12, 49, 49]) torch.Size([1, 4, 1, 49, 49])
    attn = attn.view(-1, self.num_heads, N, N)
    attn = self.softmax(attn)
else:
    attn = self.softmax(attn)
</code></pre>
<p>将 mask 加到 attention 的计算结果，并进行 softmax。mask 的值设置为 - 100，softmax 后就会忽略掉对应的值。关于 Mask，我们发现在官方代码库中的 issue38 也进行了讨论:--&gt;<a href="https://github.com/microsoft/Swin-Transformer/issues/38#issuecomment-823806591">The Question about the mask of window attention #38</a></p>
<h2 id="w-msa-和-msa-的复杂度对比">W-MSA 和 MSA 的复杂度对比<a class="anchor-link" href="#w-msa-和-msa-的复杂度对比" title="Permanent link">&para;</a></h2>
<p>在原论文中，作者提出的基于<strong>滑动窗口操作</strong>的 <code>W-MSA</code> 能大幅度减少计算量。那么两者的计算量和算法复杂度大概是如何的呢，论文中给出了一下两个公式进行对比。</p>
<p><div class="math-display"><br />
\begin{aligned}<br />
&amp;\Omega(M S A)=4 h w C^{2}+2(h w)^{2} C \<br />
&amp;\Omega(W-M S A)=4 h w C^{2}+2 M^{2} h w C<br />
\end{aligned}<br />
</div></p>
<ul>
<li><strong>h</strong>：feature map 的高度</li>
<li><strong>w</strong>：feature map 的宽度</li>
<li><strong>C</strong>：feature map 的通道数（也可以称为 embedding size 的大小）</li>
<li><strong>M</strong>：window_size 的大小</li>
</ul>
<h3 id="msa-模块的计算量">MSA 模块的计算量<a class="anchor-link" href="#msa-模块的计算量" title="Permanent link">&para;</a></h3>
<p>首先对于<code>feature map</code>中每一个<code>token</code>（一共有 <span class="math-inline">hw</span> 个 token，通道数为 C），记作<span class="math-inline">X^{h w \times C}</span>，需要通过三次线性变换 <span class="math-inline">W_q,W_k,W_v</span> ，产生对应的<code>q,k,v</code>向量，记作 <span class="math-inline">Q^{h w \times C},K^{h w \times C},V^{h w \times C}</span> （通道数为 C）。</p>
<p><div class="math-display"><br />
X^{h w \times C} \cdot W_{q}^{C \times C}=Q^{h w \times C} \<br />
X^{h w \times C} \cdot W_{k}^{C \times C}=K^{h w \times C} \<br />
X^{h w \times C} \cdot W_{v}^{C \times C}=V^{h w \times C} \<br />
</div></p>
<p>根据矩阵运算的计算量公式可以得到运算量为 <span class="math-inline">3hwC \times C</span> ，即为 <span class="math-inline">3hwC^2</span> 。</p>
<p><div class="math-display"><br />
Q^{h w \times C} \cdot K^T=A^{h w \times hw} \<br />
\Lambda^{h w \times h w}=Softmax(\frac{A^{h w \times hw}}{\sqrt(d)}+B) \<br />
\Lambda^{h w \times h w} \cdot V^{h w \times C}=Y^{h w \times C}<br />
</div></p>
<p>忽略除以<span class="math-inline">\sqrt d</span> 以及 softmax 的计算量，根据根据矩阵运算的计算量公式可得 <span class="math-inline">hwC \times hw + hw^2 \times C</span> ，即为 <span class="math-inline">2(hw^2)C</span> 。</p>
<p><div class="math-display"><br />
Y^{h w \times C} \cdot W_O^{C \times C}=O^{h w \times C}<br />
</div></p>
<p>最终再通过一个 Linear 层输出，计算量为 <span class="math-inline">hwC^2</span> 。因此整体的计算量为 <span class="math-inline">4 h w C^{2}+2(h w)^{2} C</span>​ 。</p>
<h3 id="w-msa-模块的计算量">W-MSA 模块的计算量<a class="anchor-link" href="#w-msa-模块的计算量" title="Permanent link">&para;</a></h3>
<p>对于 W-MSA 模块，首先会将<code>feature map</code>根据<code>window_size</code>分成 <span class="math-inline">\frac{hw}{M^2}</span> 的窗口，每个窗口的宽高均为<span class="math-inline">M</span>，然后在每个窗口进行 MSA 的运算。因此，可以利用上面 MSA 的计算量公式，将 <span class="math-inline">h=M，w=M</span> 带入，可以得到一个窗口的计算量为 <span class="math-inline">4 M^2 C^{2}+2M^{4} C</span> 。</p>
<p>又因为有 <span class="math-inline">\frac{hw}{M^2}</span> 个窗口，则：</p>
<p><div class="math-display"><br />
\frac{hw}{M^2} \times\left(4M^2 C^2+2M^{4} C\right)=4 h w C^{2}+2 M^{2} h w C<br />
</div></p>
<p>假设<code>feature map</code>的<span class="math-inline">h=w=112，M=7，C=128</span>，采用 W-MSA 模块会比 MSA 模块节省约 40124743680 FLOPs：</p>
<p><div class="math-display"><br />
2(h w)^{2} C-2 M^{2} h w C=2 \times 112^{4} \times 128-2 \times 7^{2} \times 112^{2} \times 128=40124743680<br />
</div></p>
<h2 id="整体流程图">整体流程图<a class="anchor-link" href="#整体流程图" title="Permanent link">&para;</a></h2>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20240328212154.png" style="zoom: 60%;" /></div>

<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20240328212206.png" style="zoom: 60%;" /></div>

<p><img alt="Hyper_parameters" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202403231022997.png" /></p>
<blockquote>
<p>参考博客：</p>
<p>https://zhuanlan.zhihu.com/p/367111046</p>
<p>联系方式：</p>
<ul>
<li>
<p>个人知乎：https://www.zhihu.com/people/shenhao-63</p>
</li>
<li>
<p>Github：https://github.com/shenhao-stu</p>
</li>
</ul>
</blockquote>
                </div>

                <!-- Comments Section (Giscus) -->
                <section class="comments-section">
                    <h3><i class="fas fa-comments"></i> 评论</h3>
                    <script src="https://giscus.app/client.js"
                        data-repo="Geeks-Z/Geeks-Z.github.io"
                        data-repo-id=""
                        data-category="Announcements"
                        data-category-id=""
                        data-mapping="pathname"
                        data-strict="0"
                        data-reactions-enabled="1"
                        data-emit-metadata="0"
                        data-input-position="bottom"
                        data-theme="preferred_color_scheme"
                        data-lang="zh-CN"
                        crossorigin="anonymous"
                        async>
                    </script>
                </section>
            </article>

            <footer class="post-footer">
                <p>© 2025 Hongwei Zhao. Built with ❤️</p>
            </footer>
        </main>
    </div>

    <!-- Theme Toggle Button -->
    <button class="theme-toggle" id="themeToggle" aria-label="切换主题">
        <i class="fas fa-moon"></i>
    </button>

    <script>
        // Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        const html = document.documentElement;
        const icon = themeToggle.querySelector('i');
        const hljsDark = document.getElementById('hljs-theme-dark');
        const hljsLight = document.getElementById('hljs-theme-light');
        
        // Check saved theme or system preference
        const savedTheme = localStorage.getItem('theme');
        const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
        
        if (savedTheme === 'dark' || (!savedTheme && prefersDark)) {
            html.setAttribute('data-theme', 'dark');
            icon.className = 'fas fa-sun';
            hljsDark.disabled = false;
            hljsLight.disabled = true;
        }
        
        themeToggle.addEventListener('click', () => {
            const isDark = html.getAttribute('data-theme') === 'dark';
            if (isDark) {
                html.removeAttribute('data-theme');
                icon.className = 'fas fa-moon';
                localStorage.setItem('theme', 'light');
                hljsDark.disabled = true;
                hljsLight.disabled = false;
            } else {
                html.setAttribute('data-theme', 'dark');
                icon.className = 'fas fa-sun';
                localStorage.setItem('theme', 'dark');
                hljsDark.disabled = false;
                hljsLight.disabled = true;
            }
            
            // Update Giscus theme
            const giscusFrame = document.querySelector('iframe.giscus-frame');
            if (giscusFrame) {
                giscusFrame.contentWindow.postMessage({
                    giscus: {
                        setConfig: {
                            theme: isDark ? 'light' : 'dark'
                        }
                    }
                }, 'https://giscus.app');
            }
        });
        
        // Highlight.js
        document.addEventListener('DOMContentLoaded', () => {
            hljs.highlightAll();
        });
        
        // KaTeX auto-render
        document.addEventListener('DOMContentLoaded', () => {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
        
        // TOC active state
        const tocLinks = document.querySelectorAll('.toc-container a');
        const headings = document.querySelectorAll('.post-content h2, .post-content h3, .post-content h4');
        
        function updateTocActive() {
            let current = '';
            headings.forEach(heading => {
                const rect = heading.getBoundingClientRect();
                if (rect.top <= 100) {
                    current = heading.getAttribute('id');
                }
            });
            
            tocLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === '#' + current) {
                    link.classList.add('active');
                }
            });
        }
        
        window.addEventListener('scroll', updateTocActive);
        updateTocActive();
    </script>
</body>
</html>
