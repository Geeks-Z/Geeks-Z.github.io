<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ViT解读</title>
    <meta name="description" content="ViT解读 - Hongwei Zhao's Blog">
    <meta name="author" content="Hongwei Zhao">
    
    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <!-- Code Highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css" id="hljs-theme-dark">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css" id="hljs-theme-light" disabled>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    
    <style>
        :root {
            /* Light Theme */
            --primary-color: #2980b9;
            --primary-hover: #1a5276;
            --link-color: #c0392b;
            --text-color: #333;
            --text-light: #666;
            --text-muted: #999;
            --bg-color: #fff;
            --bg-secondary: #f8f9fa;
            --bg-code: #f5f5f5;
            --border-color: #e5e7eb;
            --shadow: 0 1px 3px rgba(0,0,0,0.1);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.1);
        }

        [data-theme="dark"] {
            --primary-color: #5dade2;
            --primary-hover: #85c1e9;
            --link-color: #e74c3c;
            --text-color: #e5e7eb;
            --text-light: #9ca3af;
            --text-muted: #6b7280;
            --bg-color: #1a1a2e;
            --bg-secondary: #16213e;
            --bg-code: #0f0f23;
            --border-color: #374151;
            --shadow: 0 1px 3px rgba(0,0,0,0.3);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.4);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 16px;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            transition: background-color 0.3s, color 0.3s;
        }

        a {
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--primary-hover);
            text-decoration: underline;
        }

        /* Layout */
        .page-wrapper {
            display: flex;
            max-width: 1400px;
            margin: 0 auto;
            padding: 2rem;
            gap: 3rem;
        }

        /* Sidebar TOC */
        .toc-sidebar {
            width: 260px;
            flex-shrink: 0;
            position: sticky;
            top: 2rem;
            height: fit-content;
            max-height: calc(100vh - 4rem);
            overflow-y: auto;
        }

        .toc-container {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 1.5rem;
            border: 1px solid var(--border-color);
        }

        .toc-container h3 {
            font-size: 0.85rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-muted);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .toc-container ul {
            list-style: none;
        }

        .toc-container li {
            margin-bottom: 0.5rem;
        }

        .toc-container a {
            font-size: 0.9rem;
            color: var(--text-light);
            display: block;
            padding: 0.25rem 0;
            border-left: 2px solid transparent;
            padding-left: 0.75rem;
            transition: all 0.2s;
        }

        .toc-container a:hover,
        .toc-container a.active {
            color: var(--primary-color);
            border-left-color: var(--primary-color);
            text-decoration: none;
        }

        .toc-container ul ul {
            margin-left: 1rem;
        }

        /* Main Content */
        .main-content {
            flex: 1;
            min-width: 0;
            max-width: 800px;
        }

        /* Header */
        .post-header {
            margin-bottom: 2rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
            margin-bottom: 1.5rem;
        }

        .back-link:hover {
            color: var(--primary-color);
            text-decoration: none;
        }

        .post-header h1 {
            font-size: 2.25rem;
            font-weight: 700;
            line-height: 1.3;
            margin-bottom: 1rem;
            color: var(--text-color);
        }

        .post-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
        }

        .post-meta span {
            display: flex;
            align-items: center;
            gap: 0.4rem;
        }

        .post-meta i {
            color: var(--text-muted);
        }

        .tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 1rem;
        }

        .tag {
            display: inline-block;
            font-size: 0.8rem;
            background: var(--bg-secondary);
            color: var(--text-light);
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            border: 1px solid var(--border-color);
            transition: all 0.2s;
        }

        .tag:hover {
            background: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }

        /* Article Content */
        .post-content {
            font-size: 1rem;
            line-height: 1.9;
        }

        .post-content h1,
        .post-content h2,
        .post-content h3,
        .post-content h4,
        .post-content h5,
        .post-content h6 {
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-weight: 600;
            line-height: 1.4;
            color: var(--text-color);
        }

        .post-content h1 { font-size: 1.875rem; }
        .post-content h2 { 
            font-size: 1.5rem; 
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }
        .post-content h3 { font-size: 1.25rem; }
        .post-content h4 { font-size: 1.125rem; }

        .post-content p {
            margin-bottom: 1.25rem;
        }

        .post-content ul,
        .post-content ol {
            margin: 1.25rem 0;
            padding-left: 1.5rem;
        }

        .post-content li {
            margin-bottom: 0.5rem;
        }

        .post-content blockquote {
            margin: 1.5rem 0;
            padding: 1rem 1.5rem;
            background: var(--bg-secondary);
            border-left: 4px solid var(--primary-color);
            border-radius: 0 8px 8px 0;
            color: var(--text-light);
            font-style: italic;
        }

        .post-content blockquote p:last-child {
            margin-bottom: 0;
        }

        /* Code */
        .post-content code {
            font-family: 'JetBrains Mono', 'Fira Code', Consolas, monospace;
            font-size: 0.9em;
            background: var(--bg-code);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            color: var(--link-color);
        }

        .post-content pre {
            margin: 1.5rem 0;
            padding: 1.25rem;
            background: var(--bg-code);
            border-radius: 10px;
            overflow-x: auto;
            border: 1px solid var(--border-color);
        }

        .post-content pre code {
            background: none;
            padding: 0;
            color: inherit;
            font-size: 0.875rem;
            line-height: 1.6;
        }

        /* Images */
        .post-content img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            margin: 1.5rem 0;
            box-shadow: var(--shadow-lg);
        }

        /* Tables */
        .post-content table {
            width: 100%;
            margin: 1.5rem 0;
            border-collapse: collapse;
            font-size: 0.95rem;
        }

        .post-content th,
        .post-content td {
            padding: 0.75rem 1rem;
            border: 1px solid var(--border-color);
            text-align: left;
        }

        .post-content th {
            background: var(--bg-secondary);
            font-weight: 600;
        }

        .post-content tr:nth-child(even) {
            background: var(--bg-secondary);
        }

        /* Math */
        .math-display {
            margin: 1.5rem 0;
            overflow-x: auto;
            padding: 1rem;
            background: var(--bg-secondary);
            border-radius: 8px;
        }

        /* Anchor links */
        .anchor-link {
            opacity: 0;
            margin-left: 0.5rem;
            color: var(--text-muted);
            font-weight: 400;
            transition: opacity 0.2s;
        }

        .post-content h2:hover .anchor-link,
        .post-content h3:hover .anchor-link,
        .post-content h4:hover .anchor-link {
            opacity: 1;
        }

        /* Theme Toggle */
        .theme-toggle {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            background: var(--primary-color);
            color: white;
            border: none;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.25rem;
            box-shadow: var(--shadow-lg);
            transition: transform 0.2s, background 0.2s;
            z-index: 1000;
        }

        .theme-toggle:hover {
            transform: scale(1.1);
            background: var(--primary-hover);
        }

        /* Comments Section */
        .comments-section {
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-color);
        }

        .comments-section h3 {
            font-size: 1.25rem;
            margin-bottom: 1.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        /* Footer */
        .post-footer {
            margin-top: 3rem;
            padding-top: 1.5rem;
            border-top: 1px solid var(--border-color);
            text-align: center;
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        /* Responsive */
        @media (max-width: 1024px) {
            .toc-sidebar {
                display: none;
            }
            
            .page-wrapper {
                padding: 1.5rem;
            }
        }

        @media (max-width: 768px) {
            .post-header h1 {
                font-size: 1.75rem;
            }
            
            .post-meta {
                flex-direction: column;
                gap: 0.5rem;
            }
            
            .theme-toggle {
                bottom: 1rem;
                right: 1rem;
                width: 44px;
                height: 44px;
            }
        }

        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-secondary);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--border-color);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--text-muted);
        }
    </style>
</head>
<body>
    <div class="page-wrapper">
        <!-- TOC Sidebar -->
        <aside class="toc-sidebar">
            <div class="toc-container">
                <h3><i class="fas fa-list"></i> 目录</h3>
                <div class="toc">
<ul>
<li><a href="#vit-的整体流程">ViT 的整体流程</a></li>
<li><a href="#切分和映射-patch-embedding--linear-projection">切分和映射 Patch Embedding + Linear Projection</a></li>
<li><a href="#分类表征和位置信息-class-token--postional-embedding">分类表征和位置信息 Class Token + Postional Embedding</a></li>
<li><a href="#transformer-encoder">Transformer Encoder</a><ul>
<li><a href="#multi-head-attention">Multi-head Attention</a></li>
<li><a href="#mlp">MLP</a></li>
<li><a href="#layer-norm">Layer Norm</a></li>
<li><a href="#transformer-encoder-完整代码">Transformer Encoder 完整代码</a></li>
</ul>
</li>
<li><a href="#vit-完整代码">ViT 完整代码</a></li>
</ul>
</div>

            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <article>
                <header class="post-header">
                    <a href="../../../../index.html" class="back-link">
                        <i class="fas fa-arrow-left"></i> 返回博客列表
                    </a>
                    <h1>ViT解读</h1>
                    <div class="post-meta">
                        <span><i class="fas fa-calendar-alt"></i> 2026-01-28</span>
                        <span><i class="fas fa-folder"></i> AINotes/30.PyTorch/08.实战</span>
                        <span><i class="fas fa-user"></i> Hongwei Zhao</span>
                    </div>
                    <div class="tags">
                        
                    </div>
                </header>

                <div class="post-content">
                    <h1 id="vit解读">ViT解读<a class="anchor-link" href="#vit解读" title="Permanent link">&para;</a></h1>
<p>
<font size=3><b>[ViT] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.</b></font>
<br>
<font size=2>Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.</font>
<br>
<font size=2>ICLR 2021.</font>
<a href='https://arxiv.org/pdf/2010.11929v2.pdf'>[paper]</a> <a href='https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py'>[code]</a> 
</p>

<h2 id="vit-的整体流程">ViT 的整体流程<a class="anchor-link" href="#vit-的整体流程" title="Permanent link">&para;</a></h2>
<p>如下图所示，ViT 的主要思想是将图片分成一个一个的小 <code>patch</code>，将每一个 <code>patch</code> 作为序列的元素输入 Transformer 中进行计算。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202403231023776.png"/></div>

<p>其具体流程如下：<br />
1. <strong>切分和映射</strong>：对一张标准图像，我们首先将图片切分成一个一个小的 <code>patch</code>，然后将它们的维度拉平 <code>Flatten</code> 为一维的向量，最后我们将这些向量通过线性映射 <code>Linear Project</code> <span class="math-inline">\mathbf{E}</span> 到维度为 <span class="math-inline">D</span> 的空间。<br />
2. <strong>分类表征和位置信息</strong><strong>：分类表征</strong>：为了实现图像分类，我们在得到的向量中需要加入一个 <code>classs token</code>  <span class="math-inline">\mathbf{x}_\text{class}</span> 作为分类表征（如上图中标注 <span class="math-inline"><em></span> 粉色向量所示）。</em><em>位置信息</em><em>：图像和文本一样也需要注意顺序问题，因此作者通过 <code>Position Embedding</code> <span class="math-inline">\mathbf{E}_{pos}</span> 加入位置编码信息（如上图中标注 <span class="math-inline">0-9</span> 的紫色向量所示）。<br />
3. </em><em>Transformer Encoder</em><em>：然后我们将经过上面操作的 <code>token</code> 送入 <code>Transformer Encoder</code>。这里的 <code>Transformer Encoder</code> 和 <code>Transformer (Attention is All You Need)</code> 文章中实现基本一致，主要是通过多头注意力机制，对 <code>patch</code> 之间进行全局的信息提取。<br />
4. </em><em>输出与分类</em>*：对于分类任务，我们只需要获得 <code>class token</code> 经过 <code>Transformer Encoder</code> 得到的输出，加一个 <code>MLP Head</code> 进行分类学习。</p>
<blockquote>
<p>ViT代码示例来源于<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py">rwightman/timm</a></p>
</blockquote>
<h2 id="切分和映射-patch-embedding--linear-projection">切分和映射 Patch Embedding + Linear Projection<a class="anchor-link" href="#切分和映射-patch-embedding--linear-projection" title="Permanent link">&para;</a></h2>
<p>对一张标准图像 <span class="math-inline">\mathbf{x}</span>，其分辨率为 <span class="math-inline">H \times W \times C</span>。为了方便讨论，我们取 ViT 的标准输入 <span class="math-inline">H \times W \times C = 224 \times 224 \times 3</span> 进行一些具体维度的讲解。通过切分操作，我们将整个图片分成多个 <code>patch</code> <span class="math-inline">\mathbf{x}_p</span>，其大小为 <div class="math-display">P \times P \times C = 16 \times 16 \times 3 = 768。</div> 这样，一共可以得到 <code>Patch</code> 的数量为 <div class="math-display">N={(H \times W)}/{(P \times P)} = {(224 \times 224)}/{(16 \times 16)} = {(224 / 16)}\times {(224 / 16)} = 14 \times 14 = 196。</div> 所以，我们将一张 <span class="math-inline">224 \times 224 \times 3</span> 的标准图片， 通过转换得到了 <span class="math-inline">196</span> 个 <code>patch</code>，每个 <code>patch</code> 的维度是 <span class="math-inline">768</span>。</p>
<p>对得到的 <code>patch</code> 通过 <span class="math-inline">\mathbf{E} \in {\mathbb{R}^{768 \times D}}</span> 进行线性映射到维度 <span class="math-inline">D</span>，我们将映射后的 <code>patch</code> 叫做 <code>token</code>，以便于和原本 Transformer 的术语进行统一（代码中默认的 <span class="math-inline">D</span> 仍然为 <span class="math-inline">768</span>。我们认为，为了不损失信息，这里 <span class="math-inline">D</span> 满足大于等于 <span class="math-inline">768</span> 即可）。对应文中公式，上述操作可以表示为：<br />
<div class="math-display"><br />
\begin{align}<br />
[\mathbf{x}_p^1\mathbf{E}; \mathbf{x}_p^2\mathbf{E}; \cdots; \mathbf{x}_p^N\mathbf{E}], \quad \mathbf{E}\in\mathbb{R}^{(P^2\cdot C)\times D}。<br />
\end{align}<br />
</div></p>
<p>以上是按照原论文对<strong>切分和映射</strong>的讲解，在实际的代码实现过程中，切分和映射实际上是通过一个二维卷积 <code>nn.Conv2d()</code> 一步完成的。为了实现一步操作，作者将卷积核的大小 <code>kernal_size</code> 直接设置为了 <code>patch_size</code>，即 <span class="math-inline">P=16</span>。然后，将卷积核的步长 <code>stride</code> 也设置为了同样的 <code>patch_size</code>，这样就实现了不重复的切割图片。而卷积的特征输入和输出维度，分别设为了 <span class="math-inline">C=3</span> 和 <span class="math-inline">D=768</span>，对应下方代码的 <code>in_c</code> 和 <code>embed_dim</code>。</p>
<pre><code class="language-python">self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)
</code></pre>
<p>一张 <span class="math-inline">1 \times 3 \times 224 \times 224</span> 的图像（其中 <span class="math-inline">1</span> 是 <code>batch_size</code> 的维度），经过上述卷积操作得到 <span class="math-inline">1 \times 768 \times 14 \times 14</span> 的张量。（代码中将 <span class="math-inline">14 \times 14 = 196</span> 当作 <code>grid</code> 的个数，即 <code>grid_size=(14, 14)</code>）然后，对其进行拉平 <code>flatten(2)</code> 得到 <span class="math-inline">1 \times 768 \times 196</span> 的张量。因为 Transformer 需要将序列维度调整到前面，我们再通过 <code>transpose(1, 2)</code> 调整特征和序列维度，最终得到的张量大小为 <span class="math-inline">1 \times 196 \times 768</span>。切分、映射、拉平和维度调整统统经过下面一步操作得到：</p>
<pre><code class="language-python">x = self.proj(x).flatten(2).transpose(1, 2)
</code></pre>
<p>在代码中，这些操作全部被写在名为 <code>PatchEmbed</code> 的模块中，其具体的实现如下所示：</p>
<pre><code class="language-python">class PatchEmbed(nn.Module):
    &quot;&quot;&quot;
    Image --&gt; Patch Embedding --&gt; Linear Proj --&gt; Pos Embedding
    Image size -&gt; [224,224,3]
    Patch size -&gt; 16*16
    Patch num -&gt; (224^2)/(16^2)=196
    Patch dim -&gt; 16*16*3 =768
    Patch Embedding: [224,224,3] -&gt; [196,768]
    Linear Proj: [196,768] -&gt; [196,768]
    Positional Embedding: [197,768] -&gt; [196,768]
    &quot;&quot;&quot;
    def __init__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768, norm_layer=None):
        &quot;&quot;&quot;
        Args:
            img_size: 默认参数224
            patch_size: 默认参数是16
            in_c: 输入的通道数
            embed_dim: 16*16*3 = 768
            norm_layer: 是否使用norm层，默认为否
        &quot;&quot;&quot;
        super().__init__()
        img_size = (img_size, img_size) # -&gt; img_size = (224,224)
        patch_size = (patch_size, patch_size) # -&gt; patch_size = (16,16)
        self.img_size = img_size # -&gt; (224,224)
        self.patch_size = patch_size # -&gt; (16,16)
        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1]) # -&gt; grid_size = (14,14)
        self.num_patches = self.grid_size[0] * self.grid_size[1] # -&gt; num_patches = 196
        # Patch+linear proj的这个操作 [224,224,3] --&gt; [14,14,768]
        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)
        # 判断是否有norm_layer层，要是没有不改变输入
        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()

    def forward(self, x):
        # 计算各个维度的大小
        B, C, H, W = x.shape
        assert H == self.img_size[0] and W == self.img_size[1], \
            f&quot;Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).&quot;

        # flatten: [B, C, H, W] -&gt; [B, C, HW], flatten(2)代表的是从2位置开始展开
        # eg: [1,3,224,224] --&gt; [1,768,14,14] -flatten-&gt;[1,768,196]
        # transpose: [B, C, HW] -&gt; [B, HW, C]
        # eg: [1,768,196] -transpose-&gt; [1,196,768]
        x = self.proj(x).flatten(2).transpose(1, 2)
        x = self.norm(x)
        return x
</code></pre>
<p>在默认情况下，这一步是不进行 <code>layer_norm</code> 操作的，即它被设置为 <code>nn.Identity()</code>。</p>
<h2 id="分类表征和位置信息-class-token--postional-embedding">分类表征和位置信息 Class Token + Postional Embedding<a class="anchor-link" href="#分类表征和位置信息-class-token--postional-embedding" title="Permanent link">&para;</a></h2>
<p>如下图所示，左侧灰色部分为加入分类表征，中间紫色部分为加入位置信息。<br />
<img alt="图源amaarora" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202403231023777.png" /></p>
<p><strong>分类表征：Class Token</strong><br />
为了实现图像分类，我们在切分和映射后的向量 <span class="math-inline">[\mathbf{x}<em>p^1\mathbf{E}; \mathbf{x}_p^2\mathbf{E}; \cdots; \mathbf{x}_p^N\mathbf{E}]</span> 中加入一个 <code>class token</code>  <span class="math-inline">\mathbf{x}</em>\text{class} \in \mathbb{R}^{D}</span> 作为分类表征（如上图中最左侧深灰色框所示）。将这个表征放置在序列的第一个位置上，我们就得到一个维度为 <span class="math-inline">(196+1) \times 768</span> 的新张量：<br />
<div class="math-display"><br />
\begin{align}<br />
[\mathbf{x}_{\text{class}}; \mathbf{x}_p^1\mathbf{E}; \mathbf{x}_p^2\mathbf{E}; \cdots; \mathbf{x}_p^N\mathbf{E}] <br />
\end{align}<br />
</div><br />
对于具体的代码实现，我们通过 <code>nn.Parameter(torch.zeros(1, 1, 768))</code> 实例化一个可学习的 <code>cls_token</code>，然后将这个 <code>cls_token</code> 按照 <code>batch_size = x.shape[0]</code> 进行复制，最后将其和之前经过切分和映射的 <code>x</code> 并在一起 <code>torch.cat((cls_token, x), dim=1)</code>。其完整代码，如下所示：</p>
<pre><code class="language-python">cls_token = nn.Parameter(torch.zeros(1, 1, 768)) # -&gt; cls token
nn.init.trunc_normal_(self.cls_token, std=0.02) # 初始化
cls_token = cls_token.expand(x.shape[0], -1, -1) # (1,1,768) -&gt; (128,1,768)
x = torch.cat((cls_token, x), dim=1)  # [128, 197, 768]
</code></pre>
<p>其实也可以不加入这个 <code>cls token</code>，我们可以对输出 <code>token</code> 做 <code>GAP(Global Average Pooling)</code>，然后对 <code>GAP</code> 的结果进行分类。</p>
<p><strong>位置信息：Postional Embedding</strong><br />
图像和文本一样也需要注意顺序问题，因此作者通过 <code>Position Embedding</code> <span class="math-inline">\mathbf{E}<em>{\text{pos}}\in\mathbb{R}^{(N + 1)\times D}</span> 加入位置编码信息。这个 <code>Position Embedding</code> 和上面得到的分类表征张量，直接相加：<br />
<div class="math-display"><br />
\begin{align}<br />
\mathbf{z}_0 &amp;= [\mathbf{x}</em>{\text{class}}; \mathbf{x}<em>p^1\mathbf{E}; \mathbf{x}_p^2\mathbf{E}; \cdots; \mathbf{x}_p^N\mathbf{E};] + \mathbf{E}</em>{\text{pos}}, &amp; \mathbf{E}&amp;\in\mathbb{R}^{(P^2\cdot C)\times D}, \mathbf{E}_{\text{pos}}\in\mathbb{R}^{(N + 1)\times D}<br />
\end{align}<br />
</div></p>
<p>与 Transformer 使用余弦位置编码不同的是，ViT 通过<code>nn.Parameter()</code>实现了一个可以学习的位置编码。</p>
<pre><code class="language-python">num_patches = 196
pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, 768))
x = x + pos_embed
</code></pre>
<p>这里 <code>pos_embed</code> 在 <code>batch_size</code> 的维度进行了 boardcast，所以所有的样本都是同样的 <code>pos_embed</code>。</p>
<h2 id="transformer-encoder">Transformer Encoder<a class="anchor-link" href="#transformer-encoder" title="Permanent link">&para;</a></h2>
<p>下一步，我们只需要将序列 <span class="math-inline">\mathbf{z}<em>0</span> 输入 Transformer Encoder 即可。如下图所示，每个 Transformer Encoder 由 Multi-head Attention、MLP、Norm (Layer Norm,LN) 并外加 shortcut 连接实现。<br />
<div class="math-display"><br />
\begin{align}<br />
\mathbf{z}'_l &amp;= \text{MSA}(\text{LN}(\mathbf{z}</em>{l-1})) + \mathbf{z}_{l-1}, &amp; l &amp;=1\dots L, \<br />
\mathbf{z}_l &amp;= \text{MLP}(\text{LN}(\mathbf{z}'_l)) + \mathbf{z}'_l,  &amp; l &amp;=1\dots L, \<br />
\mathbf{y} &amp;= \text{LN}(\mathbf{z}_L^0)<br />
\end{align}<br />
</div></p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20240323165416.png" style="zoom: 60%;" /></div>

<h3 id="multi-head-attention">Multi-head Attention<a class="anchor-link" href="#multi-head-attention" title="Permanent link">&para;</a></h3>
<p>Multi-head Attention 或者叫做 Multi-head Self-Attention (MSA) 是由多个 Self-attention (SA) 模块组成，它们的框图可由下面所示，其中左侧为 SA，右侧为 MSA。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20240323165509.png" style="zoom: 60%;" /></div>

<p>对于一个标准的 SA 模块，我们通过对输入张量 <span class="math-inline">\mathbf{z}</span> 进行一个映射 <span class="math-inline">\mathbf{W_{SA}}</span> 得到 <span class="math-inline">Q, K, V</span><br />
<div class="math-display"><br />
[Q, K, V] = \mathbf{z} \mathbf{W}_{\text{SA}}.<br />
</div><br />
对于 MSA，我们需要对其输入再次进行切分为 <span class="math-inline">k</span> 个部分 （<span class="math-inline">k=</span><code>self.num_heads</code>），而每个部分的维度为原本维度的 <span class="math-inline">k</span> 分之一，即 <code>C // self.num_heads</code>。然后，将维度进行调整，即 <code>q, k, v</code> 到第 1 个维度， 批大小 <code>batch_size</code> 为第 2 个维度，头的数量数量 <code>num_heads</code> 为第 3 个维度，切分块的数量 <code>num_patches</code> 和每个头的特征维度 <code>embed_dim_per_head</code> 为最后两个维度。这种维度调整，将方便提取 <code>q, k, v</code>，以及后面的注意力计算。上述步骤在代码中对应：</p>
<pre><code class="language-python">self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
q, k, v = qkv[0], qkv[1], qkv[2]  # seperate q, k, v
</code></pre>
<p>现在，如果我们将每一个 <code>head</code>，看作一个独立的计算单元。我们可以对每一个<code>head</code> 进行标准的 SA 计算<br />
<div class="math-display"><br />
Attention(Q, K, V) = softmax(\frac{Q K^T}{\sqrt {D_k}}) \cdot V<br />
</div><br />
然后，这些 <code>head</code> 会被拼接在一起，计算最终的输出：<br />
<div class="math-display"><br />
\mathrm{MultiHead}(Q, K, V) = \mathrm{Concat}(\mathrm{head_1}, ...,<br />
\mathrm{head_h})W^O    \<br />
    \text{where}~\mathrm{head_i} = \mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)<br />
</div></p>
<p>其中 <span class="math-inline">W^O</span> 代表的是线性变换层，<span class="math-inline">head_i</span> 代表的是每个 <code>head</code> 的输出，其中 <span class="math-inline">W^Q_i</span>，<span class="math-inline">W^K_i</span>, <span class="math-inline">W^V_i</span>，等价于每个 <code>head</code> 的线性映射权重（如上面计算 <code>qkv</code>所讲，实际代码实现中，我们会先一起计算 <code>qkv</code>，再进行 <code>head</code> 的切分）。如果按照默认实现，一般切分为 <span class="math-inline">k=8</span> 个头，其中 <span class="math-inline">D_k=D/k = 768/8=96</span>，是为了归一化点乘的结果。</p>
<p>在代码实现的时候，作者充分考虑了多头的并行计算。通过点乘的形式对所有的 <code>head</code> 一起计算相关性 <code>(q @ k.transpose(-2, -1))</code>，然后经过 <code>softmax</code> 得到权重 <code>attn</code> （这些权重的维度为 <code>[batch_size, num_heads, num_patches + 1, num_patches + 1]</code>）。<br />
之后将这些权重 <code>attn</code> 和 <code>v</code> （其维度为 <code>[batch_size, num_heads, num_patches+1, embed_dim_per_head]</code>） 进行点乘，得到注意力的输出结果。这里在点乘的时候，我们只需要看 <code>attn</code> 和 <code>v</code>的最后两个维度，分别为<code>[num_patches + 1, num_patches + 1]</code> 和 <code>[num_patches+1, embed_dim_per_head]</code>，维持其他维度不变，我们可以得到输出的结果维度为 <code>[batch_size, num_heads, num_patches + 1, embed_dim_per_head]</code>。<br />
最后，我们通过将特征维度和多头维度交换 <code>transpose(1, 2)</code> 和 重组第2个及后面所有的维度 <code>reshape(B, N, C)</code>，就可以得到维度为 <code>[batch_size, num_patches + 1, total_embed_dim]</code> 和上面公式相同的并行多头计算结果。其完整实现如下所示</p>
<pre><code class="language-python">class Attention(nn.Module):
    def __init__(self,
                 dim,   # 输入token的dim
                 num_heads=8, # attention head的个数
                 qkv_bias=False, # 是否使用qkv bias
                 qk_scale=None,
                 attn_drop_ratio=0.,
                 proj_drop_ratio=0.):
        super(Attention, self).__init__()
        self.num_heads = num_heads
        # 计算每一个head处理的维度head_dim = dim // num_heads --&gt; 768/8 = 96
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5 # 根下dk操作
        # 使用nn.Linear生成w_q,w_k,w_v，因为本质上每一个变换矩阵都是线性变换，
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop_ratio)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop_ratio)

    def forward(self, x):
        # [batch_size, num_patches + 1, total_embed_dim]
        # total_embed_dim不是一开始展开的那个维度，是经过了一个线性变换层得到的
        B, N, C = x.shape

        # [batch_size, num_patches+1, total_embed_dim] -qkv()-&gt; [batch_size, num_patches + 1, 3 * total_embed_dim]
        # reshape: -&gt; [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]
        # permute: -&gt; [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        # q,k,v = [batch_size, num_heads, num_patches + 1, embed_dim_per_head]
        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)

        # transpose(-2,-1)在最后两个维度进行操作，输入的形状[batch_size,num_heads,num_patches+1,embed_dim_per_head]
        # transpose: -&gt; [batch_size, num_heads, embed_dim_per_head, num_patches + 1]
        # @: multiply -&gt; [batch_size, num_heads, num_patches + 1, num_patches + 1]
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        # @: multiply -&gt; [batch_size, num_heads, num_patches + 1, embed_dim_per_head]
        # transpose: -&gt; [batch_size, num_patches + 1, num_heads, embed_dim_per_head]
        # reshape: -&gt; [batch_size, num_patches + 1, total_embed_dim]
        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x
</code></pre>
<h3 id="mlp">MLP<a class="anchor-link" href="#mlp" title="Permanent link">&para;</a></h3>
<p>MLP层类似于原始Transformer中的Feed Forward Network。</p>
<blockquote>
<p>In ViT, only MLP layers are local and translationally equivariant, while the self-attention layers are global.</p>
</blockquote>
<p>为了理解这句话，即 MLP 只对局部信息进行操作，我们需要强调 <code>nn.Linear()</code> 操作只对输入张量的最后一个维度进行操作。那么，对于输入维度为 <code>[batch_size, num_patches + 1, total_embed_dim]</code>，学习到的线性层对于所有 <code>patch</code> 都是一样的。所以，它是一个局部信息的建模。对于 Attention，因为它是在不同的 <code>patch</code> 层面或者不同的序列层面进行建模，所以是全局信息建模。因此，作者使用了 MLP 和 Attention 一起进行局部和全局信息的提取。</p>
<pre><code class="language-python">class Mlp(nn.Module):
    &quot;&quot;&quot;
    in_features --&gt; hidden_features --&gt; out_features
    论文实现时：in_features.shape = out_features.shape
    &quot;&quot;&quot;
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        # 用or实现了或操作，当hidden_features/out_features为默认值None时
        # 此时out_features/hidden_features=None or in_features = in_features
        # 当对out_features或hidden_features进行输入时，or操作将会默认选择or前面的
        # 此时out_features/hidden_features = out_features/hidden_features
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        # in_features --&gt; hidden_features --&gt; out_features
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x
</code></pre>
<h3 id="layer-norm">Layer Norm<a class="anchor-link" href="#layer-norm" title="Permanent link">&para;</a></h3>
<p>Normalization 有很多种，但是它们都有一个共同的目的，那就是把输入转化成均值为 0 方差为 1 的数据（或者某个学习到的均值和方差）。我们在把数据送入激活函数之前进行 Normalization（归一化），因为我们不希望输入数据落在激活函数的饱和区。</p>
<p>Batch Norm 的作用是在对这批样本的同一维度特征做归一化，而 Layer Norm 的作用是对<strong>单个样本的所有维度特征做归一化</strong>。举一个简单的例子，对于通过编码的句子“我爱学习”，Batch Norm 是对这四个字进行归一化，而 Layer Norm 是对每个字本身的特征进行归一化。</p>
<p>对于 Layer Norm，其公式如下所示<br />
<div class="math-display">L N\left(x_i\right)=\alpha \times \frac{x_i-u_L}{\sqrt{\sigma_L^2+\epsilon}}+\beta</div><br />
可以通过 <code>nn.LayerNorm</code> 进行实现。</p>
<h3 id="transformer-encoder-完整代码">Transformer Encoder 完整代码<a class="anchor-link" href="#transformer-encoder-完整代码" title="Permanent link">&para;</a></h3>
<p>整合上面 Multi-head Attention、MLP、Norm (Layer Norm,LN) 并外加 shortcut 连接代码，我们可以得到 Transformer Encoder 的完整代码。</p>
<pre><code class="language-python">class Block(nn.Module):
    &quot;&quot;&quot;
    每一个Encoder Block的构成
    每个Encode Block的流程：norm1 --&gt; Multi-Head Attention --&gt; norm2 --&gt; MLP
    &quot;&quot;&quot;
    def __init__(self,
                 dim, # 输入mlp的维度
                 num_heads, # Multi-Head-Attention的头个数
                 mlp_ratio=4., # hidden_features / in_features = mlp_ratio
                 qkv_bias=False, # q,k,v的生成是否使用bias
                 qk_scale=None,
                 drop_ratio=0., # dropout的比例
                 attn_drop_ratio=0., # 注意力dropout的比例
                 drop_path_ratio=0.,
                 act_layer=nn.GELU, # 激活函数默认使用GELU
                 norm_layer=nn.LayerNorm): # Norm默认使用LayerNorm
        super(Block, self).__init__()
        # 第一层normalization
        self.norm1 = norm_layer(dim)
        # self.attention层的实现
        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio)
        self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio &gt; 0. else nn.Identity()
        # 第二层normalization
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio) # hidden_dim = dim * mlp_ratio
        # mlp实现
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop_ratio)

    def forward(self, x):
        # 实现了两个残差连接
        x = x + self.drop_path(self.attn(self.norm1(x)))
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x
</code></pre>
<h2 id="vit-完整代码">ViT 完整代码<a class="anchor-link" href="#vit-完整代码" title="Permanent link">&para;</a></h2>
<p>对输入图像，进行切分和影射、加入分类表征和位置信息、经过 Transformer Encoder、然后添加一个分类头进行输出，我们就完成了 ViT 所有的代码。</p>
<p>完整的 ViT 主要模块流程，见下方 <code>VisionTransformer</code>。</p>
<pre><code class="language-python">class VisionTransformer(nn.Module):
    def __init__(self,
                 img_size=224,
                 patch_size=16,
                 in_c=3,
                 num_classes=1000,
                 embed_dim=768,
                 depth=12,
                 num_heads=12,
                 mlp_ratio=4.0,
                 qkv_bias=True,
                 qk_scale=None,
                 representation_size=None,
                 distilled=False,
                 drop_ratio=0.,
                 attn_drop_ratio=0.,
                 drop_path_ratio=0.,
                 embed_layer=PatchEmbed,
                 norm_layer=None,
                 act_layer=None):
        &quot;&quot;&quot;
        Args:
            img_size (int, tuple): input image size
            patch_size (int, tuple): patch size
            in_c (int): number of input channels
            num_classes (int): number of classes for classification head
            embed_dim (int): embedding dimension
            depth (int): depth of transformer
            num_heads (int): number of attention heads
            mlp_ratio (int): ratio of mlp hidden dim to embedding dim
            qkv_bias (bool): enable bias for qkv if True
            qk_scale (float): override default qk scale of head_dim ** -0.5 if set
            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set
            distilled (bool): model includes a distillation token and head as in DeiT models
            drop_ratio (float): dropout rate
            attn_drop_ratio (float): attention dropout rate
            drop_path_ratio (float): stochastic depth rate
            embed_layer (nn.Module): patch embedding layer
            norm_layer: (nn.Module): normalization layer
        &quot;&quot;&quot;
        super(VisionTransformer, self).__init__()
        self.num_classes = num_classes
        # 每个patch的图像维度 = embed_dim
        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models
        # token的个数为1
        self.num_tokens = 2 if distilled else 1
        # 设置激活函数和norm函数
        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)
        act_layer = act_layer or nn.GELU
        # 对应的将图片打成patch的操作
        self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim)
        num_patches = self.patch_embed.num_patches
        # 设置分类的cls_token
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        # distilled 是Deit中的 这里为None
        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None
        # pos_embedding 为一个可以学习的参数
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))
        self.pos_drop = nn.Dropout(p=drop_ratio)

        dpr = [x.item() for x in torch.linspace(0, drop_path_ratio, depth)]  # stochastic depth decay rule
        # 使用nn.Sequential进行构建，ViT中深度为12
        self.blocks = nn.Sequential(*[
            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                  drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=dpr[i],
                  norm_layer=norm_layer, act_layer=act_layer)
            for i in range(depth)
        ])
        self.norm = norm_layer(embed_dim)

        # Representation layer
        if representation_size and not distilled:
            self.has_logits = True
            self.num_features = representation_size
            self.pre_logits = nn.Sequential(OrderedDict([
                (&quot;fc&quot;, nn.Linear(embed_dim, representation_size)),
                (&quot;act&quot;, nn.Tanh())
            ]))
        else:
            self.has_logits = False
            self.pre_logits = nn.Identity()

        # Classifier head(s)
        self.head = nn.Linear(self.num_features, num_classes) if num_classes &gt; 0 else nn.Identity()
        self.head_dist = None
        if distilled:
            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes &gt; 0 else nn.Identity()

        # Weight init
        nn.init.trunc_normal_(self.pos_embed, std=0.02)
        if self.dist_token is not None:
            nn.init.trunc_normal_(self.dist_token, std=0.02)

        nn.init.trunc_normal_(self.cls_token, std=0.02)
        self.apply(_init_vit_weights)

    def forward_features(self, x):
        # [B, C, H, W] -&gt; [B, num_patches, embed_dim]
        x = self.patch_embed(x)  # [B, 196, 768]
        # [1, 1, 768] -&gt; [B, 1, 768]
        cls_token = self.cls_token.expand(x.shape[0], -1, -1)
        if self.dist_token is None:
            x = torch.cat((cls_token, x), dim=1)  # [B, 197, 768]
        else:
            x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)

        x = self.pos_drop(x + self.pos_embed)
        x = self.blocks(x)
        x = self.norm(x)
        if self.dist_token is None:
            return self.pre_logits(x[:, 0])
        else:
            return x[:, 0], x[:, 1]

    def forward(self, x):
        x = self.forward_features(x)
        if self.head_dist is not None:
            x, x_dist = self.head(x[0]), self.head_dist(x[1])
            if self.training and not torch.jit.is_scripting():
                # during inference, return the average of both classifier predictions
                return x, x_dist
            else:
                return (x + x_dist) / 2
        else:
            x = self.head(x)
        return x


def _init_vit_weights(m):
    &quot;&quot;&quot;
    ViT weight initialization
    :param m: module
    &quot;&quot;&quot;
    if isinstance(m, nn.Linear):
        nn.init.trunc_normal_(m.weight, std=.01)
        if m.bias is not None:
            nn.init.zeros_(m.bias)
    elif isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight, mode=&quot;fan_out&quot;)
        if m.bias is not None:
            nn.init.zeros_(m.bias)
    elif isinstance(m, nn.LayerNorm):
        nn.init.zeros_(m.bias)
        nn.init.ones_(m.weight)
</code></pre>
<blockquote>
<p>参考：</p>
<p>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale https://arxiv.org/pdf/2010.11929.pdf<br />
Attention Is All You Need https://arxiv.org/abs/1706.03762</p>
</blockquote>
                </div>

                <!-- Comments Section (Giscus) -->
                <section class="comments-section">
                    <h3><i class="fas fa-comments"></i> 评论</h3>
                    <script src="https://giscus.app/client.js"
                        data-repo="Geeks-Z/Geeks-Z.github.io"
                        data-repo-id=""
                        data-category="Announcements"
                        data-category-id=""
                        data-mapping="pathname"
                        data-strict="0"
                        data-reactions-enabled="1"
                        data-emit-metadata="0"
                        data-input-position="bottom"
                        data-theme="preferred_color_scheme"
                        data-lang="zh-CN"
                        crossorigin="anonymous"
                        async>
                    </script>
                </section>
            </article>

            <footer class="post-footer">
                <p>© 2025 Hongwei Zhao. Built with ❤️</p>
            </footer>
        </main>
    </div>

    <!-- Theme Toggle Button -->
    <button class="theme-toggle" id="themeToggle" aria-label="切换主题">
        <i class="fas fa-moon"></i>
    </button>

    <script>
        // Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        const html = document.documentElement;
        const icon = themeToggle.querySelector('i');
        const hljsDark = document.getElementById('hljs-theme-dark');
        const hljsLight = document.getElementById('hljs-theme-light');
        
        // Check saved theme or system preference
        const savedTheme = localStorage.getItem('theme');
        const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
        
        if (savedTheme === 'dark' || (!savedTheme && prefersDark)) {
            html.setAttribute('data-theme', 'dark');
            icon.className = 'fas fa-sun';
            hljsDark.disabled = false;
            hljsLight.disabled = true;
        }
        
        themeToggle.addEventListener('click', () => {
            const isDark = html.getAttribute('data-theme') === 'dark';
            if (isDark) {
                html.removeAttribute('data-theme');
                icon.className = 'fas fa-moon';
                localStorage.setItem('theme', 'light');
                hljsDark.disabled = true;
                hljsLight.disabled = false;
            } else {
                html.setAttribute('data-theme', 'dark');
                icon.className = 'fas fa-sun';
                localStorage.setItem('theme', 'dark');
                hljsDark.disabled = false;
                hljsLight.disabled = true;
            }
            
            // Update Giscus theme
            const giscusFrame = document.querySelector('iframe.giscus-frame');
            if (giscusFrame) {
                giscusFrame.contentWindow.postMessage({
                    giscus: {
                        setConfig: {
                            theme: isDark ? 'light' : 'dark'
                        }
                    }
                }, 'https://giscus.app');
            }
        });
        
        // Highlight.js
        document.addEventListener('DOMContentLoaded', () => {
            hljs.highlightAll();
        });
        
        // KaTeX auto-render
        document.addEventListener('DOMContentLoaded', () => {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
        
        // TOC active state
        const tocLinks = document.querySelectorAll('.toc-container a');
        const headings = document.querySelectorAll('.post-content h2, .post-content h3, .post-content h4');
        
        function updateTocActive() {
            let current = '';
            headings.forEach(heading => {
                const rect = heading.getBoundingClientRect();
                if (rect.top <= 100) {
                    current = heading.getAttribute('id');
                }
            });
            
            tocLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === '#' + current) {
                    link.classList.add('active');
                }
            });
        }
        
        window.addEventListener('scroll', updateTocActive);
        updateTocActive();
    </script>
</body>
</html>
