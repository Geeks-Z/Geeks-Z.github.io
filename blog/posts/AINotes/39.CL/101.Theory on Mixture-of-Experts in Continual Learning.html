<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Untitled</title>
    <meta name="description" content="Untitled - Hongwei Zhao's Blog">
    <meta name="author" content="Hongwei Zhao">
    
    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <!-- Code Highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css" id="hljs-theme-dark">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css" id="hljs-theme-light" disabled>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    
    <style>
        :root {
            /* Light Theme */
            --primary-color: #2980b9;
            --primary-hover: #1a5276;
            --link-color: #c0392b;
            --text-color: #333;
            --text-light: #666;
            --text-muted: #999;
            --bg-color: #fff;
            --bg-secondary: #f8f9fa;
            --bg-code: #f5f5f5;
            --border-color: #e5e7eb;
            --shadow: 0 1px 3px rgba(0,0,0,0.1);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.1);
        }

        [data-theme="dark"] {
            --primary-color: #5dade2;
            --primary-hover: #85c1e9;
            --link-color: #e74c3c;
            --text-color: #e5e7eb;
            --text-light: #9ca3af;
            --text-muted: #6b7280;
            --bg-color: #1a1a2e;
            --bg-secondary: #16213e;
            --bg-code: #0f0f23;
            --border-color: #374151;
            --shadow: 0 1px 3px rgba(0,0,0,0.3);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.4);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 16px;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            transition: background-color 0.3s, color 0.3s;
        }

        a {
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--primary-hover);
            text-decoration: underline;
        }

        /* Layout */
        .page-wrapper {
            display: flex;
            max-width: 1400px;
            margin: 0 auto;
            padding: 2rem;
            gap: 3rem;
        }

        /* Sidebar TOC */
        .toc-sidebar {
            width: 260px;
            flex-shrink: 0;
            position: sticky;
            top: 2rem;
            height: fit-content;
            max-height: calc(100vh - 4rem);
            overflow-y: auto;
        }

        .toc-container {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 1.5rem;
            border: 1px solid var(--border-color);
        }

        .toc-container h3 {
            font-size: 0.85rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-muted);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .toc-container ul {
            list-style: none;
        }

        .toc-container li {
            margin-bottom: 0.5rem;
        }

        .toc-container a {
            font-size: 0.9rem;
            color: var(--text-light);
            display: block;
            padding: 0.25rem 0;
            border-left: 2px solid transparent;
            padding-left: 0.75rem;
            transition: all 0.2s;
        }

        .toc-container a:hover,
        .toc-container a.active {
            color: var(--primary-color);
            border-left-color: var(--primary-color);
            text-decoration: none;
        }

        .toc-container ul ul {
            margin-left: 1rem;
        }

        /* Main Content */
        .main-content {
            flex: 1;
            min-width: 0;
            max-width: 800px;
        }

        /* Header */
        .post-header {
            margin-bottom: 2rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
            margin-bottom: 1.5rem;
        }

        .back-link:hover {
            color: var(--primary-color);
            text-decoration: none;
        }

        .post-header h1 {
            font-size: 2.25rem;
            font-weight: 700;
            line-height: 1.3;
            margin-bottom: 1rem;
            color: var(--text-color);
        }

        .post-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
        }

        .post-meta span {
            display: flex;
            align-items: center;
            gap: 0.4rem;
        }

        .post-meta i {
            color: var(--text-muted);
        }

        .tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 1rem;
        }

        .tag {
            display: inline-block;
            font-size: 0.8rem;
            background: var(--bg-secondary);
            color: var(--text-light);
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            border: 1px solid var(--border-color);
            transition: all 0.2s;
        }

        .tag:hover {
            background: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }

        /* Article Content */
        .post-content {
            font-size: 1rem;
            line-height: 1.9;
        }

        .post-content h1,
        .post-content h2,
        .post-content h3,
        .post-content h4,
        .post-content h5,
        .post-content h6 {
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-weight: 600;
            line-height: 1.4;
            color: var(--text-color);
        }

        .post-content h1 { font-size: 1.875rem; }
        .post-content h2 { 
            font-size: 1.5rem; 
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }
        .post-content h3 { font-size: 1.25rem; }
        .post-content h4 { font-size: 1.125rem; }

        .post-content p {
            margin-bottom: 1.25rem;
        }

        .post-content ul,
        .post-content ol {
            margin: 1.25rem 0;
            padding-left: 1.5rem;
        }

        .post-content li {
            margin-bottom: 0.5rem;
        }

        .post-content blockquote {
            margin: 1.5rem 0;
            padding: 1rem 1.5rem;
            background: var(--bg-secondary);
            border-left: 4px solid var(--primary-color);
            border-radius: 0 8px 8px 0;
            color: var(--text-light);
            font-style: italic;
        }

        .post-content blockquote p:last-child {
            margin-bottom: 0;
        }

        /* Code */
        .post-content code {
            font-family: 'JetBrains Mono', 'Fira Code', Consolas, monospace;
            font-size: 0.9em;
            background: var(--bg-code);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            color: var(--link-color);
        }

        .post-content pre {
            margin: 1.5rem 0;
            padding: 1.25rem;
            background: var(--bg-code);
            border-radius: 10px;
            overflow-x: auto;
            border: 1px solid var(--border-color);
        }

        .post-content pre code {
            background: none;
            padding: 0;
            color: inherit;
            font-size: 0.875rem;
            line-height: 1.6;
        }

        /* Images */
        .post-content img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            margin: 1.5rem 0;
            box-shadow: var(--shadow-lg);
        }

        /* Tables */
        .post-content table {
            width: 100%;
            margin: 1.5rem 0;
            border-collapse: collapse;
            font-size: 0.95rem;
        }

        .post-content th,
        .post-content td {
            padding: 0.75rem 1rem;
            border: 1px solid var(--border-color);
            text-align: left;
        }

        .post-content th {
            background: var(--bg-secondary);
            font-weight: 600;
        }

        .post-content tr:nth-child(even) {
            background: var(--bg-secondary);
        }

        /* Math */
        .math-display {
            margin: 1.5rem 0;
            overflow-x: auto;
            padding: 1rem;
            background: var(--bg-secondary);
            border-radius: 8px;
        }

        /* Anchor links */
        .anchor-link {
            opacity: 0;
            margin-left: 0.5rem;
            color: var(--text-muted);
            font-weight: 400;
            transition: opacity 0.2s;
        }

        .post-content h2:hover .anchor-link,
        .post-content h3:hover .anchor-link,
        .post-content h4:hover .anchor-link {
            opacity: 1;
        }

        /* Theme Toggle */
        .theme-toggle {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            background: var(--primary-color);
            color: white;
            border: none;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.25rem;
            box-shadow: var(--shadow-lg);
            transition: transform 0.2s, background 0.2s;
            z-index: 1000;
        }

        .theme-toggle:hover {
            transform: scale(1.1);
            background: var(--primary-hover);
        }

        /* Comments Section */
        .comments-section {
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-color);
        }

        .comments-section h3 {
            font-size: 1.25rem;
            margin-bottom: 1.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        /* Footer */
        .post-footer {
            margin-top: 3rem;
            padding-top: 1.5rem;
            border-top: 1px solid var(--border-color);
            text-align: center;
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        /* Responsive */
        @media (max-width: 1024px) {
            .toc-sidebar {
                display: none;
            }
            
            .page-wrapper {
                padding: 1.5rem;
            }
        }

        @media (max-width: 768px) {
            .post-header h1 {
                font-size: 1.75rem;
            }
            
            .post-meta {
                flex-direction: column;
                gap: 0.5rem;
            }
            
            .theme-toggle {
                bottom: 1rem;
                right: 1rem;
                width: 44px;
                height: 44px;
            }
        }

        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-secondary);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--border-color);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--text-muted);
        }
    </style>
</head>
<body>
    <div class="page-wrapper">
        <!-- TOC Sidebar -->
        <aside class="toc-sidebar">
            <div class="toc-container">
                <h3><i class="fas fa-list"></i> 目录</h3>
                <div class="toc">
<ul>
<li><a href="#theory-on-mixture-of-experts-in-continual-learning">Theory on Mixture-of-Experts in Continual Learning</a></li>
<li><a href="#0-摘要">0. 摘要</a></li>
<li><a href="#1-引言">1. 引言</a></li>
<li><a href="#2-相关工作">2. 相关工作</a><ul>
<li><a href="#21-连续学习">2.1 连续学习</a></li>
<li><a href="#22-专家混合模型">2.2 专家混合模型</a></li>
<li><a href="#23-moe-在-cl-中的应用">2.3 MoE 在 CL 中的应用</a></li>
</ul>
</li>
<li><a href="#3-问题设置和-moe-模型设计">3. 问题设置和 MoE 模型设计</a><ul>
<li><a href="#31-符号表示">3.1 符号表示</a></li>
<li><a href="#32-线性模型中的-cl">3.2 线性模型中的 CL</a><ul>
<li><a href="#一般设置">一般设置</a></li>
<li><a href="#真实情况和数据集">真实情况和数据集</a></li>
</ul>
</li>
<li><a href="#33-moe-模型的关键设计">3.3 MoE 模型的关键设计</a><ul>
<li><a href="#专家模型">专家模型</a></li>
<li><a href="#门控网络参数">门控网络参数</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#4-moe-训练的理论结果">4. MoE 训练的理论结果</a></li>
<li><a href="#5-遗忘和泛化的理论结果">5. 遗忘和泛化的理论结果</a><ul>
<li><a href="#51-情况-i专家多于任务">5.1 情况 I：专家多于任务</a></li>
<li><a href="#52-情况-ii专家少于任务">5.2 情况 II：专家少于任务</a></li>
</ul>
</li>
<li><a href="#6-实验">6. 实验</a></li>
<li><a href="#7-结论">7. 结论</a></li>
<li><a href="#附录">附录</a><ul>
<li><a href="#a-实验细节和额外实验">A 实验细节和额外实验</a><ul>
<li><a href="#a1-实验计算资源">A.1 实验计算资源</a></li>
<li><a href="#a2-图-2-的实验细节">A.2 图 2 的实验细节</a></li>
<li><a href="#a3-图-3-的实验细节">A.3 图 3 的实验细节</a></li>
<li><a href="#a4-关于终止阈值和负载平衡的实验">A.4 关于终止阈值和负载平衡的实验</a></li>
</ul>
</li>
<li><a href="#b-平滑路由器">B 平滑路由器</a></li>
<li><a href="#c-引理-1-的完整版本和证明">C 引理 1 的完整版本和证明</a></li>
<li><a href="#d-损失函数分析">D 损失函数分析</a></li>
<li><a href="#e-引理-1-的完整版本和证明">E 引理 1 的完整版本和证明</a></li>
<li><a href="#f-引理-2-的完整版本和证明">F 引理 2 的完整版本和证明</a></li>
<li><a href="#f-引理-2-的完整版本和证明_1">F 引理 2 的完整版本和证明</a></li>
<li><a href="#g-引理-3-的完整版本和证明">G 引理 3 的完整版本和证明</a></li>
<li><a href="#h-引理-4-的证明">H 引理 4 的证明</a></li>
<li><a href="#i-定理-1-的证明">I 定理 1 的证明</a></li>
<li><a href="#j-定理-2-的证明">J 定理 2 的证明</a></li>
</ul>
</li>
</ul>
</div>

            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <article>
                <header class="post-header">
                    <a href="../../../index.html" class="back-link">
                        <i class="fas fa-arrow-left"></i> 返回博客列表
                    </a>
                    <h1>Untitled</h1>
                    <div class="post-meta">
                        <span><i class="fas fa-calendar-alt"></i> 2026-01-28</span>
                        <span><i class="fas fa-folder"></i> AINotes/39.CL</span>
                        <span><i class="fas fa-user"></i> Hongwei Zhao</span>
                    </div>
                    <div class="tags">
                        
                    </div>
                </header>

                <div class="post-content">
                    <h2 id="theory-on-mixture-of-experts-in-continual-learning"><a href="http://arxiv.org/abs/2406.16437">Theory on Mixture-of-Experts in Continual Learning</a><a class="anchor-link" href="#theory-on-mixture-of-experts-in-continual-learning" title="Permanent link">&para;</a></h2>
<h2 id="0-摘要">0. 摘要<a class="anchor-link" href="#0-摘要" title="Permanent link">&para;</a></h2>
<p>连续学习（CL）因其能够适应随时间到来的新任务而受到广泛关注。在模型适应新任务时，旧任务的灾难性遗忘（catastrophic forgetting）被确定为 CL 中的一个主要问题。最近，专家混合（MoE）模型已被证明可以通过使用门控网络来稀疏化和分配多个专家之间的不同任务，从而有效减轻 CL 中的灾难性遗忘。然而，对于 MoE 及其对 CL 学习性能影响的理论分析还比较缺乏。本文提供了第一个理论结果，通过过参数化线性回归任务的视角来表征 MoE 在 CL 中的影响。我们通过证明 MoE 模型可以使其专家多样化，专门处理不同任务，同时其路由器学习为每个任务选择正确的专家并在所有专家之间平衡负载，从而确立了 MoE 相对于单一专家的优势。我们的研究进一步表明了一个有趣的事实，即 MoE 在 CL 中需要在足够的训练轮次后终止门控网络的更新以实现系统收敛，这在不考虑连续任务到达的现有 MoE 研究中是不需要的。此外，我们提供了预期遗忘和整体泛化误差的显式表达式，以表征 MoE 在 CL 学习性能中的好处。有趣的是，增加更多专家需要额外的轮次才能收敛，这可能不会增强学习性能。最后，我们在合成数据和真实数据集上进行了实验，将这些来自线性模型的见解扩展到深度神经网络（DNNs），这也为 MoE 在 CL 中的实际算法设计提供了见解。</p>
<h2 id="1-引言">1. 引言<a class="anchor-link" href="#1-引言" title="Permanent link">&para;</a></h2>
<p>连续学习（CL）已成为机器学习中的一个重要范式，其中专家的目标是随时间逐一学习一系列任务。预计专家将利用从旧任务中获得的知识来促进新任务的学习，同时通过从新任务中获得的知识提升旧任务的性能。鉴于 CL 的动态性质，这里的一个主要挑战被称为灾难性遗忘，即专家在学习新任务时可能会在（即，容易忘记）之前的任务上表现不佳，如果任务的数据分布发生大的变化。当单个专家继续服务越来越多的任务时，这成为一个更严重的问题。</p>
<p>最近，稀疏门控的专家混合（MoE）模型在深度学习中取得了惊人的成功，特别是在大型语言模型（LLMs）的发展中。通过自适应地将不同的输入数据路由到多个专家中的一个，MoE 模型中的不同专家将专门掌握数据中的不同知识。因此，有了一些尝试利用 MoE 来减轻 CL 中的遗忘问题，通过训练每个专家处理特定的一组任务。然而，这些研究主要关注实验调查，而对 MoE 及其对 CL 学习性能影响的理论理解仍然缺乏。在本文中，我们的目标是通过提供第一个明确的理论结果来填补这一空白，全面理解 CL 中的 MoE。</p>
<p>为此，我们研究了 CL 中具有 M 个专家的稀疏门控 MoE 模型，通过过参数化线性回归任务的视角。在我们的 CL 设置中，每个回合有一个学习任务到达，其数据集是随机从包含 N 个未知线性模型的共享池中抽取的真实数据生成的。随后，数据被输入到参数化的门控网络中，由该网络引导的 softmax 基路由器将任务路由到 M 个专家中的一个进行模型训练。训练后的 MoE 模型然后通过梯度下降（GD）进一步更新门控网络。之后，新的任务到达，上述过程重复，直到 CL 结束。值得注意的是，分析线性模型是理解深度神经网络（DNNs）性能的重要第一步，正如许多最近研究所显示的。</p>
<p>我们的主要贡献总结如下：</p>
<p>我们提供了第一个理论分析，通过过参数化线性回归任务的视角，理解 CL 中 MoE 的行为。通过使用精心设计的损失函数更新门控网络，我们展示了在 CL 中经过足够的训练轮次（专家探索和路由器学习的顺序为 <span class="math-inline">O(M)</span>）后，MoE 模型将多样化并进入一个平衡的系统状态：每个专家将专门处理特定任务（如果 M &gt; N），或者专门处理一组类似任务（如果 M &lt; N），路由器将始终为每个任务选择正确的专家。另一个有趣的发现是，与不考虑连续任务到达的现有 MoE 研究不同，由于 CL 中任务到达的动态性，更新 MoE 的门控网络是必要的。这将确保学习系统最终收敛到一个稳定的平衡负载状态。</p>
<p>我们提供了预期遗忘和泛化误差的显式表达式，以表征 MoE 对 CL 性能的好处。1）与单一专家情况（M = 1）相比，其中任务由单一发散模型学习，具有多样化专家的 MoE 模型显著提高了学习性能，特别是在任务间数据分布变化大的情况下。2）无论专家更多（M &gt; N）还是更少（M &lt; N），遗忘和泛化误差都收敛到一个小常数。这是因为路由器在 MoE 模型收敛后始终为每个任务选择正确的专家，从长远来看有效地最小化模型误差。3）在 MoE 中，增加更多专家需要额外的探索轮次才能收敛，这不一定增强学习性能。</p>
<p>最后，我们进行了广泛的实验来验证我们的理论结果。具体来说，我们在合成数据上的实验结果不仅支持我们上述的理论发现，而且还表明负载平衡降低了平均泛化误差。这有效地提高了 MoE 模型与不平衡情况相比的能力。更重要的是，对真实数据集的实验表明，我们的理论发现可以进一步超越线性模型扩展到 DNNs，这也为 MoE 在 CL 中的实际算法设计提供了见解。</p>
<h2 id="2-相关工作">2. 相关工作<a class="anchor-link" href="#2-相关工作" title="Permanent link">&para;</a></h2>
<h3 id="21-连续学习">2.1 连续学习<a class="anchor-link" href="#21-连续学习" title="Permanent link">&para;</a></h3>
<p>在过去的十年中，提出了各种实证方法来解决 CL 中的灾难性遗忘问题，这些方法通常分为三类：1）基于正则化的方法，通过在之前任务训练的关键模型参数上引入显式的正则化项来平衡旧任务和新任务。2）基于参数隔离的方法，隔离与不同任务相关的参数以防止参数之间的干扰。3）基于记忆的方法，存储旧任务的数据或梯度信息，并在新任务的训练中重放它们。</p>
<p>另一方面，对 CL 的理论研究非常有限。其中，[36] 和 [37] 引入了 NTK 重叠矩阵来衡量任务相似性，并提出了正交梯度下降方法的变体来解决灾难性遗忘问题。[38] 考虑了一个教师 - 学生框架来检验任务相似性对学习性能的影响。[39] 提出了一个理想的 CL 框架，通过假设所有任务的数据分布为 i.i.d.来实现无遗忘。[19] 提供了过参数化线性模型中不同任务顺序的遗忘界限。进一步，[20] 基于测试误差提供了遗忘和整体泛化误差的显式形式。这些作品共同表明，当后续任务表现出显著多样化时，单一专家的学习性能往往会恶化。相比之下，我们的工作是第一个进行理论分析，以理解多个专家对 CL 的好处。</p>
<h3 id="22-专家混合模型">2.2 专家混合模型<a class="anchor-link" href="#22-专家混合模型" title="Permanent link">&para;</a></h3>
<p>MoE 模型多年来已被广泛研究，以增强深度学习中的模型容量。最近，它在大型语言模型（LLMs）等新兴领域中找到了广泛应用。为了提高训练稳定性和简化 MoE 结构，[10] 提出了稀疏化门控网络的输出。随后，[22] 建议将每个数据样本路由到单个专家而不是多个专家，这降低了计算成本，同时保持了模型质量。[21] 在分类问题的混合设置下，从理论上分析了深度学习中 MoE 的机制。请注意，这项研究侧重于单一任务设置，因此没有分析 CL 中训练一系列任务的动态。相比之下，我们的工作深入研究了 CL 中的 MoE，导致了与 [21] 不同的门控网络训练阶段。此外，我们提供了遗忘和整体泛化误差的显式表达式。</p>
<h3 id="23-moe-在-cl-中的应用">2.3 MoE 在 CL 中的应用<a class="anchor-link" href="#23-moe-在-cl-中的应用" title="Permanent link">&para;</a></h3>
<p>最近，MoE 模型已被应用于减少 CL 中的灾难性遗忘。例如，[12] 集成了多个专家的合作，基于特征表示的加权和来进行预测。[14] 提出通过将数据路由到分布重叠最小的每个专家来多样化专家，然后在任务预测期间结合专家的知识以增强学习稳定性。此外，[15] 将 MoE 应用于扩展视觉 - 语言模型的容量，减轻 CL 中的长期遗忘。然而，这些工作仅关注实证方法，缺乏对 MoE 在 CL 中表现的理论分析。</p>
<h2 id="3-问题设置和-moe-模型设计">3. 问题设置和 MoE 模型设计<a class="anchor-link" href="#3-问题设置和-moe-模型设计" title="Permanent link">&para;</a></h2>
<h3 id="31-符号表示">3.1 符号表示<a class="anchor-link" href="#31-符号表示" title="Permanent link">&para;</a></h3>
<p>对于向量 <span class="math-inline">\mathbf{w}</span>，让 <span class="math-inline">|\mathbf{w}|<em>2</span> 和 <span class="math-inline">|\mathbf{w}|</em>\infty</span> 分别表示其 <span class="math-inline">\ell_2</span> 和 <span class="math-inline">\ell_\infty</span> 范数。对于某些正常数 <span class="math-inline">c_1</span> 和 <span class="math-inline">c_2</span>，我们定义 <span class="math-inline">x = \Omega(y)</span> 如果 <span class="math-inline">x &gt; c_2|y|</span>，<span class="math-inline">x = \Theta(y)</span> 如果 <span class="math-inline">c_1|y| &lt; x &lt; c_2|y|</span>，和 <span class="math-inline">x = O(y)</span> 如果 <span class="math-inline">x &lt; c_1|y|</span>。我们还表示 <span class="math-inline">x = o(y)</span> 如果 <span class="math-inline">x/y \to 0</span>。</p>
<h3 id="32-线性模型中的-cl">3.2 线性模型中的 CL<a class="anchor-link" href="#32-线性模型中的-cl" title="Permanent link">&para;</a></h3>
<h4 id="一般设置">一般设置<a class="anchor-link" href="#一般设置" title="Permanent link">&para;</a></h4>
<p>我们考虑有 T 个训练轮次的 CL 设置。在每个轮次 <span class="math-inline">t \in [T]</span> 中，N 个任务中的一个随机到达，由具有 M 个专家的 MoE 模型学习。对于每个任务，我们考虑拟合一个线性模型 <span class="math-inline">f(X) = X^\top \mathbf{w}</span>，其中真实情况为 <span class="math-inline">\mathbf{w} \in \mathbb{R}^d</span>。然后，对于第 t 个训练轮次中的任务到达，它对应于一个线性回归问题，其中训练数据集由 <span class="math-inline">D_t = (X_t, y_t)</span> 表示。这里 <span class="math-inline">X_t \in \mathbb{R}^{d \times s_t}</span> 是具有 <span class="math-inline">s_t</span> 个样本的特征矩阵，<span class="math-inline">y_t \in \mathbb{R}^{s_t}</span> 是输出向量。在本研究中，我们关注过参数化区域，其中 <span class="math-inline">s_t &lt; d</span>。因此，存在许多可以完美拟合数据的线性模型。</p>
<h4 id="真实情况和数据集">真实情况和数据集<a class="anchor-link" href="#真实情况和数据集" title="Permanent link">&para;</a></h4>
<p>让 <span class="math-inline">W = {w_1, \cdots, w_N}</span> 表示所有 N 个任务的真实向量集合。对于任何两个任务 <span class="math-inline">n, n' \in [N]</span>，我们假设 <span class="math-inline">|w_n - w_{n'}|<em>\infty = O(\sigma_0)</span>，其中 <span class="math-inline">\sigma_0 &gt; 0</span> 表示方差。此外，我们假设任务 <span class="math-inline">n</span> 具有一个独特的特征信号 <span class="math-inline">v_n \in \mathbb{R}^d</span>，且 <span class="math-inline">|v_n|</em>\infty = O(1)</span>。</p>
<p>在每个训练轮次 <span class="math-inline">t \in [T]</span> 中，让 <span class="math-inline">n_t \in [N]</span> 表示当前任务到达的索引，其真实情况为 <span class="math-inline">w_{n_t} \in W</span>。以下，我们正式定义每个训练轮次的数据集生成。</p>
<p><strong>定义 1</strong>. 在每个训练轮次 <span class="math-inline">t \in [T]</span> 的开始，新任务到达 <span class="math-inline">n_t</span> 的数据集 <span class="math-inline">D_t = (X_t, y_t)</span> 是通过以下步骤生成的：</p>
<ol>
<li>从真实池 <span class="math-inline">W</span> 中均匀抽取一个真实情况 <span class="math-inline">w_n</span> 并让 <span class="math-inline">w_{n_t} = w_n</span>。</li>
<li>独立生成一个随机变量 <span class="math-inline">\beta_t \in (0, C]</span>，其中 C 是一个常数，满足 <span class="math-inline">C = O(1)</span>。</li>
<li>生成 <span class="math-inline">X_t</span> 作为 <span class="math-inline">s_t</span> 个样本的集合，其中一个样本由 <span class="math-inline">\beta_t v_{n_t}</span> 给出，其余的 <span class="math-inline">s_t - 1</span> 个样本从正态分布 <span class="math-inline">N(0, \sigma_t^2 I_d)</span> 中抽取，其中 <span class="math-inline">\sigma_t \geq 0</span> 是噪声水平。</li>
<li>生成输出 <span class="math-inline">y_t = X_t^\top w_{n_t}</span>。</li>
</ol>
<p>在任何训练轮次 <span class="math-inline">t \in [T]</span> 中，任务到达 <span class="math-inline">n_t</span> 的实际真实情况 <span class="math-inline">w_{n_t}</span> 是未知的。然而，根据定义 1，任务 <span class="math-inline">n_t</span> 可以根据其特征信号 <span class="math-inline">v_{n_t}</span> 被分类为 N 个簇中的一个。尽管 <span class="math-inline">v_{n_t}</span> 在特征矩阵 <span class="math-inline">X_t</span> 中的位置对于每个任务 <span class="math-inline">n_t</span> 没有指定，我们可以使用 MoE 中的单个门控网络来解决 <span class="math-inline">X_t</span> 上的这个二元分类子问题。在这种情况下，我们旨在研究 MoE 模型是否能增强 CL 中的学习性能。为了方便起见，我们假设 <span class="math-inline">s_t = s</span> 对于所有 <span class="math-inline">t \in [T]</span> 在本文中。然后我们将在以下小节中介绍 MoE 模型。</p>
<h3 id="33-moe-模型的关键设计">3.3 MoE 模型的关键设计<a class="anchor-link" href="#33-moe-模型的关键设计" title="Permanent link">&para;</a></h3>
<h4 id="专家模型">专家模型<a class="anchor-link" href="#专家模型" title="Permanent link">&para;</a></h4>
<p>让 <span class="math-inline">\mathbf{w}^{(m)}<em>t</span> 表示第 t 个训练轮次中专家 m 的模型，其中每个模型从零开始初始化，即 <span class="math-inline">\mathbf{w}^{(m)}_0 = 0</span> 对于任何 <span class="math-inline">m \in [M]</span>。在路由器确定专家 <span class="math-inline">m_t</span> 后，它将数据集 <span class="math-inline">D_t = (X_t, y_t)</span> 传输给该专家以更新 <span class="math-inline">\mathbf{w}^{(m_t)}_t</span>。对于任何其他未被选中的专家 <span class="math-inline">m \in [M]</span>（即 <span class="math-inline">m \neq m_t</span>），其模型 <span class="math-inline">\mathbf{w}^{(m)}_t</span> 保持不变，从 <span class="math-inline">\mathbf{w}^{(m)}</em>{t-1}</span> 开始。在每个轮次 <span class="math-inline">t</span> 中，训练损失由相对于数据集 <span class="math-inline">D_t</span> 的均方误差（MSE）定义：<br />
<div class="math-display"><br />
    L_{\text{tr}}^t(\mathbf{w}^{(m_t)}<em>t, D_t) = \frac{1}{s_t} |(X_t^\top \mathbf{w}^{(m_t)}_t - y_t)|_2^2.<br />
</div><br />
由于我们关注过参数化区域，存在无限多的解决方案可以完美满足 <span class="math-inline">L</em>{\text{tr}}^t(\mathbf{w}^{(m_t)}<em>t, D_t) = 0</span>。在这些解决方案中，从收敛点的前一个专家模型 <span class="math-inline">\mathbf{w}^{(m_t)}</em>{t-1}</span> 开始的梯度下降（GD）提供了一个独特的解决方案，用于最小化 <span class="math-inline">L_{\text{tr}}^t(\mathbf{w}^{(m_t)}<em>t, D_t)</span>，由以下优化问题确定：<br />
<div class="math-display"><br />
    \min</em>{\mathbf{w}} |\mathbf{w} - \mathbf{w}^{(m_t)}<em>{t-1}|_2^2, \text{ s.t. } X_t^\top \mathbf{w} = y_t.<br />
</div><br />
解决上述方程，我们更新 MoE 模型中选定的专家，以应对当前任务到达 <span class="math-inline">n_t</span>，同时保持其他专家不变：<br />
<div class="math-display"><br />
    \mathbf{w}^{(m_t)}_t = \mathbf{w}^{(m_t)}</em>{t-1} + X_t (X_t^\top X_t)^{-1} (y_t - X_t^\top \mathbf{w}^{(m_t)}_{t-1}).<br />
</div></p>
<h4 id="门控网络参数">门控网络参数<a class="anchor-link" href="#门控网络参数" title="Permanent link">&para;</a></h4>
<p>在获得 <span class="math-inline">\mathbf{w}^{(m_t)}<em>t</span> 后，MoE 使用 GD 从 <span class="math-inline">\Theta_t</span> 更新门控网络参数到 <span class="math-inline">\Theta</em>{t+1}</span>，以供下一轮训练使用。一方面，我们希望每个专家 <span class="math-inline">m</span> 的 <span class="math-inline">\theta^{(m)}_t</span> 专门处理特定任务，这有助于减轻由于不同任务的错误路由而导致的学习损失。另一方面，路由器需要在所有专家之间平衡负载，以减少模型过拟合的风险，并增强 CL 中的学习性能。为了实现这一点，我们引入了以下关键设计，用于门控网络更新。</p>
<p><strong>关键设计 I：多目标训练损失</strong>。首先，基于更新的专家模型 <span class="math-inline">\mathbf{w}^{(m)}<em>t</span>，我们提出了以下本地损失函数，用于更新 <span class="math-inline">\Theta_t</span>：<br />
<div class="math-display"><br />
    L</em>{\text{loc}}^t(\Theta_t, D_t) = \sum_{m \in [M]} \pi_m(X_t, \Theta_t) |\mathbf{w}^{(m)}<em>t - \mathbf{w}^{(m)}</em>{t-1}|<em>2^2,<br />
</div><br />
其中 <span class="math-inline">\pi_m(X_t, \Theta_t)</span> 是在方程（2）中定义的 softmax 输出。由于我们设计的本地损失在任务具有相似真实情况时被路由到同一专家 <span class="math-inline">m</span>（例如，<span class="math-inline">\mathbf{w}^{(m)}_t = \mathbf{w}^{(m)}</em>{t-1}</span>）时被最小化，它享有几个好处，如我们后面的理论结果所示：每个专家将专门处理特定任务，这导致专家模型 <span class="math-inline">\mathbf{w}^{(m)}<em>t</span> 快速收敛，CL 的遗忘和泛化误差性能将得到改善。注意，在方程（6）中，我们只需要计算单个专家 <span class="math-inline">m_t</span> 的本地损失，因为对于任何没有更新其模型的专家 <span class="math-inline">m \neq m_t</span>，<span class="math-inline">|\mathbf{w}^{(m)}_t - \mathbf{w}^{(m)}</em>{t-1}|_2^2 = 0</span>，导致计算复杂度低。</p>
<p>除了新颖的本地损失外，我们还遵循现有的 MoE 文献，其中通常定义一个辅助损失来表征专家之间的负载平衡：<br />
<div class="math-display"><br />
    L_{\text{aux}}^t(\Theta_t, D_t) = \alpha \cdot M \cdot \sum_{m \in [M]} f(m)<em>t \cdot P(m)_t,<br />
</div><br />
其中 <span class="math-inline">\alpha</span> 是常数，<span class="math-inline">f(m)_t = \frac{1}{t} \sum</em>{\tau=1}^t 1{m_\tau = m}</span> 是自 <span class="math-inline">t=1</span> 以来分配给专家 m 的任务比例，<span class="math-inline">P(m)<em>t = \frac{1}{t} \sum</em>{\tau=1}^t \pi_m(X_\tau, \Theta_\tau) \cdot 1{m_\tau = m}</span> 是路由器自 <span class="math-inline">t=1</span> 以来选择专家 m 的平均概率。方程（7）中的辅助损失鼓励所有专家的探索，因为它在均匀路由下被最小化，其中 <span class="math-inline">f(m)_t = \frac{1}{M}</span> 和 <span class="math-inline">P(m)_t = \frac{1}{M}</span>。尽管方程（7）中辅助损失的定义并不新颖，但它是必要的，并且在 MoE 模型中为 CL 平衡专家之间的负载中起着至关重要的作用。</p>
<p>基于方程（3），（6）和（7），我们最终定义每个任务到达 <span class="math-inline">n_t</span> 的任务损失如下：<br />
<div class="math-display"><br />
    L_{\text{task}}^t(\Theta_t, \mathbf{w}^{(m_t)}<em>t, D_t) = L</em>{\text{tr}}^t(\mathbf{w}^{(m_t)}<em>t, D_t) + L</em>{\text{loc}}^t(\Theta_t, D_t) + L_{\text{aux}}^t(\Theta_t, D_t).<br />
</div><br />
从初始化 <span class="math-inline">\Theta_0</span> 开始，门控网络基于 GD 更新：<br />
<div class="math-display"><br />
    \theta^{(m)}<em>{t+1} = \theta^{(m)}_t - \eta \cdot \nabla</em>{\theta(m)<em>t} L</em>{\text{task}}^t(\Theta_t, \mathbf{w}^{(m_t)}<em>t, D_t), \forall m \in [M],<br />
</div><br />
其中 <span class="math-inline">\eta &gt; 0</span> 是学习率。注意 <span class="math-inline">\mathbf{w}^{(m_t)}_t</span> 在方程（5）中也是最小化 <span class="math-inline">L</em>{\text{task}}^t(\Theta_t, \mathbf{w}^{(m_t)}<em>t, D_t)</span> 的唯一解。这是因为 <span class="math-inline">L</em>{\text{loc}}^t(\Theta_t, D_t)</span> 和 <span class="math-inline">L_{\text{aux}}^t(\Theta_t, D_t)</span> 都是在更新 <span class="math-inline">\mathbf{w}^{(m_t)}<em>t</span> 之后派生的，使得 <span class="math-inline">L</em>{\text{tr}}^t(\mathbf{w}^{(m_t)}_t, D_t)</span> 成为方程（8）中 <span class="math-inline">\mathbf{w}^{(m_t)}_t</span> 的唯一目标。</p>
<p><strong>关键设计 II：提前终止</strong>。为确保系统在专家之间平衡负载的稳定收敛状态（我们将在第 4 节中从理论上证明），在足够的（即，<span class="math-inline">T_1</span>）轮次专家探索后，我们在算法 1 中引入了提前终止策略，通过评估每个专家 m 的收敛标志 <span class="math-inline">I(m)</span>。</p>
<p>这个标志评估输出间隙，定义为 <span class="math-inline">|h_m(X_t, \theta_t) - h_{m_t}(X_t, \theta_t)|</span>，专家本身和任何选定专家 <span class="math-inline">m_t</span> 之间的差距，对于 <span class="math-inline">t &gt; T_1</span>。如果这个差距超过阈值 <span class="math-inline">\Gamma</span> 对于专家 m，表明门控网络参数 <span class="math-inline">\theta^{(m)}_t</span> 尚未收敛，则 MoE 模型继续根据方程（9）更新所有专家的 <span class="math-inline">\Theta_t</span>。否则，<span class="math-inline">\Theta_t</span> 的更新被永久终止。</p>
<h2 id="4-moe-训练的理论结果">4. MoE 训练的理论结果<a class="anchor-link" href="#4-moe-训练的理论结果" title="Permanent link">&para;</a></h2>
<p>在本节中，我们提供了对算法 1 中专家模型和门控网络训练的理论分析，进一步证明了我们在第 3 节中的关键设计。具体来说，（i）我们首先支持我们的关键设计 I，通过证明在设计的局部损失下，专家模型通过更新 <span class="math-inline">\Theta_t</span> 快速收敛。（ii）然后我们展示了我们的关键设计 II，即在 CL 中提前终止更新 <span class="math-inline">\Theta_t</span> 对于确保稳定收敛系统状态和所有专家之间的平衡负载是必要的。为了清晰起见，我们在本节中研究了 <span class="math-inline">M &gt; N</span> 的情况（结果标记为 <span class="math-inline">M &gt; N</span> 版本），并将结果扩展到 <span class="math-inline">M &lt; N</span> 的情况在附录中。</p>
<p>为了表征专家专业化，我们首先展示每个专家的门控输出由 <span class="math-inline">X_t</span> 的输入特征信号 <span class="math-inline">v_{n_t}</span> 决定。</p>
<p><strong>引理 1（M &gt; N 版本）</strong>：对于任何两个具有相同特征信号 <span class="math-inline">v_n</span> 的特征矩阵 <span class="math-inline">X</span> 和 <span class="math-inline">\tilde{X}</span>，以至少 <span class="math-inline">1 - o(1)</span> 的概率，它们对应的同一专家 <span class="math-inline">m</span> 的门控输出满足<br />
<div class="math-display"><br />
    |h_m(X, \theta^{(m)}<em>t) - h_m(\tilde{X}, \theta^{(m)}_t)| = O(\sigma</em>{1.5}^0).<br />
</div><br />
引理 1 的完整版本和证明在附录 C 中给出。根据引理 1，路由器根据其特征信号 <span class="math-inline">v_{n_t}</span> 决定任务 <span class="math-inline">n_t</span> 的专家 <span class="math-inline">m_t</span>。因此，给定 N 个任务，所有专家可以被分为 N 个集合，根据它们的专业化，即它们的门控参数 <span class="math-inline">\theta^{(m)}<em>t</span> 识别特征信号 <span class="math-inline">v_n</span>，其中每个专家集合定义为：<br />
<div class="math-display"><br />
    M_n = { m \in [M] \mid n = \arg\max</em>{j \in [N]} (\theta^{(m)}_t)^\top v_j }.<br />
</div><br />
以下命题表明，在算法 1 下，经过足够训练轮次后，专家模型在 <span class="math-inline">T_1</span> 后稳定，并保持不变直到结束 <span class="math-inline">T</span>。</p>
<p><strong>命题 1（M &gt; N 版本）</strong>：在算法 1 下，以至少 <span class="math-inline">1 - o(1)</span> 的概率，对于任何 <span class="math-inline">t &gt; T_1</span>，其中 <span class="math-inline">T_1 = \lceil \eta^{-1}M \rceil</span>，每个专家 <span class="math-inline">m \in [M]</span> 在专家集合 <span class="math-inline">M_n</span> 中稳定，并且其专家模型保持不变，满足 <span class="math-inline">\mathbf{w}^{(m)}_{T_1+1} = \cdots = \mathbf{w}^{(m)}_T</span>。</p>
<p>命题 1 的完整版本和证明在附录 E 中给出。命题 1 表明，在 <span class="math-inline">T_1</span> 轮专家探索后，每个专家将专门处理特定任务，通过最小化方程（6）中的局部损失来强制执行。之后，专家模型直到结束 <span class="math-inline">T</span> 保持不变。</p>
<p>接下来，以下命题描述了如果在算法 1 中 MoE 在任何轮次 <span class="math-inline">t</span> 继续更新 <span class="math-inline">\Theta_t</span>，则门控输出的动态。</p>
<p><strong>命题 2（M &gt; N 版本）</strong>：如果 MoE 在任何轮次 <span class="math-inline">t \in [T]</span> 继续通过方程（9）更新 <span class="math-inline">\Theta_t</span>，我们得到：1）在轮次 <span class="math-inline">t_1 = \lceil \eta^{-1}\sigma^{-0.25}<em>0 M \rceil</span>，以下属性成立<br />
<div class="math-display"><br />
    |h_m(X</em>{t_1}, \theta^{(m)}<em>{t_1}) - h</em>{m'}(X_{t_1}, \theta^{(m')}<em>{t_1})| = \begin{cases} O(\sigma</em>{1.75}^0), &amp; \text{if } m, m' \in M_n, \ \Theta(\sigma_{0.75}^0), &amp; \text{otherwise}. \end{cases}<br />
</div><br />
2）在轮次 <span class="math-inline">t_2 = \lceil \eta^{-1}\sigma^{-0.75}<em>0 M \rceil</span>，以下属性成立<br />
<div class="math-display"><br />
    |h_m(X</em>{t_2}, \theta^{(m)}<em>{t_2}) - h</em>{m'}(X_{t_2}, \theta^{(m')}<em>{t_2})| = O(\sigma</em>{1.75}^0), \forall m, m' \in [M].<br />
</div><br />
命题 2 的完整版本和证明在附录 F 中给出。根据命题 2，如果 MoE 在任何轮次 <span class="math-inline">t</span> 继续更新 <span class="math-inline">\Theta_t</span>，同一专家集合中的任意两个专家的门控输出差距在轮次 <span class="math-inline">t_1</span> 收敛到 <span class="math-inline">O(\sigma_{1.75}^0)</span>。相比之下，任何两个在不同专家集合中的专家，它们的输出差距足够大，即 <span class="math-inline">\Theta(\sigma_{0.75}^0)</span>，表明 MoE 在轮次 <span class="math-inline">t_1</span> 成功地将专家多样化到不同的集合中。然而，与单任务学习中的 MoE 可以在专家模型收敛后随时停止训练不同，CL 中的 MoE 需要随着新任务的连续到达，适当更新门控网络和专家。这是必要的，以平衡每个专家上的负载并最大化系统容量利用。然而，继续根据方程（9）更新 <span class="math-inline">\Theta_t</span> 最终将任意两个专家的输出差距减少到 <span class="math-inline">O(\sigma_{1.75}^0)</span>，在训练轮次 <span class="math-inline">t_2</span>，导致路由器为后续任务到达选择错误的专家，并产生额外的训练错误。</p>
<p>基于命题 2，提前终止更新 <span class="math-inline">\Theta_t</span> 是必要的，以保持不同集合中任意两个专家之间的足够大的输出差距，确保专家多样性，如轮次 <span class="math-inline">t_1</span> 的方程（12）。这激发了我们在算法 1 中设计的提前终止，从第 7 行到第 13 行概述。在下一个命题中，我们证明了在算法 1 中提前终止更新 <span class="math-inline">\Theta_t</span> 的好处。</p>
<p><strong>命题 3（M &gt; N 版本）</strong>：在算法 1 下，MoE 从轮次 <span class="math-inline">T_2 = O(\eta^{-1}\sigma^{-0.25}<em>0 M)</span> 开始终止更新 <span class="math-inline">\Theta_t</span>。然后对于任何任务到达 <span class="math-inline">n_t</span> 在 <span class="math-inline">t &gt; T_2</span>，路由器以相同的 <span class="math-inline">\frac{1}{|M</em>{n_t}|}</span> 概率选择任何专家 <span class="math-inline">m \in M_{n_t}</span>，其中 <span class="math-inline">|M_{n_t}|</span> 是集合 <span class="math-inline">M_{n_t}</span> 中的专家数量。</p>
<p>命题 3 的完整版本和证明在附录 G 中给出。根据算法 1，一旦 MoE 终止更新 <span class="math-inline">\Theta_t</span>，方程（1）中的随机噪声 <span class="math-inline">r(m)<em>t</span> 将引导路由器以相同的 <span class="math-inline">\frac{1}{|M</em>{n_t}|}</span> 概率选择同一专家集合中的专家，有效地平衡专家之间的负载。我们的理论分析将通过后面的实验进一步证实。</p>
<h2 id="5-遗忘和泛化的理论结果">5. 遗忘和泛化的理论结果<a class="anchor-link" href="#5-遗忘和泛化的理论结果" title="Permanent link">&para;</a></h2>
<p>对于在第 3 节中描述的 MoE 模型，我们定义 <span class="math-inline">E_t(\mathbf{w}^{(m_t)}<em>t)</span> 为第 t 轮的模型误差：<br />
<div class="math-display"><br />
    E_t(\mathbf{w}^{(m_t)}_t) = |\mathbf{w}^{(m_t)}_t - w</em>{n_t}|_2^2,<br />
</div><br />
它表征了在第 t 轮中，被选中的专家 <span class="math-inline">m_t</span> 与模型 <span class="math-inline">\mathbf{w}^{(m_t)}_t</span> 针对任务 <span class="math-inline">n_t</span> 的泛化性能。根据 CL 领域的现有文献，我们使用遗忘和整体泛化误差的指标来评估 MoE 在 CL 中的表现，定义如下：</p>
<ol>
<li><strong>遗忘</strong>：定义 <span class="math-inline">F_t</span> 为在学习任务 <span class="math-inline">n_t</span> 后遗忘旧任务的程度，对于 <span class="math-inline">t \in {2, \cdots, T}</span>：<br />
<div class="math-display"><br />
    F_t = \frac{1}{t-1} \sum_{\tau=1}^{t-1} (E_\tau(\mathbf{w}^{(m_\tau)}<em>t) - E</em>\tau(\mathbf{w}^{(m_\tau)}_\tau)).<br />
</div></li>
<li><strong>整体泛化误差</strong>：我们通过计算所有任务的平均模型误差来评估最后一轮训练 T 中模型 <span class="math-inline">\mathbf{w}^{(m)}<em>T</span> 的泛化性能：<br />
<div class="math-display"><br />
    G_T = \frac{1}{T} \sum</em>{\tau=1}^T E_\tau(\mathbf{w}^{(m_\tau)}_T).<br />
</div><br />
接下来，我们为使用单一专家（即，<span class="math-inline">M = 1</span>）的学习提供了上述两个指标的显式形式，作为基准。这里我们定义 <span class="math-inline">r := 1 - \frac{s}{d}</span> 为过参数化比率。</li>
</ol>
<p><strong>命题 4</strong>：如果 <span class="math-inline">M = 1</span>，对于任何训练轮次 <span class="math-inline">t \in {2, \cdots, T}</span>，我们有<br />
<div class="math-display"><br />
    E[F_t] = \frac{1}{t-1} \sum_{\tau=1}^{t-1} \left( \frac{r^{t-\tau}}{N} \sum_{n=1}^N |w_n|<em>2^2 + \frac{r^\tau - r^t}{N^2} \sum</em>{n=n'} |w_{n'} - w_n|_2^2 \right),<br />
</div></p>
<p><div class="math-display"><br />
    E[G_T] = \frac{r^T}{N} \sum_{n=1}^N |w_n|<em>2^2 + \frac{1 - r^T}{N^2} \sum</em>{n=n'} |w_{n'} - w_n|<em>2^2.<br />
</div><br />
命题 4 的证明在附录 H 中给出。命题 4 意味着不同任务之间的大模型差距 <span class="math-inline">|w</em>{n'} - w_n|_2^2</span> 导致 <span class="math-inline">E[F_t]</span> 和 <span class="math-inline">E[G_T]</span> 的性能差。</p>
<p>接下来，我们研究了 MoE 在 CL 中的影响，分为两种情况：（I）专家数量多于任务（M &gt; N），和（II）专家数量少于任务（M &lt; N）。通过与命题 4 中的单专家基线比较，MoE 的好处将被表征。</p>
<h3 id="51-情况-i专家多于任务">5.1 情况 I：专家多于任务<a class="anchor-link" href="#51-情况-i专家多于任务" title="Permanent link">&para;</a></h3>
<p>基于命题 1，我们得出以下定理中遗忘和整体泛化误差的显式上界。为了简化符号，我们定义 <span class="math-inline">L(m)_t := t \cdot f(m)_t</span> 作为直到轮次 <span class="math-inline">t</span> 路由到专家 <span class="math-inline">m</span> 的累计任务到达次数，其中 <span class="math-inline">f(m)_t</span> 在方程（7）中给出。</p>
<p><strong>定理 1</strong>：如果 <span class="math-inline">M = \Omega(N \ln(N))</span>，对于每个轮次 <span class="math-inline">t \in {2, \cdots, T_1}</span>，预期遗忘满足<br />
<div class="math-display"><br />
    E[F_t] &lt; \frac{1}{t-1} \sum_{\tau=1}^{t-1} \left( \frac{r^{L(m_\tau)<em>t} - r^{L(m</em>\tau)<em>\tau}}{N} \sum</em>{n=1}^N |w_n|<em>2^2 + \frac{r^{L(m</em>\tau)<em>\tau} - r^{L(m</em>\tau)<em>t}}{N^2} \sum</em>{n=n'} |w_{n'} - w_n|<em>2^2 \right).<br />
</div><br />
对于每个 <span class="math-inline">t \in {T_1 + 1, \cdots, T}</span>，我们有 <span class="math-inline">E[F_t] = \frac{T_1 - 1}{t-1} E[F</em>{T_1}]</span>。进一步，在训练最后一轮 <span class="math-inline">T</span> 的任务 <span class="math-inline">n_T</span> 后，整体泛化误差满足<br />
<div class="math-display"><br />
    E[G_T] &lt; \frac{1}{T} \sum_{\tau=1}^T \left( \frac{r^{L(m_\tau)<em>{T_1}}}{N} \sum</em>{n=1}^N |w_n|<em>2^2 + \frac{1 - r^{L(m</em>\tau)<em>{T_1}}}{N^2} \sum</em>{n=n'} |w_{n'} - w_n|_2^2 \right).<br />
</div><br />
定理 1 的证明在附录 I 中给出，我们有以下见解：</p>
<ol>
<li>
<p><strong>遗忘</strong>：如果 <span class="math-inline">t \leq T_1</span>，在方程（19）中，项<span class="math-inline">\sum_{n=1}^N |w_n|^2</span> 系数<span class="math-inline">r_{t}^{L_{t}^{\left(m_{\tau}\right)}}-r^{L_{\tau}^{\left(m_{\tau}\right)}}</span> 于 0，因为 <span class="math-inline">L(m_\tau)<em>t \geq L(m</em>\tau)<em>\tau</span> 且 <span class="math-inline">r &lt; 1</span>，表明新任务的训练增强了旧任务的性能，由于这一阶段的重复任务到达。与此同时，模型差距 <span class="math-inline">\sum</em>{n=n'} |w_{n'} - w_n|<em>2^2</span> 的系数大于 0，表明遗忘是由于专家探索不同任务。然而，如命题 1 所述，一旦专家模型在 <span class="math-inline">t = T_1</span> 收敛，后续新任务到达的训练不会导致先前任务的遗忘。因此，对于 <span class="math-inline">t \in {T_1 + 1, \cdots, T}</span>，<span class="math-inline">E[F_t] = \frac{T_1 - 1}{t-1} E[F</em>{T_1}]</span> 随着 <span class="math-inline">t</span> 的增加而减少，并随着 <span class="math-inline">T \to \infty</span> 收敛到零。这一趋势表明，与单一专家的振荡遗忘（方程 17）相比，MoE 模型可以显著减少 CL 的预期遗忘。此外，任务相似性的降低，即更大的模型差距，进一步放大了 MoE 模型的学习好处。</p>
</li>
<li>
<p><strong>泛化误差</strong>：注意当任务不太相似且模型差距大时，第二项 <span class="math-inline">\sum_{n=n'} |w_{n'} - w_n|<em>2^2</span> 主导泛化误差。在方程（22）中，模型差距 <span class="math-inline">\sum</em>{n=n'} |w_{n'} - w_n|<em>2^2</span> 的系数 <span class="math-inline">1 - r^{L(m</em>\tau)_{T_1}}</span> 小于方程（18）中的 <span class="math-inline">1 - r^T</span>，由于专家模型在 <span class="math-inline">T_1</span> 后收敛。因此，与单一专家相比，MoE 下的泛化误差减少，特别是随着 <span class="math-inline">T</span> 增加（其中 <span class="math-inline">1 - r^T</span> 接近 1）。</p>
</li>
<li>
<p><strong>专家数量</strong>：根据定理 1，对于 <span class="math-inline">t &gt; T_1</span>，<span class="math-inline">E[F_t]</span> 随着 <span class="math-inline">T_1</span> 的增加而增加，因为额外的专家探索轮次累积了更多的模型误差（方程 19）。关于 <span class="math-inline">E[G_T]</span> 在方程（20）中，更长的专家探索期 <span class="math-inline">T_1</span> 增加了模型差距的系数 <span class="math-inline">1 - r^{L(m_\tau)<em>{T_1}}</span>，导致当任务之间的模型差距大时 <span class="math-inline">E[G_T]</span> 增加。由于 <span class="math-inline">T_1</span> 随着专家数量 <span class="math-inline">M</span> 增加，增加更多专家并不增强学习性能，但延迟了收敛。注意，如果定理 1 中的 <span class="math-inline">M = 1</span>，<span class="math-inline">L(m</em>\tau)_t</span> 变为 <span class="math-inline">t</span> 单一专家，导致方程（19）和方程（20）分别专门化为方程（17）和方程（18）。</p>
</li>
</ol>
<h3 id="52-情况-ii专家少于任务">5.2 情况 II：专家少于任务<a class="anchor-link" href="#52-情况-ii专家少于任务" title="Permanent link">&para;</a></h3>
<p>接下来，我们考虑一个更一般的情况，即专家少于任务，即 <span class="math-inline">M &lt; N</span>，其中算法 1 仍然有效。特别是，我们假设 <span class="math-inline">N</span> 个真实情况在 <span class="math-inline">W</span> 中可以根据任务相似性被分类为 <span class="math-inline">K</span> 个簇，其中 <span class="math-inline">K &lt; M</span>。让 <span class="math-inline">W_k</span> 表示第 <span class="math-inline">k</span> 个任务簇。对于任何两个在同一个簇 <span class="math-inline">W_k</span> 中的任务 <span class="math-inline">n, n'</span>，我们假设 <span class="math-inline">|w_n - w_{n'}|<em>\infty = O(\sigma</em>{1.5}^0)</span>。然后我们让集合 <span class="math-inline">M_k</span> 包括所有专门处理 <span class="math-inline">k</span>- 簇任务的专家。</p>
<p>回想命题 1 表明，如果 <span class="math-inline">M &gt; N</span>，专家模型在 <span class="math-inline">T_1</span> 轮探索后收敛。然而，在专家少于任务（<span class="math-inline">M &lt; N</span>）的情况下，每个专家必须专门处理学习一组类似的任务。因此，随着相似任务持续路由到每个专家，专家模型在 <span class="math-inline">T_1</span> 轮后继续更新，与命题 1 中的 <span class="math-inline">M &gt; N</span> 情况不同。基于上述理解，我们有以下定理。</p>
<p><strong>定理 2</strong>：如果 <span class="math-inline">M &lt; N</span> 且 <span class="math-inline">M = \Omega(K \ln(K))</span>，对于任何 <span class="math-inline">t \in {1, \cdots, T_1}</span>，预期遗忘 <span class="math-inline">E[F_t]</span> 与方程（19）相同。而对于一些 <span class="math-inline">t \in {T_1 + 1, \cdots, T}</span>，预期遗忘满足<br />
<div class="math-display"><br />
E[F_t] &lt; \frac{1}{t-1} \sum_{\tau=1}^{t-1} \left( \frac{r^{L(m_\tau)<em>t} - r^{L(m</em>\tau)<em>\tau}}{N} \sum</em>{n=1}^N |w_n|<em>2^2 + \frac{1}{t-1} \sum</em>{\tau=1}^{T_1} \frac{r^{L(m_\tau)<em>i} - r^{L(m</em>\tau)<em>{T_1}}}{N^2} \sum</em>{n=n'} |w_{n'} - w_n|<em>2^2 \<br />
    + \Psi_1 \frac{1}{t-1} \sum</em>{\tau=T_1+1}^t (1 - r^{L(m_\tau)<em>t} - L(m</em>\tau)<em>\tau) r^{L(m</em>\tau)<em>t} - L(m</em>\tau)<em>{T_1} -1, \right)<br />
</div><br />
其中 <span class="math-inline">\Psi_1 = \frac{1}{N} \sum</em>{n=1}^N \frac{1}{|W_k|} \sum_{n' \in W_k} |w_{n'} - w_n|<em>2^2</span> 是同一任务簇中任意两个任务之间的预期模型差距。在训练最后一轮 <span class="math-inline">T</span> 的任务 <span class="math-inline">n_T</span> 后，整体泛化误差满足<br />
<div class="math-display"><br />
E[G_T] &lt; \frac{1}{T} \sum</em>{\tau=1}^T r^{L(m_\tau)<em>T} \left( \frac{1}{N} \sum</em>{n=1}^N |w_n|<em>2^2 + \frac{1}{T_1} \sum</em>{\tau=1}^{T_1} r^{L(m_\tau)<em>T - L(m</em>\tau)<em>{T_1}} (1 - r^{L(m</em>\tau)<em>{T_1}}) \frac{1}{N^2} \sum</em>{n=n'} |w_{n'} - w_n|<em>2^2 \right) \+ \Psi_2 \frac{1}{T} \sum</em>{\tau=T_1+1}^T (1 - r^{L(m_\tau)<em>T - L(m</em>\tau)<em>{T_1}}) +  \Psi_1 \frac{1}{T} \sum</em>{\tau=T_1+1}^T (1 - r^{L(m_\tau)<em>T - L(m</em>\tau)<em>{T_1}}),<br />
</div><br />
其中 <span class="math-inline">\Psi_2 = \frac{1}{N} \sum</em>{n=1}^N \frac{1}{K} \sum_{k=1}^K \frac{1}{|W_k|} \sum_{n' \in W_k} |w_{n'} - w_n|_2^2</span> 是随机选择的任务 <span class="math-inline">W</span> 中的任何任务与固定真实情况簇 <span class="math-inline">W_k</span> 中的任何任务之间的预期模型差距。</p>
<p>定理 2 的证明在附录 J 中给出，我们提供以下见解：</p>
<ol>
<li>
<p><strong>遗忘</strong>：与定理 1 相比，方程（21）中的 <span class="math-inline">E[F_t]</span> 引入了一个额外的项 <span class="math-inline">\Psi_1</span>，它测量了在 <span class="math-inline">{T_1 + 1, \cdots, \tau}</span> 期间由于新任务到达而更新专家模型导致的遗忘。然而，由于在 <span class="math-inline">{T_1 + 1, \cdots, T}</span> 期间路由到同一专家的任务属于同一簇，它们的小模型差距导致最小的遗忘。如果每个簇中只有一个任务，<span class="math-inline">\Psi</span> 变为 0，且 <span class="math-inline">E[F_t]</span> 与方程（19）相同。</p>
</li>
<li>
<p><strong>泛化误差</strong>：由于专家模型在任何 <span class="math-inline">t</span> 持续更新，方程（22）中的 <span class="math-inline">E[G_T]</span> 包括三个项：a) <span class="math-inline">t &lt; T_1</span> 时任意两个随机任务到达之间的预期模型差距 <span class="math-inline">\frac{1}{N^2} \sum_{n=n'} |w_{n'} - w_n|<em>2^2</span>，b) <span class="math-inline">t &gt; T_1</span> 时同一簇中两个任务之间的预期模型差距 <span class="math-inline">\Psi_1</span>，以及 c) <span class="math-inline">t &lt; T_1</span> 时随机任务到达与固定真实情况簇 <span class="math-inline">W_k</span> 中的任何任务之间的预期模型差距 <span class="math-inline">\Psi_2</span>。如果每个簇中只包含一个任务，<span class="math-inline">\Psi_2</span> 的系数简化为 <span class="math-inline">\sum</em>{\tau=T_1+1}^T (1 - r^{L(m_\tau)<em>{T_1}})</span>，给定 <span class="math-inline">L(m</em>\tau)<em>T = L(m</em>\tau)<em>{T_1}</span> 在 <span class="math-inline">T_1</span> 后没有更新。此外，<span class="math-inline">\Psi_2 = \frac{1}{N^2} \sum</em>{n=n'} |w_{n'} - w_n|_2^2</span> 且 <span class="math-inline">\Psi_1 = 0</span>，导致方程（22）专门化为方程（20）。</p>
</li>
</ol>
<p>注意，根据我们的假设 <span class="math-inline">|w_n - w_{n'}|<em>\infty = O(\sigma</em>{1.5}^0)</span>，路由器无法区分同一簇中的相似任务 <span class="math-inline">n</span> 和 <span class="math-inline">n'</span>。因此，增加更多专家不能避免方程（21）和方程（22）中的误差 <span class="math-inline">\Psi_1</span> 和 <span class="math-inline">\Psi_2</span>。因此，与定理 1 类似，当有足够的专家比簇多时（即，<span class="math-inline">M = \Omega(K \ln(K))</span>），增加更多专家并不增强学习性能，但延迟了收敛。尽管与定理 1 相比学习性能有所下降，但它仍然从 MoE 中受益，与命题 4 中的单一专家相比。</p>
<p>注意，如果我们扩展到方程（1）中的 top-k 路由策略，路由器将选择 k 个专家同时训练相同的数据。在这种情况下，方程（21）中描述的遗忘可能会减少，因为每个专家可能处理比 top-1 路由策略中更小的任务簇。然而，现在属于同一簇的相似任务可能被划分为不同的簇，并由不同的专家处理，这可能会减少这些任务之间的潜在正向知识转移。因此，top-k 情况下的泛化误差可能不比 top-1 情况小。</p>
<h2 id="6-实验">6. 实验<a class="anchor-link" href="#6-实验" title="Permanent link">&para;</a></h2>
<p>在本节中，我们对线性模型和 DNNs 进行了广泛的实验，以支持我们的理论分析。与大多数 CL 理论研究相比，我们的工作已经包括了更全面的实验调查。由于空间限制，我们将实验细节和关于终止阈值和负载平衡影响的额外实验委托给附录 A。</p>
<p><strong>关键设计的提前终止</strong>。在第一个实验中，我们旨在检查算法 1 中终止 <span class="math-inline">\Theta_t</span> 更新的必要性。这里我们设置 <span class="math-inline">T = 2000</span>，<span class="math-inline">N = 7</span>，<span class="math-inline">K = 3</span> 并变化专家数量 <span class="math-inline">M \in {1, 5, 10, 20}</span>。如图 2(a) 和图 2(c) 所示，MoE 模型的遗忘和泛化误差首先由于专家探索而增加，然后在终止更新的情况下对所有 MoE 模型收敛到几乎零，验证了定理 1 和定理 2。与此形成鲜明对比的是，图 2(b) 和图 2(d) 中没有终止的学习导致性能差且大振荡，因为路由器在持续更新 <span class="math-inline">\Theta_t</span> 后为新任务到达选择错误的专家。此外，在图 2(a) 和图 2(c) 中，MoE 模型与单一模型相比显著提高了 CL 的性能。M = 10 和 M = 20 之间的比较也表明，如果 <span class="math-inline">M &gt; N</span>，增加额外的专家会延迟收敛，而不会改善学习性能，验证了我们在定理 1 和定理 2 中的分析。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20241230153617.png" style="zoom: 80%;" /></div>

<p><strong>真实数据验证</strong>。最后，我们将算法 1 和线性模型的见解扩展到 DNNs，通过在 MNIST 数据集上进行实验。我们设置 <span class="math-inline">N = 3</span> 并变化 <span class="math-inline">M \in {1, 4, 7}</span>。在每个训练轮次中，我们通过平均 s = 100 个训练样本获得特征矩阵。为了使不同任务的模型差距多样化，我们将 d×d 矩阵转换为 d×d 维的归一化向量，作为门控网络的输入。然后我们计算输入向量中每个元素的方差 <span class="math-inline">\sigma_0</span>，这将用于算法 1 中的参数设置。图 3 显示，我们从线性模型中的理论见解也适用于 DNNs，就 MoE 和提前终止对 CL 性能的影响而言。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20241230153628.png" style="zoom: 80%;" /></div>

<h2 id="7-结论">7. 结论<a class="anchor-link" href="#7-结论" title="Permanent link">&para;</a></h2>
<p>在这项工作中，我们进行了 MoE 及其对 CL 学习性能影响的首次理论分析，重点关注过参数化线性回归问题。我们证明了 MoE 模型可以使专家多样化，专门处理不同任务，同时其路由器可以学习为每个任务选择正确的专家并在所有专家之间平衡负载。然后我们展示了，在 CL 中，终止门控网络参数更新在足够的训练轮次后是系统收敛的必要条件。此外，我们提供了预期遗忘和整体泛化误差的显式形式，以评估 MoE 的影响。最后，我们在真实数据集上使用 DNNs 进行实验，表明某些见解可以超越线性模型。</p>
<h2 id="附录">附录<a class="anchor-link" href="#附录" title="Permanent link">&para;</a></h2>
<h3 id="a-实验细节和额外实验">A 实验细节和额外实验<a class="anchor-link" href="#a-实验细节和额外实验" title="Permanent link">&para;</a></h3>
<h4 id="a1-实验计算资源">A.1 实验计算资源<a class="anchor-link" href="#a1-实验计算资源" title="Permanent link">&para;</a></h4>
<ul>
<li>操作系统：macOS Sonoma 14.2.1。</li>
<li>CPU 类型：2.6 GHz 6-Core Intel Core i7。</li>
<li>GPU 类型：Intel UHD Graphics 630 1536 MB。</li>
<li>内存：16 GB 2667 MHz DDR4。</li>
<li>执行时间：图 2 为 40 分钟，图 3 为 100 分钟，图 4 和图 5 为 80 分钟。</li>
</ul>
<h4 id="a2-图-2-的实验细节">A.2 图 2 的实验细节<a class="anchor-link" href="#a2-图-2-的实验细节" title="Permanent link">&para;</a></h4>
<ul>
<li>合成数据生成：我们首先生成 N 个真实情况及其对应的特征信号。对于每个真实情况 <span class="math-inline">w_n \in \mathbb{R}^d</span>，其中 <span class="math-inline">n \in [N]</span>，我们通过正态分布 <span class="math-inline">N(0, \sigma_0)</span> 随机生成 d 个元素。然后通过常数缩放这些真实情况以获得它们的特征信号 <span class="math-inline">v_n</span>。在每个训练轮次 <span class="math-inline">t</span> 中，我们根据真实情况池 <span class="math-inline">W</span> 和特征信号生成 <span class="math-inline">(X_t, y_t)</span>。具体来说，在抽取 <span class="math-inline">w_{n_t}</span> 后，对于 <span class="math-inline">X_t \in \mathbb{R}^{d \times s}</span>，我们随机选择一个 <span class="math-inline">s</span> 样本填充 <span class="math-inline">v_{n_t}</span>。其余 <span class="math-inline">s - 1</span> 个样本从 <span class="math-inline">N(0, \sigma_t^2 I_d)</span> 中生成。最后，我们计算输出 <span class="math-inline">y_t = X_t^\top w_{n_t}</span>。这里我们设置 <span class="math-inline">\sigma_0 = 0.4</span>，<span class="math-inline">\sigma_t = 0.1</span>，<span class="math-inline">d = 10</span> 和 <span class="math-inline">s = 6</span>。在图 2 中，我们设置 <span class="math-inline">\eta = 0.5</span>，<span class="math-inline">\alpha = 0.5</span> 和 <span class="math-inline">\lambda = 0.3</span>。</li>
</ul>
<h4 id="a3-图-3-的实验细节">A.3 图 3 的实验细节<a class="anchor-link" href="#a3-图-3-的实验细节" title="Permanent link">&para;</a></h4>
<ul>
<li>数据集：我们使用 MNIST 数据集，每个训练轮次随机选择 100 个样本进行训练，1000 个样本用于测试。</li>
<li>DNN 架构和训练细节：我们使用一个包含两个卷积层和三个全连接层的五层神经网络。第一个卷积层后跟一个步长为 2 的 2D 最大池化操作。每个任务使用 SGD 进行学习，学习率为 0.2，训练 600 个周期。遗忘和整体泛化误差分别按照方程（15）和方程（16）进行评估。这里，<span class="math-inline">E_t(\mathbf{w}^{(m_t)}_t)</span> 定义为均方测试误差，而不是方程（14）。</li>
</ul>
<h4 id="a4-关于终止阈值和负载平衡的实验">A.4 关于终止阈值和负载平衡的实验<a class="anchor-link" href="#a4-关于终止阈值和负载平衡的实验" title="Permanent link">&para;</a></h4>
<p>在额外的实验中，我们变化终止阈值 <span class="math-inline">\Gamma \in {\sigma_{0.75}^0, \sigma_0, \sigma_{1.25}^0, \sigma_{1.5}^0}</span> 来研究其对负载平衡和学习性能的影响，使用与图 2 相同的合成数据生成方法。</p>
<p>最初，我们设置 <span class="math-inline">\sigma_0 = 0.4</span>，<span class="math-inline">\lambda = \sigma_{1.25}^0</span>，<span class="math-inline">M = 5</span>，和 <span class="math-inline">N = 6</span> 与 <span class="math-inline">K = 3</span> 任务簇：<span class="math-inline">W_1 = {1, 4}</span>，<span class="math-inline">W_2 = {2, 5}</span>，和 <span class="math-inline">W_3 = {3, 6}</span>。图 4 展示了每轮任务到达及其路由专家的记录。图 4(a) 和图 4(b) 展示了如果 MoE 模型基于 <span class="math-inline">\Gamma &gt; \lambda</span> 终止更新，小噪声 <span class="math-inline">r(m)_t</span> 不能改变路由器对于每个任务簇（例如，专家 5 对于 <span class="math-inline">W_2 = {2, 5}</span>）在方程（1）中的决策，导致专家负载不平衡。而对于 <span class="math-inline">\Gamma \leq \lambda</span> 的图 4(c) 和图 4(d)，方程（1）中的随机噪声 <span class="math-inline">r(m)_t</span> 可以减少同一专家集中专家之间的门控输出差距，确保负载平衡（例如，专家 3 和 4 对于 <span class="math-inline">W_2 = {2, 5}</span>）。</p>
<p>为了进一步检查负载平衡如何影响学习性能，我们将任务数量增加到 <span class="math-inline">N = 30</span>。我们重复实验 100 次，并在图 5 中绘制平均遗忘和泛化误差。图 5(a) 展示了遗忘对于广泛的 <span class="math-inline">\Gamma</span> 是稳健的，这是由于专家模型在 <span class="math-inline">T_1</span> 轮探索后收敛。然而，图 5(b) 显示 <span class="math-inline">\Gamma \in {\sigma_{1.25}^0, \sigma_{1.5}^0}</span> 下的平衡负载导致较小的泛化误差，与 <span class="math-inline">\Gamma \in {\sigma_{0.75}^0, \sigma_0}</span> 下的不平衡负载相比。这是因为多样化的专家模型有助于减少与单一过拟合模型相比的模型误差。</p>
<h3 id="b-平滑路由器">B 平滑路由器<a class="anchor-link" href="#b-平滑路由器" title="Permanent link">&para;</a></h3>
<p>我们首先证明方程（1）确保了不同路由行为之间的平滑过渡，这使得路由器更加稳定。假设有两个不同的数据集 <span class="math-inline">(X, y)</span> 和 <span class="math-inline">(\hat{X}, \hat{y})</span> 同时作为 MoE 的输入。让 <span class="math-inline">h</span> 和 <span class="math-inline">\hat{h}</span> 分别表示两个数据集对应的门控网络输出。用 <span class="math-inline">p</span> 和 <span class="math-inline">\hat{p}</span> 表示概率向量，它们告诉每个专家为这两个数据集被路由的概率。例如，<span class="math-inline">p_m = P(\arg\max_{m'\in[M]}{h_{m'} + r(m')} = m)</span> 和 <span class="math-inline">\hat{p}<em>m = P(\arg\max</em>{m'\in[M]}{\hat{h}_{m'} + r(m')} = m)</span> 根据方程（1）。然后我们提出以下引理来证明平滑路由器。</p>
<p><strong>引理 2</strong>：两个概率向量满足 <span class="math-inline">|p - \hat{p}|<em>\infty \leq \frac{\lambda}{M^2} |h - \hat{h}|</em>\infty</span>。</p>
<p>证明：设 <span class="math-inline">m_1 = \arg\max_m {h_m + r(m)}</span> 和 <span class="math-inline">m_2 = \arg\max_m {\hat{h}<em>m + r(m)}</span>。我们首先考虑 <span class="math-inline">m_1 \neq m_2</span> 的情况。在这种情况下，我们有 <span class="math-inline">h</em>{m_1} + r(m_1) \geq h_{m_2} + r(m_2)</span>，<span class="math-inline">\hat{h}<em>{m_2} + r(m_2) \geq \hat{h}</em>{m_1} + r(m_1)</span>，这意味着 <span class="math-inline">\hat{h}<em>{m_2} - \hat{h}</em>{m_1} &gt; r(m_1) - r(m_2) \geq h_{m_2} - h_{m_1}</span>。</p>
<p>定义 <span class="math-inline">C(m_1, m_2) = \frac{\hat{h}<em>{m_2} - \hat{h}</em>{m_1} + h_{m_2} - h_{m_1}}{2}</span>。根据上述不等式，我们得到<br />
<div class="math-display"><br />
    |r(m_1) - r(m_2) - C(m_1, m_2)| \leq \frac{\hat{h}<em>{m_2} - \hat{h}</em>{m_1} - h_{m_2} + h_{m_1}}{2} \leq |\hat{h} - h|<em>\infty.<br />
</div><br />
因此，我们计算<br />
<div class="math-display"><br />
    P(\arg\max_m {h_m + r(m)} \neq \arg\max_m {\hat{h}_m + r(m)}) \leq P(\exists m_1 \neq m_2 \in [M], \text{ s.t. } |r(m_1) - r(m_2) - C(m_1, m_2)| \leq |\hat{h} - h|</em>\infty) \leq \sum_{m_1 &lt; m_2} E[P(r(m_2) + C(m_1, m_2) - |\hat{h} - h|<em>\infty \leq r(m_1) \leq r(m_2) + C(m_1, m_2) + |\hat{h} - h|</em>\infty)|r(m_2)|] \leq \lambda M^2 |\hat{h} - h|_\infty,<br />
</div><br />
其中第一个不等式由上述不等式得出，第二个不等式是由于并集界限，最后一个不等式是因为 <span class="math-inline">r(m)</span> 是从 Unif [0, <span class="math-inline">\lambda</span>] 中抽取的。</p>
<p>然后对于任何 <span class="math-inline">j \in [M]</span>，我们有<br />
<div class="math-display"><br />
    |\hat{p}<em>i - p_i| \leq E[|1(\arg\max_m {\hat{h}_m + r(m) = i}) - 1(\arg\max_m {\hat{h}_m + r(m) = i})|] \leq E[P(\arg\max_m {h_m + r(m)} \neq \arg\max_m {\hat{h}_m + r(m)})] \leq M^2 |\hat{h} - h|</em>\infty.<br />
</div><br />
这完成了引理 2 的证明。</p>
<p>直观上，引理 2 告诉我们，如果两个任务的数据集 <span class="math-inline">(X, y)</span> 和 <span class="math-inline">(\hat{X}, \hat{y})</span> 由相同的真实情况生成，那么这两个概率向量在引理 2 中满足 <span class="math-inline">|p - \hat{p}|<em>\infty = O(\lambda M^2 \sigma</em>{1.5}^0)</span>。</p>
<h3 id="c-引理-1-的完整版本和证明">C 引理 1 的完整版本和证明<a class="anchor-link" href="#c-引理-1-的完整版本和证明" title="Permanent link">&para;</a></h3>
<p><strong>引理 1（完整版本）</strong>：对于任何两个具有特征信号 <span class="math-inline">v_n</span> 和 <span class="math-inline">v'<em>n</span> 的特征矩阵 <span class="math-inline">X</span> 和 <span class="math-inline">\tilde{X}</span>，如果 <span class="math-inline">w_n = w</em>{n'}</span> 在 <span class="math-inline">M &gt; N</span> 下，或 <span class="math-inline">w_n, w_{n'} \in W_k</span> 在 <span class="math-inline">M &lt; N</span> 下，以至少 <span class="math-inline">1 - o(1)</span> 的概率，它们对应的同一专家 <span class="math-inline">m</span> 的门控输出满足<br />
<div class="math-display"><br />
    |h_m(X, \theta^{(m)}<em>t) - h_m(\tilde{X}, \theta^{(m)}_t)| = O(\sigma</em>{1.5}^0).<br />
</div><br />
证明：我们首先关注 <span class="math-inline">M &gt; N</span> 情况来证明引理 1。然后我们考虑 <span class="math-inline">M &lt; N</span> 情况来证明引理 1。对于每轮 <span class="math-inline">t</span> 根据定义 1 生成的数据集 <span class="math-inline">(X_t, y_t)</span>，我们可以假设 <span class="math-inline">X_t</span> 的第一个样本是信号向量。因此，我们重写 <span class="math-inline">X_t = [\beta_t v_n, X_{t,2}, \cdots, X_{t,s}]</span>。让 <span class="math-inline">\tilde{X}<em>t = [\beta_t v</em>{n_t}, 0, \cdots, 0]</span> 表示只保留特征信号的矩阵。</p>
<p>根据第 3 节中门控网络的定义，我们有 <span class="math-inline">h_m(X_t, \theta^{(m)}<em>t) = \sum</em>{i=1}^s (\theta^{(m)}<em>t)^\top X</em>{t,i}</span>。然后我们计算<br />
<div class="math-display"><br />
    |h_m(X_t, \theta^{(m)}<em>t) - h_m(\tilde{X}_t, \theta^{(m)}_t)| = \left| \sum</em>{i=2}^s (\theta^{(m)}<em>t)^\top X</em>{t,i} \right| \leq \max_{t,j} {\theta^{(m)}<em>{t,j}} \cdot \left| \sum</em>{i=2}^s \sum_{j=1}^d X_{t,(i,j)} \right|,<br />
</div><br />
其中 <span class="math-inline">\theta^{(m)}<em>{t,j}</span> 是向量 <span class="math-inline">\theta^{(m)}_t</span> 的第 j 个元素，<span class="math-inline">X</em>{t,(i,j)}</span> 是矩阵 <span class="math-inline">X_t</span> 的第 <span class="math-inline">(i, j)</span> 个元素。</p>
<p>然后我们应用 Hoeffding 不等式来获得<br />
<div class="math-display"><br />
    P\left( \left| \sum_{i=2}^s \sum_{j=1}^d X_{t,(i,j)} \right| &lt; s \cdot d \cdot \sigma_0 \right) \geq 1 - 2 \exp \left( -\frac{\sigma_0^2 s^2 d^2}{|X_{t,i}|<em>\infty} \right).<br />
</div><br />
由于 <span class="math-inline">X</em>{t,(i,j)} \sim N(0, \sigma_t^2)</span>，我们有 <span class="math-inline">|X_{t,i}|<em>\infty = O(\sigma_t)</span>，表明 <span class="math-inline">\exp \left( -\frac{\sigma_0^2 s^2 d^2}{|X</em>{t,i}|<em>\infty} \right) = o(1)</span>。因此，以至少 <span class="math-inline">1 - o(1)</span> 的概率，我们有 <span class="math-inline">\left| \sum</em>{i=2}^s \sum_{j=1}^d X_{t,(i,j)} \right| = O(\sigma_0)</span>。因此，我们得到<br />
<div class="math-display"><br />
    |h_m(X_t, \theta^{(m)}<em>t) - h_m(\tilde{X}_t, \theta^{(m)}_t)| = O(\sigma</em>{1.5}^0)<br />
</div><br />
由于 <span class="math-inline">\left| \sum_{i=2}^s \sum_{j=1}^d X_{t,(i,j)} \right| = O(\sigma_0)</span> 和 <span class="math-inline">\theta^{(m)}<em>{t,j} = O(\sigma</em>{0.5}^0)</span> 在稍后证明的引理 6 中。</p>
<p>如果 <span class="math-inline">M &lt; N</span>，我们计算<br />
<div class="math-display"><br />
    |h_m(X_t, \theta^{(m)}<em>t) - h_m(\tilde{X}_t, \theta^{(m)}_t)| = |(\theta^{(m)}_t)^\top \sum</em>{i=1}^s X_{t,i}| \leq |(\theta^{(m)}<em>t)^\top \sum</em>{i=2}^s X_{t,i}| + |(\theta^{(m)}<em>t)^\top (v_n - v'_n)| \leq \max</em>{t,j} {\theta^{(m)}<em>{t,j}} \cdot \left| \sum</em>{i=2}^s \sum_{j=1}^d X_{t,(i,j)} \right| + O(\sigma_0^2) = O(\sigma_{1.5}^0),<br />
</div><br />
其中第二个不等式是基于并集界限，第三个不等式是因为 <span class="math-inline">\theta^{(m)}<em>{t,j} = O(\sigma</em>{0.5}^0)</span> 和我们的假设 <span class="math-inline">|v_n - v'<em>n|</em>\infty = O(\sigma_{1.5}^0)</span>，最后一个不等式是基于我们上述 <span class="math-inline">M &gt; N</span> 情况的证明。这完成了引理 1 的完整证明。</p>
<p>基于引理 1，我们重新审视引理 2，得到以下结论，关于平滑路由器。</p>
<p><strong>推论 1</strong>：如果数据集 <span class="math-inline">(X, y)</span> 和 <span class="math-inline">(\hat{X}, \hat{y})</span> 由相同的真实情况生成，那么引理 2 中的两个概率向量满足 <span class="math-inline">|p - \hat{p}|<em>\infty = O(\lambda M^2 \sigma</em>{1.5}^0)</span>。</p>
<h3 id="d-损失函数分析">D 损失函数分析<a class="anchor-link" href="#d-损失函数分析" title="Permanent link">&para;</a></h3>
<p>在分析 MoE 之前，我们分析了门控网络参数和专家模型的损失函数。</p>
<p><strong>引理 3</strong>：根据方程（5）的更新规则，如果当前任务 <span class="math-inline">n_t</span> 路由到专家 <span class="math-inline">m_t</span> 与上一个任务 <span class="math-inline">n_\tau</span> 路由到专家 <span class="math-inline">m_t</span> 有相同的真实情况，即 <span class="math-inline">w_{n_t} = w_{n_\tau}</span>，那么专家 <span class="math-inline">m_t</span> 的模型满足 <span class="math-inline">\mathbf{w}^{(m_t)}<em>t = \mathbf{w}^{(m_t)}</em>{t-1} = \cdots = \mathbf{w}^{(m_t)}_\tau</span>。</p>
<p>引理 3 很容易通过方程（5）的更新规则证明，因此我们在这里省略证明。</p>
<p>接下来，我们检查门控网络参数的训练。</p>
<p><strong>引理 4</strong>：对于任何训练轮次 <span class="math-inline">t \geq 1</span>，我们有 <span class="math-inline">\sum_{m=1}^M \nabla_{\theta(m)<em>t} L</em>{\text{task}}^t = 0</span>。</p>
<p>证明：由于训练损失 <span class="math-inline">\nabla_{\theta(m)} L_{\text{tr}}^t = 0</span>，我们得到 <span class="math-inline">\nabla_{\theta(m)<em>t} L</em>{\text{task}}^t = \nabla_{\theta(m)<em>t} L</em>{\text{loc}}^t + \nabla_{\theta(m)<em>t} L</em>{\text{aux}}^t</span>。接下来，我们将分别证明 <span class="math-inline">\sum_{m=1}^M \nabla_{\theta(m)<em>t} L</em>{\text{loc}}^t = 0</span> 和 <span class="math-inline">\sum_{m=1}^M \nabla_{\theta(m)<em>t} L</em>{\text{aux}}^t = 0</span>。</p>
<p>根据局部损失的定义（方程 6），我们计算<br />
<div class="math-display"><br />
    \nabla_{\theta(m)<em>t} L</em>{\text{loc}}^t = \frac{\partial \pi_{m_t}(X_t, \Theta_t)}{\partial \theta(m)<em>t} |\mathbf{w}^{(m_t)}_t - \mathbf{w}^{(m_t)}</em>{t-1}|<em>2^2.<br />
</div><br />
如果 <span class="math-inline">m = m_t</span>，我们得到 <span class="math-inline">\frac{\partial \pi</em>{m_t}(X_t, \Theta_t)}{\partial \theta(m)<em>t} = \pi</em>{m_t}(X_t, \Theta_t) \cdot \left( \sum_{m' \neq m_t} \pi_{m'}(X_t, \Theta_t) \right) \cdot \sum_{i \in [s_t]} X_{t,i}</span>。</p>
<p>如果 <span class="math-inline">m \neq m_t</span>，我们得到 <span class="math-inline">\frac{\partial \pi_{m_t}(X_t, \Theta_t)}{\partial \theta(m)<em>t} = -\pi</em>{m_t}(X_t, \Theta_t) \cdot \pi_m(X_t, \Theta_t) \cdot \sum_{i \in [s_t]} X_{t,i}</span>。</p>
<p>基于上述，我们得到 <span class="math-inline">\sum_{m=1}^M \nabla_{\theta(m)<em>t} L</em>{\text{loc}}^t = |\mathbf{w}^{(m_t)}<em>t - \mathbf{w}^{(m_t)}</em>{t-1}|<em>2^2 \sum</em>{m=1}^M \frac{\partial \pi_{m_t}(X_t, \Theta_t)}{\partial \theta(m)_t} = 0</span>。</p>
<p>根据辅助损失的定义（方程 7），我们计算<br />
<div class="math-display"><br />
    \nabla_{\theta(m)<em>t} L</em>{\text{aux}}^t = \alpha M \sum_{m'=1}^M f(m')<em>t \frac{\partial P(m')_t}{\partial \theta(m)_t} = \alpha M t f(m_t)_t \frac{\partial \pi</em>{m_t}(X_t, \Theta_t)}{\partial \theta(m)<em>t},<br />
</div><br />
其中第二个等式是因为 <span class="math-inline">\frac{\partial P(m_t)_t}{\partial \theta(m)_t} = \frac{1}{t} \frac{\partial \pi</em>{m_t}(X_t, \Theta_t)}{\partial \theta(m)<em>t}</span> 且 <span class="math-inline">\frac{\partial P(m')_t}{\partial \theta(m)_t} = 0</span> 对于任何 <span class="math-inline">m' \neq m_t</span> 由方程（7）。然后基于上述，我们同样得到 <span class="math-inline">\sum</em>{m=1}^M \nabla_{\theta(m)<em>t} L</em>{\text{aux}}^t = \alpha M t f(m_t)<em>t \sum</em>{m=1}^M \frac{\partial \pi_{m_t}(X_t, \Theta_t)}{\partial \theta(m)_t} = 0</span>。</p>
<p>总结，我们最终证明了 <span class="math-inline">\sum_{m=1}^M \nabla_{\theta(m)<em>t} L</em>{\text{task}}^t = 0</span>。</p>
<p>在接下来的引理中，我们分析每个专家的损失函数梯度。</p>
<p><strong>引理 5</strong>：对于任何训练轮次 <span class="math-inline">t \in {1, \cdots, T}</span>，以下性质成立<br />
<div class="math-display"><br />
    |\nabla_{\theta(m)<em>t} L</em>{\text{task}}^t|_\infty = \begin{cases} \Omega(\sigma_0), &amp; \text{if } t \in {1, \cdots, T_1}, \ O(\sigma_0), &amp; \text{if } t \in {T_1 + 1, \cdots, T} \end{cases}<br />
</div><br />
对于任何专家 <span class="math-inline">m \in [M]</span>，其中 <span class="math-inline">T_1 = \lceil \eta^{-1}M \rceil</span> 是探索阶段的长度。</p>
<p>证明：我们通过分析 <span class="math-inline">\nabla_{\theta(m)<em>t} L</em>{\text{loc}}^t = O(\sigma_0)</span> 和 <span class="math-inline">\nabla_{\theta(m)<em>t} L</em>{\text{aux}}^t = O(\alpha M / t)</span> 来证明引理 5，分别基于方程（26）。</p>
<p>首先，我们计算 <span class="math-inline">|\nabla_{\theta(m)<em>t} L</em>{\text{loc}}^t|<em>\infty</span>。基于方程（27），我们有<br />
<div class="math-display"><br />
    \nabla</em>{\theta(m)<em>t} L</em>{\text{loc}}^t = \frac{\partial \pi_{m_t}(X_t, \Theta_t)}{\partial \theta(m)<em>t} |\mathbf{w}^{(m_t)}_t - \mathbf{w}^{(m_t)}</em>{t-1}|<em>2^2 \leq |\mathbf{w}^{(m_t)}_t - \mathbf{w}^{(m_t)}</em>{t-1}|<em>2^2 \frac{\partial \pi</em>{m_t}(X_t, \Theta_t)}{\partial \theta(m)<em>t},<br />
</div><br />
其中 <span class="math-inline">\tau</span> 是最后一个路由到专家 <span class="math-inline">m_t</span> 的任务索引，第二个等式是基于引理 3。由于 <span class="math-inline">\frac{\partial \pi</em>{m_t}(X_t, \Theta_t)}{\partial \theta(m)<em>t} = O(1)</span> 且 <span class="math-inline">w_n \sim N(0, \sigma_0^2)</span>，我们最终得到<br />
<div class="math-display"><br />
    |\nabla</em>{\theta(m)<em>t} L</em>{\text{loc}}^t|<em>\infty = O(\sigma_0).<br />
</div><br />
接下来，我们进一步计算 <span class="math-inline">|\nabla</em>{\theta(m)<em>t} L</em>{\text{aux}}^t|_\infty</span>，它包含以下两种情况。</p>
<p>如果 <span class="math-inline">t \in {1, \cdots, T_1}</span>，根据方程（30），我们有<br />
<div class="math-display"><br />
    |\nabla_{\theta(m)<em>t} L</em>{\text{aux}}^t|<em>\infty \geq |\alpha M / T_1 f(m_t)_t \frac{\partial \pi</em>{m_t}(X_t, \Theta_t)}{\partial \theta(m)<em>t}|</em>\infty \geq |\sigma_0 f(m_t)<em>t \frac{\partial \pi</em>{m_t}(X_t, \Theta_t)}{\partial \theta(m)<em>t}|</em>\infty = \Omega(\sigma_0),<br />
</div><br />
其中第二个不等式是通过设置 <span class="math-inline">\eta = \Omega(\sigma_{0.5}^0)</span> 来使 <span class="math-inline">T_1 = \lceil \sigma_{-0.5}^0 M \rceil</span>。</p>
<p>如果 <span class="math-inline">t \in {T_1 + 1, \cdots, T}</span>，我们计算<br />
<div class="math-display"><br />
    |\nabla_{\theta(m)<em>t} L</em>{\text{aux}}^t|<em>\infty \leq |\alpha M / T_1 f(m_t)_t \frac{\partial \pi</em>{m_t}(X_t, \Theta_t)}{\partial \theta(m)<em>t}|</em>\infty = O(\sigma_0).<br />
</div><br />
基于上述推导出的 <span class="math-inline">|\nabla_{\theta(m)<em>t} L</em>{\text{loc}}^t|<em>\infty</span> 和 <span class="math-inline">|\nabla</em>{\theta(m)<em>t} L</em>{\text{aux}}^t|<em>\infty</span>，我们可以根据方程（26）最终计算 <span class="math-inline">|\nabla</em>{\theta(m)<em>t} L</em>{\text{task}}^t|_\infty</span>。</p>
<p>对于 <span class="math-inline">t \in {1, \cdots, T_1}</span>，如果 <span class="math-inline">m \neq m_t</span>，我们有 <span class="math-inline">|\nabla_{\theta(m)<em>t} L</em>{\text{task}}^t|<em>\infty = |\nabla</em>{\theta(m)<em>t} L</em>{\text{loc}}^t + \nabla_{\theta(m)<em>t} L</em>{\text{aux}}^t|<em>\infty \leq |O(\sigma_0) + \alpha M / T_1 f(m_t)_t \frac{\partial \pi</em>{m_t}(X_t, \Theta_t)}{\partial \theta(m)<em>t}|</em>\infty = \Omega(\sigma_0)</span>。</p>
<p>类似地，对于任何 <span class="math-inline">t \in {T_1 + 1, \cdots, T}</span>，我们可以得出 <span class="math-inline">|\nabla_{\theta(m)<em>t} L</em>{\text{task}}^t|<em>\infty = |\nabla</em>{\theta(m)<em>t} L</em>{\text{loc}}^t + \nabla_{\theta(m)<em>t} L</em>{\text{aux}}^t|<em>\infty \geq |O(\sigma_0) + \alpha M / T_1 f(m_t)_t \frac{\partial \pi</em>{m_t}(X_t, \Theta_t)}{\partial \theta(m)<em>t}|</em>\infty = O(\sigma_0)</span>。这完成了引理 5 的证明。</p>
<p>给定 <span class="math-inline">\theta(m)_0 = 0</span> 对于任何专家 <span class="math-inline">m \in [M]</span>，在下一个引理中，我们得到了任何轮次 <span class="math-inline">t \in {1, \cdots, T}</span> 中门控网络参数 <span class="math-inline">\theta(m)_t</span> 的上界。</p>
<p><strong>引理 6</strong>：对于任何训练轮次 <span class="math-inline">t \in {1, \cdots, T}</span>，任何专家 <span class="math-inline">m</span> 的门控网络参数满足 <span class="math-inline">|\theta(m)<em>t|</em>\infty = O(\sigma_{0.5}^0)</span>。</p>
<p>证明：基于引理 5，对于任何 <span class="math-inline">t \in {1, \cdots, T_1}</span>，门控网络参数 <span class="math-inline">\theta(m)<em>t</span> 在探索阶段的累积更新满足 <span class="math-inline">|\theta(m)_t|</em>\infty \leq \eta \cdot T_1 \cdot \alpha M = O(\sigma_{0.5}^0)</span>。</p>
<p>对于任何 <span class="math-inline">t \in {T_1 + 1, \cdots, T}</span>，门控网络参数 <span class="math-inline">\theta(m)<em>t</span> 在路由器学习阶段的累积更新满足<br />
<div class="math-display"><br />
    |\theta(m)_t|</em>\infty \leq |\theta(m)<em>{T_1}|</em>\infty + \eta \cdot (T - T_1) \cdot \alpha M / T_1 = O(\sigma_{0.5}^0) + O(\sigma_{0.5}^0 - \sigma_0) = O(\sigma_{0.5}^0).<br />
</div><br />
总结，<span class="math-inline">|\theta(m)<em>t|</em>\infty = O(\sigma_{0.5}^0)</span> 对于任何轮次 <span class="math-inline">t \in {1, \cdots, T}</span>。</p>
<h3 id="e-引理-1-的完整版本和证明">E 引理 1 的完整版本和证明<a class="anchor-link" href="#e-引理-1-的完整版本和证明" title="Permanent link">&para;</a></h3>
<p><strong>引理 1（完整版本）</strong>：在算法 1 下，以至少 <span class="math-inline">1 - o(1)</span> 的概率，对于任何 <span class="math-inline">t &gt; T_1</span>，其中 <span class="math-inline">T_1 = \lceil \eta^{-1}M \rceil</span>，每个专家 <span class="math-inline">m \in [M]</span> 满足以下属性：</p>
<p>1) 如果 <span class="math-inline">M &gt; N</span>，专家 <span class="math-inline">m</span> 在专家集合 <span class="math-inline">M_n</span> 中稳定，其专家模型在时间 <span class="math-inline">T_1</span> 之后保持不变，满足 <span class="math-inline">\mathbf{w}^{(m)}<em>{T_1+1} = \cdots = \mathbf{w}^{(m)}_T</span>。<br />
2) 如果 <span class="math-inline">M &lt; N</span>，专家 <span class="math-inline">m</span> 在专家集合 <span class="math-inline">M_k</span> 中稳定，其专家模型满足 <span class="math-inline">|\mathbf{w}^{(m)}_t - \mathbf{w}^{(m)}</em>{T_1+1}|<em>\infty = O(\sigma</em>{1.5}^0)</span> 对于任何 <span class="math-inline">t \in {T_1 + 2, \cdots, T}</span>。</p>
<p>我们首先提出以下引理作为预备，然后正式在附录 E.8 中证明引理 1。</p>
<p><strong>引理 7</strong>：在任何训练轮次 <span class="math-inline">t \in {1, \cdots, T_1}</span> 中，对于任何特征信号 <span class="math-inline">v_n</span>，专家 <span class="math-inline">m \in [M]</span> 的门控网络参数满足<br />
<div class="math-display"><br />
    \langle \theta^{(m)}<em>{t+1} - \theta^{(m)}_t, v_n \rangle = \begin{cases} -O(\sigma_0), &amp; \text{if } m = m_t, \ O(M^{-1}\sigma_0), &amp; \text{if } m \neq m_t. \end{cases}<br />
</div><br />
引理 7 告诉我们，对于任何被路由器选中的专家 <span class="math-inline">m_t</span>，其在更新 <span class="math-inline">\theta^{(m_t)}</em>{t+1}</span> 时的 softmax 值由于下一个任务而减少，而对于任何未被选中的专家 <span class="math-inline">m</span>，其 softmax 值增加。这是为了确保在辅助损失函数（方程 7）下每个专家 <span class="math-inline">m</span> 的公平探索。此外，对于任何未被选中的专家 <span class="math-inline">m</span>，其门控网络参数 <span class="math-inline">\theta^{(m)}_t</span> 对所有其他信号向量 <span class="math-inline">v_n</span> 的更新速度与其他人相同。</p>
<p><strong>引理 8</strong>：在探索阶段结束时，以至少 <span class="math-inline">1 - \delta</span> 的概率，任何专家 <span class="math-inline">m \in [M]</span> 被分配的任务比例满足<br />
<div class="math-display"><br />
    \left| f(m)_{T_1} - \frac{1}{M} \right| = O(\eta^{0.5}M^{-1}).<br />
</div><br />
引理 8 告诉我们，在探索阶段，所有 <span class="math-inline">M</span> 个专家预计将被均匀探索。因此，门控网络参数 <span class="math-inline">\theta^{(m)}_t</span> 的专家 <span class="math-inline">m</span> 在所有其他人中更新相似。</p>
<p><strong>引理 9</strong>：在探索阶段结束时，即 <span class="math-inline">t = T_1</span>，以下属性成立<br />
<div class="math-display"><br />
    \left| \theta^{(m)}<em>{T_1} - \theta^{(m')}</em>{T_1} \right|_\infty = O(\eta^{-0.5}\sigma_0),<br />
</div><br />
对于任何 <span class="math-inline">m, m' \in [M]</span> 且 <span class="math-inline">m \neq m'</span>。</p>
<p>定义 <span class="math-inline">\delta_\Theta = |h_m(X_t, \theta^{(m)}<em>t) - h</em>{m'}(X_t, \theta^{(m)}_t)|</span>。然后我们得到以下引理。</p>
<p><strong>引理 10</strong>：在任何轮次 <span class="math-inline">t</span>，如果 <span class="math-inline">\delta_\Theta^t</span> 接近 0，它满足 <span class="math-inline">|\pi_m(X_t, \Theta_t) - \pi_{m'}(X_t, \Theta_t)| = O(\delta_\Theta)</span>。否则，<span class="math-inline">|\pi_m(X_t, \Theta_t) - \pi_{m'}(X_t, \Theta_t)| = \Omega(\delta_\Theta)</span>。</p>
<p><strong>引理 11</strong>：如果 <span class="math-inline">M = \Omega(N \ln(N))</span>，我们有 <span class="math-inline">|M_n| \geq 1</span> 对于所有 <span class="math-inline">n \in [N]</span> 以至少 <span class="math-inline">1 - o(1)</span> 的概率。如果 <span class="math-inline">M &lt; N</span>，给定 <span class="math-inline">M = \Omega(K \ln(K))</span>，我们有 <span class="math-inline">|M_k| \geq 1</span> 对于所有 <span class="math-inline">k \in [K]</span> 以至少 <span class="math-inline">1 - o(1)</span> 的概率。</p>
<p><strong>引理 12</strong>：在任何轮次 <span class="math-inline">t</span>，我们有以下属性：</p>
<p>1) 对于具有真实情况 <span class="math-inline">w_{n_t} = w_n</span> 的任务到达 <span class="math-inline">n_t</span> 在 <span class="math-inline">M &gt; N</span> 下，如果它被路由到正确的专家 <span class="math-inline">m_t \in M_n</span>，则对于任何专家 <span class="math-inline">m \in [M]</span>，<span class="math-inline">\nabla_{\theta(m)<em>t} L</em>{\text{loc}}^t = 0</span>。<br />
2) 对于具有真实情况 <span class="math-inline">w_{n_t} \in W_k</span> 的任务到达 <span class="math-inline">n_t</span> 在 <span class="math-inline">M &lt; N</span> 下，如果它被路由到正确的专家 <span class="math-inline">m_t \in M_k</span>，则对于任何专家 <span class="math-inline">m \in [M]</span>，<span class="math-inline">|\nabla_{\theta(m)<em>t} L</em>{\text{loc}}^t|<em>\infty = O(\sigma</em>{1.5}^0)</span>。</p>
<p>让 <span class="math-inline">X_n</span> 和 <span class="math-inline">X_{n'}</span> 表示包含特征信号 <span class="math-inline">v_n</span> 和 <span class="math-inline">v_{n'}</span> 的两个特征矩阵。</p>
<p><strong>引理 13</strong>：如果 <span class="math-inline">n</span> 和 <span class="math-inline">n'</span> 满足：1) <span class="math-inline">n \neq n'</span> 在 <span class="math-inline">M &gt; N</span> 下，或 2) <span class="math-inline">w_n \in W_k</span> 和 <span class="math-inline">w_{n'} \in W_{k'}</span> 与 <span class="math-inline">k \neq k'</span> 在 <span class="math-inline">M &lt; N</span> 下，那么如果专家 <span class="math-inline">m</span> 满足 1) <span class="math-inline">m \in M_n</span> 在 <span class="math-inline">M &gt; N</span> 下，或 2) <span class="math-inline">m \in M_k</span> 在 <span class="math-inline">M &lt; N</span> 下，在路由器学习阶段开始时 <span class="math-inline">t = T_1 + 1</span>，则以下属性在任何轮次 <span class="math-inline">t \in {T_1 + 2, \cdots, T}</span> 成立：<br />
<div class="math-display"><br />
    \pi_m(X_n, \Theta_t) &gt; \pi_m(X_{n'}, \Theta_t), \forall m \in [M].<br />
</div><br />
基于这些引理，引理 1 的证明在附录 E.8 中给出。</p>
<p><strong>引理 1 证明（E.1）</strong></p>
<p>证明：根据引理 5，对于任何轮次 <span class="math-inline">t \in {1, \cdots, T_1}</span>，辅助损失是更新 <span class="math-inline">\Theta_t</span> 的主要损失。然后基于 <span class="math-inline">\theta^{(m)}<em>t</span> 的更新规则（方程 9）和 <span class="math-inline">\nabla</em>{\theta(m_t)<em>t} L</em>{\text{aux}}^t</span> 的梯度（方程 30），我们得到<br />
<div class="math-display"><br />
    |\theta^{(m_t)}<em>{t+1} - \theta^{(m_t)}_t|</em>\infty = |\eta \cdot \nabla_{\theta(m_t)<em>t} L</em>{\text{aux}}^t|<em>\infty = O(\sigma</em>{0.5}^0 \eta) \cdot \left( \pi_{m_t}(X_t, \Theta_t) \cdot \left(1 - \pi_{m_t}(X_t, \Theta_t) \right) \cdot \sum_{i \in [s_t]} X_{t,i} \right)<em>\infty = O(\sigma_0),<br />
</div><br />
其中 <span class="math-inline">\pi</em>{m_t}(X_t, \Theta_t) \cdot (1 - \pi_{m_t}(X_t, \Theta_t)) \leq \frac{1}{4}</span> 且 <span class="math-inline">|X_t|_\infty = O(1)</span>。</p>
<p>对于任何 <span class="math-inline">m \neq m_t</span>，我们计算 <span class="math-inline">|\theta^{(m)}<em>{t+1} - \theta^{(m)}_t|</em>\infty = |\eta \cdot \nabla_{\theta(m)<em>t} L</em>{\text{aux}}^t|<em>\infty = O(\sigma</em>{0.5}^0 \eta) \cdot \left( \pi_{m_t}(X_t, \Theta_t) \cdot \pi_m(X_t, \Theta_t) \cdot \sum_{i \in [s_t]} X_{t,i} \right)<em>\infty = O(M^{-1}\sigma_0)</span>，由于 <span class="math-inline">\pi</em>{m_t}(X_t, \Theta_t) \cdot \pi_m(X_t, \Theta_t) = O(M^{-1})</span>。</p>
<p>注意，根据方程（30），我们有 <span class="math-inline">\nabla_{\theta(m_t)<em>t} L</em>{\text{aux}}^t &gt; 0</span> 和 <span class="math-inline">\nabla_{\theta(m)<em>t} L</em>{\text{aux}}^t &lt; 0</span> 对于任何 <span class="math-inline">m \neq m_t</span>。因此，对于专家 <span class="math-inline">m_t</span>，其在门控网络中的相应输出 <span class="math-inline">h_{m_t}</span> 将因为任务 <span class="math-inline">t + 1</span> 而减少 <span class="math-inline">O(\sigma_0)</span>。而对于任何专家 <span class="math-inline">m \neq m_t</span>，其相应输出 <span class="math-inline">h_m</span> 增加 <span class="math-inline">O(M^{-1}\sigma_0)</span>。</p>
<p><strong>引理 8 证明（E.2）</strong></p>
<p>证明：通过对称性，我们有对于任何 <span class="math-inline">m \in [M]</span>，<span class="math-inline">E[f(m)<em>{T_1}] = \frac{1}{M}</span>。通过 Hoeffding 不等式，我们得到<br />
<div class="math-display"><br />
    P\left( \left| f(m)</em>{T_1} - \frac{1}{M} \right| \leq \epsilon \right) \geq 1 - 2 \exp \left( -2\epsilon^2 T_1 \right).<br />
</div><br />
然后我们进一步得到<br />
<div class="math-display"><br />
    P\left( \left| f(m)<em>{T_1} - \frac{1}{M} \right| \leq \epsilon, \forall m \in [M] \right) \geq \left( 1 - 2 \exp \left( -2\epsilon^2 T_1 \right) \right)^M \geq 1 - 2M \exp \left( -2\epsilon^2 T_1 \right).<br />
</div><br />
让 <span class="math-inline">\delta = 1 - 2M \exp \left( -2\epsilon^2 T_1 \right)</span>。然后我们得到 <span class="math-inline">\epsilon = O(\eta^{0.5}M^{-1})</span>。随后，至少有 <span class="math-inline">1 - \delta</span> 的概率 <span class="math-inline">\left| f(m)</em>{T_1} - \frac{1}{M} \right| = O(\eta^{0.5}M^{-1})</span>。</p>
<p><strong>引理 9 证明（E.3）</strong></p>
<p>证明：基于引理 7 和引理 8 及其相应证明，我们可以证明引理 9 如下。</p>
<p>对于专家 <span class="math-inline">m</span> 和 <span class="math-inline">m'</span>，它们在探索阶段分别被路由器选择 <span class="math-inline">T_1 \cdot f(m)<em>{T_1}</span> 和 <span class="math-inline">T_1 \cdot f(m')</em>{T_1}</span> 次。因此，我们得到 <span class="math-inline">|\theta(m)<em>{T_1}|</em>\infty = f(m)<em>{T_1} \cdot T_1 \cdot O(\sigma_0) - (1 - f(m)</em>{T_1}) \cdot T_1 \cdot O(M^{-1}\sigma_0)</span>，<span class="math-inline">|\theta(m')<em>{T_1}|</em>\infty = f(m')<em>{T_1} \cdot T_1 \cdot O(\sigma_0) - (1 - f(m')</em>{T_1}) \cdot T_1 \cdot O(M^{-1}\sigma_0)</span>。</p>
<p>然后根据方程（9）和引理 7，我们计算 <span class="math-inline">\left| \theta(m)<em>{T_1} - \theta(m')</em>{T_1} \right|<em>\infty = \left| f(m)</em>{T_1} - f(m')<em>{T_1} \right| \cdot T_1 \cdot O(\sigma_0) = O(\eta^{-0.5}\sigma_0)</span>，其中第一个等式是基于引理 7 中的更新步骤，最后一个等式是因为 <span class="math-inline">T_1 \cdot \left| f(m)</em>{T_1} - f(m')_{T_1} \right| = O(\eta^{-0.5})</span> 由方程（32）在引理 8 中得出。</p>
<p><strong>引理 10 证明（E.4）</strong></p>
<p>证明：在任何轮次 <span class="math-inline">t</span>，我们计算<br />
<div class="math-display"><br />
    |\pi_m(X_t, \Theta_t) - \pi_{m'}(X_t, \Theta_t)| = \pi_{m'}(X_t, \Theta_t) \left| \exp(h_m(X_t, \theta^{(m)}<em>t) - h</em>{m'}(X_t, \theta^{(m')}<em>t) - \pi</em>{m'}(X_t, \Theta_t)) - 1 \right|,<br />
</div><br />
其中第一个等式是通过解方程（2）得到的。然后如果 <span class="math-inline">\delta_\Theta^t</span> 接近 0，通过应用足够小的 <span class="math-inline">\delta_\Theta</span> 的泰勒级数，我们得到<br />
<div class="math-display"><br />
    |\pi_m(X_t, \Theta_t) - \pi_{m'}(X_t, \Theta_t)| \approx \pi_{m'}(X_t, \Theta_t) |h_m(X_t, \theta^{(m)}<em>t) - h</em>{m'}(X_t, \theta^{(m')}<em>t)| = O(\delta</em>\Theta),<br />
</div><br />
最后一个等式是因为 <span class="math-inline">\pi_m(\tilde{X}_t, \Theta_t) \leq 1</span>。</p>
<p>如果 <span class="math-inline">\delta_\Theta^t</span> 不是足够小，我们得到<br />
<div class="math-display"><br />
    |\pi_m(X_t, \Theta_t) - \pi_{m'}(X_t, \Theta_t)| &gt; \pi_{m'}(X_t, \Theta_t) |h_m(X_t, \theta^{(m)}<em>t) - h</em>{m'}(X_t, \theta^{(m')}<em>t)| = \Omega(\delta</em>\Theta).<br />
</div><br />
这完成了引理 10 的证明。</p>
<p><strong>引理 11 证明（E.5）</strong></p>
<p>证明：如果 <span class="math-inline">M &gt; N</span>，通过对称性，我们有对于所有 <span class="math-inline">n \in [N]</span>，<span class="math-inline">m \in [M]</span>，<span class="math-inline">P(m \in M_n) = \frac{1}{N}</span>。因此，<span class="math-inline">|M_n|</span> 至少包含一个专家的概率是<br />
<div class="math-display"><br />
    P(|M_n| \geq 1) \geq 1 - \left( 1 - \frac{1}{N} \right)^M.<br />
</div><br />
通过应用并集界限，我们得到<br />
<div class="math-display"><br />
    P(|M_n| \geq 1, \forall n) \geq \left( 1 - \left( 1 - \frac{1}{N} \right)^M \right)^N \geq 1 - N \exp \left( -\frac{M}{N} \right) \geq 1 - \delta,<br />
</div><br />
其中第二个不等式是因为 <span class="math-inline">\left( 1 - \frac{1}{N} \right)^M</span> 足够小，最后一个不等式是因为 <span class="math-inline">M = \Omega \left( \frac{N \ln(N)}{\delta} \right)</span>。</p>
<p>如果 <span class="math-inline">M &lt; N</span>，我们可以用同样的方法证明 <span class="math-inline">P(|M_k| \geq 1, \forall k) \geq 1 - \delta</span>，给定 <span class="math-inline">M = \Omega \left( \frac{K \ln(K)}{\delta} \right)</span> 和 <span class="math-inline">P(m \in M_k) = \frac{1}{K}</span> 通过对称性。</p>
<p><strong>引理 12 证明（E.6）</strong></p>
<p>证明：在 <span class="math-inline">M &gt; N</span> 的情况下，由于 <span class="math-inline">|M_n| = 1</span>，如果任务 <span class="math-inline">n_t</span> 与 <span class="math-inline">n_t = n</span> 被路由到正确的专家 <span class="math-inline">m_t \in M_n</span>，我们有 <span class="math-inline">\mathbf{w}^{(m_t)}<em>t = \mathbf{w}^{(m_t)}</em>{t-1}</span> 通过（5）。因此，引起的局部损失 <span class="math-inline">L_{\text{loc}}^t(\Theta_t, D_t) = 0</span>，基于它在方程（6）中的定义。</p>
<p>在 <span class="math-inline">M &lt; N</span> 的情况下，由于每个簇 <span class="math-inline">k</span> 中 <span class="math-inline">|M_k| \geq 1</span>，如果任务 <span class="math-inline">n_t</span> 与 <span class="math-inline">w_{n_t} \in W_k</span> 被路由到正确的专家 <span class="math-inline">m_t \in M_k</span>，我们有 <span class="math-inline">|\mathbf{w}^{(m_t)}<em>t - \mathbf{w}^{(m_t)}</em>{t-1}|<em>\infty = |X_t (X_t^\top X_t)^{-1} (y_t - X_t^\top \mathbf{w}^{(m_t)}</em>{t-1})|<em>\infty = |X_t (X_t^\top X_t)^{-1} X_t^\top (w</em>{n_t} - \mathbf{w}^{(m_t)}<em>{t-1})|</em>\infty = O(|\mathbf{w}<em>{t} - \mathbf{w}^{(m_t)}</em>{t-1}|<em>\infty) = O(\sigma</em>{1.5}^0)</span>，其中第二个等式是因为 <span class="math-inline">y_t = X_t^\top w_{n_t}</span>，第三个等式是因为 <span class="math-inline">|X_t|<em>\infty = O(1)</span>。因此，我们通过解方程（27）得到 <span class="math-inline">\nabla</em>{\theta(m)<em>t} L</em>{\text{loc}}^t = O(\sigma_{1.5}^0)</span>，对于任何 <span class="math-inline">m \in [M]</span>。</p>
<p><strong>引理 13 证明（E.7）</strong></p>
<p>证明：我们首先关注 <span class="math-inline">M &gt; N</span> 情况来证明引理 13。基于方程（35）在引理 1 中，我们得到 <span class="math-inline">\pi_m(X_n, \Theta_{T_1}) - \pi_m(X_{n'}, \Theta_{T_1}) = \Omega(\sigma_{0.5}^0)</span> 对于 <span class="math-inline">m \in M_n</span>，给定 <span class="math-inline">|\theta^{(m)}<em>t|</em>\infty = O(\sigma_{0.5}^0)</span> 从引理 6 得出。为了证明方程（33），我们将证明 <span class="math-inline">\pi_m(X_n, \Theta_t) - \pi_m(X_{n'}, \Theta_t) = \Omega(\sigma_{0.5}^0)</span> 对任何 <span class="math-inline">t \geq T_1 + 1</span> 成立。</p>
<p>基于引理 5，我们有 <span class="math-inline">|\nabla_{\theta(m)<em>t} L</em>{\text{task}}^t|<em>\infty = O(\sigma_0)</span>，导致 <span class="math-inline">\langle \theta^{(m_t)}</em>{t+1} - \theta^{(m_t)}<em>t, v_n \rangle = -O(\sigma</em>{1.5}^0)</span> 对于专家 <span class="math-inline">m_t</span> 和 <span class="math-inline">\langle \theta^{(m)}<em>{t+1} - \theta^{(m)}_t, v_n \rangle = O(\sigma</em>{1.5}^0)</span> 对于任何其他专家 <span class="math-inline">m \neq m_t</span> 在任务 <span class="math-inline">t = T_1 + 1</span>。</p>
<p>随后，对于任何两个专家 <span class="math-inline">m \in M_n</span> 和 <span class="math-inline">m' \in M_{n'}</span>，我们计算<br />
<div class="math-display"><br />
    |\theta(m)<em>{t_1} - \theta(m')</em>{t_1}|<em>\infty \leq |\theta(m)</em>{T_1+2} - \theta(m')<em>{T_1+2}|</em>\infty + (t_1 - T_1) \cdot O(\sigma_{1.5}^0) \leq O(\sigma_0 \eta^{-0.5}) + O(\eta^{-1} \sigma_{-0.25}^0) \cdot O(\sigma_{1.5}^0) = O(\sigma_0 \eta^{-0.5}),<br />
</div><br />
其中第一个不等式是因为 <span class="math-inline">\langle \theta^{(m)}<em>{t+1} - \theta^{(m)}_t, v_n \rangle = O(\sigma</em>{1.5}^0)</span>，第二个不等式是因为 <span class="math-inline">t_1 - T_1 \leq t_1</span>。</p>
<p>由于 <span class="math-inline">|\theta(m)<em>{t_1} - \theta(m')</em>{t_1}|<em>\infty = O(\sigma_0 \eta^{-0.5})</span> 且 <span class="math-inline">|\pi_m(X_n, \Theta</em>{T_1}) - \pi_m(X_{n'}, \Theta_{T_1})| = \Omega(\sigma_{0.5}^0)</span>，方程（33）对任何 <span class="math-inline">t \in {T_1 + 2, \cdots, t_1}</span> 成立，基于我们在引理 1 的证明。</p>
<p>对于 <span class="math-inline">M &lt; N</span> 的情况，我们可以用相同的方法证明方程（33）。这完成了引理 13 的证明。</p>
<p><strong>引理 1 最终证明（E.8）</strong></p>
<p>证明：在 <span class="math-inline">M &gt; N</span> 的情况下，为了证明引理 1，我们等价地证明在探索阶段结束时 <span class="math-inline">t = T_1</span>，对于任何两个专家 <span class="math-inline">m \in M_n</span> 和 <span class="math-inline">m' \in M_{n'}</span> 且 <span class="math-inline">n \neq n'</span>，以下属性成立：<span class="math-inline">\pi_m(X_n, \Theta_t) &gt; \pi_{m'}(X_n, \Theta_t)</span>，<span class="math-inline">\pi_{m'}(X_{n'}, \Theta_t) &gt; \pi_m(X_{n'}, \Theta_t)</span>。基于方程（34），我们有每个专家 <span class="math-inline">m \in [M]</span> 在专家集合 <span class="math-inline">M_n</span> 中稳定。</p>
<p>根据引理 1，门控网络只关注特征信号。然后对于每个专家 <span class="math-inline">m</span>，我们计算 <span class="math-inline">|h_m(X_n, \theta^{(m)}<em>t) - h_m(X</em>{n'}, \theta^{(m)}<em>t)| = |\langle \theta^{(m)}_t, v_n - v</em>{n'} \rangle| = |\theta^{(m)}<em>t|</em>\infty \cdot |v_n - v_{n'}|<em>\infty = O(\sigma_0.5^0)</span>，其中最后一个等式是因为 <span class="math-inline">|\theta^{(m)}_t|</em>\infty = O(\sigma_0.5^0)</span> 在引理 6 中得出和 <span class="math-inline">|v_n - v_{n'}|_\infty = O(1)</span>。</p>
<p>然后基于引理 10，我们得到 <span class="math-inline">|\pi_m(X_n, \Theta_t) - \pi_{m'}(X_n, \Theta_t)| = \Omega(\sigma_0.5^0)</span>。（方程 35）</p>
<p>接下来，我们通过反证法证明方程（34）。假设存在两个专家 <span class="math-inline">m \in M_n</span> 和 <span class="math-inline">m' \in M_{n'}</span> 使得 <span class="math-inline">\pi_m(X_n, \Theta_t) &gt; \pi_{m'}(X_n, \Theta_t)</span>，<span class="math-inline">\pi_{m'}(X_{n'}, \Theta_t) &gt; \pi_m(X_{n'}, \Theta_t)</span>，这等同于 <span class="math-inline">\pi_m(X_n, \Theta_t) &gt; \pi_m(X_{n'}, \Theta_t) &gt; \pi_{m'}(X_{n'}, \Theta_t) &gt; \pi_{m'}(X_n, \Theta_t)</span>，（方程 36）因为 <span class="math-inline">\pi_m(X_n, \Theta_t) &gt; \pi_{m'}(X_n, \Theta_t)</span> 和 <span class="math-inline">\pi_{m'}(X_{n'}, \Theta_t) &gt; \pi_{m'}(X_n, \Theta_t)</span> 基于专家集合 <span class="math-inline">M_n</span> 的定义在方程（11）中。然后我们证明方程（36）在 <span class="math-inline">t = T_1</span> 不存在。</p>
<p>对于任务 <span class="math-inline">t = T_1</span>，我们计算 <span class="math-inline">|h_m(X_n, \theta^{(m)}<em>{T_1}) - h</em>{m'}(X_n, \theta^{(m')}<em>{T_1})| \leq |\theta^{(m)}</em>{T_1} - \theta^{(m')}<em>{T_1}|</em>\infty |v_n|<em>\infty = O(\sigma_0 \eta^{-0.5})</span>，其中第一个不等式是基于并集界限，第二个等式是因为 <span class="math-inline">|v_n|</em>\infty = O(1)</span> 和 <span class="math-inline">|\theta^{(m)}<em>{T_1} - \theta^{(m')}</em>{T_1}|_\infty = O(\sigma_0 \eta^{-0.5})</span> 从引理 9 在探索阶段结束时得出。</p>
<p>然后根据引理 10 和上述不等式，我们得到 <span class="math-inline">|\pi_m(X_n, \Theta_{T_1}) - \pi_{m'}(X_n, \Theta_{T_1})| = O(\sigma_0 \eta^{-0.5})</span>。（方程 38）</p>
<p>基于方程（36），我们进一步计算 <span class="math-inline">|\pi_m(X_n, \Theta_{T_1}) - \pi_{m'}(X_n, \Theta_{T_1})| \geq |\pi_m(X_n, \Theta_{T_1}) - \pi_m(X_{n'}, \Theta_{T_1})| = \Omega(\sigma_0.5^0)</span>，其中第一个不等式是基于方程（36），最后一个等式是基于方程（35）。这与方程（38）矛盾，因为 <span class="math-inline">\sigma_0 \eta^{-0.5} &lt; \sigma_0.5^0</span> 给定 <span class="math-inline">\eta = O(\sigma_0.5^0)</span>。因此，方程（36）在 <span class="math-inline">t = T_1</span> 不存在，方程（34）对 <span class="math-inline">t = T_1</span> 成立。</p>
<p>基于引理 13，我们得到每个专家集合 <span class="math-inline">M_n</span> 在路由器学习阶段是稳定的。因此，对于任何轮次 <span class="math-inline">t \in {T_1 + 1, \cdots, T}</span>，任务 <span class="math-inline">n_t</span> 与真实情况 <span class="math-inline">w_{n_t}</span> 将被路由到其最佳专家之一 <span class="math-inline">M_{n_t}</span>。然后基于引理 12，我们有 <span class="math-inline">\nabla_{\theta(m)<em>t} L</em>{\text{loc}}^t = 0</span> 在路由器学习阶段。随后，任何专家 <span class="math-inline">m</span> 的 <span class="math-inline">\mathbf{w}^{(m)}_t</span> 保持不变，基于引理 3。</p>
<p>在 <span class="math-inline">M &lt; N</span> 的情况下，我们类似地得到在探索阶段结束时 <span class="math-inline">t = T_1</span>，对于任何两个专家 <span class="math-inline">m \in M_k</span> 和 <span class="math-inline">m' \in M_{k'}</span> 且 <span class="math-inline">k \neq k'</span>，以下属性成立 <span class="math-inline">\pi_m(X_k, \Theta_t) &gt; \pi_{m'}(X_k, \Theta_t)</span>，<span class="math-inline">\pi_{m'}(X_{k'}, \Theta_t) &gt; \pi_m(X_{k'}, \Theta_t)</span>，其中 <span class="math-inline">X_k</span> 和 <span class="math-inline">X_{k'}</span> 包含特征信号 <span class="math-inline">v_k</span> 和 <span class="math-inline">v_{k'}</span>。</p>
<p>然后，在路由器学习阶段 <span class="math-inline">t &gt; T_1</span>，任何任务 <span class="math-inline">n_t</span> 与 <span class="math-inline">w_{n_t} \in W_k</span> 将被路由到正确的专家 <span class="math-inline">m \in M_k</span>。让 <span class="math-inline">\mathbf{w}^{(m)}</span> 表示专家 <span class="math-inline">m</span> 的最小 <span class="math-inline">\ell_2</span>- 范数离线解。基于 <span class="math-inline">\mathbf{w}^{(m_t)}<em>t</span> 的更新规则（方程 5），我们计算 <span class="math-inline">\mathbf{w}^{(m_t)}_t - \mathbf{w}^{(m)} = \mathbf{w}^{(m_t)}</em>{t-1} + X_t (X_t^\top X_t)^{-1} (y_t - X_t^\top \mathbf{w}^{(m_t)}<em>{t-1}) - \mathbf{w}^{(m)} = (I - X_t (X_t^\top X_t)^{-1} X_t^\top) \mathbf{w}^{(m_t)}</em>{t-1} + X_t (X_t^\top X_t)^{-1} X_t^\top \mathbf{w}^{(m)} - \mathbf{w}^{(m)} = (I - P_t) \cdots (I - P_{T_1+1}) (\mathbf{w}^{(m)}_{T_1} - \mathbf{w}^{(m)})</span> 对于每个专家 <span class="math-inline">m \in [M]</span>。</p>
<p>由于正交投影矩阵 <span class="math-inline">P_t</span> 是非扩张算子，因此 <span class="math-inline">\forall t \in {T_1 + 1, \cdots, T}, |\mathbf{w}^{(m)}<em>t - \mathbf{w}^{(m)}|</em>\infty \leq |\mathbf{w}^{(m)}<em>{t-1} - \mathbf{w}^{(m)}|</em>\infty \leq \cdots \leq |\mathbf{w}^{(m)}<em>{T_1} - \mathbf{w}^{(m)}|</em>\infty</span>。由于每个专家 <span class="math-inline">m \in M_k</span> 的解空间 <span class="math-inline">W_k</span> 是固定的，我们进一步得到 <span class="math-inline">|\mathbf{w}^{(m)}<em>t - \mathbf{w}^{(m)}</em>{T_1+1}|<em>\infty = |\mathbf{w}^{(m)}_t - \mathbf{w}^{(m)} + \mathbf{w}^{(m)} - \mathbf{w}^{(m)}</em>{T_1+1}|<em>\infty \leq \max</em>{w_n, w_{n'} \in W_k} |w_n - w_{n'}|<em>\infty = O(\sigma</em>{1.5}^0)</span>，其中第一个不等式是基于并集界限，第二个不等式是因为每个任务的正交投影更新 <span class="math-inline">\mathbf{w}^{(m)}<em>t</span>，最后一个等式是因为 <span class="math-inline">|w_n - w</em>{n'}|<em>\infty = O(\sigma</em>{1.5}^0)</span> 对于任何两个在相同集合 <span class="math-inline">W_k</span> 中的真实情况。</p>
<h3 id="f-引理-2-的完整版本和证明">F 引理 2 的完整版本和证明<a class="anchor-link" href="#f-引理-2-的完整版本和证明" title="Permanent link">&para;</a></h3>
<p><strong>引理 2（完整版本）</strong>：如果 MoE 在任何轮次 <span class="math-inline">t \in [T]</span> 继续通过方程（9）更新 <span class="math-inline">\Theta_t</span>，我们得到：1）在轮次 <span class="math-inline">t_1 = \lceil \eta^{-1}\sigma^{-0.25}<em>0 M \rceil</span>，以下属性成立<br />
<div class="math-display"><br />
    |h_m(X</em>{t_1}, \theta^{(m)}<em>{t_1}) - h</em>{m'}(X_{t_1}, \theta^{(m')}<em>{t_1})| = \begin{cases} O(\sigma</em>{1.75}^0), &amp; \text{if } m, m' \in M_n \text{ under } M &gt; N \text{ or } m, m' \in M_k \text{ under } M &lt; N, \ \Theta(\sigma_{0.75}^0), &amp; \text{otherwise}. \end{cases}<br />
</div><br />
2）在轮次 <span class="math-inline">t_2 = \lceil \eta^{-1}\sigma^{-0.75}<em>0 M \rceil</span>，以下属性成立<br />
<div class="math-display"><br />
    |h_m(X</em>{t_2}, \theta^{(m)}<em>{t_2}) - h</em>{m'}(X_{t_2}, \theta^{(m')}<em>{t_2})| = O(\sigma</em>{1.75}^0), \forall m, m' \in [M].<br />
</div><br />
我们首先提出以下引理作为预备，然后在附录 F.3 中证明引理 2。</p>
<p>对于任何两个专家 <span class="math-inline">m, m'</span>，定义 <span class="math-inline">\Delta_\Theta = |\pi_m(X_t, \Theta_t) - \pi_{m'}(X_t, \Theta_t)|</span>。</p>
<p><strong>引理 14</strong>：在任何轮次 <span class="math-inline">t \in {T_1 + 1, \cdots, t_1}</span>，对于所有 <span class="math-inline">m \neq m_t</span>，以下属性成立<br />
<div class="math-display"><br />
    |\nabla_{\theta(m)<em>t} L</em>{\text{task}}^t - \nabla_{\theta(m_t)<em>t} L</em>{\text{task}}^t|<em>\infty = O(\sigma_0).<br />
</div><br />
让 <span class="math-inline">X_n</span> 和 <span class="math-inline">X</em>{n'}</span> 表示包含特征信号 <span class="math-inline">v_n</span> 和 <span class="math-inline">v_{n'}</span> 的两个特征矩阵。</p>
<p><strong>引理 15</strong>：在路由器学习阶段 <span class="math-inline">t \in {T_1 + 1, \cdots, t_1}</span>，对于任何两个满足以下条件的专家：1）<span class="math-inline">m \in M_n</span> 和 <span class="math-inline">m' \in M_{n'}</span> 且 <span class="math-inline">n \neq n'</span> 在 <span class="math-inline">M &gt; N</span> 下，或 2）<span class="math-inline">m \in M_k</span> 和 <span class="math-inline">m' \in M_{k'}</span> 且 <span class="math-inline">w_n \in W_k</span>，<span class="math-inline">w_{n'} \in W_{k'}</span> 且 <span class="math-inline">k \neq k'</span> 在 <span class="math-inline">M &lt; N</span> 下，以下属性成立：<br />
<div class="math-display"><br />
    \pi_m(X_n, \Theta_t) &gt; \pi_{m'}(X_n, \Theta_t), \pi_{m'}(X_{n'}, \Theta_t) &gt; \pi_m(X_{n'}, \Theta_t).<br />
</div><br />
<strong>引理 14 证明（F.1）</strong></p>
<p>证明：基于引理 5 的证明，对于任何 <span class="math-inline">m \neq m_t</span>，我们计算 <span class="math-inline">|\nabla_{\theta(m)<em>t} L</em>{\text{task}}^t - \nabla_{\theta(m_t)<em>t} L</em>{\text{task}}^t|<em>\infty = |(|\mathbf{w}^{(m_t)}_t - \mathbf{w}^{(m_t)}</em>{t-1}|<em>2^2 + \alpha M / t f(m_t)_t) \frac{\partial \pi</em>{m_t}(X_t, \Theta_t)}{\partial \theta(m)<em>t} - \frac{\partial \pi</em>{m_t}(X_t, \Theta_t)}{\partial \theta(m_t)<em>t}|</em>\infty</span>。<br />
<div class="math-display"><br />
    = O(\sigma_0 + \alpha M / t) \cdot \left| \frac{\partial \pi_{m_t}(X_t, \Theta_t)}{\partial \theta(m)<em>t} - \frac{\partial \pi</em>{m_t}(X_t, \Theta_t)}{\partial \theta(m_t)<em>t} \right|</em>\infty = O(\sigma_0),<br />
</div><br />
最后一个等式是因为 <span class="math-inline">\pi_{m_t}(X_t, \Theta_t) &lt; 1</span>，<span class="math-inline">|X_t|_\infty = O(1)</span> 和 <span class="math-inline">\alpha M / t \leq \sigma_0</span> 对于任何 <span class="math-inline">t \in {T_1 + 1, \cdots, t_1}</span>。</p>
<p><strong>引理 15 证明（F.2）</strong></p>
<p>证明：我们将使用与引理 1 证明中相同的方法，通过反证法证明引理 15。这里，我们只证明 <span class="math-inline">M &gt; N</span> 的情况。<span class="math-inline">M &lt; N</span> 的证明是类似的。</p>
<p>回想引理 1，方程（40）在 <span class="math-inline">t = T_1 + 1</span> 时成立。基于引理 13，我们有 <span class="math-inline">|\pi_m(X_n, \Theta_{T_1}) - \pi_m(X_{n'}, \Theta_{T_1})| = \Omega(\sigma_0.5^0)</span>。然后接下来，我们<br />
<div class="math-display"><br />
    \leq \max_{w_n, w_{n'} \in W_k} |w_n - w_{n'}|<em>\infty = O(\sigma</em>{1.5}^0),<br />
</div><br />
其中第一个不等式是基于并集界限，第二个不等式是因为每个任务的正交投影更新 <span class="math-inline">\mathbf{w}^{(m)}<em>t</span>，最后一个等式是因为 <span class="math-inline">|w_n - w</em>{n'}|<em>\infty = O(\sigma</em>{1.5}^0)</span> 对于任何两个在同一集合 <span class="math-inline">W_k</span> 中的真实情况。</p>
<p>这完成了引理 1 的证明。</p>
<h3 id="f-引理-2-的完整版本和证明_1">F 引理 2 的完整版本和证明<a class="anchor-link" href="#f-引理-2-的完整版本和证明_1" title="Permanent link">&para;</a></h3>
<p><strong>引理 2（完整版本）</strong>：如果 MoE 在任何轮次 <span class="math-inline">t \in [T]</span> 继续通过方程（9）更新 <span class="math-inline">\Theta_t</span>，我们得到：1）在轮次 <span class="math-inline">t_1 = \lceil \eta^{-1}\sigma^{-0.25}<em>0 M \rceil</span>，以下属性成立<br />
<div class="math-display"><br />
    |h_m(X</em>{t_1}, \theta^{(m)}<em>{t_1}) - h</em>{m'}(X_{t_1}, \theta^{(m')}<em>{t_1})| = \begin{cases} O(\sigma</em>{1.75}^0), &amp; \text{if } m, m' \in M_n \text{ under } M &gt; N \text{ or } m, m' \in M_k \text{ under } M &lt; N, \ \Theta(\sigma_{0.75}^0), &amp; \text{otherwise}. \end{cases}<br />
</div><br />
2）在轮次 <span class="math-inline">t_2 = \lceil \eta^{-1}\sigma^{-0.75}<em>0 M \rceil</span>，以下属性成立<br />
<div class="math-display"><br />
    |h_m(X</em>{t_2}, \theta^{(m)}<em>{t_2}) - h</em>{m'}(X_{t_2}, \theta^{(m')}<em>{t_2})| = O(\sigma</em>{1.75}^0), \forall m, m' \in [M].<br />
</div><br />
我们首先提出以下引理作为预备，然后正式在附录 F.3 中证明引理 2。</p>
<p>对于任何两个专家 <span class="math-inline">m, m'</span>，定义 <span class="math-inline">\Delta_\Theta = |\pi_m(X_t, \Theta_t) - \pi_{m'}(X_t, \Theta_t)|</span>。</p>
<p><strong>引理 14</strong>：在任何轮次 <span class="math-inline">t \in {T_1 + 1, \cdots, t_1}</span>，对于所有 <span class="math-inline">m \neq m_t</span>，以下属性成立<br />
<div class="math-display"><br />
    |\nabla_{\theta(m)<em>t} L</em>{\text{task}}^t - \nabla_{\theta(m_t)<em>t} L</em>{\text{task}}^t|<em>\infty = O(\sigma_0).<br />
</div><br />
<strong>引理 15</strong>：在路由器学习阶段 <span class="math-inline">t \in {T_1 + 1, \cdots, t_1}</span>，对于任何两个满足以下条件的专家：1）<span class="math-inline">m \in M_n</span> 和 <span class="math-inline">m' \in M</em>{n'}</span> 且 <span class="math-inline">n \neq n'</span> 在 <span class="math-inline">M &gt; N</span> 下，或 2）<span class="math-inline">m \in M_k</span> 和 <span class="math-inline">m' \in M_{k'}</span> 且 <span class="math-inline">w_n \in W_k</span>，<span class="math-inline">w_{n'} \in W_{k'}</span> 且 <span class="math-inline">k \neq k'</span> 在 <span class="math-inline">M &lt; N</span> 下，以下属性成立：<br />
<div class="math-display"><br />
    \pi_m(X_n, \Theta_t) &gt; \pi_{m'}(X_n, \Theta_t), \pi_{m'}(X_{n'}, \Theta_t) &gt; \pi_m(X_{n'}, \Theta_t).<br />
</div><br />
<strong>引理 2 证明（F.1）</strong></p>
<p>证明：基于引理 5，对于任何 <span class="math-inline">m \neq m_t</span>，我们计算 <span class="math-inline">|\nabla_{\theta(m)<em>t} L</em>{\text{task}}^t - \nabla_{\theta(m_t)<em>t} L</em>{\text{task}}^t|<em>\infty = |(|w^{(m_t)}_t - w^{(m_t)}</em>{t-1}|<em>2^2 + \alpha M / t f(m_t)_t) \frac{\partial \pi</em>{m_t}(X_t, \Theta_t)}{\partial \theta(m)<em>t} - \frac{\partial \pi</em>{m_t}(X_t, \Theta_t)}{\partial \theta(m_t)}|_\infty</span>。</p>
<p>由于 <span class="math-inline">\pi_{m_t}(X_t, \Theta_t) &lt; 1</span>，<span class="math-inline">|X_t|<em>\infty = O(1)</span> 和 <span class="math-inline">\alpha M / t \leq \sigma_0</span> 对于任何 <span class="math-inline">t \in {T_1 + 1, \cdots, t_1}</span>，我们最终得到 <span class="math-inline">|\nabla</em>{\theta(m)<em>t} L</em>{\text{task}}^t - \nabla_{\theta(m_t)<em>t} L</em>{\text{task}}^t|_\infty = O(\sigma_0)</span>。</p>
<p><strong>引理 15 证明（F.2）</strong></p>
<p>证明：我们将使用与引理 1 证明相同的方法，通过反证法来证明引理 15。这里，我们仅证明 <span class="math-inline">M &gt; N</span> 的情况。<span class="math-inline">M &lt; N</span> 的情况证明类似。</p>
<p>回顾引理 1，方程（40）在 <span class="math-inline">t = T_1 + 1</span> 时成立。基于引理 13，我们有 <span class="math-inline">|\pi_m(X_n, \Theta_{T_1}) - \pi_m(X_{n'}, \Theta_{T_1})| = \Omega(\sigma_0.5^0)</span>。然后，我们的目标是证明 <span class="math-inline">|\pi_m(X_n, \Theta_{T_1}) - \pi_{m'}(X_n, \Theta_{T_1})| = O(\sigma_0 \eta^{-0.5})</span> 在方程（38）中对任何 <span class="math-inline">m \in M_n</span> 和 <span class="math-inline">m' \in M_{n'}</span> 在路由器学习阶段始终成立。然后根据引理 1 的证明，方程（40）也是真的。</p>
<p>在方程（40）在 <span class="math-inline">t = T_1 + 1</span> 时，路由器将任务 <span class="math-inline">t</span> 路由到其最佳专家 <span class="math-inline">m_t \in M_{n_t}</span>，导致 <span class="math-inline">\nabla_{\theta(m)<em>t} L</em>{\text{loc}}^t = 0</span> 通过引理 12。因此，<span class="math-inline">\nabla_{\theta(m)<em>t} L</em>{\text{task}}^t = O(\sigma_0)</span> 使得 <span class="math-inline">\langle \theta^{(m_t)}<em>{t+1} - \theta^{(m_t)}_t, v_n \rangle = -O(\sigma</em>{1.5}^0)</span> 对于专家 <span class="math-inline">m_t</span> 和 <span class="math-inline">\langle \theta^{(m)}<em>{t+1} - \theta^{(m)}_t, v_n \rangle = O(\sigma</em>{1.5}^0)</span> 对于任何其他专家 <span class="math-inline">m \neq m_t</span> 在任务 <span class="math-inline">t = T_1 + 1</span>。</p>
<p>随后，对于任何两个专家 <span class="math-inline">m \in M_n</span> 和 <span class="math-inline">m' \in M_{n'}</span>，我们计算<br />
<div class="math-display"><br />
    |\theta(m)<em>{t_1} - \theta(m')</em>{t_1}|<em>\infty \leq |\theta(m)</em>{T_1+2} - \theta(m')<em>{T_1+2}|</em>\infty + (t_1 - T_1) \cdot O(\sigma_{1.5}^0) \leq O(\sigma_0 \eta^{-0.5}) + O(\eta^{-1} \sigma_{-0.25}^0) \cdot O(\sigma_{1.5}^0) = O(\sigma_0 \eta^{-0.5}),<br />
</div><br />
其中第一个不等式是因为 <span class="math-inline">\langle \theta^{(m)}<em>{t+1} - \theta^{(m)}_t, v_n \rangle = O(\sigma</em>{1.5}^0)</span>，第二个不等式是因为 <span class="math-inline">t_1 - T_1 \leq t_1</span>。</p>
<p>由于 <span class="math-inline">|\theta(m)<em>{t_1} - \theta(m')</em>{t_1}|<em>\infty = O(\sigma_0 \eta^{-0.5})</span> 且 <span class="math-inline">|\pi_m(X_n, \Theta</em>{T_1}) - \pi_m(X_{n'}, \Theta_{T_1})| = \Omega(\sigma_{0.5}^0)</span>，方程（40）对任何 <span class="math-inline">t \in {T_1 + 2, \cdots, t_1}</span> 成立，基于我们在引理 1 的证明。</p>
<p>对于 <span class="math-inline">M &lt; N</span> 的情况，我们可以用相同的方法证明方程（40）。这完成了引理 15 的证明。</p>
<p><strong>引理 2 最终证明（F.3）</strong></p>
<p>证明：我们首先关注 <span class="math-inline">M &gt; N</span> 的情况来证明 <span class="math-inline">|h_m(X_{t_1}, \theta^{(m)}<em>{t_1}) - h</em>{m'}(X_{t_1}, \theta^{(m')}<em>{t_1})| = O(\sigma</em>{1.75}^0)</span> 对于任何两个不同的专家 <span class="math-inline">m, m' \in M_n</span> 在同一个专家集合中在方程（12）中成立。然后我们证明 <span class="math-inline">|h_m(X_{t_1}, \theta^{(m)}<em>{t_1}) - h</em>{m'}(X_{t_1}, \theta^{(m')}<em>{t_1})| = \Theta(\sigma</em>{0.75}^0)</span> 对于任何两个专家 <span class="math-inline">m \in M_n</span> 和 <span class="math-inline">m' \in M_{n'}</span> 在不同专家集合中的方程（12）。之后，我们证明在训练轮次 <span class="math-inline">t_2 = \lceil \eta^{-1}\sigma^{-0.75}_0 M \rceil</span> 的方程（13）。最后，我们证明上述分析可以推广到 <span class="math-inline">M &lt; N</span> 的情况。</p>
<p>在 <span class="math-inline">M &gt; N</span> 的情况下，让 <span class="math-inline">M'</span> 和 <span class="math-inline">m'</span> 分别表示集合 <span class="math-inline">M_n</span> 中具有最大和最小 softmax 值的两个专家。换句话说，我们有 <span class="math-inline">M' = \arg\max_{m \in M_n} \pi_m(X_t, \Theta_t)</span>，<span class="math-inline">m' = \arg\min_{m \in M_n} \pi_m(X_t, \Theta_t)</span>，其中 <span class="math-inline">v_n \in X_t</span>。如果这两个专家满足 <span class="math-inline">|h_{M'}(X_{t_1}, \theta^{(M')}<em>{t_1}) - h</em>{m'}(X_{t_1}, \theta^{(m')}<em>{t_1})| = O(\sigma</em>{1.75}^0)</span>，则这个方程对任何两个在 <span class="math-inline">M_n</span> 中的专家都成立。</p>
<p>在路由器学习阶段开始时，我们有 <span class="math-inline">|\pi_{M'}(X_{T_1}, \Theta_{T_1}) - \pi_{m'}(X_{T_1}, \Theta_{T_1})| = O(\sigma_0.75^0)</span>，基于引理 1。</p>
<p>根据方程（1）中的路由策略，如果新任务 <span class="math-inline">t</span> 有真实情况 <span class="math-inline">w_n</span>，它总是被路由到专家 <span class="math-inline">M'</span> 直到其 softmax 值减小到比其他专家小。因此，我们计算路由器学习阶段中专家 <span class="math-inline">M'</span> 的减少门控输出：<br />
<div class="math-display"><br />
    \langle \theta^{(M')}<em>{T_1+1} - \theta^{(M')}_t1, v_n \rangle \leq O(\sigma_0) \cdot \eta \cdot (t_1 - T_1) = O(\sigma_0.75^0).<br />
</div><br />
而对于专家 <span class="math-inline">m'</span>，它将不会被路由直到其 softmax 值增加到最大。因此，我们同样计算增加的门控输出 <span class="math-inline">\langle \theta^{(m')}_t1 - \theta^{(m')}</em>{T_1+1}, v_n \rangle = O(\sigma_0.75^0)</span> 对于专家 <span class="math-inline">m'</span>。</p>
<p>基于引理 4 和引理 14，专家 <span class="math-inline">m'</span> 和 <span class="math-inline">M'</span> 的门控网络参数将收敛到相同的值，误差小于 <span class="math-inline">\Theta_t</span> 的更新步骤。因此，我们得到<br />
<div class="math-display"><br />
    |\theta^{(M')}<em>t1 - \theta^{(m')}_t1|</em>\infty = |\nabla_{\theta(m_t1-1)} L_{\text{task}}^{t1-1} \cdot \eta|<em>\infty = |\nabla</em>{\theta(m_t1-1)} L_{\text{aux}}^{t1-1} \cdot \eta|<em>\infty = O(\sigma</em>{1.75}^0),<br />
</div><br />
基于 <span class="math-inline">\nabla_{\theta(m_t1-1)} L_{\text{aux}}^{t1-1} = O(\sigma_{1.25}^0)</span> 和 <span class="math-inline">\eta = O(\sigma_{0.5}^0)</span> 的事实。然后根据引理 10，我们得到 <span class="math-inline">|h_{M'}(X_{t_1}, \theta^{(M')}<em>t1) - h</em>{m'}(X_{t_1}, \theta^{(m')}<em>{t_1})| = O(\sigma</em>{1.75}^0)</span>，这也适用于任何两个在同一个专家集合 <span class="math-inline">M_n</span> 中的专家。</p>
<p>接下来，我们证明 <span class="math-inline">|h_m(X_{t_1}, \theta^{(m)}<em>{t_1}) - h</em>{m'}(X_{t_1}, \theta^{(m')}<em>{t_1})| = \Theta(\sigma</em>{0.75}^0)</span> 在方程（12）中对于专家 <span class="math-inline">m'</span> 在方程（41）中的另一个专家 <span class="math-inline">m \notin M_n</span>。</p>
<p>让 <span class="math-inline">\bar{m} \in M_{n'}</span> 表示具有最大 softmax 值的专家，对于其他专家集合中的任何数据 <span class="math-inline">X_n</span>，其中 <span class="math-inline">v_n \in X_t</span>。根据引理 1 的证明，我们得到 <span class="math-inline">\pi_{m'}(X_{T_1}, \Theta_{T_1}) - \pi_{\bar{m}}(X_{T_1}, \Theta_{T_1}) = O(\sigma_{0.75}^0)</span>。这个方程表明，在路由器学习阶段 <span class="math-inline">t \in {T_1 + 1, \cdots, t_1}</span>，任何任务到达 <span class="math-inline">n_t</span> 与真实情况 <span class="math-inline">w_{n_t} = w_n</span> 不会被路由到专家 <span class="math-inline">\bar{m}</span>。因此，<span class="math-inline">\pi_{\bar{m}}(X_t, \Theta_t)</span> 随着 <span class="math-inline">t</span> 持续增加。然后我们计算专家 <span class="math-inline">\bar{m}</span> 和 <span class="math-inline">m'</span> 每轮 <span class="math-inline">t</span> 的参数梯度差异：<br />
<div class="math-display"><br />
    \nabla_{\theta(\bar{m})<em>t} L</em>{\text{task}}^t - \nabla_{\theta(m')<em>t} L</em>{\text{task}}^t = \alpha M / t f(m_t)<em>t \pi</em>{m_t}(X_t, \Theta_t)(\pi_{m'}(X_t, \Theta_t) - \pi_{\bar{m}}(X_t, \Theta_t)) \cdot \sum_{i \in [s_t]} X_{t,i} \geq 0,<br />
</div><br />
基于 <span class="math-inline">\pi_{m'}(X_t, \Theta_t) - \pi_{\bar{m}}(X_t, \Theta_t) &gt; 0</span> 通过引理 13。由于 <span class="math-inline">\nabla_{\theta(\bar{m})<em>t} L</em>{\text{task}}^t &lt; 0</span> 和 <span class="math-inline">\nabla_{\theta(m')<em>t} L</em>{\text{task}}^t &lt; 0</span>，我们得到 <span class="math-inline">h_{m'}(X_t, \theta^{(m')}<em>t) - h</em>{\bar{m}}(X_t, \theta^{(\bar{m})}<em>t)</span> 随着 <span class="math-inline">t</span> 增加。根据我们对 <span class="math-inline">h</em>{m'}(X_t, \theta^{(m')}<em>t)</span> 的先前分析，我们得到<br />
<div class="math-display"><br />
    |h</em>{m'}(X_{t_1}, \theta^{(m')}<em>{t_1}) - h</em>{\bar{m}}(X_{t_1}, \theta^{(\bar{m})}<em>{t_1})| = |h</em>{m'}(X_{T_1}, \theta^{(m')}<em>{T_1}) - h</em>{\bar{m}}(X_{T_1}, \theta^{(\bar{m})}<em>{T_1})| + |\nabla</em>{\theta(\bar{m})<em>t} L</em>{\text{task}}^t - \nabla_{\theta(m')<em>t} L</em>{\text{task}}^t|<em>\infty \cdot \eta \cdot (t_1 - T_1) = O(\sigma</em>{0.75}^0) + \Theta(\sigma_{0.75}^0) = \Theta(\sigma_{0.75}^0),<br />
</div><br />
其中第一个等式是因为 <span class="math-inline">|\nabla_{\theta(\bar{m})<em>t} L</em>{\text{task}}^t - \nabla_{\theta(m')<em>t} L</em>{\text{task}}^t|_\infty = O(\sigma_0)</span>。这完成了方程（12）的证明。</p>
<p>随后，我们证明 <span class="math-inline">|h_m(X_{t_2}, \theta^{(m)}<em>{t_2}) - h</em>{m'}(X_{t_2}, \theta^{(m')}<em>{t_2})| = O(\sigma</em>{1.75}^0)</span> 对于任何 <span class="math-inline">m, m' \in [M]</span> 在方程（13）中</p>
<p>通过证明 <span class="math-inline">|h_{M'}(X_{t_2}, \theta^{(M')}<em>{t_2}) - h_m(X</em>{t_2}, \theta^{(m)}<em>{t_2})| = O(\sigma</em>{1.75}^0)</span> 在 <span class="math-inline">M' \in M_n</span> 的方程（41）中和任何其他专家 <span class="math-inline">m \notin M_n</span> 之间。</p>
<p>基于引理 7，我们得到<br />
<div class="math-display"><br />
    \langle \theta^{(m)}<em>{t+1} - \theta^{(m)}_t, v_n \rangle = \begin{cases} -O(\sigma</em>{1.25}^0), &amp; \text{if } m = m_t, \ O(M^{-1}\sigma_{1.25}^0), &amp; \text{if } m \neq m_t. \end{cases}<br />
</div><br />
根据我们的分析，专家 <span class="math-inline">M' \in M_n</span> 在 <span class="math-inline">t \in {t_1, \cdots, t_2}</span> 期间定期被路由器选中以训练任务 <span class="math-inline">n_t = n</span>。在专家 <span class="math-inline">M'</span> 训练任务 <span class="math-inline">n</span> 后，其门控输出 <span class="math-inline">h_{M'}(X_t, \theta^{(M')}<em>t)</span> 减少了 <span class="math-inline">O(\sigma</em>{1.25}^0)</span>。而在没有被选中的其他轮次中，其门控输出增加了 <span class="math-inline">O(M^{-1}\sigma_{1.25}^0)</span>。在这样的训练行为下，我们得到 <span class="math-inline">|h_{M'}(X_{t_1}, \theta^{(M')}<em>t1) - h</em>{M'}(X_{t_2}, \theta^{(M')}<em>t2)| = O(\sigma</em>{1.75}^0)</span>，假设 <span class="math-inline">v_n \in X_{t_1}</span> 和 <span class="math-inline">v_n \in X_{t_2}</span>。</p>
<p>而对于专家 <span class="math-inline">m \notin M_n</span>，其门控输出 <span class="math-inline">h_m(X_t, \theta^{(m)}<em>t)</span> 对于任何任务 <span class="math-inline">n</span> 的数据 <span class="math-inline">X_t</span> 持续增加。对于任何 <span class="math-inline">t \in {t_1, \cdots, t_2}</span>，我们有 <span class="math-inline">|\nabla</em>{\theta(m)<em>t} L</em>{\text{aux}}^t|<em>\infty = O(\sigma</em>{1.25}^0)</span> 对于专家 <span class="math-inline">m</span>。假设专家 <span class="math-inline">m</span> 在 <span class="math-inline">t \in {t_1, \cdots, t_2}</span> 期间从未被路由器选中以训练任务 <span class="math-inline">n_t = n</span>，我们得到 <span class="math-inline">|h_m(X_{t_2}, \theta^{(m)}<em>{t_2}) - h_m(X</em>{t_1}, \theta^{(m)}<em>{t_1})| = |\nabla</em>{\theta(m)<em>t} L</em>{\text{aux}}^t|<em>\infty \cdot (t_2 - t_1) \cdot \eta &gt; |h</em>{M'}(X_{t_1}, \theta^{(M')}<em>t1) - h_m(X</em>{t_1}, \theta^{(m)}<em>{t_1})|</span>，其中不等式是因为 <span class="math-inline">|\nabla</em>{\theta(m)<em>t} L</em>{\text{aux}}^t|<em>\infty \cdot (t_2 - t_1) \cdot \eta = O(\sigma_0.5^0)</span> 且 <span class="math-inline">|h</em>{M'}(X_{t_1}, \theta^{(M')}<em>t1) - h_m(X</em>{t_1}, \theta^{(m)}<em>{t_1})| = \Theta(\sigma</em>{0.75}^0)</span>。这个不等式表明存在一个训练轮次 <span class="math-inline">t' \in {t_1, \cdots, t_2}</span> 使得 <span class="math-inline">h_m(X_{t'}, \theta^{(m)}<em>{t'}) &gt; h</em>{M'}(X_{t'}, \theta^{(M')}<em>t')</span> 对于任务到达 <span class="math-inline">n</em>{t'} = n</span>。因此，专家 <span class="math-inline">m'</span> 被选中再次训练任务 <span class="math-inline">n</span>，意味着 <span class="math-inline">m' \in M_n</span> 在轮次 <span class="math-inline">t'</span>。然后专家 <span class="math-inline">m</span> 和 <span class="math-inline">M'</span> 的门控网络参数将收敛到相同的值，误差为 <span class="math-inline">O(\sigma_{1.75}^0)</span>，基于引理 4 和引理 14。这完成了方程（13）在 <span class="math-inline">M &gt; N</span> 情况下的证明。</p>
<p>在 <span class="math-inline">M &lt; N</span> 的情况下，让 <span class="math-inline">M'</span> 和 <span class="math-inline">m'</span> 分别表示集合 <span class="math-inline">M_k</span> 中具有最大和最小 softmax 值的专家。换句话说，我们有 <span class="math-inline">M' = \arg\max_{m \in M_k} \pi_m(X_t, \Theta_t)</span>，<span class="math-inline">m' = \arg\min_{m \in M_k} \pi_m(X_t, \Theta_t)</span>，其中任务 <span class="math-inline">t</span> 的真实情况满足 <span class="math-inline">w_n \in W_k</span>。</p>
<p>根据引理 2 的证明，门控网络参数的专家 <span class="math-inline">m'</span> 和 <span class="math-inline">M'</span> 将在路由器学习阶段结束时收敛到相同的值，误差小于 <span class="math-inline">\Theta_t</span> 的更新步骤。因此，我们得到<br />
<div class="math-display"><br />
    |\theta^{(M')}<em>t1 - \theta^{(m')}_t1|</em>\infty = |\nabla_{\theta(m_t1-1)} L_{\text{task}}^{t1-1} \cdot \eta|<em>\infty = |(\nabla</em>{\theta(m_t1-1)} L_{\text{loc}}^{t1-1} + \nabla_{\theta(m_t1-1)} L_{\text{aux}}^{t1-1}) \cdot \eta|<em>\infty = O(\sigma</em>{1.75}^0),<br />
</div><br />
基于 <span class="math-inline">\nabla_{\theta(m_t1-1)} L_{\text{aux}}^{t1-1} = O(\sigma_{1.25}^0)</span> 和 <span class="math-inline">\nabla_{\theta(m_t1-1)} L_{\text{loc}}^{t1-1} = O(\sigma_{1.5}^0)</span> 从引理 12 得出。然后我们得到 <span class="math-inline">|h_{M'}(X_{t_1}, \theta^{(M')}<em>t1) - h</em>{m'}(X_{t_1}, \theta^{(m')}<em>{t_1})| = O(\sigma</em>{1.75}^0)</span>，这也适用于任何两个在同一个专家集合 <span class="math-inline">M_k</span> 中的专家。</p>
<p>类似地，对于任何两个在不同专家集合中的专家，我们可以推导出 <span class="math-inline">|h_m(X_{t_1}, \theta^{(m)}<em>{t_1}) - h</em>{m'}(X_{t_1}, \theta^{(m')}<em>{t_1})| = \Theta(\sigma</em>{0.75}^0)</span>。对于训练轮次 <span class="math-inline">t_2</span>，<span class="math-inline">M &lt; N</span> 的情况证明与 <span class="math-inline">M &gt; N</span> 的情况相同，因此我们在这里省略。</p>
<h3 id="g-引理-3-的完整版本和证明">G 引理 3 的完整版本和证明<a class="anchor-link" href="#g-引理-3-的完整版本和证明" title="Permanent link">&para;</a></h3>
<p><strong>引理 3（完整版本）</strong>：在算法 1 下，MoE 从轮次 <span class="math-inline">T_2 = O(\eta^{-1}\sigma^{-0.25}_0 M)</span> 开始终止更新 <span class="math-inline">\Theta_t</span>。然后对于任何任务到达 <span class="math-inline">n_t</span> 在 <span class="math-inline">t &gt; T_2</span>，以下属性成立：</p>
<p>1) 如果 <span class="math-inline">M &gt; N</span>，路由器选择任何专家 <span class="math-inline">m \in M_{n_t}</span> 的概率相同，为 <span class="math-inline">\frac{1}{|M_{n_t}|}</span>，其中 <span class="math-inline">|M_{n_t}|</span> 是集合 <span class="math-inline">M_{n_t}</span> 中的专家数量。<br />
2) 如果 <span class="math-inline">M &lt; N</span> 且 <span class="math-inline">w_{n_t} \in W_k</span>，路由器选择任何专家 <span class="math-inline">m \in M_k</span> 的概率相同，为 <span class="math-inline">\frac{1}{|M_k|}</span>，其中 <span class="math-inline">|M_k|</span> 是集合 <span class="math-inline">M_k</span> 中的专家数量。</p>
<p><strong>引理 3 证明</strong></p>
<p>在 <span class="math-inline">M &gt; N</span> 的情况下，根据算法 1 和引理 2，停止更新 <span class="math-inline">\Theta_t</span> 后，以下属性成立：1）对于任何 <span class="math-inline">m \in M_n</span> 和 <span class="math-inline">m' \in M_{n'}</span>，<span class="math-inline">|h_m(X_t, \theta^{(m)}<em>t) - h</em>{m'}(X_t, \theta^{(m')}<em>t)| = \Theta(\sigma</em>{0.75}^0)</span>；2）对于任何 <span class="math-inline">m, m' \in M_n</span>，<span class="math-inline">|h_m(X_t, \theta^{(m)}<em>t) - h</em>{m'}(X_t, \theta^{(m')}<em>t)| = O(\Gamma)</span>，其中 <span class="math-inline">\Gamma = O(\sigma</em>{1.25}^0)</span>。</p>
<p>如果任务到达 <span class="math-inline">n_t</span> 的真实情况满足 <span class="math-inline">w_{n_t} = w_n</span>，对于任何专家 <span class="math-inline">m \in M_n</span> 和 <span class="math-inline">m' \notin M_n</span>，我们有<br />
<div class="math-display"><br />
    h_m(X_t, \theta^{(m)}<em>t) + r(m)_t - (h</em>{m'}(X_t, \theta^{(m')}<em>t) + r(m')_t) \geq h_m(X_t, \theta^{(m)}_t) - h</em>{m'}(X_t, \theta^{(m')}<em>t) + r(m')_t = \Theta(\sigma</em>{0.75}^0),<br />
</div><br />
给定 <span class="math-inline">r(m')<em>t = \Theta(\sigma</em>{1.25}^0)</span> 和 <span class="math-inline">h_m(X_t, \theta^{(m)}<em>t) - h</em>{m'}(X_t, \theta^{(m')}<em>t) = \Theta(\sigma</em>{0.75}^0)</span>。因此，任何专家 <span class="math-inline">m' \notin M_n</span> 都不会被选中来学习任务 <span class="math-inline">t</span>，只有集合 <span class="math-inline">M_n</span> 中的专家会被选中。</p>
<p>对于任何专家 <span class="math-inline">m \in M_n</span>，我们计算<br />
<div class="math-display"><br />
    P(m_t = m \mid m \in M_n) = P\left( \max_{m' \in M_n} {h_{m'}(X_t, \theta^{(m')}<em>t) + r(m')_t} = h_m(X_t, \theta^{(m)}_t) + r(m)_t \right) = \frac{1}{|M_n|},<br />
</div><br />
其中第三个等式是因为 <span class="math-inline">r(m)_t = \Theta(\sigma</em>{1.25}^0)</span> 和 <span class="math-inline">|h_m(X_t, \theta^{(m)}<em>t) - h</em>{m'}(X_t, \theta^{(m')}<em>t)| = O(\sigma</em>{1.25}^0)</span> 由算法 1 得出，最后一个等式是因为 <span class="math-inline">r(m)_t</span> 服从均匀分布 Unif[0, <span class="math-inline">\lambda</span>]。</p>
<p>在 <span class="math-inline">M &lt; N</span> 的情况下，我们类似地得出以下属性：1）对于任何 <span class="math-inline">m \in M_k</span> 和 <span class="math-inline">m' \in M_{k'}</span>，<span class="math-inline">|h_m(X_t, \theta^{(m)}<em>t) - h</em>{m'}(X_t, \theta^{(m')}<em>t)| = \Theta(\sigma</em>{0.75}^0)</span>；2）对于任何 <span class="math-inline">m, m' \in M_k</span>，<span class="math-inline">|h_m(X_t, \theta^{(m)}<em>t) - h</em>{m'}(X_t, \theta^{(m')}<em>t)| = O(\Gamma)</span>。基于这两个属性，<span class="math-inline">h_m(X_t, \theta^{(m)}_t) + r(m)_t - (h</em>{m'}(X_t, \theta^{(m')}<em>t) + r(m')_t) = \Theta(\sigma</em>{0.75}^0)</span> 始终对任何两个不在同一个专家集合 <span class="math-inline">M_k</span> 中的专家 <span class="math-inline">m</span> 和 <span class="math-inline">m'</span> 成立。进一步，我们可以类似地计算<br />
<div class="math-display"><br />
    P(m_t = m \mid m \in M_k) = P\left( r(m)_t &gt; r(m')_t, \forall m' \in M_k \right) = \frac{1}{|M_k|}.<br />
</div><br />
这完成了引理 3 的证明。</p>
<h3 id="h-引理-4-的证明">H 引理 4 的证明<a class="anchor-link" href="#h-引理-4-的证明" title="Permanent link">&para;</a></h3>
<p><strong>引理 4</strong>：在单专家系统中，基于方程（15）和（42）在引理 16 中，对于任何训练轮次 <span class="math-inline">t \in [T]</span> 和 <span class="math-inline">i \in {1, \cdots, t}</span>，我们计算：<br />
<div class="math-display"><br />
    E[F_t] = \frac{1}{t-1} \sum_{i=1}^{t-1} E \left[ |\mathbf{w}^{(m_i)}<em>t - w</em>{n_i}|<em>2^2 - |\mathbf{w}^{(m_i)}_i - w</em>{n_i}|<em>2^2 \right] = \frac{1}{t-1} \sum</em>{i=1}^{t-1} \left[ (r^t - r^i) E[|w_{n_i}|<em>2^2] + \sum</em>{l=1}^t (1 - r)r^{t-l} E[|w_{n_l} - w_{n_i}|<em>2^2] - i \sum</em>{j=1}^{i} (1 - r)r^{i-j} E[|w_{n_j} - w_{n_i}|_2^2] \right],<br />
</div></p>
<p><div class="math-display"><br />
    E[G_T] = \frac{1}{T} \sum_{i=1}^T E[|\mathbf{w}^{(m_i)}<em>T - w</em>{n_i}|<em>2^2] = \frac{1}{T} \sum</em>{i=1}^T \left[ r^T E[|w_{n_i}|<em>2^2] + \sum</em>{l=1}^T (1 - r)r^{T-l} E[|w_{n_l} - w_{n_i}|<em>2^2] \right].<br />
</div><br />
这里，我们让 <span class="math-inline">w</em>{n_i}</span> 表示第 <span class="math-inline">i</span> 轮任务到达的真实情况。最后的等式是基于 <span class="math-inline">E[|w_{n_i}|<em>2^2] = \frac{1}{N} \sum</em>{n=1}^N |w_n|<em>2^2</span> 和 <span class="math-inline">E[|w</em>{n_j} - w_{n_i}|<em>2^2] = E\left[ \frac{1}{N} \sum</em>{n=1}^N |w_{n_j} - w_n|<em>2^2 \right] = \frac{1}{N^2} \sum</em>{n=n'} |w_{n'} - w_n|_2^2</span> 当只有一个专家时。</p>
<h3 id="i-定理-1-的证明">I 定理 1 的证明<a class="anchor-link" href="#i-定理-1-的证明" title="Permanent link">&para;</a></h3>
<p>在证明定理 1 之前，我们首先提出以下引理。然后我们在附录 I.2 中正式证明定理 1。</p>
<p>对于专家 <span class="math-inline">m</span>，让 <span class="math-inline">\tau(m)(l) \in {1, \cdots, T_1}</span> 表示路由器在探索阶段第 <span class="math-inline">l</span> 次选择专家 <span class="math-inline">m</span> 的训练轮次。例如，<span class="math-inline">\tau(1)(2) = 5</span> 表示第 5 轮是路由器第二次选择专家 1。</p>
<p><strong>引理 16</strong>：在任何轮次 <span class="math-inline">t \in {T_1 + 1, \cdots, T}</span>，对于 <span class="math-inline">i \in {T_1 + 1, \cdots, t}</span>，我们有 <span class="math-inline">|\mathbf{w}^{(m_i)}<em>t - w</em>{n_i}|<em>2^2 = |\mathbf{w}^{(m_i)}</em>{T_1} - w_{n_i}|<em>2^2</span>。而在任何轮次 <span class="math-inline">t \in {1, \cdots, T_1}</span>，对于任何 <span class="math-inline">i \in {1, \cdots, t}</span>，我们有<br />
<div class="math-display"><br />
    E[|\mathbf{w}^{(m_i)}_t - w</em>{n_i}|<em>2^2] = r^{L(m_i)_t} E[|w</em>{n_i}|<em>2^2] + \sum</em>{l=1}^{L(m_i)<em>t} (1 - r)r^{L(m_i)_t - l} E[|\mathbf{w}^{(m_i)}</em>{\tau(m_i)(l)} - w_{n_i}|_2^2],<br />
</div><br />
其中 <span class="math-inline">L(m_i)_t = t \cdot f(m_i)_t</span> 且 <span class="math-inline">r = 1 - \frac{s}{d}</span>。</p>
<p><strong>引理 16 证明（I.1）</strong></p>
<p>证明：在任何轮次 <span class="math-inline">t \in {T_1 + 1, \cdots, T}</span>，我们有 <span class="math-inline">\mathbf{w}^{(m)}<em>t = \mathbf{w}^{(m)}</em>{T_1}</span>，基于命题 1 和命题 2。因此，对于任何轮次 <span class="math-inline">t \in {T_1 + 1, \cdots, T}</span> 和 <span class="math-inline">i \in {T_1 + 1, \cdots, t}</span>，<span class="math-inline">|\mathbf{w}^{(m_i)}<em>t - w</em>{n_i}|<em>2^2 = |\mathbf{w}^{(m_i)}</em>{T_1} - w_{n_i}|_2^2</span> 成立。</p>
<p>接下来，我们证明方程（42）对于轮次 <span class="math-inline">t \in {1, \cdots, T_1}</span>。定义 <span class="math-inline">P_t = X_t (X_t^\top X_t)^{-1} X_t^\top</span> 对于任务 <span class="math-inline">t</span>。在当前任务 <span class="math-inline">t</span>，共有 <span class="math-inline">L(m)_t = t \cdot f(m)_t</span> 个任务被路由到专家 <span class="math-inline">m</span>，其中 <span class="math-inline">f(m)_t</span> 在方程（7）中给出。</p>
<p>基于 <span class="math-inline">\mathbf{w}^{(m)}<em>t</span> 的更新规则（方程 5），我们计算<br />
<div class="math-display"><br />
    |\mathbf{w}^{(m_i)}_t - w</em>{n_i}|<em>2^2 = |\mathbf{w}^{(m_i)}</em>{\tau(m_i)(L(m_i)<em>t)} - w</em>{n_i}|<em>2^2 = |(I - P_t)\mathbf{w}^{(m_i)}</em>{\tau(m_i)(L(m_i)<em>t - 1)} + P_t w</em>{\tau(m_i)(L(m_i)<em>t)} - w</em>{n_i}|_2^2,<br />
</div><br />
其中第一个等式是因为在 <span class="math-inline">\tau(m_i)(L(m_i)_t), \cdots, t</span> 之间没有更新 <span class="math-inline">\mathbf{w}^{(m_i)}_t</span>，第二个等式是基于方程（5）。</p>
<p>由于 <span class="math-inline">P_t</span> 是 <span class="math-inline">X_t</span> 行空间的正交投影矩阵，基于标准正态分布的旋转对称性，我们有 <span class="math-inline">E|P_t(w_{\tau(m_i)(L(m_i)<em>t)} - w</em>{n_i})| = \frac{s}{d} |w_{\tau(m_i)(L(m_i)<em>t)} - w</em>{n_i}|<em>2^2</span>。然后我们进一步计算<br />
<div class="math-display"><br />
    E[|\mathbf{w}^{(m_i)}_t - w</em>{n_i}|<em>2^2] = (1 - \frac{s}{d}) E[|\mathbf{w}^{(m_i)}</em>{\tau(m_i)(L(m_i)<em>t - 1)} - w</em>{n_i}|<em>2^2] + \frac{s}{d} E[|w</em>{\tau(m_i)(L(m_i)<em>t)} - w</em>{n_i}|<em>2^2] = (1 - \frac{s}{d}) L(m_i)_t E[|\mathbf{w}^{(m_i)}_0 - w</em>{n_i}|<em>2^2] + \sum</em>{l=1}^{L(m_i)<em>t} (1 - \frac{s}{d}) \frac{s}{d} E[|w</em>{\tau(m_i)(l)} - w_{n_i}|<em>2^2] = r^{L(m_i)_t} E[|w</em>{n_i}|<em>2^2] + \sum</em>{l=1}^{L(m_i)<em>t} (1 - r)r^{L(m_i)_t - l} E[|w</em>{\tau(m_i)(l)} - w_{n_i}|_2^2],<br />
</div><br />
其中第二个等式是基于迭代计算，最后一个等式是因为 <span class="math-inline">\mathbf{w}^{(m)}_0 = 0</span> 对于任何专家 <span class="math-inline">m</span>。这里我们用 <span class="math-inline">r = 1 - \frac{s}{d}</span> 简化符号。</p>
<p><strong>定理 1 最终证明（I.2）</strong></p>
<p>证明：基于引理 16 中的方程（42），我们得到<br />
<div class="math-display"><br />
    E[|\mathbf{w}^{(m_i)}<em>i - w</em>{n_i}|<em>2^2] = r^{L(m_i)_i} E[|w</em>{n_i}|<em>2^2] + \sum</em>{l=1}^{L(m_i)<em>i} (1 - r)r^{L(m_i)_i - l} E[|\mathbf{w}^{(m_i)}</em>{\tau(m_i)(l)} - w_{n_i}|_2^2],<br />
</div><br />
其中 <span class="math-inline">\tau(m_i)(L(m_i)_i) = i</span>。</p>
<p>然后在任何轮次 <span class="math-inline">t \in {2, \cdots, T_1}</span>，我们计算预期遗忘为：<br />
<div class="math-display"><br />
    E[F_t] = \frac{1}{t-1} \sum_{i=1}^{t-1} E \left[ |\mathbf{w}^{(m_i)}<em>t - w</em>{n_i}|<em>2^2 - |\mathbf{w}^{(m_i)}_i - w</em>{n_i}|<em>2^2 \right] = \frac{1}{t-1} \sum</em>{i=1}^{t-1} \left[ (r^{L(m_i)<em>t} - r^{L(m_i)_i}) E[|w</em>{n_i}|<em>2^2] + \sum</em>{l=1}^{L(m_i)<em>t} (1 - r)r^{L(m_i)_t - l} E[|\mathbf{w}^{(m_i)}</em>{\tau(m_i)(l)} - w_{n_i}|<em>2^2] - \sum</em>{j=1}^{L(m_i)<em>i} (1 - r)r^{L(m_i)_i - j} E[|\mathbf{w}^{(m_i)}</em>{\tau(m_i)(j)} - w_{n_i}|<em>2^2] \right],<br />
</div><br />
其中 <span class="math-inline">c</em>{i,j} = (1 - r)(r^{L(m_i)_t} - L(m_i)_i + r^{L(m_i)_t} - j - r^j - L(m_i)_i)</span>。</p>
<p>由于任务 <span class="math-inline">i</span> 的真实情况 <span class="math-inline">w_{n_i}</span> 是从 <span class="math-inline">N</span> 个真实情况中随机抽取的，我们有<br />
<div class="math-display"><br />
    E[|w_{n_i}|<em>2^2] = \frac{1}{N} \sum</em>{n=1}^N |w_n|<em>2^2.<br />
</div><br />
根据引理 8 和命题 1，每个专家 <span class="math-inline">m</span> 将在 <span class="math-inline">T_1</span> 轮探索后收敛到一个专家集合 <span class="math-inline">M_n</span>。因此，我们得到<br />
<div class="math-display"><br />
    E[|\mathbf{w}^{(m_i)}</em>{\tau(m_i)(l)} - w_{n_i}|<em>2^2] &lt; \frac{1}{N^2} \sum</em>{n=n'} |w_{n'} - w_n|<em>2^2.<br />
</div><br />
而对于 <span class="math-inline">t \in {T_1 + 1, \cdots, T}</span>，基于命题 2，专家模型 <span class="math-inline">\math\mathbf{w}^{(m)}_t = \mathbf{w}^{(m)}</em>{T_1}</span> 对于任何专家 <span class="math-inline">m \in [M]</span>。因此，我们计算在最后一轮训练 <span class="math-inline">T</span> 的任务 <span class="math-inline">n_T</span> 后的整体泛化误差：<br />
<div class="math-display"><br />
    E[G_T] = \frac{1}{T} \sum_{i=1}^T E[|\mathbf{w}^{(m_i)}<em>T - w</em>{n_i}|<em>2^2] = \frac{1}{T} \sum</em>{i=1}^T \left[ r^{L(m_i)<em>T} E[|w</em>{n_i}|<em>2^2] + \sum</em>{l=1}^T (1 - r)r^{L(m_i)<em>T - l} E[|\mathbf{w}^{(m_i)}</em>{\tau(m_i)(l)} - w_{n_i}|_2^2] \right].<br />
</div><br />
在上述方程中，第一项表示所有任务的平均模型误差，第二项则是基于任务相似性和模型差距的平均误差。</p>
<h3 id="j-定理-2-的证明">J 定理 2 的证明<a class="anchor-link" href="#j-定理-2-的证明" title="Permanent link">&para;</a></h3>
<p>证明：对于任何轮次 <span class="math-inline">t \in {1, \cdots, T_1}</span>，遗忘与定理 1 中的情况相同，因此我们在这里省略证明。</p>
<p>对于 <span class="math-inline">t \in {T_1 + 1, \cdots, T}</span>，路由器将任务在同一簇中路由到每个专家。为此，我们将 <span class="math-inline">t</span> 轮分为两个子区间：<span class="math-inline">i \in {1, \cdots, T_1}</span> 和 <span class="math-inline">i \in {T_1 + 1, \cdots, t}</span>，以计算遗忘：<br />
<div class="math-display"><br />
    E[F_t] = \frac{1}{t-1} \sum_{i=1}^{t-1} E \left[ |\mathbf{w}^{(m_i)}<em>t - w</em>{n_i}|<em>2^2 - |\mathbf{w}^{(m_i)}_i - w</em>{n_i}|<em>2^2 \right] = \frac{1}{t-1} \sum</em>{i=1}^{T_1} \left[ (r^{L(m_i)<em>t} - r^{L(m_i)_i}) E[|w</em>{n_i}|<em>2^2] + \sum</em>{l=1}^{L(m_i)<em>t} (1 - r)r^{L(m_i)_t - l} E[|\mathbf{w}^{(m_i)}</em>{\tau(m_i)(l)} - w_{n_i}|_2^2] \right] +<br />
</div></p>
<p><div class="math-display"><br />
    \frac{1}{t-1} \sum_{i=T_1 + 1}^{t} \left[ (r^{L(m_i)<em>t} - r^{L(m_i)_i}) E[|w</em>{n_i}|<em>2^2] + \sum</em>{l=1}^{L(m_i)<em>t} (1 - r)r^{L(m_i)_t - l} E[|\mathbf{w}^{(m_i)}</em>{\tau(m_i)(l)} - w_{n_i}|<em>2^2] \right].<br />
</div><br />
在这里，第一部分表示在探索阶段的遗忘，第二部分表示在路由器学习阶段的遗忘。对于 <span class="math-inline">t \in {1, \cdots, T_1}</span>，我们有<br />
<div class="math-display"><br />
    E[|\mathbf{w}^{(m_i)}_t - w</em>{n_i}|<em>2^2] = r^{L(m_i)_t} E[|w</em>{n_i}|<em>2^2] + \sum</em>{l=1}^{L(m_i)<em>t} (1 - r)r^{L(m_i)_t - l} E[|\mathbf{w}^{(m_i)}</em>{\tau(m_i)(l)} - w_{n_i}|_2^2].<br />
</div><br />
对于 <span class="math-inline">t \in {T_1 + 1, \cdots, T}</span>，我们可以得到类似的表达式。</p>
                </div>

                <!-- Comments Section (Giscus) -->
                <section class="comments-section">
                    <h3><i class="fas fa-comments"></i> 评论</h3>
                    <script src="https://giscus.app/client.js"
                        data-repo="Geeks-Z/Geeks-Z.github.io"
                        data-repo-id=""
                        data-category="Announcements"
                        data-category-id=""
                        data-mapping="pathname"
                        data-strict="0"
                        data-reactions-enabled="1"
                        data-emit-metadata="0"
                        data-input-position="bottom"
                        data-theme="preferred_color_scheme"
                        data-lang="zh-CN"
                        crossorigin="anonymous"
                        async>
                    </script>
                </section>
            </article>

            <footer class="post-footer">
                <p>© 2025 Hongwei Zhao. Built with ❤️</p>
            </footer>
        </main>
    </div>

    <!-- Theme Toggle Button -->
    <button class="theme-toggle" id="themeToggle" aria-label="切换主题">
        <i class="fas fa-moon"></i>
    </button>

    <script>
        // Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        const html = document.documentElement;
        const icon = themeToggle.querySelector('i');
        const hljsDark = document.getElementById('hljs-theme-dark');
        const hljsLight = document.getElementById('hljs-theme-light');
        
        // Check saved theme or system preference
        const savedTheme = localStorage.getItem('theme');
        const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
        
        if (savedTheme === 'dark' || (!savedTheme && prefersDark)) {
            html.setAttribute('data-theme', 'dark');
            icon.className = 'fas fa-sun';
            hljsDark.disabled = false;
            hljsLight.disabled = true;
        }
        
        themeToggle.addEventListener('click', () => {
            const isDark = html.getAttribute('data-theme') === 'dark';
            if (isDark) {
                html.removeAttribute('data-theme');
                icon.className = 'fas fa-moon';
                localStorage.setItem('theme', 'light');
                hljsDark.disabled = true;
                hljsLight.disabled = false;
            } else {
                html.setAttribute('data-theme', 'dark');
                icon.className = 'fas fa-sun';
                localStorage.setItem('theme', 'dark');
                hljsDark.disabled = false;
                hljsLight.disabled = true;
            }
            
            // Update Giscus theme
            const giscusFrame = document.querySelector('iframe.giscus-frame');
            if (giscusFrame) {
                giscusFrame.contentWindow.postMessage({
                    giscus: {
                        setConfig: {
                            theme: isDark ? 'light' : 'dark'
                        }
                    }
                }, 'https://giscus.app');
            }
        });
        
        // Highlight.js
        document.addEventListener('DOMContentLoaded', () => {
            hljs.highlightAll();
        });
        
        // KaTeX auto-render
        document.addEventListener('DOMContentLoaded', () => {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
        
        // TOC active state
        const tocLinks = document.querySelectorAll('.toc-container a');
        const headings = document.querySelectorAll('.post-content h2, .post-content h3, .post-content h4');
        
        function updateTocActive() {
            let current = '';
            headings.forEach(heading => {
                const rect = heading.getBoundingClientRect();
                if (rect.top <= 100) {
                    current = heading.getAttribute('id');
                }
            });
            
            tocLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === '#' + current) {
                    link.classList.add('active');
                }
            });
        }
        
        window.addEventListener('scroll', updateTocActive);
        updateTocActive();
    </script>
</body>
</html>
