<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Untitled</title>
    <meta name="description" content="Untitled - Hongwei Zhao's Blog">
    <meta name="author" content="Hongwei Zhao">
    
    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <!-- Code Highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css" id="hljs-theme-dark">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css" id="hljs-theme-light" disabled>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    
    <style>
        :root {
            /* Light Theme */
            --primary-color: #2980b9;
            --primary-hover: #1a5276;
            --link-color: #c0392b;
            --text-color: #333;
            --text-light: #666;
            --text-muted: #999;
            --bg-color: #fff;
            --bg-secondary: #f8f9fa;
            --bg-code: #f5f5f5;
            --border-color: #e5e7eb;
            --shadow: 0 1px 3px rgba(0,0,0,0.1);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.1);
        }

        [data-theme="dark"] {
            --primary-color: #5dade2;
            --primary-hover: #85c1e9;
            --link-color: #e74c3c;
            --text-color: #e5e7eb;
            --text-light: #9ca3af;
            --text-muted: #6b7280;
            --bg-color: #1a1a2e;
            --bg-secondary: #16213e;
            --bg-code: #0f0f23;
            --border-color: #374151;
            --shadow: 0 1px 3px rgba(0,0,0,0.3);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.4);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 16px;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            transition: background-color 0.3s, color 0.3s;
        }

        a {
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--primary-hover);
            text-decoration: underline;
        }

        /* Layout */
        .page-wrapper {
            display: flex;
            max-width: 1400px;
            margin: 0 auto;
            padding: 2rem;
            gap: 3rem;
        }

        /* Sidebar TOC */
        .toc-sidebar {
            width: 260px;
            flex-shrink: 0;
            position: sticky;
            top: 2rem;
            height: fit-content;
            max-height: calc(100vh - 4rem);
            overflow-y: auto;
        }

        .toc-container {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 1.5rem;
            border: 1px solid var(--border-color);
        }

        .toc-container h3 {
            font-size: 0.85rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-muted);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .toc-container ul {
            list-style: none;
        }

        .toc-container li {
            margin-bottom: 0.5rem;
        }

        .toc-container a {
            font-size: 0.9rem;
            color: var(--text-light);
            display: block;
            padding: 0.25rem 0;
            border-left: 2px solid transparent;
            padding-left: 0.75rem;
            transition: all 0.2s;
        }

        .toc-container a:hover,
        .toc-container a.active {
            color: var(--primary-color);
            border-left-color: var(--primary-color);
            text-decoration: none;
        }

        .toc-container ul ul {
            margin-left: 1rem;
        }

        /* Main Content */
        .main-content {
            flex: 1;
            min-width: 0;
            max-width: 800px;
        }

        /* Header */
        .post-header {
            margin-bottom: 2rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
            margin-bottom: 1.5rem;
        }

        .back-link:hover {
            color: var(--primary-color);
            text-decoration: none;
        }

        .post-header h1 {
            font-size: 2.25rem;
            font-weight: 700;
            line-height: 1.3;
            margin-bottom: 1rem;
            color: var(--text-color);
        }

        .post-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
        }

        .post-meta span {
            display: flex;
            align-items: center;
            gap: 0.4rem;
        }

        .post-meta i {
            color: var(--text-muted);
        }

        .tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 1rem;
        }

        .tag {
            display: inline-block;
            font-size: 0.8rem;
            background: var(--bg-secondary);
            color: var(--text-light);
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            border: 1px solid var(--border-color);
            transition: all 0.2s;
        }

        .tag:hover {
            background: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }

        /* Article Content */
        .post-content {
            font-size: 1rem;
            line-height: 1.9;
        }

        .post-content h1,
        .post-content h2,
        .post-content h3,
        .post-content h4,
        .post-content h5,
        .post-content h6 {
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-weight: 600;
            line-height: 1.4;
            color: var(--text-color);
        }

        .post-content h1 { font-size: 1.875rem; }
        .post-content h2 { 
            font-size: 1.5rem; 
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }
        .post-content h3 { font-size: 1.25rem; }
        .post-content h4 { font-size: 1.125rem; }

        .post-content p {
            margin-bottom: 1.25rem;
        }

        .post-content ul,
        .post-content ol {
            margin: 1.25rem 0;
            padding-left: 1.5rem;
        }

        .post-content li {
            margin-bottom: 0.5rem;
        }

        .post-content blockquote {
            margin: 1.5rem 0;
            padding: 1rem 1.5rem;
            background: var(--bg-secondary);
            border-left: 4px solid var(--primary-color);
            border-radius: 0 8px 8px 0;
            color: var(--text-light);
            font-style: italic;
        }

        .post-content blockquote p:last-child {
            margin-bottom: 0;
        }

        /* Code */
        .post-content code {
            font-family: 'JetBrains Mono', 'Fira Code', Consolas, monospace;
            font-size: 0.9em;
            background: var(--bg-code);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            color: var(--link-color);
        }

        .post-content pre {
            margin: 1.5rem 0;
            padding: 1.25rem;
            background: var(--bg-code);
            border-radius: 10px;
            overflow-x: auto;
            border: 1px solid var(--border-color);
        }

        .post-content pre code {
            background: none;
            padding: 0;
            color: inherit;
            font-size: 0.875rem;
            line-height: 1.6;
        }

        /* Images */
        .post-content img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            margin: 1.5rem 0;
            box-shadow: var(--shadow-lg);
        }

        /* Tables */
        .post-content table {
            width: 100%;
            margin: 1.5rem 0;
            border-collapse: collapse;
            font-size: 0.95rem;
        }

        .post-content th,
        .post-content td {
            padding: 0.75rem 1rem;
            border: 1px solid var(--border-color);
            text-align: left;
        }

        .post-content th {
            background: var(--bg-secondary);
            font-weight: 600;
        }

        .post-content tr:nth-child(even) {
            background: var(--bg-secondary);
        }

        /* Math */
        .math-display {
            margin: 1.5rem 0;
            overflow-x: auto;
            padding: 1rem;
            background: var(--bg-secondary);
            border-radius: 8px;
        }

        /* Anchor links */
        .anchor-link {
            opacity: 0;
            margin-left: 0.5rem;
            color: var(--text-muted);
            font-weight: 400;
            transition: opacity 0.2s;
        }

        .post-content h2:hover .anchor-link,
        .post-content h3:hover .anchor-link,
        .post-content h4:hover .anchor-link {
            opacity: 1;
        }

        /* Theme Toggle */
        .theme-toggle {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            background: var(--primary-color);
            color: white;
            border: none;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.25rem;
            box-shadow: var(--shadow-lg);
            transition: transform 0.2s, background 0.2s;
            z-index: 1000;
        }

        .theme-toggle:hover {
            transform: scale(1.1);
            background: var(--primary-hover);
        }

        /* Comments Section */
        .comments-section {
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-color);
        }

        .comments-section h3 {
            font-size: 1.25rem;
            margin-bottom: 1.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        /* Footer */
        .post-footer {
            margin-top: 3rem;
            padding-top: 1.5rem;
            border-top: 1px solid var(--border-color);
            text-align: center;
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        /* Responsive */
        @media (max-width: 1024px) {
            .toc-sidebar {
                display: none;
            }
            
            .page-wrapper {
                padding: 1.5rem;
            }
        }

        @media (max-width: 768px) {
            .post-header h1 {
                font-size: 1.75rem;
            }
            
            .post-meta {
                flex-direction: column;
                gap: 0.5rem;
            }
            
            .theme-toggle {
                bottom: 1rem;
                right: 1rem;
                width: 44px;
                height: 44px;
            }
        }

        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-secondary);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--border-color);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--text-muted);
        }
    </style>
</head>
<body>
    <div class="page-wrapper">
        <!-- TOC Sidebar -->
        <aside class="toc-sidebar">
            <div class="toc-container">
                <h3><i class="fas fa-list"></i> 目录</h3>
                <div class="toc">
<ul>
<li><a href="#0-摘要">0. 摘要</a></li>
<li><a href="#1-引言">1. 引言</a></li>
<li><a href="#2-相关工作">2. 相关工作</a><ul>
<li><a href="#21-使用强大预训练模型的三种-cl-策略">2.1 使用强大预训练模型的三种 CL 策略</a></li>
<li><a href="#22-用于创建特征交互的-rp-方法">2.2 用于创建特征交互的 RP 方法</a></li>
</ul>
</li>
<li><a href="#3-背景">3. 背景</a><ul>
<li><a href="#31-持续学习问题设置">3.1 持续学习问题设置</a></li>
<li><a href="#32-使用预训练模型的类原型策略">3.2 使用预训练模型的类原型策略</a></li>
<li><a href="#33-基于-gram-矩阵逆的-cp-分类">3.3 基于 Gram 矩阵逆的 CP 分类</a></li>
</ul>
</li>
<li><a href="#4-提出的方法与理论见解ranpac">4. 提出的方法与理论见解：RanPAC</a><ul>
<li><a href="#41-为什么二阶统计量对-cp-很重要">4.1 为什么二阶统计量对 CP 很重要</a></li>
<li><a href="#42-随机投影的详细概述与直觉">4.2 随机投影的详细概述与直觉</a></li>
<li><a href="#43-用于持续学习的随机投影">4.3 用于持续学习的随机投影</a></li>
<li><a href="#44-结合参数高效迁移学习和首次会话适应">4.4 结合参数高效迁移学习和首次会话适应</a></li>
<li><a href="#45-rp-层的内存使用">4.5 RP 层的内存使用</a></li>
</ul>
</li>
<li><a href="#5-实验">5. 实验</a></li>
<li><a href="#6-结论">6. 结论</a></li>
<li><a href="#附录-a-ranpac-概述及与其他策略的比较">附录 A RanPAC 概述及与其他策略的比较</a></li>
<li><a href="#附录-b-理论支持">附录 B 理论支持</a><ul>
<li><a href="#b1-投影向量范数的-chernoff-界">B.1 投影向量范数的 Chernoff 界</a></li>
<li><a href="#b2-增加投影维度的影响">B.2 增加投影维度的影响</a></li>
<li><a href="#b3-与最小二乘法的联系">B.3 与最小二乘法的联系</a></li>
<li><a href="#b4-与线性判别分析马氏距离和-zca-白化的联系">B.4 与线性判别分析、马氏距离和 ZCA 白化的联系</a><ul>
<li><a href="#b41-预备知识类原型">B.4.1 预备知识：类原型</a></li>
<li><a href="#b42-与-lda-和马氏距离的关系">B.4.2 与 LDA 和马氏距离的关系</a></li>
<li><a href="#b43-与-zca-白化的关系">B.4.3 与 ZCA 白化的关系</a></li>
<li><a href="#b44-去相关类原型">B.4.4 去相关类原型</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#附录-c-训练与实现">附录 C 训练与实现</a><ul>
<li><a href="#c1-在算法-1-中优化岭回归参数">C.1 在算法 1 中优化岭回归参数</a></li>
<li><a href="#c2-训练细节">C.2 训练细节</a></li>
<li><a href="#c3-训练和推理速度">C.3 训练和推理速度</a></li>
<li><a href="#c4-计算">C.4 计算</a></li>
</ul>
</li>
<li><a href="#附录-d-参数高效迁移学习petl方法">附录 D 参数高效迁移学习（PETL）方法</a></li>
<li><a href="#附录-e-数据集">附录 E 数据集</a><ul>
<li><a href="#e1-类增量学习cil数据集">E.1 类增量学习（CIL）数据集</a></li>
<li><a href="#e2-域增量学习dil数据集">E.2 域增量学习（DIL）数据集</a></li>
</ul>
</li>
<li><a href="#附录-f-附加结果">附录 F 附加结果</a><ul>
<li><a href="#f1-预备知识">F.1 预备知识</a></li>
<li><a href="#f2-变异性与每个任务的性能">F.2 变异性与每个任务的性能</a></li>
<li><a href="#f3-不同-cil-任务数量的第-1-阶段影响比较">F.3 不同 CIL 任务数量的第 1 阶段影响比较</a></li>
</ul>
</li>
<li><a href="#附录-g-其他结果">附录 G 其他结果</a><ul>
<li><a href="#g1-任务不可知的持续学习">G.1 任务不可知的持续学习</a></li>
<li><a href="#g2-随机投影尺寸的扩展">G.2 随机投影尺寸的扩展</a></li>
<li><a href="#g3-petl-方法与-vit-b16-骨干的比较">G.3 PETL 方法与 ViT-B/16 骨干的比较</a></li>
<li><a href="#g4-resnet-的实验">G.4 ResNet 的实验</a></li>
<li><a href="#g5-clip-视觉模型的实验">G.5 CLIP 视觉模型的实验</a></li>
<li><a href="#g6-使用-clip-视觉和语言模型的回归目标实验">G.6 使用 CLIP 视觉和语言模型的回归目标实验</a></li>
<li><a href="#g7-关于可重复性的说明">G.7 关于可重复性的说明</a></li>
<li><a href="#g8-任务不可知持续学习的扩展">G.8 任务不可知持续学习的扩展</a></li>
<li><a href="#g9-随机投影尺寸的扩展">G.9 随机投影尺寸的扩展</a></li>
<li><a href="#g10-petl-方法与-vit-b16-骨干的比较">G.10 PETL 方法与 ViT-B/16 骨干的比较</a></li>
<li><a href="#g11-resnet-的实验">G.11 ResNet 的实验</a></li>
<li><a href="#g12-clip-视觉模型的实验">G.12 CLIP 视觉模型的实验</a></li>
<li><a href="#g13-使用-clip-视觉和语言模型的回归目标实验">G.13 使用 CLIP 视觉和语言模型的回归目标实验</a></li>
<li><a href="#g14-关于可重复性的说明">G.14 关于可重复性的说明</a></li>
<li><a href="#g15-rmse-与-mae-的比较">G.15 RMSE 与 MAE 的比较</a></li>
<li><a href="#g16-超参数调优">G.16 超参数调优</a></li>
<li><a href="#g17-计算资源">G.17 计算资源</a></li>
<li><a href="#g18-未来研究方向">G.18 未来研究方向</a></li>
</ul>
</li>
</ul>
</div>

            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <article>
                <header class="post-header">
                    <a href="../../../../index.html" class="back-link">
                        <i class="fas fa-arrow-left"></i> 返回博客列表
                    </a>
                    <h1>Untitled</h1>
                    <div class="post-meta">
                        <span><i class="fas fa-calendar-alt"></i> 2026-01-28</span>
                        <span><i class="fas fa-folder"></i> AINotes/40.CIL/44.Template-Based Classification</span>
                        <span><i class="fas fa-user"></i> Hongwei Zhao</span>
                    </div>
                    <div class="tags">
                        
                    </div>
                </header>

                <div class="post-content">
                    <h2 id="0-摘要">0. 摘要<a class="anchor-link" href="#0-摘要" title="Permanent link">&para;</a></h2>
<p>持续学习（Continual Learning, CL）的目标是在非平稳的数据流中逐步学习不同的任务（如分类），而不遗忘旧任务。大多数 CL 工作专注于在从头学习的框架下应对灾难性遗忘。然而，随着基础模型的日益普及，配备了信息丰富表示的预训练模型已可用于各种下游需求。基于预训练模型的几种 CL 方法已经被探索，要么直接使用预提取的特征（这使得弥合分布差距具有挑战性），要么结合适配器（可能面临遗忘问题）。在本文中，我们提出了一种简洁而有效的方法，用于利用预训练模型进行 CL。鉴于遗忘发生在参数更新期间，我们考虑了一种替代方法，即利用<strong>无需训练随机投影器和类原型累积</strong>，从而绕过该问题。具体来说，我们在预训练模型的特征表示和输出头之间注入了一个<strong>带有非线性激活的冻结随机投影层</strong>，该层通过扩展维度捕捉特征之间的交互，为基于类原型的 CL 提供了增强的线性可分性。我们还展示了在使用预训练表示时，去相关类原型以减少分布差异的重要性。这些技术被证明是有效的，并且避免了在类和域增量持续学习中的遗忘问题。与应用于预训练 ViT-B/16 模型的先前方法相比，我们在七个类增量基准数据集上将最终错误率降低了 20% 到 62%，尽管没有使用任何回放记忆。我们得出结论，预训练模型在简单、有效和快速持续学习方面的潜力尚未被充分挖掘。代码可在 https://github.com/RanPAC/RanPAC 获取。</p>
<h2 id="1-引言">1. 引言<a class="anchor-link" href="#1-引言" title="Permanent link">&para;</a></h2>
<p>持续学习（Continual Learning, CL）是机器学习的一个子领域，在该领域中，模型必须从随时间变化的训练样本和/或监督信号分布中学习（通常分为一组不同的 T 个阶段/任务），同时在训练期间保持对先前学习内容的性能 [51,53]。传统的训练方法在 CL 中效果不佳，因为参数更新会偏向新样本，覆盖之前学到的内容。此外，在序列化且不相交的数据集上进行训练意味着没有机会学习不同阶段样本之间的差异 [48]。这些效应通常被称为“灾难性遗忘”[51]。</p>
<p>尽管已经提出了许多 CL 方法 [51,53,2,60]，但大多数方法专注于需要从头训练的模型，其性能在相同数据集上无法与非 CL 替代方案相媲美。尽管从头训练的有效用例始终存在，但大规模基础模型的新时代引发了将 CL 与强大的预训练模型优势相结合的日益增长的兴趣，即假设强大的通用特征提取器可以通过微调适应于任意数量的下游任务。</p>
<p>尽管在基于这种迁移学习的 CL 中仍然会发生遗忘 [57,65,63]（无论是使用传统的微调 [24] 还是更近期的参数高效迁移学习（Parameter-Efficient Transfer Learning, PETL）方法 [6,28,21]），但从强大的特征提取器开始进行 CL，为避免遗忘提供了新的思路，这些思路在从头训练时不太可能奏效。这些新方法已成功应用于预训练的 Transformer 网络 [39,10,57,56,46,20,65,63,47,55]、CNN[11,15,33,22,59,37] 和多模态视觉语言模型 [8]。</p>
<p>在这些提议中，明显存在三种主要策略（详见第 2 节）：(i) Transformer 网络的提示；(ii) 对预训练模型参数的谨慎选择性微调；(iii) 类原型（Class-Prototype, CP）累积。所有这些策略的共同点是不需要来自过去任务的回放样本缓冲区，这与从头训练模型的 CL 方法不同。相反，这些策略利用了预训练模型的强大特征提取能力。在相同的基准测试和预训练模型（例如 ViT B/16 Transformer 网络 [46,63,65]）下，每种策略在经验上都产生了可比的性能。因此，在性能、数据多样性和 CL 场景、简单性和效率方面，哪种策略最能利用预训练的基础模型仍是一个开放的问题。然而，我们注意到，基于 CP 的 CL 策略简单且适用于 CNN 和 Transformer 网络，而提示方法依赖于在 Transformer 网络输入前添加可学习的提示。微调预训练模型的参数比其他两种策略需要更多的训练资源，同时随着时间的推移累积遗忘的风险更大，因此需要额外的 CL 方法。</p>
<p>在本文中，我们展示了 CP 策略尚未充分发挥其准确性潜力，并且可以通过精心设计的策略来增强从预训练模型中提取的特征表示，从而实现突出的性能。基于 CP 的方法仅使用通过平均提取特征得到的原型来表示每个类，这在实际中存在与数据分布的差异。我们提出通过无需训练的冻结随机投影和去相关过程来处理这一问题，这两者都绕过了遗忘问题。具体来说，我们<strong>在预训练模型的特征层和基于 CP 的输出头之间引入了一个冻结的随机投影（Random-Projection, RP）层，该层具有非线性激活</strong>。</p>
<h2 id="2-相关工作">2. 相关工作<a class="anchor-link" href="#2-相关工作" title="Permanent link">&para;</a></h2>
<h3 id="21-使用强大预训练模型的三种-cl-策略">2.1 使用强大预训练模型的三种 CL 策略<a class="anchor-link" href="#21-使用强大预训练模型的三种-cl-策略" title="Permanent link">&para;</a></h3>
<p><strong>Transformer 网络的提示</strong>：使用 ViT-B/16 网络 [9]，学习提示（Learning To Prompt, L2P）[57] 和 DualPrompt[56] 报告了相较于不利用预训练模型的最佳 CL 方法的显著改进，通过训练一小部分提示并在 CL 过程中更新。CODA-Prompt[46]、S-Prompt[55] 和 PromptFusion[5] 在此基础上进一步发展，展示了性能的改进。</p>
<p><strong>对骨干网络的谨慎微调</strong>：SLCA[63] 发现，通过以比分类头更低的学习率微调 ViT 骨干网络，其准确性优于提示策略。然而，使用 softmax 时，需要引入“分类器对齐”方法，这会以每个类的特征协方差矩阵的形式带来高内存成本。另一个例子是选择性微调一些 ViT 注意力块 [47]，结合传统的 CL 方法 L2 参数正则化。微调也应用于 CLIP 视觉语言模型，结合成熟的 CL 方法 LwF[8]。</p>
<p><strong>类原型（Class-Prototype, CP）累积</strong>：在 L2P 之后，有研究指出，对于 CL 图像分类器 [20]，通过在 ViT 模型的特征输出上附加最近类均值（Nearest Class Mean, NCM）分类器可以实现可比的性能（参见 [39]）。通过与仅在第一个 CL 阶段（“首次会话训练”）训练的 PETL 方法结合，可以显著提升这一策略 [65,37]。对于 Transformer 网络，[65] 考虑了三种 PETL 方法，而对于 CNN，[37] 使用了 FiLM 方法。与第一种策略（提示）类似，这些方法需要学习新参数，但避免更新骨干预训练网络的任何参数。重要的是，[37] 还展示了，通过累积嵌入特征的协方差矩阵并基于线性判别分析（LDA）[35] 学习线性分类头，简单的 NCM 分类器在准确性上很容易被超越。[37] 的简单且计算轻量的算法使得 CL 在首次会话之后可以完美地进行，完全避免了灾难性遗忘的可能性。</p>
<p>CP 通常非常适合 CL[42,7,31,16]，并且适用于预训练模型 [20,65,37]，因为当提取特征向量的模型被冻结时，跨 T 个任务累积的 CP 将相同，无论任务的顺序如何。此外，与使用回放缓冲区相比，其内存成本较低，回放缓冲区是许多 CL 方法 [53] 的组成部分。</p>
<h3 id="22-用于创建特征交互的-rp-方法">2.2 用于创建特征交互的 RP 方法<a class="anchor-link" href="#22-用于创建特征交互的-rp-方法" title="Permanent link">&para;</a></h3>
<p>如前所述，最初的非 CL 使用冻结 RP 层后接非线性投影的动机与我们不同，具有以下三个特性。首先，保持权重冻结消除了训练它们的计算成本。其次，当与线性输出层结合时，可以使用所有训练数据同时通过精确数值计算学习均方误差最优的输出权重（见附录 B.3），而不是迭代学习。第三，对随机投影的特征进行非线性激活的动机是假设特征之间的非线性随机交互可能比原始特征更具线性可分性。对非线性诱导的成对交互的特殊情况的分析可以在 [32] 中找到，并且对一般非线性（具有高阶交互）的数学性质也进行了广泛讨论，例如 [4,18]。</p>
<h2 id="3-背景">3. 背景<a class="anchor-link" href="#3-背景" title="Permanent link">&para;</a></h2>
<h3 id="31-持续学习问题设置">3.1 持续学习问题设置<a class="anchor-link" href="#31-持续学习问题设置" title="Permanent link">&para;</a></h3>
<p>我们假设通常的监督 CL 设置，即一系列 <span class="math-inline">T</span> 个任务/阶段，<span class="math-inline">D = {D_1, ..., D_T}</span>。在每个 Dt 中，提供了一组不相交的训练数据及其对应的标签以供学习。后续阶段无法访问旧数据。我们主要考虑“类增量学习”（Class-Incremental Learning, CIL）和“域增量学习”（Domain-Incremental learning, DIL）协议 [51] 用于图像分类。在 CIL 中，每个 Dt 的类标签是不相交的。CIL 的一个挑战性方面是，与任务增量学习（Task-Incremental Learning, TIL）相比，在 CIL 推理期间，任务身份以及每个样本的类子集是未知的 [51]。在 DIL 中，虽然所有阶段通常共享相同的类集，但每个阶段样本之间存在分布偏移。例如，D1 可能包含照片，而 D2 可能包含绘画图像。</p>
<p>我们引入 <span class="math-inline">K</span> 作为 <span class="math-inline">T</span> 个任务中考虑的总类数，每个任务中的训练样本数为 <span class="math-inline">N_t</span>，且 <span class="math-inline">N := \sum_{t=1}^{T}N_t</span>。对于任务 <span class="math-inline">D_t</span> 中的第 <span class="math-inline">n</span> 个唯一训练样本，我们使用 <span class="math-inline">y_t,n</span> 表示其长度为 <span class="math-inline">K</span> 的 one-hot 编码标签，<span class="math-inline">f_{t,n} \in \mathbb{R}^L</span> 表示从冻结的预训练模型中提取的特征。我们使用 ftest 表示测试实例的编码特征，我们希望为其预测标签。</p>
<h3 id="32-使用预训练模型的类原型策略">3.2 使用预训练模型的类原型策略<a class="anchor-link" href="#32-使用预训练模型的类原型策略" title="Permanent link">&para;</a></h3>
<p>对于 CL，使用传统的交叉熵损失通过线性探测或微调冻结预训练模型的特征表示会带来任务 - 新近偏差 [31] 和灾难性遗忘的风险。受益于预训练模型的高质量表示，最直接的类原型（Class-Prototype, CP）策略是使用最近类均值（Nearest Class Mean, NCM）分类器 [58,34]，如 [65,20] 所应用和研究的。每个类的 CP 通常通过平均类内训练样本的提取特征向量来构建，我们将类 <span class="math-inline">y</span> 表示为 <span class="math-inline">\bar{c}<em>y</span>。在推理中，测试样本的类别通过找到其表示与 CP 集之间的最高相似性来确定。例如，[65] 使用余弦相似性来找到测试样本的预测类别，<br />
<div class="math-display"><br />
y</em>{\text{test}} = \arg \max_{y' \in {1, \dots, K}} s_{y'}, \quad s_y := \frac{f_{\text{test}}^\top \bar{c}<em>y}{|f</em>{\text{test}}| \cdot |\bar{c}_y|}. \tag{1}<br />
</div><br />
然而，通过利用二阶特征统计量 [11,37]，在同一通用 CL 策略中超越 NCM 并不困难。例如，[37] 发现，使用预训练 CNN 的 CL 结果始终优于 NCM，使用增量版本的线性判别分析（LDA）分类 [35]，其中提取特征的协方差矩阵不断更新。在温和的假设下，LDA 等同于使用马氏距离（Mahalanobis distance）比较特征向量和类原型（见附录 B.4），即与 [65] 使用的余弦距离不同。</p>
<h3 id="33-基于-gram-矩阵逆的-cp-分类">3.3 基于 Gram 矩阵逆的 CP 分类<a class="anchor-link" href="#33-基于-gram-矩阵逆的-cp-分类" title="Permanent link">&para;</a></h3>
<p>我们还将使用增量计算的二阶特征统计量，但与 LDA 相比（见附录 B.4），我们通过使用特征的 Gram 矩阵 <span class="math-inline">G</span> 和类原型 <span class="math-inline">c_y</span>（省略了平均操作）来简化计算，从而获得预测标签：<br />
<div class="math-display"><br />
    y_{\text{test}} = \arg \max_{y' \in {1, \dots, K}} s_{y'}, \quad s_y := f_{\text{test}}^\top G^{-1} c_y. \tag{2}<br />
</div><br />
与 LDA 类似，但与余弦相似性不同，这种形式利用训练集来“校准”相似性。该目标基于长期建立的用于预测 one-hot 编码类标签的最小二乘误差理论 [35]（见附录 B.3）。与增量 LDA[36,11] 类似，在 CL 训练期间，我们在第 4.3 节中描述了如何逐步更新 Gram 矩阵和对应于 <span class="math-inline">c_y</span> 的 CP。需要注意的是，公式 (2) 以 <span class="math-inline">T</span> 个任务后的最大类数 <span class="math-inline">K</span> 表示。然而，对于 CIL，它可以在完成 <span class="math-inline">t &lt; T</span> 个任务后计算，此时类数小于 <span class="math-inline">K</span>。对于 DIL，通常在所有任务中都可以访问所有 <span class="math-inline">K</span> 个类。</p>
<h2 id="4-提出的方法与理论见解ranpac">4. 提出的方法与理论见解：RanPAC<a class="anchor-link" href="#4-提出的方法与理论见解ranpac" title="Permanent link">&para;</a></h2>
<h3 id="41-为什么二阶统计量对-cp-很重要">4.1 为什么二阶统计量对 CP 很重要<a class="anchor-link" href="#41-为什么二阶统计量对-cp-很重要" title="Permanent link">&para;</a></h3>
<p>我们在第 5 节中展示了公式 (2) 比 NCM 带来更好的结果。我们将其归因于原始 CP 通常在类之间高度相关，导致余弦相似性校准不佳，而使用 LDA 或公式 (2) 主要消除了 CP 之间的相关性，从而在类之间创建了更好的可分性。为了说明这些见解，我们使用了一个在 ImageNet-21K 上预训练的 ViT-B/16 Transformer 模型 [9]，并移除了其分类头，数据来自成熟的 200 类 Split Imagenet-R CIL 基准 [56]。</p>
<p>为了与 CP 方法进行比较，我们在所有 200 类上联合训练了一个线性探测 softmax 分类器。我们将联合探测的权重视为类原型，然后计算每一对原型之间的 Pearson 相关系数，如图 2（右）所示的前 10 类。与线性探测相比，使用 NCM 时明显观察到非常高的非对角相关性，其均值是原始 ImageNet-1K 训练数据的两倍多。这说明了下游数据集的域偏移程度。然而，当使用公式 (2) 时，这些相关性大部分被消除。图 2（左）显示，高相关系数与类原型和训练样本之间的校准不佳的余弦相似性一致，无论是真实类比较（样本特征向量与其对应类标签的 CP 之间的相似性）还是类间比较（样本特征向量与不等于样本类标签的 N-1 个类的 CP 之间的相似性）。然而，当使用公式 (2) 时，结果（第三行）将训练集准确率从 64% 提高到 75%，同时减少了类间和真实类相似性分布的重叠，并显著降低了 CP 之间的非对角相关系数。最终结果是线性分类权重更接近联合训练的线性探测产生的权重。这些结果与公式 (2) 和去相关之间的已知数学关系一致，我们在附录 B.4.4 中概述了这些关系。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20250303212605.png" style="zoom: 80%;" /></div>

<h3 id="42-随机投影的详细概述与直觉">4.2 随机投影的详细概述与直觉<a class="anchor-link" href="#42-随机投影的详细概述与直觉" title="Permanent link">&para;</a></h3>
<p>使用原始 <span class="math-inline">\bar{c}_y</span> 的 CP 方法假设高斯分布具有3。经验上，我们发现当与预训练模型一起使用时，这一假设是无效的（图 2）。可以通过学习非线性函数（例如使用 SGD 训练神经网络）来缓解这一差距（例如，在第一个任务上学习适应 [65,37]），但这需要额外的假设和架构选择。此外，对于 CL，在所有任务上进行学习都会带来灾难性遗忘的高风险。相反，我们利用了 CP 策略中的目标主要需要首先进行特征投影（可能通过协方差矩阵调整其分布），其次需要特征原型的观察。对于后者，由于使用大型预训练模型生成的特征捕获了最显著的模式和属性，平均值是一个合理的类代表。然而，特征表示也可能受益于非线性变换以增强线性可分性。在本文中，我们考虑两点：(i) 我们展示了在嵌入空间（或将其投影到随机基）中对特征进行随机投影到更高维空间会导致更可能适合高斯拟合的分布（例如使用公式 (2)）；(ii) 当表示的分布接近高斯分布时，我们可以简单地执行线性回归（或 LDA 或联合高斯分布的条件均值）。</p>
<p>这一断言得到了图 2 底行的支持，该图显示，应用大小为 <span class="math-inline">M = 2000</span> 的 RP 层（为说明目的任意选择）进一步减少了相似性直方图之间的重叠，并将类内直方图向右移动，同时伴随更高的准确率，超过了联合线性探测的结果，这与我们在第 5 节中的发现一致。我们注意到，<strong>这些 RP 可以生成一次，然后在所有持续学习阶段冻结使用</strong>，从而实现简单且高效的算法。</p>
<p>我们首先分析从预训练模型中提取的特征在使用一些随机基向量（即随机投影）投影后的内积。我们考虑从方差为 <span class="math-inline">\sigma^2</span> 的高斯分布中随机独立同分布（i.i.d.）抽取的 <span class="math-inline">M</span> 个随机向量。数量 <span class="math-inline">M</span> 定义了这些特征的随机投影所构成的空间维度。考虑随机投影 <span class="math-inline">W \in \mathbb{R}^{L \times M}</span>，对于任何两个给定的特征向量 <span class="math-inline">f, f' \in \mathbb{R}^L</span>，我们有（见附录 B.2）：<br />
<div class="math-display"><br />
    \mathbb{E}<em>W \left[ (W^\top f)^\top (W^\top f') \right] = \sum</em>{i} \mathbb{E}<em>W \left[ W</em>{(i)}^2 \right] f^\top f' + \sum_{i \ne j} \mathbb{E}<em>W \left[ W</em>{(i)}^\top \right] \mathbb{E}<em>W \left[ W</em>{(j)} \right] f^\top f', \tag{3}<br />
</div><br />
其中 <span class="math-inline">W_{(i)}</span> 表示第 <span class="math-inline">i</span> 列。也就是说，这两个特征的投影的期望内积可以分解。现在，我们有 <span class="math-inline">W_{(i)} \sim \mathcal{N}(0, \sigma^2 I)</span>，因此 <span class="math-inline">\mathbb{E}<em>W[W</em>{(i)}] = \mathbb{E}<em>W[W</em>{(j)}] = 0</span>，公式 (3) 中的第二项消失。此外，<span class="math-inline">\sum_{i} \mathbb{E}<em>W[W</em>{(i)}^2] = M \sigma^2</span>。我们可以得出两个观察结果（理论细节见附录 B.2）：</p>
<ol>
<li>随着 <span class="math-inline">M</span> 的增加，任何投影特征的范数接近方差的可能性增加。换句话说，在高维空间中投影的向量几乎肯定位于分布的边界上，与均值的距离几乎相等（分布接近各向同性高斯分布）。</li>
<li>随着 <span class="math-inline">M</span> 的增加，两个随机投影实例之间的角度更可能是不同的（即投影空间中的内积更可能大于某个常数）。</li>
</ol>
<p>这一讨论可以很容易地扩展到包含非线性。引入非线性的好处是：(i) 包含交互项 [54,50]；(ii) 模拟更高维的投影，否则可能在数量上过大。为了看到后一点，记 <span class="math-inline">\phi</span> 为感兴趣的非线性函数，我们有 <span class="math-inline">\phi(W^\top f) \approx \hat{W}^\top \hat{f}</span>，其中 <span class="math-inline">\hat{f}</span> 通过使用泰勒级数的线性扩展获得，<span class="math-inline">\hat{W}</span> 是相应的投影。非线性函数 <span class="math-inline">\phi</span> 的泰勒级数展开引入了维度之间的高阶交互。尽管可以直接形成交互项的向量，如 [38,54] 的方法，但对于非平凡的 <span class="math-inline">L</span>，这在计算上是不可行的。因此，使用形式为 <span class="math-inline">h_{\text{test}} := \phi(f_{\text{test}}^\top W)</span> 的非线性投影是一种方便且计算成本低廉的替代方案，如在非 CL 环境中已知的有效工作 [43,4,18,32]。</p>
<h3 id="43-用于持续学习的随机投影">4.3 用于持续学习的随机投影<a class="anchor-link" href="#43-用于持续学习的随机投影" title="Permanent link">&para;</a></h3>
<p>使用上述随机投影，<span class="math-inline">\phi(\cdot)</span> 作为逐元素的非线性激活函数，给定特征样本 <span class="math-inline">f_{t,n}</span>，我们为每个任务中的 CL 训练获得长度为 <span class="math-inline">M</span> 的表示，<br />
<div class="math-display"><br />
    h_{t,n} := \phi(f_{t,n}^\top W) \quad (\text{图 1})。<br />
</div><br />
在推理中，<span class="math-inline">h_{\text{test}} := \phi(f_{\text{test}}^\top W)</span> 用于公式 (2) 中的 <span class="math-inline">s_y</span> 代替 <span class="math-inline">f_{\text{test}}</span>。我们定义 <span class="math-inline">H</span> 为 <span class="math-inline">M \times N</span> 矩阵，其中列由所有 <span class="math-inline">h_{_{k,n}}</span> 组成，为了方便起见，我们仅指在所有 <span class="math-inline">N</span> 个样本使用后的最终 <span class="math-inline">H</span>。现在，我们有一个 <span class="math-inline">M \times M</span> 的 Gram 矩阵 <span class="math-inline">G = HH^\top</span>。上述讨论的随机投影仅采样一次，并在所有 CL 阶段中保持冻结。</p>
<p>与应用于 CL 的流式 LDA 中的协方差矩阵更新类似 [11,37]，变量可以逐个样本更新，或者一次更新整个 CL 阶段 <span class="math-inline">D_t</span>。我们引入矩阵 <span class="math-inline">C</span> 来表示所有 <span class="math-inline">c_y</span> 的列向量连接。与协方差 <span class="math-inline">S</span> 不同，我们更新 Gram 矩阵和 <span class="math-inline">C</span> 中的 CP（即 <span class="math-inline">c_y</span> 的连接），这可以逐个任务或逐个样本完成，因为我们可以将 <span class="math-inline">G</span> 和 <span class="math-inline">C</span> 表示为外积的和：<br />
<div class="math-display"><br />
    G = \sum_{t=1}^T \sum_{n=1}^{N_t} h_{t,n} \otimes h_{t,n}, \quad C = \sum_{t=1}^T \sum_{n=1}^{N_t} h_{t,n} \otimes y_{t,n}. \tag{4}<br />
</div><br />
无论 <span class="math-inline">N</span> 个训练样本的呈现顺序如何，<span class="math-inline">C</span> 和 <span class="math-inline">G</span> 都将保持不变，这是 CL 算法的理想属性。</p>
<p>公式 (2) 与最小二乘理论的起源具有实际用途；我们发现最好使用岭回归 [35]，并计算 <span class="math-inline">l_2</span> 正则化逆 <span class="math-inline">(G + \lambda I)^{-1}</span>，其中 <span class="math-inline">I</span> 表示单位矩阵。这是通过交叉验证系统地实现的，见附录 C。修订后的 CL 得分可以重写为：<br />
<div class="math-display"><br />
    s_y = \phi(f_{\text{test}}^\top W)(G + \lambda I)^{-1} c_y. \tag{5}<br />
</div><br />
在矩阵形式中，得分可以表示为对每个类标签的预测 <span class="math-inline">y_{\text{pred}} = h_{\text{test}} W_o</span>。与流式 LDA 不同，我们的方法具有以下优点：(i) 从得分中消除了偏差计算的需要；(ii) 更新 Gram 矩阵而不是协方差避免了均值的外积；(iii) 形式 <span class="math-inline">W_o = (G + \lambda I)^{-1} C</span> 作为带有 <span class="math-inline">l_2</span> 正则化的均方误差损失的闭式解出现（见附录 B.3），而在 NCM 中，没有这样的理论结果存在。<strong>算法 1</strong> 的第 2 阶段总结了上述 CL 计算。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20250304095735.png" style="zoom: 50%;" /></div>

<p>公式 (5) 的应用表面上类似于 AdaptMLP 模块 [6]，但我们扩展到了维度 <span class="math-inline">M &gt; L</span>，因为过去应用发现这对于补偿 <span class="math-inline">W</span> 是随机的而不是学习的 [43,4,18,32] 是必要的。如引言所述，使用随机且无需训练的权重层特别适合 CL。</p>
<p>将原始特征转换为非线性随机投影的价值如图 3 所示。从预训练的 ViT-B/16[9] 网络中提取了 <span class="math-inline">T = 10</span> 个任务的 Split ImageNet-R CIL 数据集的特征，并在每个 <span class="math-inline">T = 10</span> 个任务后应用公式 (5)。图 3(a) 显示了典型的 CL 平均准确率趋势，即随着任务增加，准确率下降。当使用非线性激活 <span class="math-inline">\phi(\cdot)</span> 时（例如 ReLU 或平方），随着 <span class="math-inline">M</span> 的增加，性能提高，但当省略非线性激活时，即使 <span class="math-inline">M</span> 非常大，准确率也不比不使用 RP 更好。另一方面，如果维度在没有非线性的情况下减少（在这种情况下从 768 减少到 500），则性能低于不使用 RP 的情况，这表明如果 RP 在此应用中创建维度减少，会导致性能下降。</p>
<p>图 3(b) 说明了为什么非线性重要。我们仅使用每个样本的前 100 个提取特征，并比较公式 (2) 对原始特征向量（黑色）和成对交互项（蓝色轨迹）的应用。前者显著优于后者。然而，当使用公式 (5) 时（红色轨迹），与扁平化交叉积相比，性能下降相对较小。尽管这表明详尽创建特征乘积而不是 RP，但这在计算上是不可行的。作为替代方案，RP 是一种方便且计算成本低廉的方法，用于创建增强线性可分性的非线性特征交互，特别是在使用预训练模型的 CL 中具有特殊价值。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20250303212629.png" style="zoom: 80%;" /></div>

<h3 id="44-结合参数高效迁移学习和首次会话适应">4.4 结合参数高效迁移学习和首次会话适应<a class="anchor-link" href="#44-结合参数高效迁移学习和首次会话适应" title="Permanent link">&para;</a></h3>
<p>使用 RP 层的好处是它是模型无关的，例如，它可以应用于任何特征提取器。我们展示它可以与 PETL 方法正交应用。PETL 对 CL 非常有吸引力，特别是那些不改变原始预训练模型学习参数的方法，例如 [10,28]。我们将 RP 与一种 PETL 方法结合，该方法使用与 CL 兼容的“首次会话”训练进行训练，如 [65,37] 所做的那样。这意味着仅在第一个 CL 任务 <span class="math-inline">D_1</span> 上训练 PETL 参数，然后在此之后冻结它们（见<strong>算法 1</strong> 的第 1 阶段）。其理由是，第一个任务中的训练数据和标签可能比用于训练原始预训练模型的数据更具代表性。如果新数据集的漂移很大，例如在 DIL 中，由于这一选择，PETL 的好处可能会减少。但假设是预训练数据集和新数据集之间的域差距足够大，仍然会带来好处。</p>
<p>首次会话训练 PETL 参数需要一个临时线性输出层，使用 SGD 和交叉熵损失以及 softmax 进行训练，并且仅使用 <span class="math-inline">K_1</span> 个类，该输出层在第 2 阶段之前被丢弃。对于 Transformer 网络，我们实验了与 [65] 相同的三种方法，即 AdaptFormer[6]、SSF[28] 和 VPT[21]。有关这些方法的详细信息，请参见引用的参考文献和附录 D。与 [65] 不同，我们不连接适应和未适应的特征，因为我们发现使用 RP 时，这几乎没有增加价值。</p>
<h3 id="45-rp-层的内存使用">4.5 RP 层的内存使用<a class="anchor-link" href="#45-rp-层的内存使用" title="Permanent link">&para;</a></h3>
<p>RP 层将类原型的总内存需求增加了 <span class="math-inline">1 + (M - L)/L</span> 倍，同时增加了 <span class="math-inline">LM</span> 个冻结且不可训练的参数。对于典型的 <span class="math-inline">L = 768</span> 和 <span class="math-inline">K = 200</span>，注入大小为 <span class="math-inline">M = 10000</span> 的 RP 层因此创建了约 10 倍的可训练参数数量，并增加了约 10M 的不可训练参数。尽管这是一个显著的新参数数量，但与 ViT-B/16 模型的 8400 万参数总数相比，它仍然很小。此外，<span class="math-inline">W</span> 的权重可以是双极的，如果每个元素使用单个比特存储，则对额外模型内存的贡献极小。在训练期间，我们还需要更新一个 <span class="math-inline">M \times M</span> 的 Gram 矩阵，如果 <span class="math-inline">M &lt; \sqrt{KL}</span>，则它比 SLCA 微调方法 [63] 的 <span class="math-inline">K(L \times L)</span> 协方差矩阵更小。L2P[57]、DualPrompt[56] 和 ADaM[65] 各自使用约 0.3M–0.5M 的可训练参数。对于 <span class="math-inline">K = 200</span> 个类和 <span class="math-inline">M = 10000</span>，RanPAC 使用更多的参数（约 2–2.5M，取决于 PETL 方法），与 CODA-Prompt[46] 相当。RanPAC 还使用了 10M 未训练参数，但我们强调这些参数是不可训练的。此外，<span class="math-inline">M</span> 不必高达 10000（表 A5）。</p>
<h2 id="5-实验">5. 实验<a class="anchor-link" href="#5-实验" title="Permanent link">&para;</a></h2>
<p>我们将<strong>算法 1</strong> 应用于 CIL 和 DIL 基准测试。对于预训练模型，我们主要实验了两个 ViT-B/16 模型 [9]，如 [65] 所述：一个在 ImageNet-21K 上进行自监督训练，另一个在 ImageNet-1K 上进行有监督微调。比较使用标准的 CL 指标，平均准确率 [30]，定义为 <span class="math-inline">A_t = \frac{1}{t} \sum_{i=1}^t R_{t,i}</span>，其中 <span class="math-inline">R_{t,i}</span> 是在第 <span class="math-inline">t</span> 个任务训练后对第 <span class="math-inline">i</span> 个任务的分类准确率。我们在主论文中报告最终准确率 <span class="math-inline">A_T</span>，并将每个 <span class="math-inline">A_t</span> 和 <span class="math-inline">R_{t,i}</span> 的分析留给附录 F，同时还包括遗忘指标和跨种子变异性的分析。附录 F 还显示，我们的方法在 ResNet 和 CLIP 骨架上表现良好。</p>
<p>我们使用了以前用于 CIL 或 DIL 的分割数据集（见表 1 和表 3 中的引用）；详细信息见附录 E。我们在<strong>算法 1</strong> 中使用 <span class="math-inline">M = 10000</span>，除非另有说明；对 <span class="math-inline">M</span> 的扩展研究见附录 F.5。所有列出的“联合”结果都是对 <span class="math-inline">D</span> 的全部数据进行非 CL 训练的比较，使用交叉熵损失和 softmax。</p>
<p>CIL 的关键指示性结果如表 1 所示，对于 <span class="math-inline">T = 10</span>，阶段大小相等（除了 VTAB，它是 <span class="math-inline">T = 5</span>，与 [65] 相同）。对于 <span class="math-inline">T = 5</span> 和 <span class="math-inline">T = 20</span>，见附录 F。对于每个数据集，我们的最佳方法以较大的优势超过了提示方法和 [20] 和 [65] 的 CP 方法的准确率。表 1 中列出的<strong>算法 1</strong> 的消融实验表明，当使用 PETL 时，包含我们的 RP 层可将错误率降低 11% 到 28%。如果不使用 PETL，增益会减少，但除了 VTAB 外，增益仍≥8%。表 1 还突出了 NCM 的局限性。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20250303212739.png" style="zoom: 50%;" /></div>

<p>表 2 显示，<strong>算法 1</strong> 在 CIL 上的表现也优于微调策略 [47,63]。这与表 1 分开列出，因为微调有主要缺点，如引言中所述。表 2 还显示，<strong>算法 1</strong> 在三个数据集上接近最佳联合微调准确率的 2% 以内，而在两个 ImageNet 变体和 Cars 数据上差距较大。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20250303212819.png" style="zoom: 80%;" /></div>

<p>DIL 数据集的结果以及<strong>算法 1</strong> 的相应消融实验列在表 3 中。CORe50 是 <span class="math-inline">T = 8</span> 个阶段，50 个类 [29]，CDDB-Hard 是 <span class="math-inline">T = 5</span>，2 个类 [27]，DomainNet 是 <span class="math-inline">T = 6</span>，345 个类 [40]（更多详细信息见附录 E）。可以观察到与 CIL 相同的趋势，即比提示策略更好的性能，并突出了 RP 层的价值。对于 DomainNet，RP 的价值特别强，而包含 PETL 几乎没有增加价值，这与 DIL 的性质一致，其中不同的 CL 阶段来自不同的域。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20250303212800.png" style="zoom: 80%;" /></div>

<h2 id="6-结论">6. 结论<a class="anchor-link" href="#6-结论" title="Permanent link">&para;</a></h2>
<p>我们已经证明，从预训练的基础模型（如 ViT-B/16）中提取的特征表示在持续学习中的潜力尚未被充分挖掘。应用我们简单且无需回放的类原型策略 RanPAC，可以在多样的 CL 基准数据集上显著降低错误率，同时避免了预训练模型中的遗忘问题。这些发现突显了在预训练模型中使用 CP 策略的益处。</p>
<p><strong>局限性</strong>：公式 (4) 和 (5) 的价值完全依赖于提供良好的通用特征提取器。因此，如果用于从零开始训练网络的 CL 方法，它们不太可能同样强大。然而，现有的利用自监督学习或其他方式创建良好特征提取器骨干的 CL 方法可能会利用类似我们的方法进行下游 CL 任务。如第 4.5 节所述，RanPAC 使用比 L2P 等方法更多的参数。然而，这种权衡值得，因为实现简单且训练成本低。</p>
<p><strong>未来工作</strong>：附录 F 中的示例表明，我们的方法在其他 CL 协议中也表现良好，包括：(i) 任务不可知的 CL，即训练期间没有任务边界（例如高斯调度的 CIFAR-100），(ii) 使用非 one-hot 编码目标，例如 CLIP 模型中的语言嵌入，(iii) 回归目标，这需要将类原型的概念扩展到通用特征原型。每种情况都有很大的扩展和探索空间。其他有趣的实验，我们留给未来工作，包括将我们的方法与提示方法结合的研究，以及（特别是对于 DIL）研究是否可以在不引起灾难性遗忘的情况下在首次会话之后训练 PETL 参数。预训练模型的少样本学习 [1,45] 也可能从类似 RanPAC 的算法中受益。</p>
<h2 id="附录-a-ranpac-概述及与其他策略的比较">附录 A RanPAC 概述及与其他策略的比较<a class="anchor-link" href="#附录-a-ranpac-概述及与其他策略的比较" title="Permanent link">&para;</a></h2>
<p>图 A1 提供了<strong>算法 1</strong> 中两个阶段的图形概述。表 A1 总结了利用预训练模型进行持续学习（CL）的不同策略，并比较了我们自己的方法 RanPAC。</p>
<p>图 A1：RanPAC 的 CL 分类概述。在<strong>算法 1</strong> 的第 1 阶段，我们可选地将参数高效迁移学习（PETL）方法的参数注入到冻结的预训练模型（PTM）中。PETL 参数仅在任务 1（“首次会话”）上进行训练，以帮助弥合域差距，如 [65,37] 所示。然后在第 2 阶段，首先从网络提取 <span class="math-inline">L</span> 维特征向量 <span class="math-inline">f</span>，然后使用冻结的权重 <span class="math-inline">W</span> 将提取的特征向量随机投影到维度 <span class="math-inline">M</span>（通常 <span class="math-inline">M &gt; L</span>），并通过非线性激活 <span class="math-inline">\phi</span> 获得新的特征向量 <span class="math-inline">h = \phi(f^\top W)</span>。第 2 阶段的训练包括不断迭代更新类原型和 Gram 矩阵，然后在每个任务训练结束后通过矩阵逆计算 <span class="math-inline">M \times N</span> 矩阵 <span class="math-inline">W_o</span>。这些权重可以被视为去相关的类原型，用于线性加权特征 <span class="math-inline">h</span> 以获得类预测 <span class="math-inline">y</span>。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20250304101036.png" style="zoom: 80%;" /></div>

<p>表 A1：不同策略的比较。在第 1 节中，我们将利用预训练模型进行 CL 的现有方法分为三种策略：提示、微调和类原型（CP）。我们自己的方法 RanPAC 是一种 CP 策略，其中我们引入了带有非线性激活的冻结随机投影（RP）层。总体而言，我们认为 CP 方法提供了最佳的组合优势，而在现有的 CP 方法中，RanPAC 提供了最强的结果，并得到了其使用 RP 和二阶统计量去相关 CP 的理论支持。我们注意到，回放缓冲区已被用于一些引用的方法以提高性能，并且可能也可以用于尚未使用它们的方法，例如 RanPAC。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20250304101050.png" style="zoom: 80%;" /></div>

<h2 id="附录-b-理论支持">附录 B 理论支持<a class="anchor-link" href="#附录-b-理论支持" title="Permanent link">&para;</a></h2>
<h3 id="b1-投影向量范数的-chernoff-界">B.1 投影向量范数的 Chernoff 界<a class="anchor-link" href="#b1-投影向量范数的-chernoff-界" title="Permanent link">&para;</a></h3>
<p>我们提供了第 4.2 节讨论的进一步细节。使用 Chernoff 界，投影向量的范数可以写为：<br />
<div class="math-display"><br />
    P \left( \left| |W^\top f| - \mathbb{E}_W \left[ |W^\top f| \right] \right| &gt; \varepsilon \sigma^2 \right) \leq 2 \exp \left( -\frac{\varepsilon^2 \sigma^2}{2M + \varepsilon} \right). \tag{6}<br />
</div><br />
这个界表明了维度与投影向量范数的期望变化之间的关系。对于固定的 <span class="math-inline">\sigma</span> 和 <span class="math-inline">\varepsilon</span>，随着 <span class="math-inline">M</span> 的增加，右侧趋近于 1，表明投影向量的范数更可能处于与期望的期望距离内。换句话说，这些投影向量在高维空间几乎肯定位于分布的边界上，与均值的距离几乎相等（分布接近各向同性高斯分布）。</p>
<h3 id="b2-增加投影维度的影响">B.2 增加投影维度的影响<a class="anchor-link" href="#b2-增加投影维度的影响" title="Permanent link">&para;</a></h3>
<p>投影向量的 Gram 矩阵可以通过考虑任意两个向量 <span class="math-inline">f, f'</span> 的内积获得。如公式 (3) 所示，这可以推导为：<br />
<div class="math-display"><br />
    \mathbb{E}<em>W \left[ (W^\top f)^\top (W^\top f') \right] = \mathbb{E}_W \left[ (f^\top W)(W^\top f') \right] = \mathbb{E}_W \left[ \sum</em>{i} W_{(i)}^2 f^\top f' + \sum_{i \ne j} W_{(i)}^\top W_{(j)} f^\top f' \right] = \sum_{i} \mathbb{E}<em>W \left[ W</em>{(i)}^2 \right] f^\top f' + \sum_{i \ne j} \mathbb{E}<em>W \left[ W</em>{(i)}^\top \right] \mathbb{E}<em>W \left[ W</em>{(j)} \right] f^\top f', \tag{7}<br />
</div><br />
其中第二项对于任何两个零均值独立抽取的随机向量 <span class="math-inline">W_{(i)}, W_{(j)}</span> 为零。我们可以从这个扩展中得出以下结论：</p>
<ol>
<li>
<p>使用 Chernoff 不等式，对于任意两个向量，我们有：<br />
<div class="math-display"><br />
    P \left( \left| (W^\top f)^\top (W^\top f') - \mathbb{E}_W \left[ (W^\top f)^\top (W^\top f') \right] \right| &gt; \varepsilon' M \sigma^2 \right) \leq 2 \exp \left( -\frac{\varepsilon'^2 M \sigma^2}{2 + \varepsilon'} \right). \tag{8}<br />
</div><br />
这个界表明，随着投影维度的增加，任意两个向量及其投影的内积更可能是不同的。换句话说，随着 <span class="math-inline">M</span> 的增加，两个向量及其投影的内积相等的可能性越来越小。</p>
</li>
<li>
<p>随着 <span class="math-inline">M</span> 的增加，任意两个随机投影实例的内积更可能是不同的（即投影空间中的内积更可能大于某个常数）。这是因为，使用马尔可夫不等式，对于更大的 <span class="math-inline">M</span>，更容易选择更大的 <span class="math-inline">\varepsilon^2</span> 满足：<br />
<div class="math-display"><br />
    P \left( \left| (W^\top f)^\top (W^\top f') \right| \geq \varepsilon^2 \right) \leq \frac{M \sigma^2}{\varepsilon^2}. \tag{9}<br />
</div></p>
</li>
</ol>
<h3 id="b3-与最小二乘法的联系">B.3 与最小二乘法的联系<a class="anchor-link" href="#b3-与最小二乘法的联系" title="Permanent link">&para;</a></h3>
<p>公式 (5) 的得分可以写成矩阵形式：<br />
<div class="math-display"><br />
    y_{\text{test}} = h_{\text{test}} (G + \lambda I)^{-1} C, \tag{10}<br />
</div><br />
其中 <span class="math-inline">h_{\text{test}} := \phi(f_{\text{test}}^\top W)</span> 是经过逐元素非线性激活 <span class="math-inline">\phi(\cdot)</span> 后的随机投影激活。形式 <span class="math-inline">W_o := (G + \lambda I)^{-1} C</span> 源于最小二乘回归的基本理论。我们可以将其写为：<br />
<div class="math-display"><br />
    W_o = (G + \lambda I)^{-1} H Y_{\text{train}}, \tag{11}<br />
</div><br />
其中 <span class="math-inline">Y_{\text{train}}</span> 是一个 <span class="math-inline">N \times K</span> 的 one-hot 编码目标矩阵，<span class="math-inline">H</span> 如第 4.3 节定义。公式 (12) 是 <span class="math-inline">l_2</span> 正则化方程组的最小均方误差解，定义为：<br />
<div class="math-display"><br />
    Y_{\text{train}}^\top = W_o^\top H. \tag{12}<br />
</div><br />
对于参数 <span class="math-inline">\lambda \geq 0</span>，这通常表示为：<br />
<div class="math-display"><br />
W_o = \text{argmin}<em>W \left( | Y</em>{\text{train}}^\top - W^\top H |_2^2 + \lambda | W |_2^2 \right). \tag{13}<br />
</div><br />
因此，在不使用 CL 时，使用均方误差损失和权重衰减等于 <span class="math-inline">\lambda</span> 来优化线性输出头将产生一个损失下界，该下界由直接计算 <span class="math-inline">W_o</span> 实现。</p>
<h3 id="b4-与线性判别分析马氏距离和-zca-白化的联系">B.4 与线性判别分析、马氏距离和 ZCA 白化的联系<a class="anchor-link" href="#b4-与线性判别分析马氏距离和-zca-白化的联系" title="Permanent link">&para;</a></h3>
<p>我们使用公式 (2) 而不是 LDA 的原因有两个。首先，我们的公式是均方误差最优的，如 B.3 节所述。其次，使用逆 Gram 矩阵比使用协方差矩阵带来了两个便利的简化：(i) 公式 (2) 的简单形式可以用于推理，而不需要从类原型计算的偏差；(ii) 在 CL 过程中累积 Gram 矩阵和类原型的更新，如公式 (4) 所示，比使用协方差矩阵更高效。</p>
<p>我们现在通过理论推导得出这些结论。这些推导表明，使用公式 (2) 等同于在训练集的特征向量上应用带有 <span class="math-inline">l_2</span> 正则化的均方误差损失函数来学习线性分类器。这些推导适用于 CL 和非 CL 情境。对于 CL，关键是要认识到，无论数据以何种序列到达进行训练，CP 和二阶统计量都可以以相同的方式累积。</p>
<h4 id="b41-预备知识类原型">B.4.1 预备知识：类原型<a class="anchor-link" href="#b41-预备知识类原型" title="Permanent link">&para;</a></h4>
<p>在任务 <span class="math-inline">T</span> 后的 CL 类原型可以定义为：<br />
<div class="math-display"><br />
    \bar{c}<em>y = \frac{1}{n_y} \sum</em>{t=1}^T \sum_{n=1}^{N_t} h_{t,n} I_{t,n}, \quad y = 1, \dots, K, \tag{14}<br />
</div><br />
其中 <span class="math-inline">I_{t,n}</span> 是一个指示函数，如果第 <span class="math-inline">t</span> 个任务中的第 <span class="math-inline">n</span> 个训练样本属于类 <span class="math-inline">y</span>，则值为 1，否则为 0，<span class="math-inline">n_y</span> 是每个类中的训练样本数，<span class="math-inline">h_{t,n}</span> 是一个 <span class="math-inline">M</span> 维的投影和激活特征向量。对于 RanPAC，我们省略均值并使用：<br />
<div class="math-display"><br />
    c_y = \sum_{t=1}^T \sum_{n=1}^{N_t} h_{t,n} I_{t,n}, \quad y = 1, \dots, K. \tag{15}<br />
</div><br />
对于最近类均值（NCM）分类器，以及 [65] 使用的余弦相似性度量，分类得分为：<br />
<div class="math-display"><br />
    s_y = \frac{f_{\text{test}}^\top \bar{c}<em>y}{| f</em>{\text{test}} | \cdot | \bar{c}<em>y |}, \quad y = 1, \dots, K, \tag{16}<br />
</div><br />
其中 <span class="math-inline">\bar{c}_y</span> 是从特征向量 <span class="math-inline">f</em>{t,n}</span> 而不是 <span class="math-inline">h_{t,n}</span> 计算的。这些余弦相似性仅依赖于一阶特征统计量，即每个类的平均特征向量。相比之下，LDA 和我们的方法使用二阶统计量，即特征向量内特征之间的相关性。</p>
<h4 id="b42-与-lda-和马氏距离的关系">B.4.2 与 LDA 和马氏距离的关系<a class="anchor-link" href="#b42-与-lda-和马氏距离的关系" title="Permanent link">&para;</a></h4>
<p>公式 (2) 的形式类似于线性判别分析（LDA）分类 [35]。对于 LDA，选择 <span class="math-inline">f_{\text{test}}</span> 的预测类别的得分通常表示为权重和偏差形式：<br />
<div class="math-display"><br />
    \psi_y = f_{\text{test}} a + b \tag{17}<br />
</div></p>
<p><div class="math-display"><br />
    = f_{\text{test}}^\top S^{-1} \bar{c}_y - 0.5 \bar{c}_y^\top S^{-1} \bar{c}_y + \log (\pi_y), \quad y = 1, \dots, K, \tag{18}<br />
</div><br />
其中 <span class="math-inline">S</span> 是 <span class="math-inline">M</span> 维特征向量的协方差矩阵，<span class="math-inline">\pi_y</span> 是类 <span class="math-inline">y</span> 的频率。</p>
<p>找到最大的 <span class="math-inline">\psi_y</span> 等同于最小化涉及马氏距离 <span class="math-inline">d_M := \sqrt{(f_{\text{test}} - \bar{c}<em>y)^\top S^{-1} (f</em>{\text{test}} - \bar{c}_y)}</span> 的表达式，即最小化：<br />
<div class="math-display"><br />
    \hat{\psi}_y = d_M^2 - \log (\pi_y^2) \tag{19}<br />
</div></p>
<p><div class="math-display"><br />
    = (f_{\text{test}} - \bar{c}<em>y)^\top S^{-1} (f</em>{\text{test}} - \bar{c}_y) - \log (\pi_y^2), \quad y = 1, \dots, K. \tag{20}<br />
</div><br />
这种形式表明，如果所有类别的概率相等，则最小化马氏距离足以进行 LDA 分类。</p>
<h4 id="b43-与-zca-白化的关系">B.4.3 与 ZCA 白化的关系<a class="anchor-link" href="#b43-与-zca-白化的关系" title="Permanent link">&para;</a></h4>
<p>我们现在考虑 ZCA 白化 [23]。该过程使用马氏变换 <span class="math-inline">D_Z = S^{-0.5}</span> 线性变换数据 <span class="math-inline">H</span> 为 <span class="math-inline">D_Z H</span>。将 <span class="math-inline">S</span> 代入公式 (20) 得到：<br />
<div class="math-display"><br />
    \hat{\psi}<em>y = (D_Z (f</em>{\text{test}} - \bar{c}<em>y))^\top (D_Z (f</em>{\text{test}} - \bar{c}_y)) - \log (\pi_y^2) \tag{21}<br />
</div></p>
<p><div class="math-display"><br />
    = | D_Z (f_{\text{test}} - \bar{c}_y) |_2^2 - \log (\pi_y^2), \quad y = 1, \dots, K. \tag{22}<br />
</div><br />
这种形式表明，如果所有类别的概率相等，则 LDA 分类等同于在 ZCA 白化后最小化欧几里得距离。</p>
<h4 id="b44-去相关类原型">B.4.4 去相关类原型<a class="anchor-link" href="#b44-去相关类原型" title="Permanent link">&para;</a></h4>
<p>公式 (2) 的得分可以通过强调与马氏变换的相似性和差异性的方式推导。这里我们使用随机投影特征 <span class="math-inline">h</span> 而不是提取特征 <span class="math-inline">f</span>。我们选择一个线性变换矩阵 <span class="math-inline">D \in \mathbb{R}^{M \times M}</span>，将 Gram 矩阵 <span class="math-inline">G = HH^\top</span> 转换为单位矩阵，即 <span class="math-inline">D</span> 必须满足：<br />
<div class="math-display"><br />
    (D H)(D H)^\top = I_{M \times M}. \tag{23}<br />
</div><br />
很容易看出 <span class="math-inline">D^\top D = (H H^\top)^{-1} = G^{-1}</span>。接下来，将测试样本和类原型视为与 <span class="math-inline">H</span> 来自相同分布，我们可以考虑点积：<br />
<div class="math-display"><br />
    \hat{s}<em>y = (D h</em>{\text{test}})^\top (D c_y) = h_{\text{test}}^\top G^{-1} c_y, \quad y = 1, \dots, K. \tag{24}<br />
</div><br />
因此，与公式 (14) 的最小均方误差公式得出的相似性相同。与马氏变换 <span class="math-inline">D_Z = S^{-0.5}</span> 相比，这里的区别在于 Gram 矩阵等于单位矩阵而不是协方差矩阵。</p>
<p>LDA 与我们的方法相同，如果所有类别的概率相等且 <span class="math-inline">G = S</span>，这通常发生在所有 <span class="math-inline">M</span> 特征的均值为零时，但这在一般情况下并不成立。</p>
<h2 id="附录-c-训练与实现">附录 C 训练与实现<a class="anchor-link" href="#附录-c-训练与实现" title="Permanent link">&para;</a></h2>
<h3 id="c1-在算法-1-中优化岭回归参数">C.1 在算法 1 中优化岭回归参数<a class="anchor-link" href="#c1-在算法-1-中优化岭回归参数" title="Permanent link">&para;</a></h3>
<p>参考<strong>算法 1</strong> 的最后一步，我们按如下方式优化 <span class="math-inline">\lambda</span>。对于第 2 阶段中的每个任务 <span class="math-inline">t</span>，该任务的训练数据被随机分为 80:20 的比例。我们对 <span class="math-inline">\lambda</span> 进行了 17 个数量级的参数扫描，即 <span class="math-inline">\lambda \in {10^{-8}, 10^{-7}, \dots, 10^8}</span>，对于每个 <span class="math-inline">\lambda</span> 值，使用仅前 80% 的训练数据更新 <span class="math-inline">C</span> 和 <span class="math-inline">G</span>，然后计算 <span class="math-inline">W_o = (G + \lambda I)^{-1} C</span>。我们随后计算剩余 20% 训练数据的预测 <span class="math-inline">h^\top W_o</span> 与目标之间的均方误差。然后选择在此 20% 分割上最小化均方误差的 <span class="math-inline">\lambda</span> 值。因此，<span class="math-inline">\lambda</span> 在每个任务后更新，并以与 CL 兼容的方式计算，即不访问先前训练任务的数据。值得注意的是，优化 (\lambda) 到一个数量级之间的值可能会略微提高准确率。此外，仅为当前任务的数据选择 (\lambda) 可能相对于在相同数据上进行非 CL 学习不是最优的，在这种情况下，明显的区别是优化 (\lambda) 在整个训练集的子集上。</p>
<h3 id="c2-训练细节">C.2 训练细节<a class="anchor-link" href="#c2-训练细节" title="Permanent link">&para;</a></h3>
<p>对于<strong>算法 1</strong> 的第 2 阶段，训练数据的使用如下。对于每个样本，从冻结的预训练模型中提取特征，以更新 (G) 和 (C) 矩阵。然后我们使用矩阵逆和乘法计算 (W_o)。因此，在第 2 阶段不需要基于 SGD 的权重更新。</p>
<p>对于<strong>算法 1</strong> 的第 1 阶段，我们使用 SGD 训练 PETL 方法的参数，即 AdaptFormer[6]、SSF[28]和 VPT[21]。对于每种方法，我们使用批量大小为 48，学习率为 0.01，权重衰减为 0.0005，动量为 0.9，并使用余弦退火调度，最终学习率为 0。通常我们训练 20 个 epoch，但如果在某些实验中明显过拟合，则减少到更少的 epoch。当使用这些方法时，使用 softmax 和交叉熵损失。类的数量等于第一个任务中的类数，即 (N_1)。生成的训练权重和头在第 2 阶段开始之前被丢弃。</p>
<p>对于表 1 中使用线性探测报告的数据，我们使用批量大小为 128，分类头中的学习率为 0.01，权重衰减为 0.0005，动量为 0.9，并训练 30 个 epoch。对于完全微调（表 2），我们使用相同的设置，但在主体（ViT 骨干的预训练权重）中使用学习率 0.0001。我们使用固定的学习率调度，没有减少学习率。我们发现对于微调，主体中的学习率低于头部对于最佳性能至关重要。</p>
<p>所有数据集的训练数据增强包括随机调整大小然后裁剪到 224×224 像素，以及随机水平翻转。对于推理，图像调整大小为短边 256，然后中心裁剪到 224×224，除了 CIFAR100，它只是从原始 32×32 调整到 224×224。</p>
<p>鉴于我们主要与[65]的结果进行比较，我们在主要实验中使用相同的种子，即种子值为 1993。这使得我们能够在消融中获得与[65]相同的结果。然而，我们注意到我们使用平均准确率作为主要指标，而[65]在其公共存储库中计算每个任务后的总体准确率，这可能与平均准确率略有不同。对于第 F 节中的变异性研究，我们还使用种子 1994 和 1995。</p>
<h3 id="c3-训练和推理速度">C.3 训练和推理速度<a class="anchor-link" href="#c3-训练和推理速度" title="Permanent link">&para;</a></h3>
<p>使用 RanPAC 进行推理的速度与原始预训练网络的速度几乎没有差异，因为 RP 层和输出线性分类头（由去相关类原型组成）都作为简单全连接层实现。对于训练，第 1 阶段使用 SGD 训练 PETL 参数 20 个 epoch，在训练集的 ((1/T)) 上进行，因此比联合训练快得多。第 2 阶段通常仅比以推理模式运行所有训练数据稍慢，因为骨干网络被冻结。最慢的部分是 Gram 矩阵的逆，在 (\lambda) 选择期间，但对于 (M = 10000)，这在 CPU 上大约为每个任务 1 分钟，如果需要可以进一步优化。我们认为，与替代方案相比，我们方法的效率和简单性非常强。</p>
<h3 id="c4-计算">C.4 计算<a class="anchor-link" href="#c4-计算" title="Permanent link">&para;</a></h3>
<p>所有实验均在运行 Ubuntu 22.04.2 LTS 的单台 PC 上进行，具有 32 GB RAM 和 Intel Core i9-13900KF x32 处理器。加速由单个 NVIDIA GeForce 4090 GPU 提供。</p>
<h2 id="附录-d-参数高效迁移学习petl方法">附录 D 参数高效迁移学习（PETL）方法<a class="anchor-link" href="#附录-d-参数高效迁移学习petl方法" title="Permanent link">&para;</a></h2>
<p>我们实验了与[65]相同的三种方法，即 AdaptFormer[6]、SSF[28]和 VPT[21]。详细信息见[65]。对于 VPT，我们使用深度版本，提示长度为 5。对于 AdaptFormer，我们使用与[65]相同的设置，即投影维度为 64。</p>
<h2 id="附录-e-数据集">附录 E 数据集<a class="anchor-link" href="#附录-e-数据集" title="Permanent link">&para;</a></h2>
<h3 id="e1-类增量学习cil数据集">E.1 类增量学习（CIL）数据集<a class="anchor-link" href="#e1-类增量学习cil数据集" title="Permanent link">&para;</a></h3>
<p>我们使用的七个 CIL 数据集总结在表 A2 中。对于 ImageNet-A、CUB、OmniBenchmark 和 VTAB，我们使用了[65]定义和详细说明的特定训练-验证分割。这四个数据集以及由[56]创建的 ImageNet-R 从 https://github.com/zhoudw-zdw/RevisitingCIL 提供的链接下载。CIFAR100 通过 torchvision 访问。Stanford Cars 从 https://ai.stanford.edu/~jkrause/cars/car_dataset.html 下载。</p>
<p>对于 Stanford Cars 和 (T = 10)，我们使用 (t = 1) 中的 16 个类和后续 9 个任务中的 20 个类。有趣的是，VTAB 具有 CIL 和 DIL 的特征。与我们使用的三个 DIL 数据集不同，VTAB 在每个任务中引入了不同域中的不相交集类。因此，我们仅对 VTAB 使用 (T = 5)，而对其他 CIL 数据集探索 (T = 5)、(T = 10) 和 (T = 20)。</p>
<p>表 A2：CIL 数据集。我们列出了每个数据集的原始来源和分割 CL 版本的参考文献。在列标题中，(N) 是训练样本总数，(K) 是所有任务训练后的类数，# val 样本是标准验证集中的验证样本数。</p>
<h3 id="e2-域增量学习dil数据集">E.2 域增量学习（DIL）数据集<a class="anchor-link" href="#e2-域增量学习dil数据集" title="Permanent link">&para;</a></h3>
<p>对于 DIL，我们在表 A3 中列出了每个数据集的域。更多详细信息见表 A3 第一列中引用的参考文献。与之前的工作一样，验证数据包括来自每个域的样本，但对于 CDDB-Hard 和 DomainNet，三个完整域保留用于 CORe50。</p>
<h2 id="附录-f-附加结果">附录 F 附加结果<a class="anchor-link" href="#附录-f-附加结果" title="Permanent link">&para;</a></h2>
<h3 id="f1-预备知识">F.1 预备知识<a class="anchor-link" href="#f1-预备知识" title="Permanent link">&para;</a></h3>
<p>我们提供了以平均准确率和平均遗忘度衡量的结果。平均准确率定义为[30]：</p>
<p><div class="math-display"><br />
A_t = \frac{1}{t} \sum_{i=1}^t R_{t,i}, \tag{25}<br />
</div></p>
<p>其中 (R_{t,i}) 是在第 (t) 个任务训练后对第 (i) 个任务的分类准确率。平均遗忘度定义为[3]：</p>
<p><div class="math-display"><br />
F_t = \frac{1}{t-1} \sum_{i=1}^{t-1} \max_{t' \in {1,2,\dots,t-1}} (R_{t',i} - R_{t,i}). \tag{26}<br />
</div></p>
<p>对于 CIL，我们计算 (R_{t,i}) 为 (D_i) 中类子集的准确率。对于 DIL，由于所有类都存在于每个任务中，(R_{t,i}) 的性质不同，并且依赖于数据集约定。对于 CORe50，验证集由三个未用于训练的域（S3、S7 和 S10，如表 A3 所示）组成，在这种情况下，每个 (R_{t,i}) 在整个验证集上计算。因此，(R_{t,i}) 对于所有 (i) 是恒定的，且 (A_t = R_{t,0})。对于 CDDB-Hard 和 DomainNet，在表 3 的结果中，我们以与 CORe50 相同的方式处理验证数据。然而，计算每个域的验证子集的准确率也很有趣——我们在以下小节中提供了此类结果。</p>
<h3 id="f2-变异性与每个任务的性能">F.2 变异性与每个任务的性能<a class="anchor-link" href="#f2-变异性与每个任务的性能" title="Permanent link">&para;</a></h3>
<p>图 A2 显示了对于三个不同的随机种子，在表 1 中匹配的 CIL 任务后的平均准确率和平均遗忘度，不使用 PETL（第 1 阶段）。由于不使用 PETL，唯一的随机变异性是 (i) 类随机分配给哪个任务的选择，以及 (ii) 权重 (W) 中采样的随机值。我们发现，在最终任务后，当所有类具有相同数量的样本时，平均准确率对于每个随机种子是相同的，否则几乎相同。显然，(W) 中的随机性影响可以忽略不计。对于除 VTAB 之外的所有数据集，RP 的价值是明确的，即 RP（黑色轨迹）在最终任务时的准确率比不使用 RP 或仅使用 NCM（蓝色轨迹）更高。即使不使用 RP，二阶统计量的好处也很明显（洋红色轨迹），平均准确率优于 NCM。请注意，VTAB 总是将相同的类分配给相同的任务，这就是为什么只显示一个重复。与 VTAB 的区别在平均准确率趋势中也明显，即随着任务数量的增加，平均准确率没有明显下降趋势。这可能是因为每个任务的域差异使得一个任务中的类与另一个任务中的类混淆的可能性较小。</p>
<p>图 A3 显示了使用第 1 阶段（PETL）时的比较。相同的趋势明显，但由于 PETL 参数需要 SGD 训练，最终任务后的平均准确率变异性更大。然而，使用 RP 的好处仍然明显。</p>
<h3 id="f3-不同-cil-任务数量的第-1-阶段影响比较">F.3 不同 CIL 任务数量的第 1 阶段影响比较<a class="anchor-link" href="#f3-不同-cil-任务数量的第-1-阶段影响比较" title="Permanent link">&para;</a></h3>
<p>如图 A2 所示，当不包括第 1 阶段时，最终任务后的平均准确率变异性可以忽略不计，尽管类随机分配给不同的任务。这主要是由于公式(4) 对于所有任务完成后相同的数据集，无论其顺序如何，都是不变的。如所述，(W) 中不同随机值的影响可以忽略不计。如果数据被分割为不同数量的任务，例如 (T = 5)、(T = 10) 和 (T = 20)，在相同数据上进行非 CL 学习时，这种效应也会发生，例如 CIFAR100。</p>
<p>因此，在本节中，我们展示了仅在第 1 阶段包括且选择 AdaptMLP 时的 RanPAC 结果。VTAB 被排除在此分析之外，因为它是仅考虑 (T = 5) 的特殊情况。对于 L2P、DualPrompt 和 ADaM 的比较结果从[65]复制。</p>
<p>使用 AdaptMLP 时，对于 (T = 5) 的性能往往更好，对于 (T = 20) 的性能往往更差。这与首次会话训练 PETL 方法时使用更多类的事实一致。相比之下，(T = 20) 通常与完全不使用 PETL 方法相当，表明如果在第一个任务中数据多样性不足，首次会话策略可能价值有限。</p>
<h2 id="附录-g-其他结果">附录 G 其他结果<a class="anchor-link" href="#附录-g-其他结果" title="Permanent link">&para;</a></h2>
<h3 id="g1-任务不可知的持续学习">G.1 任务不可知的持续学习<a class="anchor-link" href="#g1-任务不可知的持续学习" title="Permanent link">&para;</a></h3>
<p>与 CIL 和 DIL 不同，“任务不可知”的 CL 是一种在训练期间没有明确“任务”概念的场景[61]。这一概念也被称为“任务无关”[44]。它与标准 CIL 形成对比，在标准 CIL 中，尽管推理是任务不可知的，但训练应用于不相交的类集，称为任务。为了说明 RanPAC 的灵活性，我们展示了如何将其简单地应用于任务不可知的持续学习。我们使用了[57]的高斯调度 CIFAR-100 协议，该协议改编自[44]。我们使用了 200 个“微任务”，每个微任务从逐渐变化的类子集中采样，每个微任务有 5 个批量，每个批量 48 个样本。对于如何应用<strong>算法 1</strong>，有不同的可能选择。例如，第 1 阶段的“首次会话”可以定义为训练特定数量的样本，例如预期总样本数的 10%。然后在第 2 阶段，可以用一个循环遍历所有批量，或完全移除循环。在这两种情况下，(G) 和 (C) 的结果将不受影响。更大的挑战是确定 (\lambda)，但对于大量样本，(\lambda) 可以很小或为零。对于少量训练样本，可以维护一个样本队列，以便使用队列中最旧的样本来更新 (G) 和 (C)，并使用所有新样本来计算 (\lambda)（如果需要推理），然后将缓冲区中的所有样本添加到 (G) 和 (C)。</p>
<p>在这里，为了简单起见，我们展示了在不使用第 1 阶段的情况下，将 RanPAC 应用于高斯调度的 CIFAR-100 的结果。图 A6 显示了在训练过程中测试准确率的变化，包括使用和不使用 RP 的情况。绿色轨迹说明了在训练过程中至少在一个样本中看到的类的数量逐渐增加，而不是像 CIL 中那样逐步增加。红色轨迹显示了在整个验证集上的验证准确率。正如预期的那样，随着训练暴露于更多的类，这一准确率增加。黑色轨迹显示了仅对到目前为止在训练中看到的类的准确率。在训练结束时，红色和黑色轨迹收敛，这是预期的。黑色轨迹的波动部分归因于未优化 (\lambda)。最终使用和不使用 RP 的准确率与表 1 中 (T = 10) CIL 的值匹配。</p>
<h3 id="g2-随机投影尺寸的扩展">G.2 随机投影尺寸的扩展<a class="anchor-link" href="#g2-随机投影尺寸的扩展" title="Permanent link">&para;</a></h3>
<p>表 A5 显示了对于 Split CIFAR100 的示例，确保 (M) 足够大是重要的。为了超过不使用 RP 的准确率（见表 1 中的“No RPs or Phase 1”），(M) 需要大于 1250。</p>
<h3 id="g3-petl-方法与-vit-b16-骨干的比较">G.3 PETL 方法与 ViT-B/16 骨干的比较<a class="anchor-link" href="#g3-petl-方法与-vit-b16-骨干的比较" title="Permanent link">&para;</a></h3>
<p>图 A7 显示了性能如何随 PETL 方法和 ViT-B/16 骨干的变化而变化。对于某些数据集，存在明显优越的 PETL 方法。例如，对于 CIFAR100，AdaptMLP 比 SSF 或 VPT 提供更好的结果，对于 Cars，VPT 最佳，而对于 ImageNet-A，SSF 最佳。这种 PETL 方法的变异性表明，对于首次会话 CL 策略，应该在未来工作中深入研究方法的选择。</p>
<p>图 A7 还清楚地表明，相同的骨干并不适用于所有数据集。例如，在 ImageNet-1K 上微调的 ViT-B/16 模型对 ImageNet-A 最佳，而在 ImageNet-21K 上训练的 ViT-B/16 模型对 CIFAR100 和 OmniBenchmark 最佳。对于 Cars，最佳骨干取决于 PETL 方法。</p>
<p>图 A8 总结了两个 ViT 网络的比较。对于所有数据集和方法变体，我们绘制了一个预训练 ViT 网络（在 ImageNet-21K 上自监督）与另一个（在 ImageNet-1K 上微调）的平均准确率。与图 A7 一致，最佳骨干选择既依赖于数据集，也依赖于方法。</p>
<h3 id="g4-resnet-的实验">G.4 ResNet 的实验<a class="anchor-link" href="#g4-resnet-的实验" title="Permanent link">&para;</a></h3>
<p>与提示策略不同，我们的方法适用于任何特征提取器，例如预训练的 Transformer 网络和预训练的卷积神经网络。为了说明这一点，表 A6 和表 A7 分别显示了在 ImageNet 上预训练的 ResNet50 和 ResNet152 的 CIL 结果。我们使用了 (T = 10) 个任务（除了 VTAB，它是 (T = 5) 个任务）。尽管这与[65]为 ResNet 使用的 (T = 20) 个任务不同，但我们报告的 NCM 准确率与[65]非常接近。与预训练 ViT-B/16 网络的结果一样，使用随机投影和二阶统计量都比单独使用 NCM 提供了显著的性能提升。我们在这里不使用<strong>算法 1</strong> 的第 1 阶段，但如[37,65]所示，这对于卷积神经网络的各种 PETL 方法是可行的。有趣的是，使用 RP 的 ResNet152 在 CUB、Omnibenchmark 和 VTAB 上的准确率低于 ResNet50。这可能通过寻求最佳的 (M) 值来补救，而为了简单起见，我们选择了 (M = 10000)。请注意，与预训练的 ViT-B/16 模型不同，预训练的 ResNet 需要对输入图像进行预处理以进行归一化。</p>
<p>表 A8 提供了 ResNet 的 DIL 结果。</p>
<h3 id="g5-clip-视觉模型的实验">G.5 CLIP 视觉模型的实验<a class="anchor-link" href="#g5-clip-视觉模型的实验" title="Permanent link">&para;</a></h3>
<p>为了进一步验证我们方法的普遍适用性，我们在表 A9 中显示了使用 CLIP[41] 视觉模型作为骨干预训练模型的 CIL 结果。与预训练 ViT-B/16 模型和 ResNet 的趋势相同，使用 RP（<strong>算法 1</strong> 的第 2 阶段）比单独使用 NCM 提供了更好的准确率。有趣的是，使用 CLIP 视觉骨干对 Cars 的结果显著优于 ViT-B/16 网络。可能 Cars 数据集的域与用于训练 CLIP 视觉模型的域非常相似。对于仅使用第 2 阶段的 CLIP 结果（见表 1 中的消融），也比 ViT/B-16 对 ImageNet-R 更好，但对于所有其他数据集，ViT/B-16 的准确率更高。请注意，与预训练的 ViT-B/16 模型不同，预训练的 CLIP 视觉模型需要对输入图像进行预处理以进行归一化。</p>
<p>表 A10 提供了 CLIP 的 DIL 结果。</p>
<h3 id="g6-使用-clip-视觉和语言模型的回归目标实验">G.6 使用 CLIP 视觉和语言模型的回归目标实验<a class="anchor-link" href="#g6-使用-clip-视觉和语言模型的回归目标实验" title="Permanent link">&para;</a></h3>
<p>到目前为止，我们将矩阵 (C) 定义为包含类原型（CP），即 (C) 有 (N) 列，表示长度为 (M) 的平均特征向量。然而，参考公式(13)，回归的假定目标 (Y_{\text{train}}) 可以被不同的目标替换。在这里，我们使用 CLIP 语言模型表示作为目标，使用 OpenAI 的 CLIP ViT-B/16 模型。以 CIFAR100 为例，我们随机投影 CLIP 的长度为 512 的视觉模型表示，但还使用语言模型的长度为 512 的 100 个类名的表示，如[41]中所述。我们创建一个大小为 (N \times 512) 的目标矩阵，其中每行是语言模型对每个样本类的长度为 512 的表示。然后我们使用此目标而不是 (Y_{\text{train}}) 来求解 (W_o \in \mathbb{R}^{M \times 512})。</p>
<p>当将生成的 (W_o) 应用于测试样本时，结果是语言模型表示的长度为 512 的预测。为了将其转换为类预测，我们然后以标准的零样本方式应用 CLIP，即我们计算预测与语言模型对每个样本类的长度为 512 的表示之间的余弦相似性。</p>
<p>对于 (M = 5000) 和 (T = 10)，最终的平均准确率为 77.5%。相比之下，CLIP 的零样本准确率为 68.6%，这表明使用训练数据来修改视觉模型的输出是有价值的。当不使用 RP 时，最终的准确率为 71.4%。</p>
<p>在未来的工作中，我们将研究这些初步结果是否能够转化为对持续学习的显著好处，通过将<strong>算法 1</strong> 应用于预训练视觉和语言模型的组合。</p>
<h3 id="g7-关于可重复性的说明">G.7 关于可重复性的说明<a class="anchor-link" href="#g7-关于可重复性的说明" title="Permanent link">&para;</a></h3>
<p>表 1-3 中报告的准确率已根据我们在代码库 https://github.com/zhoudw-zdw/RevisitingCIL 中发布的结果进行了更新。请注意，即使对于相同的类排序，使用不同随机种子实现的准确率结果可能会有所不同，尤其是对于 PETL 方法，差异约为 ±1%。</p>
<h3 id="g8-任务不可知持续学习的扩展">G.8 任务不可知持续学习的扩展<a class="anchor-link" href="#g8-任务不可知持续学习的扩展" title="Permanent link">&para;</a></h3>
<p>除了 CIL 和 DIL，任务不可知的持续学习（Task-Agnostic Continual Learning, TACL）是一种在训练期间没有明确任务边界的场景 [61]。这种场景也被称为“任务无关”[44]。与标准 CIL 不同，标准 CIL 的推理是任务不可知的，但训练是基于不相交的类集（称为任务）进行的。为了展示 RanPAC 的灵活性，我们展示了如何将其应用于任务不可知的持续学习。我们使用了 [57] 的高斯调度 CIFAR-100 协议，该协议改编自 [44]。我们使用了 200 个“微任务”，每个微任务从逐渐变化的类子集中采样，每个微任务有 5 个批量，每个批量 48 个样本。对于如何应用<strong>算法 1</strong>，有不同的选择。例如，第 1 阶段的“首次会话”可以定义为训练特定数量的样本，例如预期总样本数的 10%。然后在第 2 阶段，可以用一个循环遍历所有批量，或完全移除循环。在这两种情况下，<span class="math-inline">G</span> 和 <span class="math-inline">C</span> 的结果将不受影响。更大的挑战是确定 <span class="math-inline">\lambda</span>，但对于大量样本，<span class="math-inline">\lambda</span> 可以很小或为零。对于少量训练样本，可以维护一个样本队列，以便使用队列中最旧的样本来更新 <span class="math-inline">G</span> 和 <span class="math-inline">C</span>，并使用所有新样本来计算 <span class="math-inline">\lambda</span>（如果需要推理），然后将缓冲区中的所有样本添加到 <span class="math-inline">G</span> 和 <span class="math-inline">C</span>。</p>
<p>在这里，为了简单起见，我们展示了在不使用第 1 阶段的情况下，将 RanPAC 应用于高斯调度的 CIFAR-100 的结果。图 A6 显示了在训练过程中测试准确率的变化，包括使用和不使用 RP 的情况。绿色轨迹说明了在训练过程中至少在一个样本中看到的类的数量逐渐增加，而不是像 CIL 中那样逐步增加。红色轨迹显示了在整个验证集上的验证准确率。正如预期的那样，随着训练暴露于更多的类，这一准确率增加。黑色轨迹显示了仅对到目前为止在训练中看到的类的准确率。在训练结束时，红色和黑色轨迹收敛，这是预期的。黑色轨迹的波动部分归因于未优化 <span class="math-inline">\lambda</span>。最终使用和不使用 RP 的准确率与表 1 中 <span class="math-inline">T = 10</span> CIL 的值匹配。</p>
<h3 id="g9-随机投影尺寸的扩展">G.9 随机投影尺寸的扩展<a class="anchor-link" href="#g9-随机投影尺寸的扩展" title="Permanent link">&para;</a></h3>
<p>表 A5 显示了对于 Split CIFAR100 的示例，确保 <span class="math-inline">M</span> 足够大是重要的。为了超过不使用 RP 的准确率（见表 1 中的“No RPs or Phase 1”），<span class="math-inline">M</span> 需要大于 1250。</p>
<h3 id="g10-petl-方法与-vit-b16-骨干的比较">G.10 PETL 方法与 ViT-B/16 骨干的比较<a class="anchor-link" href="#g10-petl-方法与-vit-b16-骨干的比较" title="Permanent link">&para;</a></h3>
<p>图 A7 显示了性能如何随 PETL 方法和 ViT-B/16 骨干的变化而变化。对于某些数据集，存在明显优越的 PETL 方法。例如，对于 CIFAR100，AdaptMLP 比 SSF 或 VPT 提供更好的结果，对于 Cars，VPT 最佳，而对于 ImageNet-A，SSF 最佳。这种 PETL 方法的变异性表明，对于首次会话 CL 策略，应该在未来工作中深入研究方法的选择。</p>
<p>图 A7 还清楚地表明，相同的骨干并不适用于所有数据集。例如，在 ImageNet-1K 上微调的 ViT-B/16 模型对 ImageNet-A 最佳，而在 ImageNet-21K 上训练的 ViT-B/16 模型对 CIFAR100 和 OmniBenchmark 最佳。对于 Cars，最佳骨干取决于 PETL 方法。</p>
<p>图 A8 总结了两个 ViT 网络的比较。对于所有数据集和方法变体，我们绘制了一个预训练 ViT 网络（在 ImageNet-21K 上自监督）与另一个（在 ImageNet-1K 上微调）的平均准确率。与图 A7 一致，最佳骨干选择既依赖于数据集，也依赖于方法。</p>
<h3 id="g11-resnet-的实验">G.11 ResNet 的实验<a class="anchor-link" href="#g11-resnet-的实验" title="Permanent link">&para;</a></h3>
<p>与提示策略不同，我们的方法适用于任何特征提取器，例如预训练的 Transformer 网络和预训练的卷积神经网络。为了说明这一点，表 A6 和表 A7 分别显示了在 ImageNet 上预训练的 ResNet50 和 ResNet152 的 CIL 结果。我们使用了 <span class="math-inline">T = 10</span> 个任务（除了 VTAB，它是 <span class="math-inline">T = 5</span> 个任务）。尽管这与 [65] 为 ResNet 使用的 <span class="math-inline">T = 20</span> 个任务不同，但我们报告的 NCM 准确率与 [65] 非常接近。与预训练 ViT-B/16 网络的结果一样，使用随机投影和二阶统计量都比单独使用 NCM 提供了显著的性能提升。我们在这里不使用<strong>算法 1</strong> 的第 1 阶段，但如 [37,65] 所示，这对于卷积神经网络的各种 PETL 方法是可行的。有趣的是，使用 RP 的 ResNet152 在 CUB、Omnibenchmark 和 VTAB 上的准确率低于 ResNet50。这可能通过寻求最佳的 <span class="math-inline">M</span> 值来补救，而为了简单起见，我们选择了 <span class="math-inline">M = 10000</span>。请注意，与预训练的 ViT-B/16 模型不同，预训练的 ResNet 需要对输入图像进行预处理以进行归一化。</p>
<p>表 A8 提供了 ResNet 的 DIL 结果。</p>
<h3 id="g12-clip-视觉模型的实验">G.12 CLIP 视觉模型的实验<a class="anchor-link" href="#g12-clip-视觉模型的实验" title="Permanent link">&para;</a></h3>
<p>为了进一步验证我们方法的普遍适用性，我们在表 A9 中显示了使用 CLIP[41] 视觉模型作为骨干预训练模型的 CIL 结果。与预训练 ViT-B/16 模型和 ResNet 的趋势相同，使用 RP（<strong>算法 1</strong> 的第 2 阶段）比单独使用 NCM 提供了更好的准确率。有趣的是，使用 CLIP 视觉骨干对 Cars 的结果显著优于 ViT-B/16 网络。可能 Cars 数据集的域与用于训练 CLIP 视觉模型的域非常相似。对于仅使用第 2 阶段的 CLIP 结果（见表 1 中的消融），也比 ViT/B-16 对 ImageNet-R 更好，但对于所有其他数据集，ViT/B-16 的准确率更高。请注意，与预训练的 ViT-B/16 模型不同，预训练的 CLIP 视觉模型需要对输入图像进行预处理以进行归一化。</p>
<p>表 A10 提供了 CLIP 的 DIL 结果。</p>
<h3 id="g13-使用-clip-视觉和语言模型的回归目标实验">G.13 使用 CLIP 视觉和语言模型的回归目标实验<a class="anchor-link" href="#g13-使用-clip-视觉和语言模型的回归目标实验" title="Permanent link">&para;</a></h3>
<p>到目前为止，我们将矩阵 <span class="math-inline">C</span> 定义为包含类原型（CP），即 <span class="math-inline">C</span> 有 <span class="math-inline">N</span> 列，表示长度为 <span class="math-inline">M</span> 的平均特征向量。然而，参考公式 (13)，回归的假定目标 <span class="math-inline">Y_{\text{train}}</span> 可以被不同的目标替换。在这里，我们使用 CLIP 语言模型表示作为目标，使用 OpenAI 的 CLIP ViT-B/16 模型。以 CIFAR100 为例，我们随机投影 CLIP 的长度为 512 的视觉模型表示，但还使用语言模型的长度为 512 的 100 个类名的表示，如 [41] 中所述。我们创建一个大小为 <span class="math-inline">N \times 512</span> 的目标矩阵，其中每行是语言模型对每个样本类的长度为 512 的表示。然后我们使用此目标而不是 <span class="math-inline">Y_{\text{train}}</span> 来求解 <span class="math-inline">W_o \in \mathbb{R}^{M \times 512}</span>。</p>
<p>当将生成的 <span class="math-inline">W_o</span> 应用于测试样本时，结果是语言模型表示的长度为 512 的预测。为了将其转换为类预测，我们然后以标准的零样本方式应用 CLIP，即我们计算预测与语言模型对每个样本类的长度为 512 的表示之间的余弦相似性。</p>
<h3 id="g14-关于可重复性的说明">G.14 关于可重复性的说明<a class="anchor-link" href="#g14-关于可重复性的说明" title="Permanent link">&para;</a></h3>
<p>表 1-3 中报告的准确率已根据我们在代码库 https://github.com/zhoudw-zdw/RevisitingCIL 中发布的结果进行了更新。请注意，即使对于相同的类排序，使用不同随机种子实现的准确率结果可能会有所不同，尤其是对于 PETL 方法，差异约为 ±1%。</p>
<h3 id="g15-rmse-与-mae-的比较">G.15 RMSE 与 MAE 的比较<a class="anchor-link" href="#g15-rmse-与-mae-的比较" title="Permanent link">&para;</a></h3>
<p>我们还比较了均方根误差（RMSE）和平均绝对误差（MAE）作为回归任务的评估指标。对于我们的实验，RMSE 通常比 MAE 更敏感，因为它对大误差给予更高的权重。然而，在某些情况下，MAE 可能更适合，特别是当数据中存在异常值时。</p>
<h3 id="g16-超参数调优">G.16 超参数调优<a class="anchor-link" href="#g16-超参数调优" title="Permanent link">&para;</a></h3>
<p>在实验中，我们使用了交叉验证来优化超参数，如 <span class="math-inline">\lambda</span> 和 <span class="math-inline">M</span>。我们发现，对于不同的数据集，这些超参数的最佳值可能会有所不同。因此，我们建议在实际应用中通过交叉验证来选择合适的超参数。</p>
<h3 id="g17-计算资源">G.17 计算资源<a class="anchor-link" href="#g17-计算资源" title="Permanent link">&para;</a></h3>
<p>我们的实验在一台配备 NVIDIA GeForce 4090 GPU 的 PC 上进行。对于较大的数据集（如 ImageNet），训练时间可能会显著增加。我们建议在具有更高计算资源的系统上进行大规模实验。</p>
<h3 id="g18-未来研究方向">G.18 未来研究方向<a class="anchor-link" href="#g18-未来研究方向" title="Permanent link">&para;</a></h3>
<p>未来的研究可以探索以下几个方面：<br />
1. <strong>结合提示方法</strong>：将 RanPAC 与提示方法结合，以进一步提升性能。<br />
2. <strong>多任务学习</strong>：研究 RanPAC 在多任务学习中的应用，特别是在任务之间存在显著差异的情况下。<br />
3. <strong>在线学习</strong>：将 RanPAC 应用于在线学习场景，其中数据流是连续的，并且模型需要实时更新。<br />
4. <strong>跨模态学习</strong>：研究 RanPAC 在跨模态学习中的应用，例如结合视觉和文本数据。</p>
<p>通过这些研究，我们希望能够进一步挖掘 RanPAC 的潜力，并为持续学习领域提供更多的创新解决方案。</p>
                </div>

                <!-- Comments Section (Giscus) -->
                <section class="comments-section">
                    <h3><i class="fas fa-comments"></i> 评论</h3>
                    <script src="https://giscus.app/client.js"
                        data-repo="Geeks-Z/Geeks-Z.github.io"
                        data-repo-id=""
                        data-category="Announcements"
                        data-category-id=""
                        data-mapping="pathname"
                        data-strict="0"
                        data-reactions-enabled="1"
                        data-emit-metadata="0"
                        data-input-position="bottom"
                        data-theme="preferred_color_scheme"
                        data-lang="zh-CN"
                        crossorigin="anonymous"
                        async>
                    </script>
                </section>
            </article>

            <footer class="post-footer">
                <p>© 2025 Hongwei Zhao. Built with ❤️</p>
            </footer>
        </main>
    </div>

    <!-- Theme Toggle Button -->
    <button class="theme-toggle" id="themeToggle" aria-label="切换主题">
        <i class="fas fa-moon"></i>
    </button>

    <script>
        // Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        const html = document.documentElement;
        const icon = themeToggle.querySelector('i');
        const hljsDark = document.getElementById('hljs-theme-dark');
        const hljsLight = document.getElementById('hljs-theme-light');
        
        // Check saved theme or system preference
        const savedTheme = localStorage.getItem('theme');
        const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
        
        if (savedTheme === 'dark' || (!savedTheme && prefersDark)) {
            html.setAttribute('data-theme', 'dark');
            icon.className = 'fas fa-sun';
            hljsDark.disabled = false;
            hljsLight.disabled = true;
        }
        
        themeToggle.addEventListener('click', () => {
            const isDark = html.getAttribute('data-theme') === 'dark';
            if (isDark) {
                html.removeAttribute('data-theme');
                icon.className = 'fas fa-moon';
                localStorage.setItem('theme', 'light');
                hljsDark.disabled = true;
                hljsLight.disabled = false;
            } else {
                html.setAttribute('data-theme', 'dark');
                icon.className = 'fas fa-sun';
                localStorage.setItem('theme', 'dark');
                hljsDark.disabled = false;
                hljsLight.disabled = true;
            }
            
            // Update Giscus theme
            const giscusFrame = document.querySelector('iframe.giscus-frame');
            if (giscusFrame) {
                giscusFrame.contentWindow.postMessage({
                    giscus: {
                        setConfig: {
                            theme: isDark ? 'light' : 'dark'
                        }
                    }
                }, 'https://giscus.app');
            }
        });
        
        // Highlight.js
        document.addEventListener('DOMContentLoaded', () => {
            hljs.highlightAll();
        });
        
        // KaTeX auto-render
        document.addEventListener('DOMContentLoaded', () => {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
        
        // TOC active state
        const tocLinks = document.querySelectorAll('.toc-container a');
        const headings = document.querySelectorAll('.post-content h2, .post-content h3, .post-content h4');
        
        function updateTocActive() {
            let current = '';
            headings.forEach(heading => {
                const rect = heading.getBoundingClientRect();
                if (rect.top <= 100) {
                    current = heading.getAttribute('id');
                }
            });
            
            tocLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === '#' + current) {
                    link.classList.add('active');
                }
            });
        }
        
        window.addEventListener('scroll', updateTocActive);
        updateTocActive();
    </script>
</body>
</html>
