<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Untitled</title>
    <meta name="description" content="Untitled - Hongwei Zhao's Blog">
    <meta name="author" content="Hongwei Zhao">
    
    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <!-- Code Highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css" id="hljs-theme-dark">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css" id="hljs-theme-light" disabled>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    
    <style>
        :root {
            /* Light Theme - 明亮清新配色 */
            --primary-color: #4A90D9;
            --primary-hover: #3678C2;
            --link-color: #E86B5F;
            --text-color: #2D2D2D;
            --text-light: #5A5A5A;
            --text-muted: #8A8A8A;
            --bg-color: #FFFFFF;
            --bg-secondary: #F5F7FA;
            --bg-code: #F8F9FC;
            --border-color: #E8ECF0;
            --shadow: 0 2px 8px rgba(0,0,0,0.06);
            --shadow-lg: 0 8px 24px rgba(0,0,0,0.08);
        }

        [data-theme="dark"] {
            --primary-color: #5dade2;
            --primary-hover: #85c1e9;
            --link-color: #e74c3c;
            --text-color: #e5e7eb;
            --text-light: #9ca3af;
            --text-muted: #6b7280;
            --bg-color: #1a1a2e;
            --bg-secondary: #16213e;
            --bg-code: #0f0f23;
            --border-color: #374151;
            --shadow: 0 1px 3px rgba(0,0,0,0.3);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.4);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 16px;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            transition: background-color 0.3s, color 0.3s;
        }

        a {
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--primary-hover);
            text-decoration: underline;
        }

        /* Layout */
        .page-wrapper {
            display: flex;
            max-width: 1400px;
            margin: 0 auto;
            padding: 2rem;
            gap: 3rem;
        }

        /* Sidebar TOC */
        .toc-sidebar {
            width: 260px;
            flex-shrink: 0;
            position: sticky;
            top: 2rem;
            height: fit-content;
            max-height: calc(100vh - 4rem);
            overflow-y: auto;
        }

        .toc-container {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 1.5rem;
            border: 1px solid var(--border-color);
        }

        .toc-container h3 {
            font-size: 0.85rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-muted);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .toc-container ul {
            list-style: none;
        }

        .toc-container li {
            margin-bottom: 0.5rem;
        }

        .toc-container a {
            font-size: 0.9rem;
            color: var(--text-light);
            display: block;
            padding: 0.25rem 0;
            border-left: 2px solid transparent;
            padding-left: 0.75rem;
            transition: all 0.2s;
        }

        .toc-container a:hover,
        .toc-container a.active {
            color: var(--primary-color);
            border-left-color: var(--primary-color);
            text-decoration: none;
        }

        .toc-container ul ul {
            margin-left: 1rem;
        }

        /* Main Content */
        .main-content {
            flex: 1;
            min-width: 0;
            max-width: 800px;
        }

        /* Header */
        .post-header {
            margin-bottom: 2rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
            margin-bottom: 1.5rem;
        }

        .back-link:hover {
            color: var(--primary-color);
            text-decoration: none;
        }

        .post-header h1 {
            font-size: 2.25rem;
            font-weight: 700;
            line-height: 1.3;
            margin-bottom: 1rem;
            color: var(--text-color);
        }

        .post-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
        }

        .post-meta span {
            display: flex;
            align-items: center;
            gap: 0.4rem;
        }

        .post-meta i {
            color: var(--text-muted);
        }

        .tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 1rem;
        }

        .tag {
            display: inline-block;
            font-size: 0.8rem;
            background: var(--bg-secondary);
            color: var(--text-light);
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            border: 1px solid var(--border-color);
            transition: all 0.2s;
        }

        .tag:hover {
            background: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }

        /* Article Content */
        .post-content {
            font-size: 1rem;
            line-height: 1.9;
        }

        .post-content h1,
        .post-content h2,
        .post-content h3,
        .post-content h4,
        .post-content h5,
        .post-content h6 {
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-weight: 600;
            line-height: 1.4;
            color: var(--text-color);
        }

        .post-content h1 { font-size: 1.875rem; }
        .post-content h2 { 
            font-size: 1.5rem; 
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }
        .post-content h3 { font-size: 1.25rem; }
        .post-content h4 { font-size: 1.125rem; }

        .post-content p {
            margin-bottom: 1.25rem;
        }

        .post-content ul,
        .post-content ol {
            margin: 1.25rem 0;
            padding-left: 1.5rem;
        }

        .post-content li {
            margin-bottom: 0.5rem;
        }

        .post-content blockquote {
            margin: 1.5rem 0;
            padding: 1rem 1.5rem;
            background: var(--bg-secondary);
            border-left: 4px solid var(--primary-color);
            border-radius: 0 8px 8px 0;
            color: var(--text-light);
            font-style: italic;
        }

        .post-content blockquote p:last-child {
            margin-bottom: 0;
        }

        /* Code */
        .post-content code {
            font-family: 'JetBrains Mono', 'Fira Code', Consolas, monospace;
            font-size: 0.9em;
            background: var(--bg-code);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            color: var(--link-color);
        }

        .post-content pre {
            margin: 1.5rem 0;
            padding: 1.25rem;
            background: var(--bg-code);
            border-radius: 10px;
            overflow-x: auto;
            border: 1px solid var(--border-color);
        }

        .post-content pre code {
            background: none;
            padding: 0;
            color: inherit;
            font-size: 0.875rem;
            line-height: 1.6;
        }

        /* Images */
        .post-content img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            margin: 1.5rem 0;
            box-shadow: var(--shadow-lg);
        }

        /* Tables */
        .post-content table {
            width: 100%;
            margin: 1.5rem 0;
            border-collapse: collapse;
            font-size: 0.95rem;
        }

        .post-content th,
        .post-content td {
            padding: 0.75rem 1rem;
            border: 1px solid var(--border-color);
            text-align: left;
        }

        .post-content th {
            background: var(--bg-secondary);
            font-weight: 600;
        }

        .post-content tr:nth-child(even) {
            background: var(--bg-secondary);
        }

        /* Math */
        .math-display {
            margin: 1.5rem 0;
            overflow-x: auto;
            padding: 1rem;
            background: var(--bg-secondary);
            border-radius: 8px;
        }

        /* Anchor links */
        .anchor-link {
            opacity: 0;
            margin-left: 0.5rem;
            color: var(--text-muted);
            font-weight: 400;
            transition: opacity 0.2s;
        }

        .post-content h2:hover .anchor-link,
        .post-content h3:hover .anchor-link,
        .post-content h4:hover .anchor-link {
            opacity: 1;
        }

        /* Theme Toggle */
        .theme-toggle {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            background: var(--primary-color);
            color: white;
            border: none;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.25rem;
            box-shadow: var(--shadow-lg);
            transition: transform 0.2s, background 0.2s;
            z-index: 1000;
        }

        .theme-toggle:hover {
            transform: scale(1.1);
            background: var(--primary-hover);
        }

        /* Comments Section */
        .comments-section {
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-color);
        }

        .comments-section h3 {
            font-size: 1.25rem;
            margin-bottom: 1.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        /* Footer */
        .post-footer {
            margin-top: 3rem;
            padding-top: 1.5rem;
            border-top: 1px solid var(--border-color);
            text-align: center;
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        /* Responsive */
        @media (max-width: 1024px) {
            .toc-sidebar {
                display: none;
            }
            
            .page-wrapper {
                padding: 1.5rem;
            }
        }

        @media (max-width: 768px) {
            .post-header h1 {
                font-size: 1.75rem;
            }
            
            .post-meta {
                flex-direction: column;
                gap: 0.5rem;
            }
            
            .theme-toggle {
                bottom: 1rem;
                right: 1rem;
                width: 44px;
                height: 44px;
            }
        }

        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-secondary);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--border-color);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--text-muted);
        }
    </style>
</head>
<body>
    <div class="page-wrapper">
        <!-- TOC Sidebar -->
        <aside class="toc-sidebar">
            <div class="toc-container">
                <h3><i class="fas fa-list"></i> 目录</h3>
                <div class="toc">
<ul>
<li><a href="#weighted-ensemble-models-are-strong-continual-learners">Weighted Ensemble Models Are Strong Continual Learners</a></li>
<li><a href="#0-摘要">0. 摘要</a></li>
<li><a href="#1-引言">1. 引言</a></li>
<li><a href="#2-相关工作">2. 相关工作</a><ul>
<li><a href="#21-基于-ptm-的持续学习">2.1 基于 PTM 的持续学习</a></li>
<li><a href="#22-输出空间权重空间集成">2.2 输出空间/权重空间集成</a></li>
</ul>
</li>
<li><a href="#3-方法">3. 方法</a><ul>
<li><a href="#31-问题表述与概述">3.1 问题表述与概述</a><ul>
<li><a href="#311-基于预训练模型的持续学习">3.1.1 基于预训练模型的持续学习</a></li>
</ul>
</li>
<li><a href="#32-持续模型平均coma">3.2 持续模型平均（CoMA）</a></li>
<li><a href="#33-持续-fisher-加权模型平均cofima">3.3 持续 Fisher 加权模型平均（CoFiMA）</a></li>
</ul>
</li>
<li><a href="#4-实验">4. 实验</a><ul>
<li><a href="#41-实验设置">4.1 实验设置</a></li>
<li><a href="#42-最先进方法的比较">4.2 最先进方法的比较</a></li>
<li><a href="#43-消融研究">4.3 消融研究</a><ul>
<li><a href="#431-模型平均的分析">4.3.1 模型平均的分析</a></li>
<li><a href="#432-ptm-的影响">4.3.2 PTM 的影响</a></li>
<li><a href="#433-平衡旧任务和新任务的信息">4.3.3 平衡旧任务和新任务的信息</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#5-结论">5. 结论</a></li>
</ul>
</div>

            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <article>
                <header class="post-header">
                    <a href="../../../../index.html" class="back-link">
                        <i class="fas fa-arrow-left"></i> 返回博客列表
                    </a>
                    <h1>Untitled</h1>
                    <div class="post-meta">
                        <span><i class="fas fa-calendar-alt"></i> 2026-02-04</span>
                        <span><i class="fas fa-folder"></i> AINotes/40.CIL/30.Parameter Regularization</span>
                        <span><i class="fas fa-user"></i> Hongwei Zhao</span>
                    </div>
                    <div class="tags">
                        
                    </div>
                </header>

                <div class="post-content">
                    <h2 id="weighted-ensemble-models-are-strong-continual-learners">Weighted Ensemble Models Are Strong Continual Learners<a class="anchor-link" href="#weighted-ensemble-models-are-strong-continual-learners" title="Permanent link">&para;</a></h2>
<h2 id="0-摘要">0. 摘要<a class="anchor-link" href="#0-摘要" title="Permanent link">&para;</a></h2>
<p>在这项工作中，我们研究了持续学习（Continual Learning, CL）问题，目标是在一系列任务上学习一个模型，假设在学习当前任务数据时，之前任务的数据不可用。持续学习本质上是在学习新任务（即可塑性）和保持先前学习概念的性能（即稳定性）之间进行平衡。为了解决稳定性与可塑性之间的权衡问题，我们提出对先前任务和当前任务的模型参数进行加权集成。这种加权集成模型，我们称之为<strong>持续模型平均</strong>（<strong>CoMA</strong>），通过利用可塑性在当前任务上获得高准确率，同时不会偏离先前的权重配置太远，从而确保稳定性。我们还提出了 CoMA 的改进版本，名为<strong>持续 Fisher 加权模型平均</strong>（<strong>CoFiMA</strong>），它通过利用模型权重的 Fisher 信息选择性地对权重集成中的每个参数进行加权。这两个变体在概念上简单，易于实现，并且在多个标准 CL 基准测试中达到了最先进的性能。代码可在以下网址获取：https://github.com/lemProg/CoFiMA。</p>
<p>关键词：持续学习，模型平均。</p>
<h2 id="1-引言">1. 引言<a class="anchor-link" href="#1-引言" title="Permanent link">&para;</a></h2>
<p>从一系列任务中持续学习一个统一的模型是一个具有挑战性的问题，主要原因是<strong>灾难性遗忘</strong>（Catastrophic Forgetting, CF）[15]——这种现象表现为模型在先前见过的数据上性能下降。持续学习（CL）作为解决 CF 的一种方案，允许模型在吸收新任务信息的同时保留对先前学习类别的分类能力 [36]。直到最近，CL 方法主要集中在相对较小的网络，通常是 ResNets[18]，从随机初始化开始 [36, 76]。最近，大规模预训练模型（Pre-Trained Models, PTMs）[11, 27, 51]——如在大规模数据集（如 ImageNet[55]、LAION-400M[58]）上预训练的 Vision Transformer（ViT）[27, 35]——的兴起，导致了许多 CL 方法的涌现，这些方法利用了 PTMs 的强大表示能力，引发了 CL 领域的范式转变 [70, 77, 78, 79, 88]。具体来说，许多基于 PTM 的 CL 方法 [10, 40, 82] 通过实验验证了，通过大规模和多样化的预训练获得良好的初始表示，有助于增量学习，因为新任务可以通过较少的训练步骤学习。然而，顺序地对 PTM 骨干进行全微调会导致原始 PTM 表示的退化，同时显著遗忘先前学习的任务 [38, 47, 86, 88]。为了防止过拟合，许多方法提出了启发式地限制 PTM 微调仅在第一次适应会话中进行 [38, 47, 88]，或者谨慎选择低学习率来微调骨干 [86]。然而，<strong>在快速积累新任务知识的同时保持 PTMs 的泛化能力</strong>仍然是一个开放的研究问题。</p>
<p>在追求实现 PTMs 的鲁棒微调过程中，一些研究探讨了权重平均（Weight Averaging, WA）方法的应用 [1, 25, 37, 42, 80, 81]。这些方法的本质是通过集成多个微调后的 PTMs 来获得一个单一模型，该模型封装了多个模型的表示能力。虽然据我们所知，WA 尚未在 CL 中进行研究，但最近的两项研究 [80, 81] 激发了我们对 WA 在 CL 中可行性的兴趣。首先，WISE-FT[81] 通过平均微调模型和预训练模型的权重，提高了零样本分类器（如 CLIP）的微调过程的鲁棒性（图 1(a)）。在这里，生成的集成模型在目标分布上表现出高准确率，同时保留了原始 PTM 的分布外（Out-of-Distribution, OOD）性能。重要的是要认识到，模型鲁棒性和 CL 是相互关联的：<strong>OOD 性能与目标性能之间的平衡</strong>反映了 CL 中的<strong>稳定性与可塑性权衡</strong>，目标是在适应新任务的同时保持对先前任务的有效性 [1, 25, 42]。其次，“Model soups”[80] 表明，通过 WA 结合同一 PTM 的多个微调模型，可以提高在分布内和分布外任务上的性能（图 1(b)）。他们的实验强调了 WA 在相对较大的模型池（例如 32 个）中的潜力，这与 CL 中通常遇到的任务数量相呼应。</p>
<p>图 1：<strong>现有模型平均技术与我们提出的 CL 技术的比较。</strong> (a) 平均预训练模型和微调模型的权重，同时提高了分布外和目标数据集的性能。(b) Model soups 结合了多个微调模型，生成了一个鲁棒的统一模型。(c) 在提出的 CoFiMA 中，当前和过去模型的权重基于它们的 Fisher 信息矩阵（用 <span class="math-inline">\mathcal{F}</span> 表示）进行加权，从而在当前和旧任务上实现了平衡的性能。</p>
<h2 id="2-相关工作">2. 相关工作<a class="anchor-link" href="#2-相关工作" title="Permanent link">&para;</a></h2>
<h3 id="21-基于-ptm-的持续学习">2.1 基于 PTM 的持续学习<a class="anchor-link" href="#21-基于-ptm-的持续学习" title="Permanent link">&para;</a></h3>
<p>不久之前，CL 的主要焦点是从头开始顺序训练深度神经网络，旨在熟练地获取新任务，同时减轻对先前任务的遗忘。典型的 CL 策略包括<strong>基于正则化</strong>的方法 [2, 8, 26, 33, 85]，这些方法保持初始模型并选择性地稳定参数或预测变化；<strong>基于回放</strong>的方法 [4, 50, 73, 83]，这些方法试图近似并重新生成先前学习的数据分布；以及<strong>基于架构</strong>的方法 [56, 59, 84]，这些方法为每个任务分配离散的参数子空间。</p>
<p>不同的是，最近的 CL 研究探讨了 PTMs 的优势 [10, 78, 79]。预训练获得的表示不仅促进了知识转移，还在下游持续学习中表现出对灾难性遗忘的抵抗力 [40, 52]。此外，在预训练阶段学习大量基础类允许 CL 以最小的适应进行 [82]。例如，L2P[79] 利用了 NLP 中预训练知识利用的技术，采用了一组额外的可学习参数，称为“提示”，这些提示指导预训练表示层学习增量任务。DualPrompt[78] 扩展了这一概念，通过将额外的提示附加到预训练表示层，以促进任务不变和任务特定指令的学习。尽管基于提示的方法已被证明显著优于传统的 CL 基线，但它们引入了额外的推理成本。最近，Zhang 等人 [86] 表明，使用 PTMs 进行小学习率的顺序微调优于传统的 CL 方法。Wang 等人 [74, 75] 提出了一种架构，该架构采用多个较窄的子网络来管理增量任务，有效减少了 CL 中的泛化误差。然而，这种方法引入了增加的复杂性。</p>
<p>与先前在 CL 中通过提示 [78, 79]、专家集成 [57, 74, 75] 或回放缓冲区 [4, 50, 73, 83] 增强 PTMs 的方法不同，CoFiMA 采用了不同的策略。CoFiMA 通过解锁所有模型参数进行微调来实现可塑性，与 [38, 78, 79] 不同。此外，它通过平均先前模型的参数权重来减少训练期间的遗忘。与 EWC[26] 相比，EWC 使用 Fisher 信息矩阵（FIM）作为任务间 L2 转移的正则化项，而 CoFiMA 将 FIM 作为加权因子来评估每个任务的权重重要性，而不施加任何正则化约束（更多细节见补充材料）。</p>
<h3 id="22-输出空间权重空间集成">2.2 输出空间/权重空间集成<a class="anchor-link" href="#22-输出空间权重空间集成" title="Permanent link">&para;</a></h3>
<p>传统的集成方法，或输出空间集成，结合了多个分类器的预测，通常优于单一模型，并在分布变化下提供更校准的不确定性估计 [16, 31, 37, 46, 64]。然而，这些输出空间集成在推理时需要大量的计算资源。权重空间集成通过插值模型权重提供了一种计算效率高的替代方案 [22, 44, 66, 80, 81]。Wortsman 等人 [81] 通过插值零样本 CLIP 和微调模型权重实现了这一点，从而在微调任务和分布变化下都获得了性能提升。Matena 等人 [37] 提出了一种使用 Fisher 值进行不同文本分类任务的高级 WA 技术，但他们没有研究这种方法在 CL 中的可行性。在联邦学习（Federated Learning, FL）中，模型平均（特别是 FedAvg）是一种基本技术，用于在保护隐私的同时整合来自分散数据的见解 [39]。这种方法涉及在分布式节点上训练本地模型，并平均它们的参数以更新全局模型，从而提高学习效率和数据隐私 [24, 32, 39]。我们的方法与现有的 WA 方法 [37, 80, 81] 不同，因为它专注于顺序微调与权重平均，以适应 CL 设置，我们在每个任务中迭代执行我们的过程，使用每个任务的平均模型作为下一个任务的初始化。</p>
<p>WA 还与 Frankle 等人 [14] 引入的线性模型连通性概念密切相关。这个概念确定了在两个独立网络的权重之间进行线性插值时保持准确性的条件。神经网络权重的插值在各种场景中保持高准确性，沿着共享的优化轨迹 [7, 12, 14, 22, 80, 81]。类似地，Neyshabur 等人 [43] 证明了预训练模型与全新初始化模型之间存在的连接。他们指出，来自预训练模型的解决方案之间没有性能障碍，但不同随机初始化模型的解决方案之间可能存在障碍。Mizradeh 等人 [41] 在多任务学习和 CL 的背景下研究了线性连通性。他们表明，存在一个线性路径解决方案，连接两个分别在任务“A”和“B”上训练的模型：一个在任务 A 上表现出色，另一个在任务 A 和 B 上进行了微调。这些工作为线性插值在模型性能中的有效性提供了坚实的理论和实证基础。基于这些发现，我们的工作提出了一个针对 CL 的新解决方案，这是一个尚未探索的领域。</p>
<h2 id="3-方法">3. 方法<a class="anchor-link" href="#3-方法" title="Permanent link">&para;</a></h2>
<h3 id="31-问题表述与概述">3.1 问题表述与概述<a class="anchor-link" href="#31-问题表述与概述" title="Permanent link">&para;</a></h3>
<h4 id="311-基于预训练模型的持续学习">3.1.1 基于预训练模型的持续学习<a class="anchor-link" href="#311-基于预训练模型的持续学习" title="Permanent link">&para;</a></h4>
<p>我们考虑一个分类模型 <span class="math-inline">M_{\varphi}(\cdot)=h(f(\cdot))</span>，其中 <span class="math-inline">f(\cdot)</span> 是特征提取器，<span class="math-inline">h(\cdot)</span> 是分类头，两者都由统一的参数集 <span class="math-inline">\varphi</span> 参数化。特征提取器 <span class="math-inline">f_{\boldsymbol{\theta}}</span> 使用 PTM 的参数 <span class="math-inline">\boldsymbol{\theta}<em>{0}</span> 进行初始化，然后在一系列增量任务上顺序训练，每个任务由相应的训练集 <span class="math-inline">\mathcal{D}</em>{t}</span> 表示，其中 <span class="math-inline">t\in{1,\ldots,T}</span>。主要目标是在这些任务的相关测试集上实现鲁棒性能。具体来说，对于每个任务 <span class="math-inline">t</span>，数据集 <span class="math-inline">\mathcal{D}<em>{t}</span> 定义为 <span class="math-inline">\mathcal{D}</em>{t}=\bigcup_{c\in C_{t}}\mathcal{D}<em>{t}^{c}</span>，其中 <span class="math-inline">\mathcal{D}</em>{t}^{c}=(\mathbf{x}<em>{n}^{c},\boldsymbol{y}</em>{n}^{c})<em>{n=1}^{N</em>{c}}</span>，<span class="math-inline">C_{t}</span> 表示任务 <span class="math-inline">t</span> 中引入的新类别集合。这里，<span class="math-inline">N_{c}</span> 表示每个类别 <span class="math-inline">c</span> 的训练实例数量，<span class="math-inline">(\mathbf{x}<em>{n}^{c},\boldsymbol{y}</em>{n}^{c})</span> 表示第 <span class="math-inline">n</span> 个训练实例及其对应的标签。在类增量学习（Class-Incremental Learning, CIL）中，评估是在所有观察到的类别上进行的，而不需要任务索引标签 [69]。</p>
<p>这个问题提出了两个主要挑战：(i) 需要将从 PTM 中获得的知识适应新任务；(ii) 在吸收新任务的同时，保持模型的全面学习能力以避免遗忘先前获得的知识。</p>
<p><strong>概述</strong>。在这项工作中，我们提出<strong>模型平均</strong>作为基于 PTM 的 CIL 的有效解决方案。由于在新任务上微调 PTM 会导致权重偏离原始 PTM 和先前任务的配置，模型平均通过平均先前任务和当前任务的模型权重来避免对先前任务的遗忘，并保持 PTM 的泛化能力。每个任务结束时，使用平均模型对所有已见任务进行推理，并将其作为下一个任务的初始化。我们将这种方法称为<strong>持续模型平均</strong>（CoMA），并将在第 3.2 节中详细讨论。</p>
<p>我们通过引入<strong>持续 Fisher 加权模型平均</strong>（CoFiMA）扩展了 CoMA，其中我们在任何任务 <span class="math-inline">t</span> 上基于由 Fisher 信息 [9, 62] 确定的额外加权系数对两个模型进行平均。给定模型参数的 Fisher 信息决定了该参数对任务的重要性。该过程在第 3.3 节中详细说明。</p>
<h3 id="32-持续模型平均coma">3.2 持续模型平均（CoMA）<a class="anchor-link" href="#32-持续模型平均coma" title="Permanent link">&para;</a></h3>
<p>暂时忽略 CIL 中典型的内存限制，提出了一种通过模型集成的实用方法：从预训练模型开始，依次为每个任务训练单独的模型，并在每个任务后保存模型。假设我们已经在一系列 <span class="math-inline">T</span> 个任务上进行了训练：这种方法将产生 <span class="math-inline">T</span> 个不同的网络，每个网络都有自己的一组参数，记为 <span class="math-inline">\boldsymbol{\theta}<em>{1},\ldots,\boldsymbol{\theta}</em>{T}</span>。目标是创建一个具有参数 <span class="math-inline">\boldsymbol{\theta}</span> 的复合神经网络，确保在所有任务上具有良好的性能。</p>
<p>我们将复合参数 <span class="math-inline">\boldsymbol{\theta}</span> 的后验建模为各向同性高斯分布 <span class="math-inline">\boldsymbol{\theta}\sim\mathcal{N}(\boldsymbol{\theta}<em>{t},\mathbf{I})</span>，其中 <span class="math-inline">\mathbf{I}</span> 是单位矩阵 [37, 81]。我们将模型 <span class="math-inline">\boldsymbol{\theta}</em>{1},\ldots,\boldsymbol{\theta}<em>{T}</span> 视为复合模型 <span class="math-inline">\boldsymbol{\theta}</span> 的独立观测值，并最大化 <span class="math-inline">\boldsymbol{\theta}</span> 在所有 <span class="math-inline">T</span> 个任务上的后验分布的对数似然，得到以下优化问题：<br />
<div class="math-display"><br />
    \boldsymbol{\theta}^{<em>}=\arg\max_{\boldsymbol{\theta}}\frac{1}{T}\sum_{t=1}^{ T}\log p(\boldsymbol{\theta}|\boldsymbol{\theta}_{t}). \tag{1}<br />
</div><br />
这个优化问题的解是模型参数的简单平均 [37, 39]。稍微滥用符号，我们用求和运算符表示跨集合的元素求和，可以写成：<br />
<div class="math-display"><br />
    \boldsymbol{\theta}^{</em>}=\frac{1}{T}\sum</em>{t=1}^{T}\boldsymbol{\theta}_{t}. \tag{2}<br />
</div><br />
除了得到广泛支持的似然最大化框架的支持外，如式 (2) 所示的模型平均还得到了 Mirzadeh 等人 [41] 的见解的进一步验证。他们的研究表明，当两个模型分别在两个不同的任务上训练时，一个在两个任务上都表现出色的模型通常存在于它们的参数空间的线性插值中。下面，我们详细说明如何将这种模型平均方法适应于 CIL 的约束条件，即任务数据按顺序到达，并且不存储先前数据。</p>
<p>首先，从相同的预训练模型 <span class="math-inline">\boldsymbol{\theta}<em>{0}</span> 开始每个任务的训练可能会导致收敛到参数空间的不同区域 [14, 53]。这种情况如图 2(a) 所示。如果学习到的参数相距太远，后验分布的高斯假设不再有效，平均网络可能会导致高损失区域和聚合模型的性能下降（见第 4.3 节）。因此，对于每个任务 <span class="math-inline">t</span>，我们从 <span class="math-inline">\boldsymbol{\theta}^{*}</em>{t-1}</span> 开始微调，而不是从初始预训练模型 <span class="math-inline">\boldsymbol{\theta}_{0}</span> 开始。这种变化限制了达到遥远参数区域的风险。</p>
<p>其次，我们的方法的目标之一是防止模型数量随 <span class="math-inline">t</span> 线性增长。因此，我们从同时平均所有模型 <span class="math-inline">\boldsymbol{\theta}<em>{1},\ldots,\boldsymbol{\theta}</em>{T}</span> 转变为在每个任务 <span class="math-inline">t</span> 上迭代计算平均 <span class="math-inline">\boldsymbol{\theta}^{<em>}_{t}</span>：<br />
<div class="math-display"><br />
    \boldsymbol{\theta}^{</em>}<em>{t}=\lambda\boldsymbol{\theta}</em>{t}+(1-\lambda)\boldsymbol {\theta}^{<em>}_{t-1}, \tag{3}<br />
</div><br />
其中 <span class="math-inline">\lambda\in[0,1]</span>。为了初始化这个递归，我们从 <span class="math-inline">t!=!1</span> 开始，<span class="math-inline">\boldsymbol{\theta}^{</em>}<em>{1}</span> 设置为在第一个任务结束时获得的模型参数。注意，当 <span class="math-inline">\lambda=\frac{1}{t}</span> 时，式 (3) 严格等同于式 (2) 中的平均（见补充材料），与最大似然解一致，并符合我们最初对各向同性高斯后验的假设。然而，我们的实验表明，这种参数选择可能会导致次优性能，并且为每个模型分配可变权重是有利的。这种现象可能归因于在所有任务特定模型之间遇到高损失值区域的可能性（见图 2(b)）。因此，我们建议执行非均匀平均，给予最新任务更高的权重，如图 2(c) 所示。这是通过使用恒定权重参数 <span class="math-inline">\lambda</span> 实现的。给予最新任务更多权重的动机是，早期模型仅在最初几个任务上训练，而最后一个模型通过顺序微调编码了旧任务和最近任务的知识（见补充材料）。通过这种方式，预计在这条轨迹上会存在更好的权衡，并且两个端点 <span class="math-inline">\bm{\theta}</em>{t-1}</span> 和 <span class="math-inline">\bm{\theta}_{t}</span> 会平滑连接，而不会出现显著的损失障碍或性能下降 [14]。</p>
<p>在内存方面，与在所有任务上进行简单的顺序微调相比，我们的方法在每个训练阶段的存储开销仅限于单个模型的大小。然而，在过渡到后续任务时，只需要存储 <span class="math-inline">\bm{\theta}^{*}_{t}</span>。</p>
<p><strong>处理分类器参数</strong>。在每个新任务 <span class="math-inline">t</span> 中，存在一些独特的参数（特别是与新类别相关的新头部参数），这些参数在之前的模型中不存在，因此不参与平均。为了适应这一点，我们将平均过程（如式 (3) 所示）限制在跨模型共有的参数上（包括骨干参数和头部参数的共享部分），同时排除新头部权重（与新类别相关）的平均。</p>
<h3 id="33-持续-fisher-加权模型平均cofima">3.3 持续 Fisher 加权模型平均（CoFiMA）<a class="anchor-link" href="#33-持续-fisher-加权模型平均cofima" title="Permanent link">&para;</a></h3>
<p>均匀权重平均隐含地假设模型的所有参数对训练任务 <span class="math-inline">t</span> 具有相同的重要性。虽然有效，但为所有参数赋予相同的重要性可能会导致权重集成模型位于高损失的误差盆地中 [80]。为了缓解这个问题，我们旨在根据每个权重参数对给定任务的重要性选择性地集成权重。受弹性权重巩固（Elastic Weight Consolidation, EWC）[26] 和 Matena 等人 [37] 最近工作的启发，我们利用 Fisher 信息 [9, 62] 在模型平均过程中对模型参数进行加权。Fisher 信息固有地捕捉了每个权重参数在模型训练的数据集（或任务）上的重要性。我们将这种 CoMA 变体称为<strong>持续 Fisher 加权模型平均</strong>（<strong>CoFiMA</strong>），如图 1(c) 所示。CoFiMA 仅存储先前任务的 Fisher 值，从而使 Fisher 加权平均与 CL 约束兼容。CoFiMA 保持了计算效率，仅需在当前任务数据上进行一次前向和反向传播以估计 Fisher 值 [26, 48]，并在多个标准 CL 基准测试中超越了 CoMA 和现有的基于 PTM 的 CL 解决方案，达到了最先进的性能。</p>
<p>我们的<strong>贡献</strong>总结如下：</p>
<ul>
<li>我们<strong>在鲁棒微调和持续学习之间建立了联系</strong>，并表明模型平均是解决<strong>基于 PTM 的 CL 问题</strong>的简单而有效的解决方案。我们提出了 CoMA，一种基于权重集成的 CL 方法，解决了稳定性与可塑性权衡的挑战性任务。</li>
<li>我们通过使用 Fisher 信息自适应地加权先前任务和当前任务模型的参数，扩展了 CoMA 到 CoFiMA。</li>
<li>我们在多个标准 CL 基准测试上进行了广泛的实验，并证明 CoFiMA 虽然简单，但始终优于基于 PTM 的 CL 解决方案。</li>
</ul>
<h2 id="4-实验">4. 实验<a class="anchor-link" href="#4-实验" title="Permanent link">&para;</a></h2>
<p>在本节中，我们首先简要描述实验设置，然后展示实验结果。</p>
<h3 id="41-实验设置">4.1 实验设置<a class="anchor-link" href="#41-实验设置" title="Permanent link">&para;</a></h3>
<p><strong>数据集和设置</strong>。我们使用 PILOT[65] 框架进行实验。我们在四个 CIL 基准测试上进行了实验：CIFAR-100[30]、ImageNet-R[19]、CUB-200[72] 和 Cars-196[28]。CIFAR-100 数据集 [30] 包含 100 类自然图像，每类有 500 张训练图像。ImageNet-R 数据集 [19] 包含 200 类图像，分为 24,000 张训练图像和 6,000 张测试图像。这些图像虽然与 ImageNet-21K 相关，但由于它们是 ImageNet 中的困难样本或不同风格的新图像，因此对 PTM 具有挑战性。CUB-200 数据集 [72] 包含 200 类鸟类图像，每类大约 60 张图像，一半用于训练，一半用于测试。Cars-196 数据集 [28] 包含 196 类汽车图像，分为 8,144 张训练图像和 8,040 张测试图像，保持相似的类别比例。前两个数据集专注于细粒度分类，而后两个数据集（即 CIFAR-100 和 ImageNet-R）是 CL 的标准基准测试。根据 SLCA[86]，我们将每个基准测试分为 10 个任务。我们在类增量设置中报告结果，即在推理时不知道任务 ID。</p>
<p><strong>指标</strong>。我们报告在学习每个增量任务后所有已见类别的平均分类准确率（记为_Inc-Acc_（%））和学习最后一个任务后的准确率（记为_Last-Acc_（%））。</p>
<p><strong>基线和竞争对手</strong>。我们与最先进的基于 PTM 的 CIL 方法 L2P[79]、DualPrompt[78]、SLCA[86] 和 RanPAC[38] 进行了比较。我们还使用相同的 PTM 作为经典 CL 方法 GDumb[50]、LvPF[33]、DER[4]、BIC[83] 和 EWC[26] 的初始化。此外，我们报告了以下基线：顺序微调模型（记为 Seq FT）和 Prototype-classifier[23]，后者是基于 PTM 提取特征的余弦相似度分类器。Joint-Training 是一个上限，其中模型同时在所有任务上进行训练。</p>
<p><strong>实现细节</strong>。我们在实验中采用了两种 PTM：在 ImageNet-21K[55] 上监督预训练的 ViT-B/16[27] 骨干，默认 PTM；以及在 ImageNet-1K 上使用 MoCo-V3[5] 进行自监督预训练的 ViT-B/16 骨干。我们遵循 SLCA[86] 的实现，采用 0.0001 的小学习率用于表示层，0.01 的学习率用于分类层，以及类对齐策略。我们在所有实验中将批量大小设置为 128，<span class="math-inline">\lambda!=!0.4</span> 用于监督预训练，<span class="math-inline">\lambda!=!0.2</span> 用于自监督预训练。</p>
<h3 id="42-最先进方法的比较">4.2 最先进方法的比较<a class="anchor-link" href="#42-最先进方法的比较" title="Permanent link">&para;</a></h3>
<p>本节分析了 CoMA 和 CoFiMA 在各种 CL 基准测试中的性能。我们在表 1 和表 2 中报告了 CoMA 和 CoFiMA 与使用 ViT-B/16 骨干的最先进 CL 方法的比较，该骨干分别在监督和自监督预训练下进行。</p>
<p>如表 1 所示，我们提出的 CoMA 在所有基准测试中始终优于表现最佳的 CIL 基线 SLCA[86] 和 RanPAC[38]。这证实了模型平均在 CIL 中的优势。此外，CoFiMA 作为 CoMA 的改进版本，进一步提高了性能，在 CIL 中达到了新的最先进结果。这突显了基于参数重要性进行自适应模型平均的必要性。具体来说，CoFiMA 在 CUB-200 上实现了 87.11% 的 Last-Acc 和 91.87% 的 Inc-Acc，超过了 SLCA 的表现，分别提高了 +<strong>2.4%</strong>和 +<strong>0.93%</strong>。同样，在 Cars-196 上，CoFiMA 在 Last-Acc 和 Inc-Acc 上分别比 SLCA 提高了 +<strong>9.23%</strong>和 +<strong>5.72%</strong>。在 Imagenet-R 上，CoFiMA 在 Last-Acc 和 Inc-Acc 上分别比 SLCA 提高了 +<strong>1.25%</strong>和 +<strong>0.31%</strong>。</p>
<p>从表 2 中我们观察到，CoMA 和 CoFiMA 在使用自监督预训练的 PTM 时，均大幅超越了最先进的方法。结果表明，访问自监督 PTM 足以在 CIL 中达到令人满意的性能，尽管与表 1 相比，所有方法的绝对性能较低。具体来说，在 CUB-200 上，CoFiMA 的 Last-Acc 为 77.65%，Inc-Acc 为 83.54%，继续显示出相对于 SLCA 的优势。CoFiMA 在 Cars-196、CIFAR-100 和 ImageNet-R 上也处于领先地位，展示了其在不同数据集上的一致性能。</p>
<p>值得注意的是，CoFiMA 的性能在表 1 和表 2 中均接近联合训练的基线。例如，在表 1 中，CIFAR-100 的 CoFiMA 的 Last-Acc 仅比联合训练的基线 93.22% 低<strong>0.45%</strong>。在其他基准测试中，这一差距进一步缩小，表明 CoFiMA 在接近 CL 性能上限方面的有效性。CoFiMA 通过有效平衡旧知识的保留与新信息的获取，在使用监督和自监督 PTM 时均表现出强大的性能。</p>
<h3 id="43-消融研究">4.3 消融研究<a class="anchor-link" href="#43-消融研究" title="Permanent link">&para;</a></h3>
<h4 id="431-模型平均的分析">4.3.1 模型平均的分析<a class="anchor-link" href="#431-模型平均的分析" title="Permanent link">&para;</a></h4>
<p>本节评估了我们的持续模型平均方法与两个基线的比较：</p>
<ul>
<li><strong>权重集成</strong>，它均匀地平均模型权重（例如 <span class="math-inline">\lambda</span>=1/<span class="math-inline">t</span>），从预训练的 PTM（<span class="math-inline">\bm{\theta}<em>{0}</span>）或先前任务的参数（<span class="math-inline">\bm{\theta}</em>{t-1}</span>）初始化每个模型 <span class="math-inline">M_{t}</span>。</li>
<li><strong>指数移动平均（EMA）</strong>[67]，一种在每次梯度下降迭代 <span class="math-inline">\bm{m}</span> 时计算模型参数的运行平均的技术，如下所示：<span class="math-inline">\bm{\theta}<em>{m}=\beta\bm{\theta}</em>{m}+(1-\beta)\bm{\theta}_{m-1}</span>，其中 <span class="math-inline">\beta=0.999</span>。</li>
</ul>
<p>表 3 展示了结果。CoFiMA 在所有数据集上表现出优越的性能。从 <span class="math-inline">\bm{\theta}<em>{t-1}</span> 初始化的<strong>权重集成</strong>在 CIFAR-100（Last-Acc: 91.69%）上表现出竞争力，但在 Cars-196（Last-Acc: 71.82%）等数据集上表现更好。从 <span class="math-inline">\bm{\theta}</em>{0}</span> 开始的<strong>权重集成</strong>表现较低。这表明重新初始化到 <span class="math-inline">\bm{\theta}_{0}</span> 的局限性；由于模型在不同任务上训练，它会导致不同的最优解，如图 2(a) 所示。</p>
<p>然而，从 <span class="math-inline">\bm{\theta}<em>{t-1}</span> 初始化的<strong>权重集成</strong>优于从 <span class="math-inline">\bm{\theta}</em>{0}</span> 开始的变体，强调了初始化的重要性。从 <span class="math-inline">\bm{\theta}<em>{t-1}^{*}</span> 初始化 <span class="math-inline">\bm{\theta}</em>{t}</span> 可以提高任务 <span class="math-inline">t</span> 和 <span class="math-inline">t-1</span> 的性能，正如 [81, 21] 中所做的那样。这种性能提升表明，对连续模型进行平均可以在当前任务 <span class="math-inline">t</span> 上获得良好的性能，同时保留先前的知识。</p>
<p>对所有训练到任务 <span class="math-inline">t</span> 的模型进行平均是次优的，因为任务 <span class="math-inline">t</span> 的权重可能与任务 <span class="math-inline">t=1</span> 的权重显著不同。这种平均会将 <span class="math-inline">\bm{\theta}_{t}^{<em>}</span> 的值推向次优的最小值，导致性能下降（参见图 2(b)）。相比之下，CoFiMA 通过仅与前一个参数 <span class="math-inline">\bm{\theta}_{t-1}^{</em>}</span> 进行平均来避免这种情况，从而获得更好的性能。</p>
<p><strong>EMA</strong> 方法虽然在大多数情况下优于两种<strong>权重集成</strong>变体，但仍不及 CoFiMA 的性能。这种差异可能源于 <strong>EMA</strong> 中过多的平均，其中 <span class="math-inline">\bm{\theta}_{t}^{*}</span> 在同一任务内被多次修改，可能导致次优结果。不同的是，我们的方法仅在完成每个任务后应用权重平均，通过选择性地关注相关模型权重并保留先前的知识来提高计算效率。</p>
<h4 id="432-ptm-的影响">4.3.2 PTM 的影响<a class="anchor-link" href="#432-ptm-的影响" title="Permanent link">&para;</a></h4>
<p>在本节中，我们评估了 CoFiMA 方法在各种骨干架构上的性能，包括自监督（MAE [17]、MoCoV3 [5] 和 DINOv2 [45]）和监督（ViT-Tiny [27] 和 ViT-B/16-SAM [13]）模型。这一综合分析旨在确定 CoFiMA 在不同训练范式中的适应性和性能一致性。结果如图 3 所示。</p>
<p>我们的结果表明，CoFiMA 在几乎所有测试的骨干上相对于基线 SLCA 方法都提高了性能。例如，使用 ViT-Tiny 骨干时，CoFiMA 将 SLCA 的 Last-Accuracy 从 80.25% 提高到 82.96%。这种趋势在 ViT-Large 上也类似，CoFiMA 达到了 86.81% 的准确率，超过了 SLCA 的 85.93%。只有在 ViT-B/16-DINOv2 的情况下，性能略有下降，这可能是由于基准测试达到了饱和点：SLCA 和 CoFiMA 的表现都接近联合学习的黄金标准。</p>
<p>在自监督学习模型的背景下，CoFiMA 通过在 ViT-B/16-MAE 和 ViT-B/16-MoCoV3 骨干上优于 SLCA 展示了其有效性。重要的是，ViT-B/16-SAM 骨干 [13] 在评估模型中表现最佳。这可以归因于 SAM 骨干中固有的有效泛化特征，这是使用 SAM 优化器训练的结果。该优化器以其增强模型泛化能力而闻名，这反映在我们的实验中观察到的优越性能指标中，正如 Mehta 等人 [40] 的工作中所提到的。我们还注意到，自监督预训练通常会导致持续学习基线与联合训练之间的性能差距更大。特别是对于 ViT-B/16-MAE，正如 Zhang 等人 [86] 的工作中所指出的，因为使用 MAE 进行联合训练需要较小的更新来学习所有任务，而使用 SLCA 或 CoFiMA 进行增量学习则需要更大的更新。</p>
<p>与联合训练相比，联合训练在 CL 设置中始终处于领先地位，正如 ViT-Large（94.45%）和 ViT-B/16-SAM（91.87%）所看到的那样，CoFiMA 仍然接近，特别是在 ViT-B/16-SAM 上，它达到了 90.48%。这些结果表明，CoFiMA 是一种多功能的方法，在监督和自监督学习背景下，使用各种骨干都能有效提高性能。然而，我们方法的性能提升因骨干（大小）及其预训练范式的选择而异。</p>
<h4 id="433-平衡旧任务和新任务的信息">4.3.3 平衡旧任务和新任务的信息<a class="anchor-link" href="#433-平衡旧任务和新任务的信息" title="Permanent link">&para;</a></h4>
<p>在 CL 中，一个主要目标是平衡从先前任务中保留的知识与从当前任务中获取的新信息。本研究考察了在平均过程中用于平衡模型稳定性/可塑性的 <span class="math-inline">\lambda</span> 的影响。</p>
<p>除了我们的方法外，我们还纳入了 WiSE-FT [81] 的聚合方案的适应版本，我们称之为 <strong>WiSE-FT-CL</strong>。在学习任务 <span class="math-inline">t</span> 后，通过线性插值在预训练模型和微调模型之间实现稳定性：<span class="math-inline">\boldsymbol{\theta}^{<em>}<em>{t}=\lambda\boldsymbol{\theta}</em>{t}+(1-\lambda)\boldsymbol {\theta}_{0}</span>。对于每个新任务 <span class="math-inline">t</span>，微调从 <span class="math-inline">\boldsymbol{\theta}^{</em>}_{t-1}</span> 开始。这种方法与 WiSE-FT [81] 一致，仅依赖于初始预训练来实现稳定性。</p>
<p>根据图 4，CoFiMA 在各种 <span class="math-inline">\lambda</span> 设置下均表现出优于 WiSE-FT-CL 的性能。这种改进表明，我们的方法在保留旧信息的同时更有效地整合了新知识。最佳性能在 <span class="math-inline">\lambda=0.3</span> 时实现，这给出了学习新任务 <span class="math-inline">t</span>（即可塑性）和保留先前任务知识（即稳定性）之间的最佳权衡。</p>
<p>WiSE-FT-CL 表现不佳的原因在于与模型 <span class="math-inline">\bm{\theta}<em>{0}</span> 的平均影响，由于微调，<span class="math-inline">\bm{\theta}</em>{0}</span> 的参数值与 <span class="math-inline">\bm{\theta}_{t}</span> 显著不同。因此，这种平均过程导致次优的参数值，如第 3 节所述。对于 CoFiMA，较高的 <span class="math-inline">\lambda</span> 值更强调任务 <span class="math-inline">t</span> 的当前模型，从而导致对先前任务的遗忘。</p>
<p>总之，我们的方法 CoFiMA 通过利用连续模型之间的权重平均，有效地保持了保留旧任务信息和适应新任务数据之间的平衡。这里的关键好处是，任务 <span class="math-inline">t</span> 和 <span class="math-inline">t-1</span> 的模型参数值不会显著偏离，确保平均参数对两个任务都有效 [6, 21]（更多细节见补充材料）。</p>
<h2 id="5-结论">5. 结论<a class="anchor-link" href="#5-结论" title="Permanent link">&para;</a></h2>
<p>在这项工作中，我们提出了 CoFiMA，这是第一个基于权重平均技术来解决 CIL 设置中灾难性遗忘的方法。该方法基于两个支柱：首先，它利用模型平均，提供了一种平衡机制，既能保留先前的知识，又能适应新信息。其次，它结合了 Fisher 信息，智能地对参数的平均进行加权。这种改进允许根据每个参数的 Fisher 信息确定其重要性来调整其值，从而有效减少灾难性遗忘。</p>
<p>我们在各种 PTM 骨干和基准数据集上的基准测试表明，CoFiMA 始终优于最先进的 CIL 方法。我们的研究结果强调了 CoFiMA 在减轻遗忘方面的有效性，并突出了其在不同 PTM 骨干和基准数据集上的多功能性。</p>
                </div>

                <!-- Comments Section (Giscus) -->
                <section class="comments-section">
                    <h3><i class="fas fa-comments"></i> 评论</h3>
                    <script src="https://giscus.app/client.js"
                        data-repo="Geeks-Z/Geeks-Z.github.io"
                        data-repo-id=""
                        data-category="Announcements"
                        data-category-id=""
                        data-mapping="pathname"
                        data-strict="0"
                        data-reactions-enabled="1"
                        data-emit-metadata="0"
                        data-input-position="bottom"
                        data-theme="preferred_color_scheme"
                        data-lang="zh-CN"
                        crossorigin="anonymous"
                        async>
                    </script>
                </section>
            </article>

            <footer class="post-footer">
                <p>© 2025 Hongwei Zhao. Built with ❤️</p>
            </footer>
        </main>
    </div>

    <!-- Theme Toggle Button -->
    <button class="theme-toggle" id="themeToggle" aria-label="切换主题">
        <i class="fas fa-moon"></i>
    </button>

    <script>
        // Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        const html = document.documentElement;
        const icon = themeToggle.querySelector('i');
        const hljsDark = document.getElementById('hljs-theme-dark');
        const hljsLight = document.getElementById('hljs-theme-light');
        
        // Check saved theme or system preference
        const savedTheme = localStorage.getItem('theme');
        const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
        
        if (savedTheme === 'dark' || (!savedTheme && prefersDark)) {
            html.setAttribute('data-theme', 'dark');
            icon.className = 'fas fa-sun';
            hljsDark.disabled = false;
            hljsLight.disabled = true;
        }
        
        themeToggle.addEventListener('click', () => {
            const isDark = html.getAttribute('data-theme') === 'dark';
            if (isDark) {
                html.removeAttribute('data-theme');
                icon.className = 'fas fa-moon';
                localStorage.setItem('theme', 'light');
                hljsDark.disabled = true;
                hljsLight.disabled = false;
            } else {
                html.setAttribute('data-theme', 'dark');
                icon.className = 'fas fa-sun';
                localStorage.setItem('theme', 'dark');
                hljsDark.disabled = false;
                hljsLight.disabled = true;
            }
            
            // Update Giscus theme
            const giscusFrame = document.querySelector('iframe.giscus-frame');
            if (giscusFrame) {
                giscusFrame.contentWindow.postMessage({
                    giscus: {
                        setConfig: {
                            theme: isDark ? 'light' : 'dark'
                        }
                    }
                }, 'https://giscus.app');
            }
        });
        
        // Highlight.js
        document.addEventListener('DOMContentLoaded', () => {
            hljs.highlightAll();
        });
        
        // KaTeX auto-render
        document.addEventListener('DOMContentLoaded', () => {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
        
        // TOC active state
        const tocLinks = document.querySelectorAll('.toc-container a');
        const headings = document.querySelectorAll('.post-content h2, .post-content h3, .post-content h4');
        
        function updateTocActive() {
            let current = '';
            headings.forEach(heading => {
                const rect = heading.getBoundingClientRect();
                if (rect.top <= 100) {
                    current = heading.getAttribute('id');
                }
            });
            
            tocLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === '#' + current) {
                    link.classList.add('active');
                }
            });
        }
        
        window.addEventListener('scroll', updateTocActive);
        updateTocActive();
    </script>
</body>
</html>
