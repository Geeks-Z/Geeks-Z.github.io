<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Untitled</title>
    <meta name="description" content="Untitled - Hongwei Zhao's Blog">
    <meta name="author" content="Hongwei Zhao">
    
    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <!-- Code Highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css" id="hljs-theme-dark">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css" id="hljs-theme-light" disabled>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    
    <style>
        :root {
            /* Light Theme */
            --primary-color: #2980b9;
            --primary-hover: #1a5276;
            --link-color: #c0392b;
            --text-color: #333;
            --text-light: #666;
            --text-muted: #999;
            --bg-color: #fff;
            --bg-secondary: #f8f9fa;
            --bg-code: #f5f5f5;
            --border-color: #e5e7eb;
            --shadow: 0 1px 3px rgba(0,0,0,0.1);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.1);
        }

        [data-theme="dark"] {
            --primary-color: #5dade2;
            --primary-hover: #85c1e9;
            --link-color: #e74c3c;
            --text-color: #e5e7eb;
            --text-light: #9ca3af;
            --text-muted: #6b7280;
            --bg-color: #1a1a2e;
            --bg-secondary: #16213e;
            --bg-code: #0f0f23;
            --border-color: #374151;
            --shadow: 0 1px 3px rgba(0,0,0,0.3);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.4);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 16px;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            transition: background-color 0.3s, color 0.3s;
        }

        a {
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--primary-hover);
            text-decoration: underline;
        }

        /* Layout */
        .page-wrapper {
            display: flex;
            max-width: 1400px;
            margin: 0 auto;
            padding: 2rem;
            gap: 3rem;
        }

        /* Sidebar TOC */
        .toc-sidebar {
            width: 260px;
            flex-shrink: 0;
            position: sticky;
            top: 2rem;
            height: fit-content;
            max-height: calc(100vh - 4rem);
            overflow-y: auto;
        }

        .toc-container {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 1.5rem;
            border: 1px solid var(--border-color);
        }

        .toc-container h3 {
            font-size: 0.85rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-muted);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .toc-container ul {
            list-style: none;
        }

        .toc-container li {
            margin-bottom: 0.5rem;
        }

        .toc-container a {
            font-size: 0.9rem;
            color: var(--text-light);
            display: block;
            padding: 0.25rem 0;
            border-left: 2px solid transparent;
            padding-left: 0.75rem;
            transition: all 0.2s;
        }

        .toc-container a:hover,
        .toc-container a.active {
            color: var(--primary-color);
            border-left-color: var(--primary-color);
            text-decoration: none;
        }

        .toc-container ul ul {
            margin-left: 1rem;
        }

        /* Main Content */
        .main-content {
            flex: 1;
            min-width: 0;
            max-width: 800px;
        }

        /* Header */
        .post-header {
            margin-bottom: 2rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
            margin-bottom: 1.5rem;
        }

        .back-link:hover {
            color: var(--primary-color);
            text-decoration: none;
        }

        .post-header h1 {
            font-size: 2.25rem;
            font-weight: 700;
            line-height: 1.3;
            margin-bottom: 1rem;
            color: var(--text-color);
        }

        .post-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
        }

        .post-meta span {
            display: flex;
            align-items: center;
            gap: 0.4rem;
        }

        .post-meta i {
            color: var(--text-muted);
        }

        .tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 1rem;
        }

        .tag {
            display: inline-block;
            font-size: 0.8rem;
            background: var(--bg-secondary);
            color: var(--text-light);
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            border: 1px solid var(--border-color);
            transition: all 0.2s;
        }

        .tag:hover {
            background: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }

        /* Article Content */
        .post-content {
            font-size: 1rem;
            line-height: 1.9;
        }

        .post-content h1,
        .post-content h2,
        .post-content h3,
        .post-content h4,
        .post-content h5,
        .post-content h6 {
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-weight: 600;
            line-height: 1.4;
            color: var(--text-color);
        }

        .post-content h1 { font-size: 1.875rem; }
        .post-content h2 { 
            font-size: 1.5rem; 
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }
        .post-content h3 { font-size: 1.25rem; }
        .post-content h4 { font-size: 1.125rem; }

        .post-content p {
            margin-bottom: 1.25rem;
        }

        .post-content ul,
        .post-content ol {
            margin: 1.25rem 0;
            padding-left: 1.5rem;
        }

        .post-content li {
            margin-bottom: 0.5rem;
        }

        .post-content blockquote {
            margin: 1.5rem 0;
            padding: 1rem 1.5rem;
            background: var(--bg-secondary);
            border-left: 4px solid var(--primary-color);
            border-radius: 0 8px 8px 0;
            color: var(--text-light);
            font-style: italic;
        }

        .post-content blockquote p:last-child {
            margin-bottom: 0;
        }

        /* Code */
        .post-content code {
            font-family: 'JetBrains Mono', 'Fira Code', Consolas, monospace;
            font-size: 0.9em;
            background: var(--bg-code);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            color: var(--link-color);
        }

        .post-content pre {
            margin: 1.5rem 0;
            padding: 1.25rem;
            background: var(--bg-code);
            border-radius: 10px;
            overflow-x: auto;
            border: 1px solid var(--border-color);
        }

        .post-content pre code {
            background: none;
            padding: 0;
            color: inherit;
            font-size: 0.875rem;
            line-height: 1.6;
        }

        /* Images */
        .post-content img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            margin: 1.5rem 0;
            box-shadow: var(--shadow-lg);
        }

        /* Tables */
        .post-content table {
            width: 100%;
            margin: 1.5rem 0;
            border-collapse: collapse;
            font-size: 0.95rem;
        }

        .post-content th,
        .post-content td {
            padding: 0.75rem 1rem;
            border: 1px solid var(--border-color);
            text-align: left;
        }

        .post-content th {
            background: var(--bg-secondary);
            font-weight: 600;
        }

        .post-content tr:nth-child(even) {
            background: var(--bg-secondary);
        }

        /* Math */
        .math-display {
            margin: 1.5rem 0;
            overflow-x: auto;
            padding: 1rem;
            background: var(--bg-secondary);
            border-radius: 8px;
        }

        /* Anchor links */
        .anchor-link {
            opacity: 0;
            margin-left: 0.5rem;
            color: var(--text-muted);
            font-weight: 400;
            transition: opacity 0.2s;
        }

        .post-content h2:hover .anchor-link,
        .post-content h3:hover .anchor-link,
        .post-content h4:hover .anchor-link {
            opacity: 1;
        }

        /* Theme Toggle */
        .theme-toggle {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            background: var(--primary-color);
            color: white;
            border: none;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.25rem;
            box-shadow: var(--shadow-lg);
            transition: transform 0.2s, background 0.2s;
            z-index: 1000;
        }

        .theme-toggle:hover {
            transform: scale(1.1);
            background: var(--primary-hover);
        }

        /* Comments Section */
        .comments-section {
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-color);
        }

        .comments-section h3 {
            font-size: 1.25rem;
            margin-bottom: 1.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        /* Footer */
        .post-footer {
            margin-top: 3rem;
            padding-top: 1.5rem;
            border-top: 1px solid var(--border-color);
            text-align: center;
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        /* Responsive */
        @media (max-width: 1024px) {
            .toc-sidebar {
                display: none;
            }
            
            .page-wrapper {
                padding: 1.5rem;
            }
        }

        @media (max-width: 768px) {
            .post-header h1 {
                font-size: 1.75rem;
            }
            
            .post-meta {
                flex-direction: column;
                gap: 0.5rem;
            }
            
            .theme-toggle {
                bottom: 1rem;
                right: 1rem;
                width: 44px;
                height: 44px;
            }
        }

        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-secondary);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--border-color);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--text-muted);
        }
    </style>
</head>
<body>
    <div class="page-wrapper">
        <!-- TOC Sidebar -->
        <aside class="toc-sidebar">
            <div class="toc-container">
                <h3><i class="fas fa-list"></i> 目录</h3>
                <div class="toc">
<ul>
<li><a href="#continual-learning-for-large-language-models-a-survey">Continual Learning for Large Language Models: A Survey</a></li>
</ul>
</div>

            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <article>
                <header class="post-header">
                    <a href="../../../../index.html" class="back-link">
                        <i class="fas fa-arrow-left"></i> 返回博客列表
                    </a>
                    <h1>Untitled</h1>
                    <div class="post-meta">
                        <span><i class="fas fa-calendar-alt"></i> 2026-01-28</span>
                        <span><i class="fas fa-folder"></i> AINotes/40.CIL/00.Survey</span>
                        <span><i class="fas fa-user"></i> Hongwei Zhao</span>
                    </div>
                    <div class="tags">
                        
                    </div>
                </header>

                <div class="post-content">
                    <h2 id="continual-learning-for-large-language-models-a-survey">Continual Learning for Large Language Models: A Survey<a class="anchor-link" href="#continual-learning-for-large-language-models-a-survey" title="Permanent link">&para;</a></h2>
<p>有效且高效地将静态预训练的大语言模型（LLM）适应不断变化的数据分布仍然是一个主要挑战。为特定需求定制的预训练 LLM 在先前知识领域中通常会出现显著的性能下降——这一现象被称为“灾难性遗忘”。虽然在持续学习（CL）社区中对此问题进行了广泛研究，但在 LLM 领域中这一问题呈现出新的表现形式。在本综述中，我们提供了当前在持续学习背景下对 LLM 的研究进展的全面概述和详细讨论。除了介绍基础知识外，本综述还分为四个主要部分：首先，我们描述了对 LLM 持续学习的概述，涵盖了两个连续性的方向：纵向连续性（或纵向持续学习），即从一般能力到具体能力的持续适应，以及横向连续性（或横向持续学习），即跨时间和领域的持续适应（第3节）。在纵向连续性方面，我们总结了现代 CL 背景下学习 LLM 的三个阶段：持续预训练（CPT）、领域自适应预训练（DAP）和持续微调（CFT）（第4节）。然后，我们概述了持续学习 LLM 的评估协议以及当前可用的数据源（第5节）。最后，我们讨论了关于 LLM 持续学习的一些有趣问题（第6节）。本综述揭示了在持续预训练、适应和微调大语言模型这一相对未被充分研究的领域中，需要更多的关注。需要立即关注的关键领域包括开发实用且易于访问的评估基准，以及专门设计的方法来应对遗忘并在不断发展的 LLM 学习范式中实现知识转移。完整的文献列表可在 https://github.com/Wang-ML-Lab/llm-continual-learning-survey 查看。</p>
<p><strong>CCS 概念</strong>：<br />
• 计算方法 → 终身机器学习；自然语言处理；神经网络。<br />
<strong>其他关键词和短语</strong>： 大语言模型，持续学习。</p>
<p><strong>1 引言</strong></p>
<p>近年来，大语言模型（LLM）在实现人工通用智能（AGI）方面表现出了相当大的潜力 [2, 8, 27, 51, 128, 224, 240, 294, 295]。研究人员观察到，随着参数规模的增加，诸如多步推理、少样本上下文学习和指令跟随等复杂能力得到了提高 [204, 317, 319, 320, 354]。LLM 的发展具有深远影响和革命性，促使机器学习从业者重新考虑一些曾经具有挑战性的任务（如问答系统、机器翻译和对话系统）的传统计算范式 [12, 64, 148]。然而，LLM 通常是在静态、预收集的通用领域数据集上训练的，随着时间的推移，其性能会逐渐下降 [7, 67, 122, 123, 131, 182]，并且在不同内容领域之间表现出不同 [46, 56, 90, 92, 131, 135, 237, 238, 282]。此外，单一预训练的大模型无法满足每个用户的需求，需进一步微调 [13, 21, 48, 137, 236, 322, 323, 323, 363, 385, 385]。虽然重新收集预训练数据并重新训练模型是一种潜在的解决方案，但这种方法在现实场景中是昂贵且不切实际的。</p>
<p>为了在最小化先前知识领域性能下降的情况下有效地适应下游任务，研究人员采用了持续学习的方法，也被称为终身学习或增量学习 [49, 230, 296, 302]。受人脑增量学习模式的启发 [55, 132, 177, 198, 199, 223, 226, 347]，持续学习涉及在一系列任务上顺序训练机器学习模型，并期望在所有任务上保持性能 [29, 74, 75, 81, 145, 166, 249, 367]。在整个训练过程中，模型只能有限或无法访问先前的数据，这在当前任务学习期间会导致无法保留过去的知识，因为未见先前数据的优化约束不存在 [29, 38, 100, 166, 180, 249, 270, 275]。这种挑战，被称为灾难性遗忘 [200]，自其出现以来一直是持续学习研究的中心焦点。多年来，研究人员探索了各种技术来减轻机器学习模型中的遗忘问题。这些技术包括基于重放的方法 [29, 38, 249, 263, 270]、参数正则化 [5, 145, 250, 279] 和模型架构扩展 [245, 301]。这些技术共同显著推进了在不同任务、模型架构和学习范式中实现零遗忘的目标。</p>
<p>在顺序训练和适应 LLM 的背景下，持续学习的重要性也正在发生语义上的转变。为了更好地突显这一持续转变，本综述首次将持续学习 LLM 分为需要从业者解决的两个连续性方向（第3节）：<br />
- 纵向连续性（或纵向持续学习），指的是 LLM 在从大规模通用领域向小规模特定领域过渡时的持续适应，涉及学习目标和执行实体的转变。例如，医疗机构可能会开发针对医学领域的 LLM，同时保留其为用户提供一般推理和问答能力的能力。<br />
- 横向连续性（或横向持续学习），指的是跨时间和领域的持续适应，通常涉及多个训练阶段，且容易遗忘。例如，社交媒体平台不断更新 LLM 以反映最新趋势，确保下游服务（如广告和推荐）的准确定位，不会影响现有用户的体验。</p>
<p>显式地分离纵向和横向持续学习不仅是现有持续学习类型（如领域增量学习）的简单修改，还提供了一个稳健的概念框架，用于分析和描述复杂的 LLM 持续学习范式。例如，循环调优旨在同时保留纵向和横向连续性 [237]，未来的设计可能包括在纵向和横向持续学习之间交替的“之字形”持续学习。</p>
<p>在图1中，沿着纵向连续性，我们勾勒出了现代持续学习 LLM 的三个关键阶段：持续预训练（CPT）、领域自适应预训练（DAP）和持续微调（CFT）（第4节）。在持续预训练阶段，现有研究主要研究了三种类型的分布转变：时间上的、内容层次上的和语言层次上的。每种转变都有不同的关注点和挑战。在领域自适应预训练阶段，虽然主要被视为为下游任务准备 LLM 的过程，但 CL 评估和技术经常被使用。然而，考虑到传统 CL 社区的成熟度，这些技术的多样性明显不足。在持续微调阶段，我们关注于新兴的 LLM 学习领域，涵盖诸如持续指令调优（CIT）、持续模型优化（CMR）、持续模型对齐（CMA）和持续多模态 LLM（CMLLMs）等主题。接下来，我们提供了当前公开可用的评估协议和基准（第5节）。最后，我们在第6节讨论了关于 LLM 持续学习的一些有趣问题，包括传统 CL 类型和内存限制在 LLM 持续学习背景下的角色变化，以及该主题的未来研究方向。</p>
<p>综上所述，本文详细介绍了现有 LLM 持续学习研究，显著区别于现有相关主题的文献 [22, 136, 302, 330, 353]。我们的综述突显了在持续预训练（CPT）和领域自适应预训练（DAP）领域持续发展的 LLM 这一未被充分研究的研究领域。我们强调需要社区更多关注，尤其是开发实用、易于访问和广泛认可的评估基准。此外，还需要设计专门的方法来应对不断出现的 LLM 学习范式中的遗忘问题。我们希望本综述能为快速变化的 LLM 持续学习领域提供系统且新颖的视角，帮助持续学习社区在更高效、可靠和可持续地发展 LLM 的目标上做出贡献 [10, 32, 122, 280, 341]。</p>
<p><strong>组织结构</strong>。本文的其余部分组织如下。我们首先在第</p>
<p>2节介绍大语言模型和持续学习的背景和基础知识。然后，我们在第3节展示了现代大语言模型持续学习的概述。在纵向上，我们将其粗略地分为三个持续训练 LLM 的阶段，并在第4节逐一介绍每个阶段。在第4.3节，我们将介绍 LLM 持续微调的独特方面，包括持续指令调优（第4.3.3节）、持续模型优化（第4.3.4节）、持续模型对齐（第4.3.5节）和持续多模态大语言模型（第4.3.6节）。在第5节，我们全面介绍了公开可用的 LLM 持续学习的评估协议和基准。最后，在第6节，我们对大语言模型时代持续学习的角色进行了一系列讨论，包括大规模持续 LLM 的新兴能力（第6.1节）、三种类型的持续学习（第6.2节）、LLM 持续学习中的内存角色（第6.3节）和未来的研究方向（第6.4节）。</p>
<p><strong>2 预备知识</strong></p>
<p>在本节中，我们将概述大语言模型（LLM）和持续学习（CL）的基本概念。我们首先介绍本文中使用的符号。随后，我们讨论 LLM 的预训练和下游适应，以及主流的 LLM 家族（第2.1节），接着介绍社区研究的基本持续学习技术（第2.2节）。</p>
<p><strong>符号</strong>。我们用小写字母表示标量，用小写粗体字母表示向量，用大写粗体字母表示矩阵。向量的 l2 范数和矩阵的 Frobenius 范数分别用 ∥·∥2 表示。对于向量 v = [v1, v2, ···, vn]⊤，∥v∥2 = (∑ni=1 v2i)1/2；对于矩阵 A ∈ Rm×n，∥A∥2 = (∑ij A2ij)1/2。我们用 𝜖D 和 LD 表示误差函数和损失函数，其中下标用于表示在数据分布 D 上取期望测量的误差/损失。我们进一步用 L̂S 表示在样本集 S 上评估的经验损失函数。概率和期望分别用 P 和 E 表示。我们用 [m] 表示从 1 到 m 的正整数集合，{1, ···, m}。</p>
<p><strong>2.1 大语言模型</strong></p>
<p>在过去的二十年里，神经语言建模已成为深度学习领域的主导，标志着显著和快速的进步。主要基于 Transformer 架构，预训练语言模型（PLM）如 BERT，通过在大规模未标注文本语料库上的广泛预训练，建立了一个通用的隐藏嵌入空间。遵循预训练和微调的范式，PLM 在经过少量任务特定数据的微调后，在各种自然语言处理任务中表现出色 [66, 178, 243]。研究表明，增加模型规模可以提高语言模型的能力 [109, 133]。通过将参数扩展到数十亿甚至数千亿，并在海量文本数据集上训练，PLM 不仅表现出卓越的语言理解和生成能力，还展现出诸如上下文学习、指令跟随和多步推理等新兴能力，这些能力在小规模语言模型如 BERT 中是不存在的 [204, 317, 319, 320, 354]。这些较大的模型通常被称为大语言模型（LLM）。</p>
<p><strong>2.1.1 大语言模型的预训练</strong>。预训练对于语言模型获取广泛的语言表示至关重要。解码器模型通常在预训练过程中采用概率语言建模（LM）任务。在此背景下，LM 特指自回归 LM。给定一系列标记 x = [x1, x2, ···, xN]，LM 根据所有先前的标记 x&lt;t = [x1, x2, ···, xt−1] 自回归地预测下一个标记 xt，并通过最小化负对数似然来训练整个网络：</p>
<p>LLM (x) ≜ −∑Nt=1 log P (xt | x&lt;t), (1)</p>
<p>其中 P (x1 | x&lt;1) ≜ P (x1) 是第一个标记的无条件概率估计。三大家族的解码器模型最受欢迎：GPT、PaLM 和 LLaMA。由 OpenAI 开发的 GPT 家族包括 GPT-2 [240]、GPT-3 [27]、ChatGPT [224] 和 GPT-4 [2]。值得注意的是，GPT-3 是第一个展现出新兴能力的 LLM，而这些能力在较小的 PLM 中并未出现。另一个值得注意的家族是由谷歌开发的 Gemini，其与 GPT 家族相当 [248, 288]。虽然 GPT 和 Gemini 家族都是闭源的，Meta 发布的 LLaMA 是目前最受欢迎的开源 LLM 家族 [294, 295]。这些模型的权重在非商业许可证下向研究界开放。</p>
<p>掩码语言建模（MLM）任务是编码器模型如 BERT 的常见预训练目标 [66, 178]。在 MLM 中，对于输入序列 x，将一部分输入标记 m(x) 掩码并用特殊的 [MASK] 标记替换。预训练目标是利用未掩码的部分 x\m(x) 预测掩码部分 m(x)。总之，MLM 的总体目标是最小化负对数似然：</p>
<p>LMLM (x) ≜ −∑x∈m(x) log P (x | x\m(x)) . (2)</p>
<p>一些编码器-解码器架构模型，如 T5 [243]，也使用序列到序列 MLM 任务作为预训练目标。他们将掩码句子作为编码器输入，并利用解码器顺序预测掩码标记。</p>
<p><strong>2.1.2 大语言模型的适应</strong>。预训练后，需要有效地适应 LLM 以更好地服务于下游任务。已经为特定目标提出了一系列适应方法。由于 LLM 在预训练期间主要关注生成语言连贯的文本，因此其性能可能并不一定符合人类用户的实际需求或符合人类的价值观、偏好和原则。此外，由于预训练数据的时效性问题，LLM 可能还会遇到知识截止或谬误问题。因此，提出了指令调优、模型优化和模型对齐来解决这些问题 [60, 225, 241, 372]。下面是 LLM 的三种适应任务的正式定义。</p>
<p><strong>定义 2.1（指令调优，IT）</strong>。设 h(x) 是一个以数据 x（通常由自然语言指令或查询组成）为输入的语言模型。指令调优（IT）是一种专门的训练方法，旨在增强模型准确有效地响应特定指令的能力。IT 的目标是通过使用从 IT 数据分布 DI 中抽取的一组训练示例 I = {(xi, yi)}Ni=1 来调整 h 的参数，其中 yi 表示 x 的期望输出。这个集合旨在针对需要提高性能的特定任务或功能。形式上，IT 旨在找到满足以下条件的最优优化假设 h*：</p>
<p>h* ≜ argmin h' E(x,y)∼DI [−log P (y | x, h')] ≈ argmin h' ∑Ni=1 −log P (yi | xi, h'). (3)</p>
<p>备注。模型对齐（MA）任务通常以与 IT 相同的问题定义形式出现，使用大小为 M 的对齐数据集 A = {(xa, ya, ya)}Ma=1，其中 ya 表示输入 xa 的模型原始决策，ya 表示符合指定伦理指南或期望结果的对齐决策。</p>
<p><strong>定义 2.2（模型优化，MR）</strong>。假设我们有一个以数据 x（例如自然语言查询）作为输入的模型 h(x)。考虑一个大小为 N 的编辑集 E = {(xe, ye, ye)}Ne=1，其中 ye 表示 xe 的真实标签，但模型对于 xe 错误地输出 ye。模型优化（MR）旨在有效地将模型从 h 更新为 h'，使其正确预测编辑集 E，同时保留 E 以外的原始输出。形式上，我们旨在找到满足以下条件的 h'：</p>
<p>h'(x0) = { y0 如果 (x0, y0) ∈ E, h(x0) 其他情况。 (4)</p>
<p><strong>2.2 持续学习</strong></p>
<p>人类逐渐积累跨任务的知识和技能，而在先前任务上的性能不会显著下降 [55, 132, 177, 198, 199, 223, 226, 347]。相反，机器学习模型通常以数据为中心，最小化后续任务上的训练损失会导致</p>
<p>模型在旧任务上失效，这一现象被称为“灾难性遗忘”。解决这一挑战是持续学习研究的焦点。如何在不遗忘的情况下高效地将模型适应一系列任务的问题在持续学习社区中进行了广泛研究 [49, 230, 296, 302]。这些研究通常在以下持续学习的内存约束下进行。</p>
<p><strong>定义 2.3（持续学习的内存约束）</strong>。假设 T 组观察集 {St ∼ Tt}Tt=1 按顺序到达，其中 {Tt}Tt=1 表示 T 个任务分布。在学习阶段 t &gt; 1 时，{Si}t−1i=1 的观察集是不可访问的（强约束）或部分可访问的（松散约束）。</p>
<p>备注。在 CL 的早期阶段，工作主要集中在强内存约束上 [5, 145, 166, 180]；随着研究领域的进展，更多的关注点放在将内存约束放松到用于重放的小缓冲区上 [29, 38, 247, 270]；一些现代 CL 工作完全抛弃了内存约束，但关注于计算预算 [31, 235, 297]。</p>
<p><strong>2.2.1 三种类型的持续学习</strong>。有三种突出的持续学习场景：任务增量学习（TIL）、领域增量学习（DIL）和类增量学习（CIL）。为了为后续讨论奠定基础（如表3和第6.2节所示），我们遵循 [144, 296, 302] 提出的概念框架，并为这三种持续学习场景提供正式定义。</p>
<p><strong>定义 2.4（任务增量学习，TIL）</strong>。假设 T 组任务分布 {Tt}Tt=1 按顺序到达，其中 Tt 表示第 t 个任务的输入空间和标签空间 (Xt, Yt) 的联合分布。记 X ≜ ∪Tt=1 Xt 和 Y ≜ ∪Tt=1Yt 为输入和标签空间的并集。在定义 2.3 中定义的内存约束下，任务增量学习（TIL）旨在找到满足以下条件的最优假设 h<em>： h</em> = argmin h ∑Tt=1 E(x,y)∼Tt [1h(x,t)≠y]。 (5)</p>
<p><strong>定义 2.5（领域增量学习，DIL）</strong>。假设 T 组领域分布 {Dt}Tt=1 按顺序到达，其中 Dt 表示共享输入空间和标签空间 (X, Y) 的第 t 个联合分布。在定义 2.3 中定义的内存约束下，领域增量学习（DIL）旨在找到满足以下条件的最优假设 h<em>： h</em> = argmin h ∑Tt=1 E(x,y)∼Dt [1h(x)≠y]。 (6)</p>
<p><strong>定义 2.6（类增量学习，CIL）</strong>。假设 T 组任务分布 {Tt}Tt=1 按顺序到达，其中 Tt 表示第 t 个任务的输入空间和标签空间 (Xt, Yt) 的联合分布。记 X ≜ ∪Tt=1 Xt 和 Y ≜ ∪Tt=1Yt 为输入和标签空间的并集。在定义 2.3 中定义的内存约束下，类增量学习（CIL）旨在找到满足以下条件的最优假设 h<em>： h</em> = argmin h ∑Tt=1 E(x,y)∼Tt [1h(x)≠(t,y)]。 (7)</p>
<p>备注。在 TIL 中，通常会有共享输入空间 X = Xt，∀t ∈ [T]，但标签分布空间 Yt 可以是不同的（Yi ∩ Yj = ∅，∀i ≠ j），部分共享（Yi ∩ Yj ≠ ∅，∃i ≠ j），或在不同任务之间共享（Y = Yt，∀t ∈ [T]）。在 DIL 中，任务的定义形式相同，即相同的输入空间 X 和相同的输出空间 Y。在推理期间，不提供任务 ID 给假设，这意味着持续学习模型需要捕捉领域不变特征和标签之间的模式。DIL 通常被认为比 TIL 更难。CIL 通常被认为是最具挑战性的持续学习场景，因为模型需要同时推断标签和任务 ID。另一种可能的 CIL 表述是将其表示为 DIL，但输出标签空间是离散的，Yi ∩ Yj = ∅，∀i ≠ j。</p>
<p><strong>2.2.2 持续学习技术</strong>。CL 的目标是找到一个在所有任务/领域上最小化风险的假设。以 DIL 为例 [270]，在第 t 学习阶段，理想的训练目标 L(h) 定义为 L(h) ≜ ∑t−1i=1 LDi(h)︸过去领域 + LDt(h)︸当前领域。 (8) 由于内存约束（定义 2.3），过去领域的目标通常难以测量或优化。因此，设计 CL 算法的核心在于在不违反内存约束的情况下识别第一个项的代理学习目标。现有的 CL 技术可以大致分为五类：（i）基于重放的，（ii）基于正则化的，（iii）基于架构的，（iv）基于优化的，以及（v）基于表示的 [61, 302]。这里，我们简要介绍前三类持续学习技术，因为它们在持续学习大语言模型中广泛应用。</p>
<p>基于重放的方法。基于重放的方法采用松散的内存约束，为每个任务 Ti 保留一个小缓冲区 {Mi}t−1i=1 的观察数据。形式上，他们寻求优化以下经验训练目标：L̂重放(h) ≜ ∑t−1i=1 L̂Mi(h)︸过去领域代理 + L̂St(h)︸当前领域， (9) 其中 L̂S 表示在样本集 S 上评估的经验损失项。尽管被视为 CL 的简化解决方案，但基于重放的方法可能在理论上导致宽松的泛化界限 [270]。尽管如此，它们因其简洁性、稳定性和高性能而受到重视，即使使用小的情景记忆 [38, 249]。例如，DER++ [29] 通过重放一小部分过去的样本及其对数（称为暗经验重放）表现出一致的性能提升。ESM-ER [259] 引入错误敏感性调制（ESM），以缓解新样本带来的突变表示漂移。基于重放的 CL 主要关注样本效率的提高以维护缓冲区。例如，[247] 优先选择示例选择基于放牧以准确建模类均值在类增量学习中的表示。[381] 提议存储低保真示例以实现内存高效的示例集维护。RM（彩虹记忆）[16] 引入基于每个样本不确定性估计和数据增强的多样性感知记忆更新，以进行类增量学习。</p>
<p>基于正则化的方法。假设 h𝜃t−1 是第 t-1 阶段训练后得到的假设，由 𝜃t−1 参数化。基于正则化的方法利用正则化项作为过去领域损失的代理，通过参数空间中的距离来确定。L̂正则(h𝜃) ≜ 𝜆 · ∥𝜃 − 𝜃t−1∥𝚺︸过去领域代理 + L̂St(h𝜃)︸当前领域， (10) 其中 ∥v∥𝚺 = v⊤𝚺v 是在正定矩阵 𝚺 上评估的向量范数，𝜆 是引入的正则化系数，用于平衡过去知识保留和当前知识学习。引入的矩阵 𝚺 用于衡量每个参数在保留过去知识中的重要性及其相关性。在实践中，为了减少计算开销，通常设计对角矩阵以仅编码每个参数的重要性。例如，弹性权重巩固（EWC）[145] 采用贝叶斯视角，使用 Fisher 信息矩阵（FIM）的对角值作为参数 Hessian 矩阵的近似值。这形成了一个连续的最大后验（MAP）优化用于持续学习。记忆感知突触（MAS）[5] 在线和无监督地计算参数重要性，通过在训练期间累积绝对梯度来定义重要性。还值得注意的是，当 𝚺 = 𝑰 简化为单位矩阵时，正则化项简化为基本的 l2</p>
<p>-惩罚项，均等惩罚每个参数，这在某些持续 LLM 中可能非常有效 [252]。</p>
<p>基于架构的方法。动态扩展网络架构以吸收新知识被认为是最有效的 CL 形式 [315, 316]。这种方法主要解决适应挑战，并且在推理期间可用或可以正确推断任务 ID 时可以实现零遗忘。然而，由于任务 ID 推断的困难，架构扩展主要用于 TIL，但在 DIL 或 CIL 中很少被探索。渐进神经网络（PNN）[256] 提议在出现新任务时学习横向连接的神经元，确保不遗忘并能够在未来任务中转移先前学到的神经元。结合预训练的主干大模型如 ViT [70]，CoLoR [324] 为不同任务训练各种低秩适配（LoRA）[111] 模块。在测试期间，它估计并存储每个任务的原型，并利用预训练模型的自然聚类能力推断任务 ID，选择相应的 LoRA 组件生成预测。在持续 LLM 的领域中，随着参数高效微调（PEFT）[6, 65, 111, 151, 163, 268] 的兴起，架构扩展重新流行起来，这是我们稍后将深入探讨的话题 [123, 131, 154, 228, 306, 326, 346, 349]。</p>
<p><strong>2.2.3 持续学习的评估指标</strong>。在传统持续学习的领域中，当任务流以分类形式出现时，许多指标依赖于准确性矩阵的概念 [181, 270]。将这一概念扩展到持续学习 LLM 的背景下，我们引入性能矩阵 P ∈ RT×T，其中 T 表示训练阶段的总数。P 的每个条目对应于模型评估的性能指标，如预训练数据上的困惑度 [46, 90, 131]，下游数据上的零样本/少样本评估指标而无需微调 [11, 53, 62, 222, 254, 327]，下游任务上的微调准确性 [7, 46, 123, 237]，以及在下游任务上评估的微调附加组件的探测准确性 [189, 284, 385]。在 P 中，Pi,j 表示在任务 i 上训练后在任务 j 上评估的模型性能。基于性能矩阵的定义，我们引入了广泛采用的主要评估协议。</p>
<p>总体性能（OP）。总体性能（OP）[137, 370, 376] 是平均准确性概念的自然扩展。在训练阶段 t 之后测量的 OP 是在 t 阶段后训练的模型的平均性能。记作 OPt，我们有：OPt ≜ 1t∑ti=1Pt,i。 (11) 正如 [270] 所述，OP 对应于定义 2.4、2.5 和 2.6 中定义的主要优化目标。在大多数持续学习文献中，一旦所有 T 任务完成，报告最终 OP（OPT），通常省略下标 T 以简洁。在一些工作中，OP 以任务重要性加权 ÕP ≜ 1T∑Ti=1wiPt,i，其中 wi = Ni/∑Tj=1 Nj 表示数据比例。</p>
<p>遗忘（F）。定义 Ft 为到任务 t 的遗忘，表示在整个训练过程中观察到的最大性能下降，平均在 t 个训练阶段：Ft ≜ 1t−1∑t−1j=1[maxl∈[t−1]{Pl,j − Pt,j}]。 (12) 通常，研究人员在整个训练过程结束时报告平均遗忘 F = FT。遗忘量化了学习新任务对先前获得知识的影响。理想情况下，一个健壮的持续学习框架应该实现反向迁移（BWT），即学习新任务增强先前任务的性能。这种增强通常通过否定遗忘来衡量，从而表明在早期任务上的性能有所提升。</p>
<p>正向迁移（FWT）。正向迁移衡量持续学习算法的泛化能力。形式上，正向迁移 FWTt 到训练阶段 t 定义为：FWTt ≜ 1t−1∑ti=2Pi−1,i − bi， (13) 其中 bi 是在进行持续学习之前评估的模型在任务 i 上的基线性能。严格来说，bi 的定义与之前的工作 [181, 270] 中定义的不完全相同，在那里它用于表示模型随机初始化的性能。</p>
<p><strong>3 持续学习与大语言模型的结合：概述</strong></p>
<p>大语言模型（LLM）在各个维度上都很广泛，包括模型参数的大小、预训练数据集、计算资源、项目团队和开发周期 [2, 8, 27, 51, 224, 240, 294, 295]。LLM 的大规模给开发团队带来了显著的挑战，特别是在快速变化的环境中保持其更新 [7, 67, 122, 123, 131]。例如，2023年，每天新推文的平均涌入量超过5亿条1，甚至对这一大数据量的一个子集进行训练也是负担不起的。可循环调优 [237] 是第一个明确概述现代 LLM 生产流水线中供应商-消费者结构的工作。这一结构允许我们从涉及的各个角色的角度剖析持续 LLM 的挑战。在供应商方面，模型在一系列大规模未标注数据集上持续预训练。在每次预训练模型发布后，消费者需要利用更强大和最新的上游模型进行下游任务。与上游供应商相比，下游用户通常缺乏收集和存储大规模数据、维护大规模硬件系统以及自己训练 LLM 的能力。因此，可循环调优主要关注于如何高效地将更新的预训练 LLM 持续适应下游任务。在本综述中，我们进一步提出了一个涵盖持续 LLM 预训练、适应和部署的现代生产流水线的全面框架（图1）。我们的框架与现有研究 [330] 的不同之处在于纳入了两个连续性方向：纵向连续性和横向连续性。</p>
<p><strong>3.1 纵向连续性（纵向持续学习）</strong></p>
<p><strong>定义</strong>。纵向连续性（或纵向持续学习）长期以来在现有文献中得到了隐式或显式的研究。纵向连续性的特点是包含数据包容性、任务范围和计算资源的层次结构。具体而言，训练任务逐渐从通用预训练过渡到下游任务，通常由生产流水线中的不同实体承担 [89, 92, 237, 252, 341, 346]。图1显示了 LLM 纵向连续性的典型流水线，即“预训练”→“领域自适应训练”→“下游微调”[53, 62, 89, 93, 94, 117, 158, 191, 193, 252, 326, 327, 346, 390]：</p>
<ul>
<li><strong>预训练</strong>。在预训练阶段，需要大量来自不同领域的数据以开发通用 LLM。此阶段需要专门的研发团队来训练和基准测试模型，以及大量的计算资源。</li>
<li><strong>领域自适应预训练</strong>。随后，下游机构可能会选择领域自适应预训练，使用上游供应商不可用的领域特定数据来定制模型以适应特定任务。</li>
<li><strong>微调</strong>。最后，LLM 在下游任务的标注数据上进行微调，然后部署。</li>
</ul>
<p>在整个过程中，未标注的领域特定数据集的规模小于上游预训练阶段，但大于最终下游任务微调阶段。这一模式延伸到计算资源、团队规模和其他因素。需要注意的是，纵向连续性可以涉及多个阶段 [117, 171, 222, 254]。在现实应用中，在领域自适应预训练期间，可以添加额外的层以适应多个实体，例如在同一领域内具有不同目标的各个部门。</p>
<p><strong>纵向遗忘</strong>。我们将经历纵向持续学习的模型在通用知识上的性能下降称为“纵向遗忘”。如图2所示，对于纵向持续学习，上游任务的数据分布部分覆盖下游，这意味着模型可能以良好的初始状态开始后续阶段的训练。为了防止纵向遗忘，需要解决两个重要的挑战：</p>
<p>1<br />
数据来源：https://www.omnicoreagency.com/twitter-statistics</p>
<ul>
<li><strong>任务异质性</strong>。由于上游任务和下游任务的构成固有不同，任务异质性可能导致模型结构和训练方案的差异，这一直被认为是一个主要障碍 [144, 166, 219, 247, 332]。为了缓解这一问题，从业者通常在下游阶段冻结共享参数或重新构建下游任务以匹配预训练任务</li>
</ul>
<p>的结构 [154, 228, 306, 326, 346, 349]。<br />
- <strong>不可访问的上游数据</strong>。这一挑战主要源于承担纵向持续学习的各个实体之间的保密程度不同。根据不同协议收集和整理的数据可能对某些下游实体不可访问。这一场景甚至比传统 CL 中提出的严格内存约束（定义 2.3）更具挑战性，因为后者的算法依赖于在特定时间点访问先前数据以测量参数重要性 [5, 145] 或进行重放 [29, 38, 249, 270]。为了应对不可访问的上游数据的挑战，现有方法要么使用公共数据集，要么生成伪示例来创建代理预训练数据集 [236]。</p>
<p><strong>3.2 横向连续性（横向持续学习）</strong></p>
<p><strong>定义</strong>。横向连续性（或横向持续学习）指的是跨时间和领域的持续适应，这是持续学习社区中广泛探讨的话题。保持横向连续性的主要原因在于数据分布随时间的动态变化。为了跟上这些内容变化，LLM 必须增量学习新出现的数据。否则，重新训练的成本将变得难以承受且不切实际 [7, 67, 122, 123, 131, 182, 355]。实验证据一致表明，尽管 LLM 具有令人印象深刻的能力，但在面对时间或领域变化时，它们在未来未见数据上的泛化能力存在不足 [7, 67, 122, 123]。此外，在适应新时间领域时，它们难以完全保留过去的知识，尽管它们在应对灾难性遗忘方面表现出更高的鲁棒性 [189, 201, 284, 385]。是否需要使用复杂的 CL 算法来解决 LLM 中的挑战仍然是一个悬而未决的问题。例如，在大规模持续预训练期间，大型机构通常能够承担保留所有历史数据的存储成本，使得内存约束不适用。一些研究表明，在完全访问历史数据的情况下，简单的稀疏重放技术可以有效减轻遗忘 [82, 235, 264, 284, 291]。相比之下，许多持续学习研究展示了优于简单解决方案的卓越性能，表明在 LLM 训练中持续学习技术的重要性 [46, 122, 131, 238]。</p>
<p><strong>横向遗忘</strong>。我们非正式地将“横向遗忘”定义为模型在进行横向持续学习时对先前任务的性能下降。如图2所示，横向持续学习通常涉及规模相似的训练阶段，其数据之间可能存在分布重叠。总之，为了横向持续学习 LLM，需要解决两个主要挑战：</p>
<ul>
<li><strong>长任务序列</strong>。横向持续学习理想情况下涉及许多增量阶段，特别是为了适应数据分布的时间变化。更长的任务序列意味着模型的更多更新步骤，导致不可避免的遗忘先前学到的任务。为了应对这一挑战，研究人员采用了具有更强约束的既定持续学习技术，例如持续模型集成 [245]。</li>
<li><strong>突发分布转变</strong>。与纵向连续性中分布转变通常是可预测的不同，横向持续学习对任务属性没有限制。证据表明，任务分布的突变会导致模型的显著横向遗忘 [30, 259]。</li>
</ul>
<p><strong>4 持续学习的大语言模型的学习阶段</strong></p>
<p>图1概述了持续学习 LLM。沿着纵向连续性轴，现代持续学习出现了三大层次。顶层是持续预训练（CPT），涉及在新收集的数据和现有数据上持续预训练 LLM（第4.1节）。中层是领域自适应预训练（DAP），通过在领域特定的未标注数据上进行额外的预训练，为领域特定的应用准备 LLM（第4.2节）。底层是持续微调（CFT），针对消费者侧的最终下游任务，在部署后需要为指定任务更新模型（第4.3节）。</p>
<p><strong>4.1 持续预训练（CPT）</strong></p>
<p><strong>4.1.1 CPT：有效性和效率</strong>。在深入探讨持续预训练（CPT）的细节之前，需要解决两个基本问题：首先，关于有效性，CPT 能否提高下游任务的性能，超越在广泛数据领域上的初始训练？大量研究不仅展示了为了改进下游性能而进行 CPT 的必要性 [46, 92, 122, 123, 131, 238]，还表明当分布转变是渐进的 [122, 355] 或某种程度上相关时 [92]，CPT 可以有效帮助模型泛化到未见数据。第二个问题是关于效率：考虑到 LLM 的参数和数据（无论是旧数据还是新数据）的庞大规模，我们能否以计算上高效的方式实现适应和知识保留？关于效率，大多数研究集中在有效的知识保留技术上 [122, 123, 131, 154]，这些技术与应对灾难性遗忘的 CL 文献高度重叠 [5, 29, 245, 247, 249, 250, 256, 263, 270, 301]。与先前充分利用新数据的方法相比，一些研究认识到在实际生产环境中这一方法的不可行性。相反，它们集中于进一步提高适应效率。例如，ELLE [238] 采用功能保留的模型扩展来促进高效的知识增长；[7] 和 [341] 基于新颖性和多样性子样本训练数据，以提高训练效率，实现优于全数据训练的性能。尽管目前尚未充分探索，但在持续预训练中有效的适应随着最近强调数据质量对 LLM 泛化能力的重要性 [72, 162, 276, 339]，这一领域将变得越来越重要。</p>
<p><strong>4.1.2 持续预训练的一般观察</strong>。表1总结了现有的持续预训练（CPT）研究，以下是我们对 CPT 的一些关键观察。</p>
<ul>
<li><strong>OBS-1</strong>：专门针对 CPT 的先进技术的发展处于起步阶段，需进一步探索。在被检查的论文中，只有约一半提出了新的 CPT 技术 [7, 46, 56, 67, 92, 135, 237, 238, 282]，其余一半要么仅关注纯适应的效果而不考虑 CL 技术 [83, 90, 182]，要么对现有 CL 技术的简单应用进行经验研究 [122, 123, 131, 154]。</li>
<li><strong>OBS-2</strong>：CPT 中采用的 CL 技术的多样性仍然有限。CPT 中大多数实际应用的 CL 技术主要集中在 LLM 的架构扩展上 [7, 46, 56, 67, 92, 237]，只有少数明确利用了重放 [46, 237] 和参数正则化 [7, 46]。</li>
<li><strong>OBS-3</strong>：现有研究与实际生产环境之间存在明显差距。除了最近的研究 [355] 在159个领域中进行 CPT 外，探索的最长预训练阶段序列是8 [92, 131]。然而，这远远低于在实际场景中持续预训练频繁发生并持续数月或数年的情况。这些方法在这种长期情景下的有效性尚不确定。此外，研究在无任务边界数据流设置中的 CPT 也是未来需要探索的重要领域。</li>
</ul>
<p><strong>4.1.3 CPT 中的分布转变</strong>。本综述将 CPT 的分布转变分为三大类型：（i）语言转变：LLM 顺序学习不同的语言语料库，例如英语→汉语 [83, 154]。（ii）内容转变：LLM 顺序学习不同领域的语料库，例如化学→生物学 [46, 56, 90, 92, 131, 237]。（iii）时间转变：分布转变随着时间发生，例如2021年的新闻→2022年的新闻，主要关注时间戳敏感的知识保留和更新 [7, 67, 122, 123, 131]。一些研究持续在由错误构建的数据集上进行预训练 [379]，重新加权样本 [47] 或标记 [172]，无法正确分类，我们在表1中用“其他”表示它们。</p>
<p><strong>语言转变</strong>。[83] 关注于评估 LLM 自然顺序学习新语言（英语、挪威语和冰岛语）的能力。没有使用明确的 CL 技术来防止横向遗忘，研究观察到一致的正向知识迁移，无论学习顺序如何，都有助于新语言的习得。另一方面，遗忘成为一个显著的挑战，通过增加 LLM 规模无法缓解</p>
<p>。在 [154] 中，研究了 LLM 适应新语言时先前学到的语言的遗忘程度。评估了各种 CL 技术，包括参数冻结、LoRA [111] 和（IA）3 [174]，在输出语言、通用知识保留和可靠性等多个维度上的表现。初步实验结果突显了在语言转变下处理 CPT 的非平凡性问题。我们认为，对语言转变下 CPT 的研究处于初步阶段，原因有二：首先，数据集的规模，包括语言数量和总标记数，仍然较小。其次，尚未提出针对语言转变的特定方法；仅评估了现有持续学习技术的基本组合。</p>
<p><strong>内容转变</strong>。没有使用复杂的 CL 技术，[355] 探索了在159个内容领域中的大规模 CPT，并对领域结构和模型属性进行了关键观察。研究表明，与在单个领域上进行 DAP 相比，在各种领域上进行 CPT 可以有效提高模型的适应能力。同样，[90] 继续对 Pythia [21] 进行预训练阶段，没有使用复杂的 CL 技术。研究集中于通过简单的学习率重新热启动（re-warming）来改进 CPT。他们发现重新热启动一致地改善了从头开始训练的模型。基于这一简单观察，[121] 进一步表明，适当结合学习率重新热启动和重新衰减，以及重放先前数据，足以实现与完全重新训练相当的性能。</p>
<p>另一个开创性工作，LLPT [131]，为一系列内容级分布转变建立了全面的训练和评估协议，称为“领域增量数据流”。他们评估了多种 CL 方法，与 [83] 中的发现类似，发现一致的正向知识迁移，但横向遗忘仍然显著。此外，与通常认为经验重放（ER，[38]）是防止遗忘的最有效方法不同，作者发现其在 CPT 中无效。他们推测 ER 的无效可能源于过拟合问题 [131, 362]。可循环调优 [237] 是第一个同时考虑上游 LLM 供应商和下游消费者的研究。研究表明，如果上游供应商持续预训练 LLM，无论是否重放，消费者侧效率都可以通过回收先前学习的更新组件得到提升。在这种背景下，初始化过时组件和知识蒸馏的两种 CL 技术相互补充，以提高适应效率。</p>
<p>其他方法包括为新内容领域训练额外的领域专家。DEMix [92] 通过增量训练和集成新专家（DEMix 层）来应对 CPT。为了在测试时没有领域信息时确保合理的推理性能，DEMix 提出了一个无参数的概率方法，动态估计领域的加权混合。引入新的领域变量 Dt 以及每个单词 xt，作者通过对所有专家进行边缘化来估计下一个单词的概率 p(xt | x&lt;t)：</p>
<p>p(xt | x&lt;t) = ∑nj=1 p(xt | x&lt;t, Dt = j) · p(Dt = j | x&lt;t) = ∑nj=1 p(xt | x&lt;t, Dt = j) · [p(x&lt;t | Dt=j) ·p(Dt=j)∑nj′=1 p(x&lt;t | Dt=j′) ·p(Dt=j′)]，2</p>
<p>其中条件概率项 p(·|·, Dt) 由使用特定领域专家计算。DEMix 框架的模块化已被证明可以促进高效的领域自适应预训练，促进相关知识在推理中的作用，并允许移除组件。Lifelong-MoE [46] 类似于 DEMix [92]，为新领域增量训练领域专家。然而，Lifelong-MoE 与 DEMix 不同的是，它使用令牌级门控函数激活多个专家进行中间嵌入计算。在训练期间，先前专家的参数和门控函数保持冻结，使用知识蒸馏损失调节参数更新，从而使 Lifelong-MoE 对横向遗忘具有鲁棒性。</p>
<p>需要注意的是，一些论文对 CPT 的重要性得出了几乎相反的结论。例如，[56] 在五个科学领域中对基于 BERT 的模型 [66, 178] 进行了持续预训练，并评估了下游情感分析的性能。他们观察到，即使是简单的顺序预训练也不会表现出严重的遗忘，提出了关于 CPT 必要性的合理问题。</p>
<p><strong>时间转变</strong>。在内容转变背景下，MTL（多任务学习）通常被认为是可达到的上限 [230, 270, 302]。然而，当考虑 CL 在时间转变下时，这一观点并不完全成立 [67, 122, 123]，因为时间转变可能引入冲突信息，给 LLM 带来挑战。例如，“梅西为巴塞罗那队效力”这一陈述在2004年至2021年期间是准确的，但到2024年变为错误，因为“梅西为国际迈阿密队效力”成为正确的陈述。</p>
<p>因此，正如 CKL [123] 和 TemporalWiki [122] 所倡导的那样，LLM 在适应时间转变时必须同时实现三个目标：（i）保留旧知识，（ii）获取新知识，以及（iii）更新过时的知识。他们评估了同一组持续学习基线方法 [44, 103, 112, 305]，每种方法突出了不同方面的影响。CKL [123] 观察到，参数扩展在所有实验条件下始终表现出稳健的性能。相反，基于重放的方法在高效适应新知识获取和更新过时知识方面表现不佳，导致在训练过程中快速遗忘新学到的信息。TemporalWiki [122] 从维基百科的顺序快照中构建了一系列时间语料库及其差分集，发现对这些差分集进行更新大幅增强了新知识获取和更新，显著减少了计算资源需求，并且在此过程中多种 CL 技术在减轻横向遗忘方面证明是有效的。LLPT [131] 引入了对顺序语料库预训练的 LLM 的时间泛化评估。通过在大规模按时间顺序排列的推特流上进行实验，作者证明了结合 CL 技术的 CPT 在知识获取和时间泛化方面优于特定任务 LM。然而，这些初步实验并未得出哪个特定 CL 方法比其他方法更优的结论。</p>
<p>另一条工作线是时间语言模型（TLMs），通过将时间信息集成到模型中，以不同方式解决知识保留、获取和更新问题 [67, 253, 280]。在训练期间，他们通过特殊标记 [253]、明确的年份信息 [67] 或语法引导的结构信息 [280] 将时间信息注入训练示例作为前缀。在 TempoT5 [67] 进行的顺序训练实验中，比较了持续预训练和联合预训练的语言模型，证明当过去数据的重放率适当设置时，CPT 能更好地平衡适应和遗忘。</p>
<p><strong>其他</strong>。作为逐步获取新知识的一种技术，CPT 可用于优化 LLM 的行为。CEM [379] 收集模型响应错误的示例，并将模型持续训练在这些示例及补充数据集上。RHO-1 [172] 提出选择性语言建模（SLM），利用参考模型评估训练语料库中每个标记的困惑度，并对高困惑度标记进行持续预训练。类似地，IR-DRO [47] 重新训练模型在原始预训练数据集中重新加权的示例，更多关注高损失序列。</p>
<p>通过 CPT 解决时间转变的重要性已在若干工业研究中得到了强调。例如，[7] 使用动态词汇扩展算法和高效子样本程序，对大规模新兴推文数据进行 CPT。相反，[182] 在没有明确措施限制模型更新的情况下，按季度发布一系列基于 BERT 的 LLM，在新推文数据上进行增量训练。初步实验结果表明，持续预训练的 LLM 在下游任务上相对于基 BERT 模型显著提高。尽管一些研究对沿时间轴持续适应 LLM 的必要性提出质疑，例如出于环境原因减少 CO2 排放 [10]，但社区普遍接受 CPT 作为比传统的“结合并重新训练”方法更高效的学习范式。</p>
<p><strong>4.2 领域自适应预训练（DAP）</strong></p>
<p><strong>DAP 的背景</strong>。无论规模大小，机构通常拥有大量未标注的领域特定数据。这些数据弥合了在各种语料库上训练的通用 LLM 和设计用于特定下游任务的微调 LLM 之间的差距。利用这些数据作为准备阶段，可以促进 LLM 更有效地适应下游任务。这样的过程被称为“继续/持续/连续预训练”[11, 53</p>
<p>, 62, 222, 254, 327]、领域自适应预训练（DAP）[93, 94, 158, 191, 193, 390]，或语言模型的持续自适应（CALM）[89, 326, 341, 346]。图1的第二层展示了 DAP，位于持续预训练（CPT）和最终下游任务微调（CFT）之间。</p>
<p><strong>4.2.1 DAP 的重要性</strong>。DAP 重要性在于它既提高了最终微调模型的性能，又减少了用于标注的下游数据的需求。以文献中 DAP 的一般性观察（表2）为背景，我们观察到以下现象。</p>
<ul>
<li><strong>OBS-4</strong>：DAP 的重要性取决于上游 LLM 的训练数据和 DAP 的领域特定数据之间的距离。近年来，LLM 的规模显著扩大，随着开放域预训练数据的多样性显著增加，领域适应的有效性取决于领域特定数据的独特性 [158]。例如，[62] 在一个庞大且无界的开放域数据集上预训练 GPT-2 后，在专门的临床数据集 MIMIC-III 上进行了领域适应。结果表明，领域自适应（GPT-2 + MIMIC-III）模型在医疗领域的任务上优于在同一领域上从头训练的模型。</li>
<li><strong>OBS-5</strong>：为了进一步提高下游任务性能，DAP 的数据选择和预处理变得至关重要。例如，[254] 提出了一个基于数据依赖度的领域适应方法，通过在不同的训练阶段选择适合的未标注数据来优化模型性能。相比之下，[222] 引入了多阶段 DAP 方法，在每个阶段分别使用不同的数据源，以逐步提高模型性能。</li>
<li><strong>OBS-6</strong>：DAP 的有效性随着领域特定数据量的增加而增加，表明在数据丰富的情况下，领域适应是一个值得投资的过程。例如，[93, 94] 表明，随着训练数据规模的增加，模型在领域特定任务上的性能显著提高。</li>
</ul>
<p><strong>4.2.2 DAP 中的挑战</strong>。尽管领域自适应预训练（DAP）在提高模型性能方面表现出色，但在实际应用中面临以下挑战：</p>
<ul>
<li><strong>数据质量和选择</strong>。在现实应用中，领域特定数据的质量和选择对 DAP 的效果至关重要。数据中的噪音和偏差可能导致模型性能下降。因此，研究人员提出了多种数据预处理和选择技术，以确保用于 DAP 的数据具有高质量和代表性。例如，[222] 提出了一个基于数据依赖度的领域适应方法，通过在不同的训练阶段选择适合的未标注数据来优化模型性能。</li>
<li><strong>计算资源和效率</strong>。DAP 需要大量计算资源和时间，特别是当领域特定数据量巨大时。因此，研究人员提出了多种方法来提高 DAP 的效率，例如使用参数高效微调技术（PEFT）[111, 154] 和基于样本效率的训练方法[7, 341]。</li>
</ul>
<p><strong>4.2.3 DAP 的未来方向</strong>。随着 LLM 的规模和复杂性的增加，领域自适应预训练（DAP）将继续在提高模型性能和适应能力方面发挥重要作用。未来的研究可以在以下几个方面进行探索：</p>
<ul>
<li><strong>自动化数据选择和预处理</strong>。通过自动化和智能化的数据选择和预处理技术，可以提高 DAP 的效率和效果。例如，使用机器学习技术自动筛选和过滤领域特定数据，以确保数据质量和代表性。</li>
<li><strong>高效的领域适应方法</strong>。开发新的高效领域适应方法，以减少计算资源和时间的需求。例如，利用参数高效微调技术（PEFT）和基于样本效率的训练方法，提高 DAP 的效率和效果。</li>
<li><strong>跨领域适应</strong>。研究如何在不同领域之间进行知识迁移和适应，以提高模型的通用性和适应能力。例如，探索在一个领域进行预训练，并在其他领域进行微调，以实现跨领域的知识迁移和适应。</li>
</ul>
<p><strong>4.3 持续微调（CFT）</strong></p>
<p><strong>4.3.1 持续微调的背景</strong>。持续微调（CFT）是指在部署后，针对特定下游任务持续更新和优化模型，以适应新的数据和需求。CFT 通过在下游任务的标注数据上进行增量训练，确保模型能够随着时间的推移保持高性能和适应性。</p>
<p><strong>4.3.2 CFT 的重要性</strong>。CFT 的重要性在于它能够在部署后持续优化模型性能，适应新的数据和需求。通过不断更新和优化模型，CFT 能够确保模型在面对新的挑战时保持高性能和适应性。</p>
<p><strong>4.3.3 CFT 的技术挑战</strong>。尽管 CFT 在提高模型性能和适应能力方面具有重要作用，但在实际应用中面临以下技术挑战：</p>
<ul>
<li><strong>数据可用性和质量</strong>。在实际应用中，新的数据可能不容易获得，或者数据质量可能较低。因此，CFT 需要有效的数据收集和预处理技术，以确保用于增量训练的数据具有高质量和代表性。</li>
<li><strong>计算资源和效率</strong>。CFT 需要大量计算资源和时间，特别是当新的数据量巨大时。因此，研究人员提出了多种方法来提高 CFT 的效率，例如使用参数高效微调技术（PEFT）[111, 154] 和基于样本效率的训练方法[7, 341]。</li>
</ul>
<p><strong>4.3.4 持续指令调优（CIT）</strong>。持续指令调优（CIT）是指在部署后，通过不断优化模型的指令响应能力，确保模型能够准确有效地响应用户指令。CIT 通过在特定任务的标注数据上进行增量训练，优化模型的指令响应能力。</p>
<p><strong>4.3.5 持续模型优化（CMR）</strong>。持续模型优化（CMR）是指在部署后，通过不断优化模型的参数和结构，确保模型在各种任务上的高性能和适应性。CMR 通过在特定任务的标注数据上进行增量训练，优化模型的参数和结构。</p>
<p><strong>4.3.6 持续多模态大语言模型（CMLLMs）</strong>。持续多模态大语言模型（CMLLMs）是指在部署后，通过不断优化和扩展模型的多模态能力，确保模型在各种模态下的高性能和适应性。CMLLMs 通过在多模态数据上进行增量训练，优化和扩展模型的多模态能力。</p>
<p><strong>5 持续学习大语言模型的评估协议和基准</strong></p>
<p><strong>5.1 评估协议</strong>。评估持续学习大语言模型的关键在于使用合适的评估协议和基准，以全面衡量模型在不同任务和数据上的性能和适应能力。以下是一些常用的评估协议：</p>
<ul>
<li><strong>总体性能（OP）</strong>。总体性能（OP）衡量模型在所有任务上的平均性能，通常在所有任务完成后报告最终 OP（OPT）。</li>
<li><strong>遗忘（F）</strong>。遗忘（F）量化模型在学习新任务时对先前任务的性能下降，通常在整个训练过程结束时报告平均遗忘（FT）。</li>
<li><strong>正向迁移（FWT）</strong>。正向迁移（FWT）衡量模型在学习新任务时对未来任务的泛化能力，通常在每个训练阶段后报告。</li>
</ul>
<p><strong>5.2 基准数据集</strong>。为了全面评估持续学习大语言模型的性能和适应能力，需要使用多样化和具有挑战性的基准数据集。以下是一些常用的基准数据集：</p>
<ul>
<li><strong>开放域数据集</strong>。开放域数据集包括来自不同领域和主题的广泛数据，适用于评估模型的通用能力和适应性。</li>
<li><strong>领域特定数据集</strong>。领域特定数据集包括特定领域的数据，适用于评估模型在特定任务和领域上的性能和适应性。</li>
<li><strong>时间序列数据集</strong>。时间序列数据集包括按时间顺序排列的数据，适用于评估模型在时间变化下的适应能力和知识保留。</li>
</ul>
<p><strong>6 持续学习大语言模型的未来方向</strong></p>
<p><strong>6.1 持续学习大语言模型的潜力</strong>。持续学习大语言模型在各个领域具有广泛的应用潜力，例如自然语言处理、对话系统、机器翻译和信息检索。通过不断优化和适应，持续学习大语言模型能够实现更高效、更准确的任务执行和知识迁移。</p>
<p><strong>6.2 持续学习技术的改进</strong>。未来的研究可以在以下几个方面改进持续学习技术：</p>
<ul>
<li><strong>高效的知识保留方法</strong>。开发新的高效知识保留方法，以减少模型在学习新任务时的遗忘。例如，利用参数高效微调技术（PEFT）和</li>
</ul>
<p>基于样本效率的训练方法，提高知识保留的效率和效果。<br />
- <strong>自动化和智能化的数据选择</strong>。通过自动化和智能化的数据选择技术，提高模型在不同任务和数据上的适应能力。例如，使用机器学习技术自动筛选和过滤训练数据，以确保数据质量和代表性。<br />
- <strong>跨任务和领域的知识迁移</strong>。研究如何在不同任务和领域之间进行知识迁移和适应，提高模型的通用性和适应能力。例如，探索在一个任务或领域进行预训练，并在其他任务或领域进行微调，实现跨任务和领域的知识迁移。</p>
<p><strong>6.3 持续学习大语言模型的应用</strong>。持续学习大语言模型在各个领域具有广泛的应用前景，例如：</p>
<ul>
<li><strong>对话系统</strong>。通过持续学习，对话系统可以不断适应用户需求和语言变化，提高交互的自然性和准确性。</li>
<li><strong>机器翻译</strong>。通过持续学习，机器翻译系统可以不断适应新的语言和领域，提高翻译的准确性和流畅性。</li>
<li><strong>信息检索</strong>。通过持续学习，信息检索系统可以不断优化检索算法和模型，提高检索的效率和准确性。</li>
</ul>
<p><strong>6.4 未来研究方向</strong>。未来的研究可以在以下几个方面进行探索：</p>
<ul>
<li><strong>高效的持续学习算法</strong>。开发新的高效持续学习算法，提高模型在不同任务和数据上的适应能力和性能。</li>
<li><strong>多模态持续学习</strong>。研究如何在多模态数据上进行持续学习，提高模型在多模态任务上的适应能力和性能。</li>
<li><strong>跨领域和跨任务的持续学习</strong>。探索跨领域和跨任务的持续学习方法，实现知识的广泛迁移和适应，提高模型的通用性和适应能力。</li>
</ul>
<p><strong>7 结论</strong></p>
<p>本文综述了大语言模型的持续学习方法和技术，介绍了当前的研究进展和挑战，并提出了未来的研究方向。通过不断优化和适应，大语言模型在各个领域具有广泛的应用前景，未来的研究将进一步推动大语言模型的持续学习技术的发展。</p>
                </div>

                <!-- Comments Section (Giscus) -->
                <section class="comments-section">
                    <h3><i class="fas fa-comments"></i> 评论</h3>
                    <script src="https://giscus.app/client.js"
                        data-repo="Geeks-Z/Geeks-Z.github.io"
                        data-repo-id=""
                        data-category="Announcements"
                        data-category-id=""
                        data-mapping="pathname"
                        data-strict="0"
                        data-reactions-enabled="1"
                        data-emit-metadata="0"
                        data-input-position="bottom"
                        data-theme="preferred_color_scheme"
                        data-lang="zh-CN"
                        crossorigin="anonymous"
                        async>
                    </script>
                </section>
            </article>

            <footer class="post-footer">
                <p>© 2025 Hongwei Zhao. Built with ❤️</p>
            </footer>
        </main>
    </div>

    <!-- Theme Toggle Button -->
    <button class="theme-toggle" id="themeToggle" aria-label="切换主题">
        <i class="fas fa-moon"></i>
    </button>

    <script>
        // Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        const html = document.documentElement;
        const icon = themeToggle.querySelector('i');
        const hljsDark = document.getElementById('hljs-theme-dark');
        const hljsLight = document.getElementById('hljs-theme-light');
        
        // Check saved theme or system preference
        const savedTheme = localStorage.getItem('theme');
        const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
        
        if (savedTheme === 'dark' || (!savedTheme && prefersDark)) {
            html.setAttribute('data-theme', 'dark');
            icon.className = 'fas fa-sun';
            hljsDark.disabled = false;
            hljsLight.disabled = true;
        }
        
        themeToggle.addEventListener('click', () => {
            const isDark = html.getAttribute('data-theme') === 'dark';
            if (isDark) {
                html.removeAttribute('data-theme');
                icon.className = 'fas fa-moon';
                localStorage.setItem('theme', 'light');
                hljsDark.disabled = true;
                hljsLight.disabled = false;
            } else {
                html.setAttribute('data-theme', 'dark');
                icon.className = 'fas fa-sun';
                localStorage.setItem('theme', 'dark');
                hljsDark.disabled = false;
                hljsLight.disabled = true;
            }
            
            // Update Giscus theme
            const giscusFrame = document.querySelector('iframe.giscus-frame');
            if (giscusFrame) {
                giscusFrame.contentWindow.postMessage({
                    giscus: {
                        setConfig: {
                            theme: isDark ? 'light' : 'dark'
                        }
                    }
                }, 'https://giscus.app');
            }
        });
        
        // Highlight.js
        document.addEventListener('DOMContentLoaded', () => {
            hljs.highlightAll();
        });
        
        // KaTeX auto-render
        document.addEventListener('DOMContentLoaded', () => {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
        
        // TOC active state
        const tocLinks = document.querySelectorAll('.toc-container a');
        const headings = document.querySelectorAll('.post-content h2, .post-content h3, .post-content h4');
        
        function updateTocActive() {
            let current = '';
            headings.forEach(heading => {
                const rect = heading.getBoundingClientRect();
                if (rect.top <= 100) {
                    current = heading.getAttribute('id');
                }
            });
            
            tocLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === '#' + current) {
                    link.classList.add('active');
                }
            });
        }
        
        window.addEventListener('scroll', updateTocActive);
        updateTocActive();
    </script>
</body>
</html>
