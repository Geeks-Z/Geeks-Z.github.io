<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Untitled</title>
    <meta name="description" content="Untitled - Hongwei Zhao's Blog">
    <meta name="author" content="Hongwei Zhao">
    
    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <!-- Code Highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css" id="hljs-theme-dark">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css" id="hljs-theme-light" disabled>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    
    <style>
        :root {
            /* Light Theme */
            --primary-color: #2980b9;
            --primary-hover: #1a5276;
            --link-color: #c0392b;
            --text-color: #333;
            --text-light: #666;
            --text-muted: #999;
            --bg-color: #fff;
            --bg-secondary: #f8f9fa;
            --bg-code: #f5f5f5;
            --border-color: #e5e7eb;
            --shadow: 0 1px 3px rgba(0,0,0,0.1);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.1);
        }

        [data-theme="dark"] {
            --primary-color: #5dade2;
            --primary-hover: #85c1e9;
            --link-color: #e74c3c;
            --text-color: #e5e7eb;
            --text-light: #9ca3af;
            --text-muted: #6b7280;
            --bg-color: #1a1a2e;
            --bg-secondary: #16213e;
            --bg-code: #0f0f23;
            --border-color: #374151;
            --shadow: 0 1px 3px rgba(0,0,0,0.3);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.4);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 16px;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            transition: background-color 0.3s, color 0.3s;
        }

        a {
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--primary-hover);
            text-decoration: underline;
        }

        /* Layout */
        .page-wrapper {
            display: flex;
            max-width: 1400px;
            margin: 0 auto;
            padding: 2rem;
            gap: 3rem;
        }

        /* Sidebar TOC */
        .toc-sidebar {
            width: 260px;
            flex-shrink: 0;
            position: sticky;
            top: 2rem;
            height: fit-content;
            max-height: calc(100vh - 4rem);
            overflow-y: auto;
        }

        .toc-container {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 1.5rem;
            border: 1px solid var(--border-color);
        }

        .toc-container h3 {
            font-size: 0.85rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-muted);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .toc-container ul {
            list-style: none;
        }

        .toc-container li {
            margin-bottom: 0.5rem;
        }

        .toc-container a {
            font-size: 0.9rem;
            color: var(--text-light);
            display: block;
            padding: 0.25rem 0;
            border-left: 2px solid transparent;
            padding-left: 0.75rem;
            transition: all 0.2s;
        }

        .toc-container a:hover,
        .toc-container a.active {
            color: var(--primary-color);
            border-left-color: var(--primary-color);
            text-decoration: none;
        }

        .toc-container ul ul {
            margin-left: 1rem;
        }

        /* Main Content */
        .main-content {
            flex: 1;
            min-width: 0;
            max-width: 800px;
        }

        /* Header */
        .post-header {
            margin-bottom: 2rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
            margin-bottom: 1.5rem;
        }

        .back-link:hover {
            color: var(--primary-color);
            text-decoration: none;
        }

        .post-header h1 {
            font-size: 2.25rem;
            font-weight: 700;
            line-height: 1.3;
            margin-bottom: 1rem;
            color: var(--text-color);
        }

        .post-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
        }

        .post-meta span {
            display: flex;
            align-items: center;
            gap: 0.4rem;
        }

        .post-meta i {
            color: var(--text-muted);
        }

        .tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 1rem;
        }

        .tag {
            display: inline-block;
            font-size: 0.8rem;
            background: var(--bg-secondary);
            color: var(--text-light);
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            border: 1px solid var(--border-color);
            transition: all 0.2s;
        }

        .tag:hover {
            background: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }

        /* Article Content */
        .post-content {
            font-size: 1rem;
            line-height: 1.9;
        }

        .post-content h1,
        .post-content h2,
        .post-content h3,
        .post-content h4,
        .post-content h5,
        .post-content h6 {
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-weight: 600;
            line-height: 1.4;
            color: var(--text-color);
        }

        .post-content h1 { font-size: 1.875rem; }
        .post-content h2 { 
            font-size: 1.5rem; 
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }
        .post-content h3 { font-size: 1.25rem; }
        .post-content h4 { font-size: 1.125rem; }

        .post-content p {
            margin-bottom: 1.25rem;
        }

        .post-content ul,
        .post-content ol {
            margin: 1.25rem 0;
            padding-left: 1.5rem;
        }

        .post-content li {
            margin-bottom: 0.5rem;
        }

        .post-content blockquote {
            margin: 1.5rem 0;
            padding: 1rem 1.5rem;
            background: var(--bg-secondary);
            border-left: 4px solid var(--primary-color);
            border-radius: 0 8px 8px 0;
            color: var(--text-light);
            font-style: italic;
        }

        .post-content blockquote p:last-child {
            margin-bottom: 0;
        }

        /* Code */
        .post-content code {
            font-family: 'JetBrains Mono', 'Fira Code', Consolas, monospace;
            font-size: 0.9em;
            background: var(--bg-code);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            color: var(--link-color);
        }

        .post-content pre {
            margin: 1.5rem 0;
            padding: 1.25rem;
            background: var(--bg-code);
            border-radius: 10px;
            overflow-x: auto;
            border: 1px solid var(--border-color);
        }

        .post-content pre code {
            background: none;
            padding: 0;
            color: inherit;
            font-size: 0.875rem;
            line-height: 1.6;
        }

        /* Images */
        .post-content img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            margin: 1.5rem 0;
            box-shadow: var(--shadow-lg);
        }

        /* Tables */
        .post-content table {
            width: 100%;
            margin: 1.5rem 0;
            border-collapse: collapse;
            font-size: 0.95rem;
        }

        .post-content th,
        .post-content td {
            padding: 0.75rem 1rem;
            border: 1px solid var(--border-color);
            text-align: left;
        }

        .post-content th {
            background: var(--bg-secondary);
            font-weight: 600;
        }

        .post-content tr:nth-child(even) {
            background: var(--bg-secondary);
        }

        /* Math */
        .math-display {
            margin: 1.5rem 0;
            overflow-x: auto;
            padding: 1rem;
            background: var(--bg-secondary);
            border-radius: 8px;
        }

        /* Anchor links */
        .anchor-link {
            opacity: 0;
            margin-left: 0.5rem;
            color: var(--text-muted);
            font-weight: 400;
            transition: opacity 0.2s;
        }

        .post-content h2:hover .anchor-link,
        .post-content h3:hover .anchor-link,
        .post-content h4:hover .anchor-link {
            opacity: 1;
        }

        /* Theme Toggle */
        .theme-toggle {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            background: var(--primary-color);
            color: white;
            border: none;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.25rem;
            box-shadow: var(--shadow-lg);
            transition: transform 0.2s, background 0.2s;
            z-index: 1000;
        }

        .theme-toggle:hover {
            transform: scale(1.1);
            background: var(--primary-hover);
        }

        /* Comments Section */
        .comments-section {
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-color);
        }

        .comments-section h3 {
            font-size: 1.25rem;
            margin-bottom: 1.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        /* Footer */
        .post-footer {
            margin-top: 3rem;
            padding-top: 1.5rem;
            border-top: 1px solid var(--border-color);
            text-align: center;
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        /* Responsive */
        @media (max-width: 1024px) {
            .toc-sidebar {
                display: none;
            }
            
            .page-wrapper {
                padding: 1.5rem;
            }
        }

        @media (max-width: 768px) {
            .post-header h1 {
                font-size: 1.75rem;
            }
            
            .post-meta {
                flex-direction: column;
                gap: 0.5rem;
            }
            
            .theme-toggle {
                bottom: 1rem;
                right: 1rem;
                width: 44px;
                height: 44px;
            }
        }

        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-secondary);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--border-color);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--text-muted);
        }
    </style>
</head>
<body>
    <div class="page-wrapper">
        <!-- TOC Sidebar -->
        <aside class="toc-sidebar">
            <div class="toc-container">
                <h3><i class="fas fa-list"></i> 目录</h3>
                <div class="toc">
<ul>
<li><a href="#memory-efficient-incremental-learning-through-feature-adaptation">Memory-Efficient Incremental Learning Through Feature Adaptation</a></li>
<li><a href="#0-摘要">0. 摘要</a></li>
<li><a href="#1-引言">1. 引言</a><ul>
<li><a href="#本文贡献">本文贡献</a></li>
</ul>
</li>
<li><a href="#2-相关工作">2. 相关工作</a><ul>
<li><a href="#21-非重放方法">2.1 非重放方法</a></li>
<li><a href="#22-基于重放的样本保存">2.2 基于重放的样本保存</a></li>
<li><a href="#23-基于生成图像的重放">2.3 基于生成图像的重放</a></li>
<li><a href="#24-基于特征的方法">2.4 基于特征的方法</a></li>
</ul>
</li>
<li><a href="#3-增量学习背景">3. 增量学习背景</a><ul>
<li><a href="#31-问题定义">3.1 问题定义</a></li>
<li><a href="#32-分类器">3.2 分类器</a></li>
<li><a href="#33-知识蒸馏">3.3 知识蒸馏</a></li>
<li><a href="#34-旧类别数据的保存">3.4 旧类别数据的保存</a></li>
</ul>
</li>
<li><a href="#4-高效内存增量学习">4. 高效内存增量学习</a><ul>
<li><a href="#41-网络训练">4.1 网络训练</a></li>
<li><a href="#42-特征适配">4.2 特征适配</a></li>
<li><a href="#43-特征分类器的训练">4.3 特征分类器的训练</a></li>
</ul>
</li>
<li><a href="#5-实验">5. 实验</a><ul>
<li><a href="#51-实验设置">5.1 实验设置</a></li>
<li><a href="#52-内存占用的影响">5.2 内存占用的影响</a></li>
<li><a href="#53-与现有方法的比较">5.3 与现有方法的比较</a><ul>
<li><a href="#表-1imagenet-100-和-imagenet-1000-的平均增量准确性和内存占用">表 1：ImageNet-100 和 ImageNet-1000 的平均增量准确性和内存占用</a></li>
</ul>
</li>
<li><a href="#54-参数的影响">5.4 参数的影响</a></li>
</ul>
</li>
<li><a href="#6-结论">6. 结论</a></li>
</ul>
</div>

            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <article>
                <header class="post-header">
                    <a href="../../../../index.html" class="back-link">
                        <i class="fas fa-arrow-left"></i> 返回博客列表
                    </a>
                    <h1>Untitled</h1>
                    <div class="post-meta">
                        <span><i class="fas fa-calendar-alt"></i> 2026-01-28</span>
                        <span><i class="fas fa-folder"></i> AINotes/40.CIL/41.Feature Rectify</span>
                        <span><i class="fas fa-user"></i> Hongwei Zhao</span>
                    </div>
                    <div class="tags">
                        
                    </div>
                </header>

                <div class="post-content">
                    <h2 id="memory-efficient-incremental-learning-through-feature-adaptation"><a href="http://arxiv.org/abs/2004.00713">Memory-Efficient Incremental Learning Through Feature Adaptation</a><a class="anchor-link" href="#memory-efficient-incremental-learning-through-feature-adaptation" title="Permanent link">&para;</a></h2>
<h2 id="0-摘要">0. 摘要<a class="anchor-link" href="#0-摘要" title="Permanent link">&para;</a></h2>
<p>我们提出了一种增量学习的方法，该方法保存了来自先前学习类别的训练图像的特征描述符，而非图像本身，这与大多数现有方法不同。通过保存低维的特征嵌入，可以显著减少内存占用。假设模型会随着新的类别逐步更新，每次仅有新数据可用。这需要在无法访问相应原始训练图像的情况下，将先前存储的特征向量适配到更新后的特征空间。特征适配是通过多层感知机（MLP）学习的，该模型以训练图像上原始网络与更新网络的输出特征对作为训练数据。实验验证表明，这种变换对先前类别的特征具有良好的泛化能力，并将特征映射到特征空间中的一个判别性子空间。因此，分类器可以在没有旧类别图像的情况下联合优化新旧类别的分类任务。实验结果表明，我们的方法在增量学习基准测试中达到了最先进的分类准确性，同时与保存图像的方法相比，内存占用降低了至少一个数量级。</p>
<h2 id="1-引言">1. 引言<a class="anchor-link" href="#1-引言" title="Permanent link">&para;</a></h2>
<p>深度神经网络在许多计算机视觉问题上表现出色，例如图像分类【15,22,37】和目标检测【14,31】。然而，大多数常见模型需要大量标注数据进行训练，并假设所有可能类别的数据在训练时可同时获取。  </p>
<p>与此不同的是，类别增量学习【30】处理一种场景：训练数据是按顺序接收的，先前类别的数据会随着新类别数据的到来被丢弃。因此，所有类别不会一次性学习。理想情况下，模型应该能够在学习新类别的同时保留从先前类别中学到的知识。然而，这会引发一个重要问题，因为神经网络已知会快速遗忘过去学到的知识——这种现象被称为灾难性遗忘（catastrophic forgetting）【27】。最近的方法通过增加正则化项【20,23】鼓励网络保持与其先前状态相似，或者通过保存先前类别的一部分数据【30】来缓解神经网络的灾难性遗忘问题。</p>
<p>Rebuffi 等人【30】提出了一种成功增量学习系统的一个重要标准：“计算需求和内存占用应保持有界，或者至少随着已经见过的类别数量缓慢增长。”在我们的工作中，我们显著改进了增量学习系统的内存占用。我们提出保存一部分特征描述符而非图像的策略。这使得我们能够通过低维嵌入压缩来自先前类别的信息。例如，在使用 ResNet-18 进行 ImageNet 分类时，保存一个 512 维的特征向量所需的存储仅为 256×256×3 图像存储需求的约 1%（详见 5.3 节）。实验表明，与现有方法相比，我们的方法在增量学习基准测试中实现了更好的分类准确性，同时内存占用至少降低了一个数量级。</p>
<p>我们保存特征描述符而不是图像的策略面临一个严重潜在问题：随着模型对更多类别进行训练，特征提取器会发生变化，使得先前特征提取器生成的特征描述符变得不再适用。为了解决这一问题，我们提出了一种特征适配方法，可以学习两个特征空间之间的映射。如图 1 所示，我们的新方法可以学习特征空间的变化，并将保存的特征描述符适配到新特征空间。在所有图像特征位于相同特征空间后，我们可以训练一个特征分类器，对已见类别的特征进行正确分类。</p>
<h3 id="本文贡献">本文贡献<a class="anchor-link" href="#本文贡献" title="Permanent link">&para;</a></h3>
<p>本文的主要贡献如下：</p>
<ul>
<li>我们提出了一种增量学习框架，其中保存的是先前特征描述符，而非先前图像。  </li>
<li>我们提出了一种特征适配方法，用于在模型更新时将先前的特征描述符映射到其正确值。  </li>
<li>我们在流行的类别增量学习基准测试上验证了我们的方法，并表明其在 ImageNet 上相较于其他最新方法实现了最优的准确性，同时显著降低了内存占用。</li>
</ul>
<h2 id="2-相关工作">2. 相关工作<a class="anchor-link" href="#2-相关工作" title="Permanent link">&para;</a></h2>
<p>深度学习时代之前的增量学习文献包括增量训练的支持向量机【5】、随机森林【32】以及能够推广到新类别的度量学习方法【28】。本文的重点主要放在更近期的基于深度学习的方法上。这些方法的核心通常是“重放”（rehearsal）的概念，即在用新类别数据更新模型时，保存并重用先前类别数据【33】。</p>
<h3 id="21-非重放方法">2.1 非重放方法<a class="anchor-link" href="#21-非重放方法" title="Permanent link">&para;</a></h3>
<p>非重放方法不保存先前类别的任何数据。常见方法包括：</p>
<ol>
<li>为新类别集增加网络容量【35,38】；  </li>
<li>权重合并（Weight Consolidation），即识别对先前类别重要的权重，并减缓这些权重的学习速率【20】。  </li>
</ol>
<p>Chaudhry 等人【6】通过加入基于 KL 散度的正则化项改进了权重合并方法。Liu 等人【24】通过旋转网络的参数空间，展示了权重合并在旋转参数空间中更为有效。Aljundi 等人【1】无需标注数据，以无监督的方式计算每个参数的重要性。  </p>
<p>Learning without Forgetting (LwF)【23】（将在第 3 节中详细讨论）通过在损失函数中加入知识蒸馏【16】项，缓解了灾难性遗忘。该蒸馏项鼓励新类别的网络输出接近原始网络输出。Learning without Memorizing【8】通过增加基于注意力图的蒸馏项扩展了 LwF 方法。Zhang 等人【45】指出，LwF 生成的模型通常对旧类别或新类别存在偏向性。他们为新类别训练一个单独的模型，并通过无标注辅助数据整合两个模型。Yu 等人【44】通过使用新类别质心估计特征表示漂移，更新了旧类别的质心以进行 NME 分类【30】。</p>
<h3 id="22-基于重放的样本保存">2.2 基于重放的样本保存<a class="anchor-link" href="#22-基于重放的样本保存" title="Permanent link">&para;</a></h3>
<p>Lopez-Paz 和 Ranzato【25】在梯度更新中加入约束，同时在学习新类别时将信息传递到先前类别。Rebuffi 等人提出的增量分类器与表示学习（iCARL）【30】保存了一部分图像（称为“样本”），并在更新新类别的网络时包含这些样本。样本选择通过一种高效算法“herding”实现【39】。研究表明，当使用均值类向量【28】而非网络的学习分类器进行分类时，分类准确性更高。iCARL 是文献中最有效的方法之一，将作为我们的主要基线。Castro 等人【4】通过端到端的方式学习网络和分类器，扩展了 iCARL 方法。类似地，Javed 和 Shafait【18】通过提出动态阈值移动算法，端到端学习分类器。最近的研究还包括通过纠正偏差和在损失函数中引入附加约束【2,17,41】扩展 iCARL。</p>
<h3 id="23-基于生成图像的重放">2.3 基于生成图像的重放<a class="anchor-link" href="#23-基于生成图像的重放" title="Permanent link">&para;</a></h3>
<p>这些方法使用生成模型（如 GANs【10】）生成伪造图像以模拟过去的数据，并在学习新类别时使用生成的图像更新网络【36,40】。He 等人【13】使用多个生成器增加容量，以适应新类别。然而，这些方法的主要缺点是它们通常仅适用于低分辨率图像的数据集，或者其成功依赖于将生成的图像与真实图像结合使用。</p>
<h3 id="24-基于特征的方法">2.4 基于特征的方法<a class="anchor-link" href="#24-基于特征的方法" title="Permanent link">&para;</a></h3>
<p>早期的基于特征生成的工作主要关注零样本学习【3,42】。Kemker 等人【19】使用双记忆系统，包括快速学习新类别的短期记忆和存储旧类别的长期记忆。他们在内存中存储特征向量的统计信息，例如均值向量和协方差矩阵。Xiang 等人【43】也存储特征向量的统计信息，并学习一个特征生成器生成旧类别的特征向量。这些方法的缺点在于它们依赖于预训练的网络。这与其他方法（如 LwF 和 iCARL）不同，后者从零开始学习网络。</p>
<p>本文提出了一种基于特征的重放方法。与现有基于特征的方法不同，我们不从类别统计中生成特征描述符，而是将保存的特征描述符适配到新的特征空间。我们的方法允许从头开始训练网络，不依赖预训练模型（如【19,43】）。与现有的重放方法相比，我们的方法通过保存特征而非图像显著降低了内存占用。</p>
<p>我们的方法受 Hariharan 和 Girshick 提出的特征生成器【12】启发。他们的方法通过学习类内特征变换作为少样本学习问题中的数据增强手段。我们的方法有所不同，我们在网络的两个不同增量状态下学习同一图像的特征对之间的变换函数。最后，尽管 Yu 等人【44】通过插值估计特征质心的变化，但我们的方法学习了适用于所有保存特征的通用变换函数。</p>
<h2 id="3-增量学习背景">3. 增量学习背景<a class="anchor-link" href="#3-增量学习背景" title="Permanent link">&para;</a></h2>
<p>本节介绍了增量学习任务，并总结了用于训练网络和应对灾难性遗忘的常见策略，包括知识蒸馏和旧数据的保存方法。</p>
<h3 id="31-问题定义">3.1 问题定义<a class="anchor-link" href="#31-问题定义" title="Permanent link">&para;</a></h3>
<p>我们给定一组图像 <span class="math-inline">X</span> 和其对应的类别标签 <span class="math-inline">Y</span>，这些标签属于类别集合 <span class="math-inline">C</span>。这定义了数据集 <span class="math-inline">D = {(x, y) | x \in X, y \in Y}</span>。在类别增量学习中，我们希望将一个现有模型扩展为可以分类新类别的模型。假设有 <span class="math-inline">T</span> 个任务，将 <span class="math-inline">C</span> 划分为 <span class="math-inline">T</span> 个互不重叠的子集 <span class="math-inline">C_1, C_2, \ldots, C_T</span>，其中 <span class="math-inline">C = C_1 \cup C_2 \cup \cdots \cup C_T</span>，并且对于 <span class="math-inline">i \neq j</span>，<span class="math-inline">C_i \cap C_j = \emptyset</span>。第 <span class="math-inline">t</span> 个任务引入了一个新的类别子集 <span class="math-inline">C_t</span>，使用的数据集为 <span class="math-inline">D_t = {(x, y) | y \in C_t}</span>。我们将 <span class="math-inline">X^t = {x | (x, y) \in D_t}</span> 和 <span class="math-inline">Y^t = {y | (x, y) \in D_t}</span> 分别定义为任务 <span class="math-inline">t</span> 中的训练图像和标签。目标是训练一个分类器，该分类器能够准确地分类新类别 <span class="math-inline">C_t</span> 中的样本，同时仍能正确分类属于旧类别 <span class="math-inline">C_i (i &lt; t)</span> 的样本。</p>
<h3 id="32-分类器">3.2 分类器<a class="anchor-link" href="#32-分类器" title="Permanent link">&para;</a></h3>
<p>学习的分类器通常是一个卷积神经网络（CNN），记作 <span class="math-inline">f_{\theta,W}: X \to \mathbb{R}^K</span>，其中 <span class="math-inline">K</span> 是类别的数量。可学习参数 <span class="math-inline">\theta</span> 和 <span class="math-inline">W</span> 对应网络的两个组件：特征提取器 <span class="math-inline">h_\theta: X \to \mathbb{R}^d</span> 和网络分类器 <span class="math-inline">g_W: \mathbb{R}^d \to \mathbb{R}^K</span>。特征提取器 <span class="math-inline">h_\theta</span> 将输入图像映射到 <span class="math-inline">d</span> 维特征向量，分类器 <span class="math-inline">g_W</span> 将特征向量映射到 <span class="math-inline">K</span> 维向量以提供每个类别的分类分数。整体网络 <span class="math-inline">f_{\theta,W}</span> 的映射可以表示为：<br />
<div class="math-display"><br />
    f_{\theta,W}(x) := g_W(h_\theta(x)).<br />
</div><br />
通过某种损失函数（如交叉熵损失）来训练网络参数 <span class="math-inline">\theta</span> 和 <span class="math-inline">W</span>。交叉熵损失定义为：<br />
<div class="math-display"><br />
    L_\text{CE}(x, y) := -\sum_{k=1}^K y_k \log(\sigma(f_{\theta,W}(x))_k),<br />
</div><br />
其中 <span class="math-inline">y \in \mathbb{R}^K</span> 是标签向量，<span class="math-inline">\sigma</span> 是 softmax 或 sigmoid 函数。</p>
<p>在增量学习中，随着任务的增加，模型输出的类别数量不断增长。在任务 <span class="math-inline">t</span> 中，类别总数为 <span class="math-inline">K_t = \sum_{i=1}^t |C_i|</span>。在任务 <span class="math-inline">t</span> 中，模型需要分类比任务 <span class="math-inline">t-1</span> 多出的 <span class="math-inline">|C_t|</span> 个类别。网络 <span class="math-inline">f_{\theta,W}</span> 只使用当前任务数据 <span class="math-inline">X^t</span> 进行训练，但仍需要准确分类来自所有先前任务类别的样本。</p>
<h3 id="33-知识蒸馏">3.3 知识蒸馏<a class="anchor-link" href="#33-知识蒸馏" title="Permanent link">&para;</a></h3>
<p>增量学习中的主要挑战之一是灾难性遗忘【11,27】。在任务 <span class="math-inline">t</span> 中，我们希望扩展先前模型的能力以分类新类别 <span class="math-inline">C_t</span>。我们初始化新模型 <span class="math-inline">f_\theta^t, W^t</span> 为 <span class="math-inline">f_\theta^{t-1}, W^{t-1}</span>。在任务训练开始之前，冻结上一任务模型 <span class="math-inline">f_\theta^{t-1}, W^{t-1}</span> 的副本作为参考。然而，我们只能访问当前任务数据 <span class="math-inline">X^t</span>，而无法访问先前任务的数据 <span class="math-inline">X^i (i &lt; t)</span>。当通过交叉熵损失（公式 (2)）对当前任务数据 <span class="math-inline">X^t</span> 更新网络时，其先前任务的知识会因灾难性遗忘而迅速丧失。</p>
<p>Learning without Forgetting (LwF)【23】通过引入知识蒸馏损失【16】来缓解该问题。这种损失是一种改进的交叉熵损失，鼓励当前任务模型 <span class="math-inline">f_\theta^t, W^t</span> 模仿先前任务模型 <span class="math-inline">f_\theta^{t-1}, W^{t-1}</span> 的输出：<br />
<div class="math-display"><br />
    L_\text{KD}(x) := -\sum_{k=1}^{K_{t-1}} \sigma(f_\theta^{t-1}, W^{t-1}(x))<em>k \log(\sigma(f</em>\theta^t, W^t(x))<em>k),<br />
</div><br />
其中 <span class="math-inline">x \in X^t</span>。知识蒸馏损失项与分类损失（公式 (2)）结合，构成整体损失函数：<br />
<div class="math-display"><br />
    L(x, y) := L</em>\text{CE}(x, y) + \lambda L_\text{KD}(x),<br />
</div><br />
其中 <span class="math-inline">\lambda</span> 通常设为 1【30】。注意，在任务 <span class="math-inline">t</span> 中，网络 <span class="math-inline">f_\theta^t, W^t</span> 会不断更新，而网络 <span class="math-inline">f_\theta^{t-1}, W^{t-1}</span> 保持冻结状态，并在任务 <span class="math-inline">t</span> 结束后不会再保存。</p>
<h3 id="34-旧类别数据的保存">3.4 旧类别数据的保存<a class="anchor-link" href="#34-旧类别数据的保存" title="Permanent link">&para;</a></h3>
<p>一种常见的方法是保存一些旧类别的图像，并在训练新任务时使用它们【30】。在任务 <span class="math-inline">t</span> 中，新类别数据为 <span class="math-inline">X^t</span>，旧类别数据为先前任务中看到的数据 <span class="math-inline">X^i (i &lt; t)</span>。在每个任务 <span class="math-inline">t</span> 结束后，从 <span class="math-inline">X^t</span> 中创建一个新的样本集 <span class="math-inline">P^t</span>。样本集 <span class="math-inline">P^t</span> 包含用于训练未来任务的图像子集。因此，在任务 <span class="math-inline">t</span> 中的训练会使用图像 <span class="math-inline">X^t</span> 和 <span class="math-inline">P^i (i &lt; t)</span>。</p>
<p>对旧类别数据的训练可以有效缓解先前任务类别的灾难性遗忘。在 iCARL【30】中，样本选择算法“herding”被用于生成 <span class="math-inline">P^t</span>，以使选定的样本集能够很好地近似类别的均值向量。通过这种方法，可以对存储样本的内存需求进行有效限制。</p>
<h2 id="4-高效内存增量学习">4. 高效内存增量学习<a class="anchor-link" href="#4-高效内存增量学习" title="Permanent link">&para;</a></h2>
<p>我们的目标是保存紧凑的特征描述符（即 <span class="math-inline">v := h_\theta(x)</span>），而不是来自旧类别的图像。这使我们能够显著提高内存效率，或者在相同的内存限制下为每个类别存储更多的样本。 保存特征描述符的主要挑战在于，随着特征提取器<span class="math-inline">h_\theta</span> 新数据上进行训练，特征描述符的特性可能随时间变化。这为新任务引入了问题：我们希望使用所有保存的特征描述符共同训练一个能够覆盖所有类别的特征分类器<span class="math-inline">g_{\tilde{W}}</span>，但由于<span class="math-inline">h_\theta</span> 生变化，保存的特征描述符与新任务提取的特征描述符可能不兼容。此外，如果我们无法访问旧图像，就无法重新提取特征描述符。</p>
<p>为了解决这一问题，我们提出了一种特征适配方法，该方法通过特征适配网络<span class="math-inline">\phi_\psi</span> 接更新特征描述符。每个任务的训练包括以下步骤：</p>
<ol>
<li>使用分类和蒸馏损失训练主干卷积神经网络（CNN）；</li>
<li>学习特征适配网络以适配存储的特征到当前任务的特征空间；</li>
<li>使用当前任务的特征和已适配的旧任务特征训练特征分类器。</li>
</ol>
<p>特征分类器<span class="math-inline">g_{\tilde{W}}</span> 于分类测试图像的特征。它独立于网络分类器<span class="math-inline">g_W</span>，后者用于训练网络。图 1 给出了我们方法的视觉概述，附录 A 中的算法 1 提供了完整步骤。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20250113164527.png" style="zoom: 80%;" /></div>

<h3 id="41-网络训练">4.1 网络训练<a class="anchor-link" href="#41-网络训练" title="Permanent link">&para;</a></h3>
<p>这一部分描述了主干卷积神经网络<span class="math-inline">f_{\theta,W}</span> 训练过程。我们的实现与第 3 节中的训练设置相同，但增加了两个额外组件：<strong>余弦归一化</strong>和<strong>特征蒸馏</strong>。</p>
<p><strong>余弦归一化</strong>：在多个学习任务中提出过【23,26】，包括增量学习【17】。网络预测（公式 (1)）基于余弦相似性，而非简单的点积。这等价于<span class="math-inline">\hat{W}^\top \hat{v}</span>，其中<span class="math-inline">\hat{W}</span> 分类器参数<span class="math-inline">W</span> 列方向<span class="math-inline">\ell_2</span> 一化版本，<span class="math-inline">\hat{v}</span> 特征向量<span class="math-inline">v</span> <span class="math-inline">\ell_2</span> 一化版本。 </p>
<p><strong>特征蒸馏</strong>：特征描述符上的附加蒸馏项。类似于公式 (3)，在损失函数中增加一个约束，鼓励新特征提取器<span class="math-inline">h_\theta^t</span> 仿旧特征提取器<span class="math-inline">h_\theta^{t-1}</span> 输出：<br />
<div class="math-display"><br />
    L_\text{FD}(x) := 1 - \cos(h_\theta^t(x), h_\theta^{t-1}(x)),<br />
</div><br />
其中<span class="math-inline">x \in X^t</span>，<span class="math-inline">h_\theta^{t-1}</span> 上一任务冻结的特征提取器。特征蒸馏损失与其他损失项一起最小化，形成总损失函数：<br />
<div class="math-display"><br />
    L(x, y) := L_\text{CE}(x, y) + \lambda L_\text{KD}(x) + \gamma L_\text{FD}(x),<br />
</div><br />
其中<span class="math-inline">\gamma</span> 需要调节的超参数，其影响将在 5.4 节中讨论。 </p>
<p>特征蒸馏已经被用于增量学习中，替代知识蒸馏损失【17】，但仅用于保存图像的特征向量。它也与注意力蒸馏【8】在理念上相似，后者对两个模型生成的注意力图增加约束。 余弦归一化和特征蒸馏提高了我们方法和基线的准确性。其实际影响将在 5 节中详细研究。 </p>
<h3 id="42-特征适配">4.2 特征适配<a class="anchor-link" href="#42-特征适配" title="Permanent link">&para;</a></h3>
<p><strong>概述</strong> 特征适配在每个任务的 CNN 训练完成后进行。以下我们首先描述针对前两个任务的特征适配过程，然后将其扩展到后续任务。 </p>
<p>在任务<span class="math-inline">t = 1</span> ，网络用属于类别<span class="math-inline">C_1</span> 图像<span class="math-inline">X^1</span> 行训练。训练完成后，我们提取特征描述符<span class="math-inline">V^1 = {h_\theta^1(x) | x \in X^1}</span>，其中<span class="math-inline">h_\theta^1(x)</span> <span class="math-inline">f_{\theta,W}^1</span> 特征提取器组件。我们将这些特征存储到内存<span class="math-inline">M^1 = V^1</span> ，并根据内存需求减少存储的特征数量（这一过程将在本节稍后解释）。 在任务<span class="math-inline">t = 2</span> ，我们获得属于新类别<span class="math-inline">C_2</span> 图像<span class="math-inline">X^2</span>。网络<span class="math-inline">f_{\theta,W}^2</span> <span class="math-inline">f_{\theta,W}^1</span> 始化，并在训练期间固定<span class="math-inline">f_{\theta,W}^1</span> 为参考（使用公式 (6) 中的损失）。训练完成后，我们提取新特征<span class="math-inline">V^2 = {h_\theta^2(x) | x \in X^2}</span>。 </p>
<p>此时我们有两个特征集合：来自任务 1 的<span class="math-inline">M^1</span> 来自任务 2 的<span class="math-inline">V^2</span>。需要注意的是，这两个集合的特征分别由不同的特征提取器<span class="math-inline">h_\theta^1</span> <span class="math-inline">h_\theta^2</span> 取，因此它们位于不同的特征空间中，互不兼容。因此，我们必须将<span class="math-inline">M^1</span> 换到与<span class="math-inline">V^2</span> 同的特征空间。为此，我们训练一个特征适配网络<span class="math-inline">\phi_\psi^{1 \to 2}</span>，将<span class="math-inline">M^1</span> 射到与<span class="math-inline">V^2</span> 同的空间（训练过程将在下文中描述）。 </p>
<p>一旦特征适配网络训练完成，我们使用<span class="math-inline">\phi_\psi^{1 \to 2}</span> <span class="math-inline">M^1</span> 换到当前特征空间，并创建一个新的内存集<span class="math-inline">M^2 = V^2 \cup \phi_\psi^{1 \to 2}(M^1)</span>。最终的<span class="math-inline">M^2</span> 含来自当前任务的新特征和来自前一个任务的适配特征，可用于训练判别特征分类器（详见第 4.3 节）。此时，<span class="math-inline">M^1</span> <span class="math-inline">f_{\theta,W}^1</span> 不再需要保存。 对于后续任务<span class="math-inline">t &gt; 2</span>，我们遵循相同的过程。给定新任务的图像<span class="math-inline">X^t</span> 类别<span class="math-inline">C_t</span>，我们完成训练后提取特征<span class="math-inline">V^t = {h_\theta^t(x) | x \in X^t}</span>，训练一个特征适配网络<span class="math-inline">\phi_\psi^{(t-1) \to t}</span>，并使用该网络生成内存集<span class="math-inline">M^t = V^t \cup \phi_\psi^{(t-1) \to t}(M^{t-1})</span>。最终，<span class="math-inline">M^t</span> 含所有任务<span class="math-inline">i \leq t</span> 类别的特征，全部转换到当前任务的特征空间。此时，<span class="math-inline">M^{t-1}</span> <span class="math-inline">f_{\theta,W}^{t-1}</span> 再需要保存。 </p>
<p><strong>特征适配网络的训练</strong> 在任务<span class="math-inline">t</span> ，我们通过训练一个变换函数<span class="math-inline">\phi_\psi^{(t-1) \to t}: \mathbb{R}^d \to \mathbb{R}^d</span>，将先前的特征提取器<span class="math-inline">h_\theta^{t-1}</span> 输出映射到当前特征提取器<span class="math-inline">h_\theta^t</span> 输出。为此，我们使用当前任务数据<span class="math-inline">X^t</span>。 令<span class="math-inline">V' = {(h_\theta^{t-1}(x), h_\theta^t(x)) | x \in X^t}</span>，其中<span class="math-inline">(v, v') \in V'</span>。对于每个<span class="math-inline">x \in X^t</span>，<span class="math-inline">v</span> 示其由<span class="math-inline">h_\theta^{t-1}</span> 取的特征，<span class="math-inline">v'</span> 示由<span class="math-inline">h_\theta^t</span> 取的特征。通过学习<span class="math-inline">v</span> <span class="math-inline">v'</span> 间的映射，我们可以将内存中的特征<span class="math-inline">M^{t-1}</span> 换到与<span class="math-inline">V^t</span> 同的特征空间。 训练特征适配网络<span class="math-inline">\phi_\psi^{(t-1) \to t}</span> 损失函数定义为【12】：<br />
<div class="math-display"><br />
    L_\text{fa}(v, v', y) := \alpha L_\text{sim}(v', \phi_\psi(v)) + L_\text{cls}(g_W, \phi_\psi(v), y),<br />
</div><br />
其中<span class="math-inline">y</span> 特征<span class="math-inline">v</span> 应的类别标签。 <br />
- 第一项<span class="math-inline">L_\text{sim}(v', \phi_\psi(v)) = 1 - \cos(v', \phi_\psi(v))</span> 励适配特征<span class="math-inline">\phi_\psi(v)</span> 目标特征<span class="math-inline">v'</span> 似。这与特征蒸馏损失（公式 (5)）相同。 <br />
- 第二项<span class="math-inline">L_\text{cls}(g_W, \phi_\psi(v), y)</span> 交叉熵损失，<span class="math-inline">g_W</span> 固定的网络分类器。该项鼓励适配后的特征属于正确的类别<span class="math-inline">y</span>。 </p>
<p><strong>减少<span class="math-inline">M^t</span> 大小</strong> 为了满足特定的内存需求，可以减少内存集中特征的数量。我们使用 herding 算法【30,39】选择与类别均值最接近的特征子集。在更新任务<span class="math-inline">t</span> 的内存时，我们仅保留每个类别固定数量<span class="math-inline">L</span> 特征。 </p>
<h3 id="43-特征分类器的训练">4.3 特征分类器的训练<a class="anchor-link" href="#43-特征分类器的训练" title="Permanent link">&para;</a></h3>
<p>我们的目标是分类属于<span class="math-inline">K_t = \sum_{i=1}^t |C_i|</span> 类别的测试图像，其中包括所有已学习任务的类别。正如第 3 节所述，学习到的网络<span class="math-inline">f_{\theta,W}^t</span> 一个从图像到<span class="math-inline">K_t</span> 别的映射，可用于对测试图像进行分类。然而，由于网络仅在当前任务的数据<span class="math-inline">X^t</span> 训练，即使在训练过程中使用了蒸馏损失（公式 (5)），对先前任务的遗忘仍然不可避免，这导致模型的性能次优。 </p>
<p>我们通过利用来自先前任务的适配特征描述符训练一个更准确的特征分类器来解决这个问题。 </p>
<p>在任务<span class="math-inline">t</span> 束时，我们使用内存<span class="math-inline">M^t</span> 练一个新的特征分类器<span class="math-inline">g_{\tilde{W}}^t</span>，内存<span class="math-inline">M^t</span> 含来自先前任务的适配特征描述符以及当前任务的新特征描述符。这与网络分类器<span class="math-inline">g_W^t</span> 同，后者是网络<span class="math-inline">f_{\theta,W}^t</span> 一部分。对于测试图像，我们提取其特征表示<span class="math-inline">h_\theta^t(x)</span>，并使用特征分类器<span class="math-inline">g_{\tilde{W}}^t</span> 其进行分类。 在实际操作中，特征分类器<span class="math-inline">g_{\tilde{W}}^t</span> 一个线性分类器，可以通过多种方式训练，例如线性支持向量机（SVM）或随机梯度下降（SGD）等。在我们的实验中，特征分类器<span class="math-inline">g_{\tilde{W}}^t</span> 使用线性 SVM 进行训练。</p>
<h2 id="5-实验">5. 实验<a class="anchor-link" href="#5-实验" title="Permanent link">&para;</a></h2>
<p>我们描述了实验设置，然后在每个数据集上报告分类准确性的结果。此外，我们测量了特征适配方法的质量，这是独立于分类任务的性能指标。最后，我们详细研究了关键实现选择和参数的影响。</p>
<h3 id="51-实验设置">5.1 实验设置<a class="anchor-link" href="#51-实验设置" title="Permanent link">&para;</a></h3>
<p><strong>数据集</strong><br />
我们在 CIFAR-100【21】、ImageNet-100 和 ImageNet-1000 上进行了实验。ImageNet-100【30】是 ImageNet-1000 数据集【34】的一个子集，包含从原始 1000 个类别中随机抽样的 100 个类别。我们的实验设置与 iCARL【30】一致。网络以类别增量的方式进行训练，每次仅考虑当前任务可用的数据。我们用 <span class="math-inline">M</span> 表示每次任务中的类别数，总任务数用 <span class="math-inline">T</span> 表示。在每个任务后，对所有已学习的类别进行分类。CIFAR-100 和 ImageNet-100 的每个实验都重复 5 次，每次使用随机的类别顺序，结果取 5 次运行的平均值。</p>
<p><strong>评价指标</strong><br />
我们报告两个评价指标：<br />
1. 随任务增加的所有类别分类准确性曲线；<br />
2. 平均增量准确性（average incremental accuracy），即第一项曲线中的点的平均值。CIFAR-100 使用 Top-1 准确性，ImageNet 使用 Top-5 准确性。</p>
<p><strong>基线方法</strong><br />
我们的主要基线是文献中扩展的两种方法：Learning Without Forgetting (LwF)【23】和 iCARL【30】。<br />
- LwF 不保存先前任务的数据，通过分类和蒸馏损失（公式 (4)）进行训练。我们使用 Rebuffi 等人【30】提出的多分类版本（LwF.MC）。<br />
- iCARL 在先前任务中保存代表性训练图像作为样本集，并在更新网络时包含这些样本。我们在实现时将 iCARL 中每类样本数量 <span class="math-inline">P</span> 固定，与原始方法不同（原始方法中总样本数固定，随类别数量变化调整每类样本数）。  </p>
<p>我们还对 iCARL 和 LwF 进行了扩展，添加了余弦归一化和特征蒸馏损失（详见第 4.1 节），这些改进已被证明可以提高准确性。改进后的方法分别称为 <span class="math-inline">\gamma</span>-iCARL 和 <span class="math-inline">\gamma</span>-LwF，其中 <span class="math-inline">\gamma</span> 是控制特征蒸馏（公式 (5)）的参数。</p>
<p><strong>实现细节</strong><br />
- 特征提取网络 <span class="math-inline">h_\theta</span> 为 ResNet-32【15】（CIFAR-100）和 ResNet-18【15】（ImageNet-100 和 ImageNet-1000）。<br />
- 特征分类器 <span class="math-inline">g_{\tilde{W}}</span> 使用线性 SVM【7,29】。<br />
- 特征适配网络 <span class="math-inline">\phi_\psi</span> 是一个两层多层感知机（MLP），带有 ReLU 激活【9】，输入/输出维度为 <span class="math-inline">d</span>，隐藏层维度为 <span class="math-inline">16d</span>。<br />
- 损失函数（公式 (4)）中的知识蒸馏权重 <span class="math-inline">\lambda</span> 设置为 1，因此激活函数 <span class="math-inline">\sigma</span> 为 sigmoid。  </p>
<p>其他超参数与 Rebuffi 等人【30】一致：<br />
- 批大小为 128，权重衰减为 <span class="math-inline">1 \times 10^{-5}</span>，学习率为 2.0。<br />
- CIFAR-100 的每个任务训练 70 个 epoch，在第 50 和 64 个 epoch 将学习率降低 5 倍；<br />
- ImageNet 训练 60 个 epoch，在第 20、30、40 和 50 个 epoch 将学习率降低 5 倍。</p>
<h3 id="52-内存占用的影响">5.2 内存占用的影响<a class="anchor-link" href="#52-内存占用的影响" title="Permanent link">&para;</a></h3>
<p>我们的主要目标是改进增量学习框架的内存需求。我们首先在内存占用和平均增量准确性之间比较我们的方法与基线方法。内存占用是指为数据集的所有类别保存的数据（特征或图像）的大小。</p>
<p>图 2 显示了不同方法在 CIFAR-100、ImageNet-100 和 ImageNet-1000 上的内存占用与平均增量准确性的关系。对于 <span class="math-inline">\gamma</span>-iCARL，通过更改每个类别保存的图像数量 <span class="math-inline">P</span> 来调整内存需求。对于我们的方法，通过更改每个类别保存的特征描述符数量 <span class="math-inline">L</span> 调整内存需求。我们还展示了一种混合变体 Ours-hybrid，其中保存 <span class="math-inline">P</span> 张图像和 <span class="math-inline">L</span> 个特征描述符。在这种变体中，我们固定 <span class="math-inline">L = 100</span>（CIFAR-100）或 <span class="math-inline">L = 250</span>（ImageNet-100 和 ImageNet-1000），并调整 <span class="math-inline">P</span> 以满足内存要求。</p>
<p>从图 2 可以看出，在 CIFAR-100 上，我们的方法依然可以实现较高的准确性，但内存节省相对有限。这是因为 CIFAR-100 图像分辨率较低（32×32×3，3.072 KB），而保存特征描述符（<span class="math-inline">d = 64</span> 个浮点数，占 0.256 KB）对内存的影响较小。然而，由于 CIFAR-100 计算复杂度较低，我们主要利用其进行超参数调优（详见第 5.4 节）。对于 ImageNet 数据集，我们的方法在内存节省方面优势更明显。ImageNet 图像分辨率为 256×256×3，存储单个图像需占用 192 KB，而存储一个 <span class="math-inline">d = 512</span> 的特征描述符仅需 2 KB，仅为图像存储需求的约 1%。  </p>
<p>即使忽略特征相较于图像的内存节省，我们的方法在大多数情况下仍然优于 <span class="math-inline">\gamma</span>-iCARL。对于 ImageNet-1000，当每个任务的类别数 <span class="math-inline">M \geq 100</span> 时，我们的方法的准确性明显优于 <span class="math-inline">\gamma</span>-iCARL，同时内存需求减少至少一个数量级。</p>
<p>图 3 显示了 ImageNet-1000 上保存的特征数量（我们的方法）和图像数量（<span class="math-inline">\gamma</span>-iCARL）对平均增量准确性的影响。即使仅从保存的数据点数量的角度出发，我们的方法在大多数情况下仍优于 <span class="math-inline">\gamma</span>-iCARL。</p>
<h3 id="53-与现有方法的比较">5.3 与现有方法的比较<a class="anchor-link" href="#53-与现有方法的比较" title="Permanent link">&para;</a></h3>
<p>表 1 显示了我们的方法与文献中现有方法在内存总占用和平均增量准确性方面的对比。图 4 展示了每个任务的分类准确性曲线。我们报告了在每类保存 <span class="math-inline">L = 250</span> 个特征的情况下，我们方法的平均增量准确性。Ours-hybrid 是我们的混合变体，保存 <span class="math-inline">L = 250</span> 个特征和 <span class="math-inline">P = 10</span> 张图像，而基线和现有方法保存 <span class="math-inline">P = 20</span> 张图像。可以明显看出，我们的方法是首个采用特征描述符保存并适配的工作，在内存使用显著减少的同时，分类准确性有所提高。</p>
<hr />
<h4 id="表-1imagenet-100-和-imagenet-1000-的平均增量准确性和内存占用">表 1：ImageNet-100 和 ImageNet-1000 的平均增量准确性和内存占用<a class="anchor-link" href="#表-1imagenet-100-和-imagenet-1000-的平均增量准确性和内存占用" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>方法</th>
<th>ImageNet-100 内存（MB）</th>
<th>ImageNet-100 准确性</th>
<th>ImageNet-1000 内存（MB）</th>
<th>ImageNet-1000 准确性</th>
</tr>
</thead>
<tbody>
<tr>
<td>原始 LwF【23】†*</td>
<td>-</td>
<td>0.642</td>
<td>-</td>
<td>0.566</td>
</tr>
<tr>
<td>原始 iCARL【30】†*</td>
<td>375</td>
<td>0.836</td>
<td>3750</td>
<td>0.637</td>
</tr>
<tr>
<td>EEIL【4】†</td>
<td>375</td>
<td>0.904</td>
<td>3750</td>
<td>0.694</td>
</tr>
<tr>
<td>重平衡（Rebalancing）【17】</td>
<td>375</td>
<td>0.681</td>
<td>3750</td>
<td>0.643</td>
</tr>
<tr>
<td>BiC（不校正）【41】†</td>
<td>375</td>
<td>0.872</td>
<td>3750</td>
<td>0.776</td>
</tr>
<tr>
<td>BiC【41】†</td>
<td>375</td>
<td>0.906</td>
<td>3750</td>
<td>0.840</td>
</tr>
<tr>
<td><span class="math-inline">\gamma</span>-LwF</td>
<td>-</td>
<td>0.891</td>
<td>-</td>
<td>0.804</td>
</tr>
<tr>
<td><span class="math-inline">\gamma</span>-iCARL</td>
<td>375</td>
<td>0.914</td>
<td>3750</td>
<td>0.802</td>
</tr>
<tr>
<td><strong>Ours</strong></td>
<td><strong>48.8</strong></td>
<td><strong>0.913</strong></td>
<td><strong>488.3</strong></td>
<td><strong>0.843</strong></td>
</tr>
<tr>
<td><strong>Ours-hybrid</strong></td>
<td><strong>236.3</strong></td>
<td><strong>0.927</strong></td>
<td><strong>2863.3</strong></td>
<td><strong>0.846</strong></td>
</tr>
</tbody>
</table>
<p><strong>注释</strong>：<br />
- † 表示结果来自原文。<br />
- * 表示数字从原文图表中估计得出。  </p>
<hr />
<p>从结果可以看出，与保存图像的方法相比，我们的方法（基于特征保存和适配）在内存占用方面显著降低，同时在准确性上与最先进的方法持平或更优。此外，Ours-hybrid 通过结合图像和特征保存进一步提升了准确性。</p>
<h3 id="54-参数的影响">5.4 参数的影响<a class="anchor-link" href="#54-参数的影响" title="Permanent link">&para;</a></h3>
<p>我们分析了方法中关键超参数的影响。本节所有实验均在 CIFAR-100 数据集上进行，其中 10% 的原始训练数据被保留作为验证集。</p>
<hr />
<p><strong>余弦分类器的影响</strong><br />
我们在基础网络（即 LwF.MC）上评估了余弦分类器的效果。当使用余弦分类器时，平均增量准确性为 48.7%，而不使用时为 45.2%。因此，我们在所有基线和方法中都包含了余弦分类器。</p>
<hr />
<p><strong>参数 <span class="math-inline">\alpha</span> 的影响</strong><br />
参数 <span class="math-inline">\alpha</span> 控制特征适配网络训练时，相似性项（公式 (7)）与分类项的相对重要性。图 5 顶部 (a) 展示了不同 <span class="math-inline">\alpha</span> 值下的准确性。重构约束（由 <span class="math-inline">\alpha</span> 控制）需要一个较大的值。我们在实验中将 <span class="math-inline">\alpha</span> 设置为 <span class="math-inline">10^2</span>。</p>
<hr />
<p><strong>隐藏层维度 <span class="math-inline">d'</span> 的影响</strong><br />
我们评估了特征适配网络 <span class="math-inline">\phi_\psi</span> 中隐藏层维度 <span class="math-inline">d'</span> 的影响（图 5 顶部 (b)）。将特征向量投影到更高维空间是有益的，验证集上的准确性在 <span class="math-inline">d' = 1024</span> 时达到最大值。在实验中，我们设置 <span class="math-inline">d' = 16d</span>。</p>
<hr />
<p><strong>网络深度的影响</strong><br />
我们研究了特征适配网络 <span class="math-inline">\phi_\psi</span> 中隐藏层数量的影响（图 5 顶部 (c)）。准确性在隐藏层数量为 2 时达到峰值，此后开始下降，可能是由于网络的过拟合。在实验中，我们使用了 2 个隐藏层。</p>
<hr />
<p><strong>特征蒸馏的影响</strong><br />
我们对特征蒸馏（公式 (5)）的超参数 <span class="math-inline">\gamma</span> 进行了评估，分别在 <span class="math-inline">\gamma</span>-LwF 和我们的方法中测试（图 5 顶部 (d)）。在实验中，我们将 <span class="math-inline">\gamma</span> 设置为 0.05，并在所有基线和方法中包含特征蒸馏。</p>
<hr />
<p><strong>特征适配质量</strong><br />
我们通过测量适配特征与其真实值之间的平均相似性，评估特征适配过程的质量。给定图像 <span class="math-inline">x</span>，真实向量 <span class="math-inline">h_\theta^t(x)</span> 表示如果在任务 <span class="math-inline">t</span> 中仍然拥有图像 <span class="math-inline">x</span> 时的特征表示，而适配特征为内存中的对应向量 <span class="math-inline">v</span>。特征适配质量通过内积计算：<br />
<div class="math-display"><br />
    \omega = v^\top h_\theta^t(x)。<br />
</div><br />
这衡量了适配特征相对于真实特征的准确性。我们重复验证实验，测量所有特征的平均 <span class="math-inline">\omega</span> 而非准确性（图 5 底部）。图中可以看出，大多数趋势是相关的，即更好的特征适配带来了更高的准确性。一个主要的例外是特征蒸馏（公式 (5)）中 <span class="math-inline">\gamma</span> 的行为：较大的 <span class="math-inline">\gamma</span> 会提高 <span class="math-inline">\omega</span>，但会降低分类准确性。这是可以预期的，因为较大的 <span class="math-inline">\gamma</span> 强迫网络在不同任务之间的特征提取器变化最小，从而使特征适配更成功，但特征表示的判别性降低。</p>
<hr />
<p><strong>平衡特征分类器的影响</strong><br />
Wu 等人【41】指出，基于类别不平衡的训练会导致对新类别的偏置。我们在附录 C 中研究了这一问题。实验表明，当使用平衡的类别样本数训练特征分类器 <span class="math-inline">g_{\tilde{W}}</span> 时，准确性会有所提升。</p>
<h2 id="6-结论">6. 结论<a class="anchor-link" href="#6-结论" title="Permanent link">&para;</a></h2>
<p>我们提出了一种新颖的增量学习方法，该方法保存特征描述符而非图像。我们的方法引入了特征适配函数，该函数可以在模型更新新类别时准确地更新保存的特征描述符。通过对分类准确性和特征适配质量的全面评估，实验结果表明：在显著降低内存占用的同时，我们的方法能够实现最先进的分类准确性。</p>
<p>此外，我们的方法与现有工作【23,30】是正交的，可以结合使用以进一步提高准确性，同时保持低内存需求。</p>
                </div>

                <!-- Comments Section (Giscus) -->
                <section class="comments-section">
                    <h3><i class="fas fa-comments"></i> 评论</h3>
                    <script src="https://giscus.app/client.js"
                        data-repo="Geeks-Z/Geeks-Z.github.io"
                        data-repo-id=""
                        data-category="Announcements"
                        data-category-id=""
                        data-mapping="pathname"
                        data-strict="0"
                        data-reactions-enabled="1"
                        data-emit-metadata="0"
                        data-input-position="bottom"
                        data-theme="preferred_color_scheme"
                        data-lang="zh-CN"
                        crossorigin="anonymous"
                        async>
                    </script>
                </section>
            </article>

            <footer class="post-footer">
                <p>© 2025 Hongwei Zhao. Built with ❤️</p>
            </footer>
        </main>
    </div>

    <!-- Theme Toggle Button -->
    <button class="theme-toggle" id="themeToggle" aria-label="切换主题">
        <i class="fas fa-moon"></i>
    </button>

    <script>
        // Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        const html = document.documentElement;
        const icon = themeToggle.querySelector('i');
        const hljsDark = document.getElementById('hljs-theme-dark');
        const hljsLight = document.getElementById('hljs-theme-light');
        
        // Check saved theme or system preference
        const savedTheme = localStorage.getItem('theme');
        const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
        
        if (savedTheme === 'dark' || (!savedTheme && prefersDark)) {
            html.setAttribute('data-theme', 'dark');
            icon.className = 'fas fa-sun';
            hljsDark.disabled = false;
            hljsLight.disabled = true;
        }
        
        themeToggle.addEventListener('click', () => {
            const isDark = html.getAttribute('data-theme') === 'dark';
            if (isDark) {
                html.removeAttribute('data-theme');
                icon.className = 'fas fa-moon';
                localStorage.setItem('theme', 'light');
                hljsDark.disabled = true;
                hljsLight.disabled = false;
            } else {
                html.setAttribute('data-theme', 'dark');
                icon.className = 'fas fa-sun';
                localStorage.setItem('theme', 'dark');
                hljsDark.disabled = false;
                hljsLight.disabled = true;
            }
            
            // Update Giscus theme
            const giscusFrame = document.querySelector('iframe.giscus-frame');
            if (giscusFrame) {
                giscusFrame.contentWindow.postMessage({
                    giscus: {
                        setConfig: {
                            theme: isDark ? 'light' : 'dark'
                        }
                    }
                }, 'https://giscus.app');
            }
        });
        
        // Highlight.js
        document.addEventListener('DOMContentLoaded', () => {
            hljs.highlightAll();
        });
        
        // KaTeX auto-render
        document.addEventListener('DOMContentLoaded', () => {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
        
        // TOC active state
        const tocLinks = document.querySelectorAll('.toc-container a');
        const headings = document.querySelectorAll('.post-content h2, .post-content h3, .post-content h4');
        
        function updateTocActive() {
            let current = '';
            headings.forEach(heading => {
                const rect = heading.getBoundingClientRect();
                if (rect.top <= 100) {
                    current = heading.getAttribute('id');
                }
            });
            
            tocLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === '#' + current) {
                    link.classList.add('active');
                }
            });
        }
        
        window.addEventListener('scroll', updateTocActive);
        updateTocActive();
    </script>
</body>
</html>
