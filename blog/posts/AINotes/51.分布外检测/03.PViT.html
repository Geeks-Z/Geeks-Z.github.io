<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Untitled</title>
    <meta name="description" content="Untitled - Hongwei Zhao's Blog">
    <meta name="author" content="Hongwei Zhao">
    
    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <!-- Code Highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css" id="hljs-theme-dark">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css" id="hljs-theme-light" disabled>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    
    <style>
        :root {
            /* Light Theme */
            --primary-color: #2980b9;
            --primary-hover: #1a5276;
            --link-color: #c0392b;
            --text-color: #333;
            --text-light: #666;
            --text-muted: #999;
            --bg-color: #fff;
            --bg-secondary: #f8f9fa;
            --bg-code: #f5f5f5;
            --border-color: #e5e7eb;
            --shadow: 0 1px 3px rgba(0,0,0,0.1);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.1);
        }

        [data-theme="dark"] {
            --primary-color: #5dade2;
            --primary-hover: #85c1e9;
            --link-color: #e74c3c;
            --text-color: #e5e7eb;
            --text-light: #9ca3af;
            --text-muted: #6b7280;
            --bg-color: #1a1a2e;
            --bg-secondary: #16213e;
            --bg-code: #0f0f23;
            --border-color: #374151;
            --shadow: 0 1px 3px rgba(0,0,0,0.3);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.4);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 16px;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            transition: background-color 0.3s, color 0.3s;
        }

        a {
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--primary-hover);
            text-decoration: underline;
        }

        /* Layout */
        .page-wrapper {
            display: flex;
            max-width: 1400px;
            margin: 0 auto;
            padding: 2rem;
            gap: 3rem;
        }

        /* Sidebar TOC */
        .toc-sidebar {
            width: 260px;
            flex-shrink: 0;
            position: sticky;
            top: 2rem;
            height: fit-content;
            max-height: calc(100vh - 4rem);
            overflow-y: auto;
        }

        .toc-container {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 1.5rem;
            border: 1px solid var(--border-color);
        }

        .toc-container h3 {
            font-size: 0.85rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-muted);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .toc-container ul {
            list-style: none;
        }

        .toc-container li {
            margin-bottom: 0.5rem;
        }

        .toc-container a {
            font-size: 0.9rem;
            color: var(--text-light);
            display: block;
            padding: 0.25rem 0;
            border-left: 2px solid transparent;
            padding-left: 0.75rem;
            transition: all 0.2s;
        }

        .toc-container a:hover,
        .toc-container a.active {
            color: var(--primary-color);
            border-left-color: var(--primary-color);
            text-decoration: none;
        }

        .toc-container ul ul {
            margin-left: 1rem;
        }

        /* Main Content */
        .main-content {
            flex: 1;
            min-width: 0;
            max-width: 800px;
        }

        /* Header */
        .post-header {
            margin-bottom: 2rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
            margin-bottom: 1.5rem;
        }

        .back-link:hover {
            color: var(--primary-color);
            text-decoration: none;
        }

        .post-header h1 {
            font-size: 2.25rem;
            font-weight: 700;
            line-height: 1.3;
            margin-bottom: 1rem;
            color: var(--text-color);
        }

        .post-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
        }

        .post-meta span {
            display: flex;
            align-items: center;
            gap: 0.4rem;
        }

        .post-meta i {
            color: var(--text-muted);
        }

        .tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 1rem;
        }

        .tag {
            display: inline-block;
            font-size: 0.8rem;
            background: var(--bg-secondary);
            color: var(--text-light);
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            border: 1px solid var(--border-color);
            transition: all 0.2s;
        }

        .tag:hover {
            background: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }

        /* Article Content */
        .post-content {
            font-size: 1rem;
            line-height: 1.9;
        }

        .post-content h1,
        .post-content h2,
        .post-content h3,
        .post-content h4,
        .post-content h5,
        .post-content h6 {
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-weight: 600;
            line-height: 1.4;
            color: var(--text-color);
        }

        .post-content h1 { font-size: 1.875rem; }
        .post-content h2 { 
            font-size: 1.5rem; 
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }
        .post-content h3 { font-size: 1.25rem; }
        .post-content h4 { font-size: 1.125rem; }

        .post-content p {
            margin-bottom: 1.25rem;
        }

        .post-content ul,
        .post-content ol {
            margin: 1.25rem 0;
            padding-left: 1.5rem;
        }

        .post-content li {
            margin-bottom: 0.5rem;
        }

        .post-content blockquote {
            margin: 1.5rem 0;
            padding: 1rem 1.5rem;
            background: var(--bg-secondary);
            border-left: 4px solid var(--primary-color);
            border-radius: 0 8px 8px 0;
            color: var(--text-light);
            font-style: italic;
        }

        .post-content blockquote p:last-child {
            margin-bottom: 0;
        }

        /* Code */
        .post-content code {
            font-family: 'JetBrains Mono', 'Fira Code', Consolas, monospace;
            font-size: 0.9em;
            background: var(--bg-code);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            color: var(--link-color);
        }

        .post-content pre {
            margin: 1.5rem 0;
            padding: 1.25rem;
            background: var(--bg-code);
            border-radius: 10px;
            overflow-x: auto;
            border: 1px solid var(--border-color);
        }

        .post-content pre code {
            background: none;
            padding: 0;
            color: inherit;
            font-size: 0.875rem;
            line-height: 1.6;
        }

        /* Images */
        .post-content img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            margin: 1.5rem 0;
            box-shadow: var(--shadow-lg);
        }

        /* Tables */
        .post-content table {
            width: 100%;
            margin: 1.5rem 0;
            border-collapse: collapse;
            font-size: 0.95rem;
        }

        .post-content th,
        .post-content td {
            padding: 0.75rem 1rem;
            border: 1px solid var(--border-color);
            text-align: left;
        }

        .post-content th {
            background: var(--bg-secondary);
            font-weight: 600;
        }

        .post-content tr:nth-child(even) {
            background: var(--bg-secondary);
        }

        /* Math */
        .math-display {
            margin: 1.5rem 0;
            overflow-x: auto;
            padding: 1rem;
            background: var(--bg-secondary);
            border-radius: 8px;
        }

        /* Anchor links */
        .anchor-link {
            opacity: 0;
            margin-left: 0.5rem;
            color: var(--text-muted);
            font-weight: 400;
            transition: opacity 0.2s;
        }

        .post-content h2:hover .anchor-link,
        .post-content h3:hover .anchor-link,
        .post-content h4:hover .anchor-link {
            opacity: 1;
        }

        /* Theme Toggle */
        .theme-toggle {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            background: var(--primary-color);
            color: white;
            border: none;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.25rem;
            box-shadow: var(--shadow-lg);
            transition: transform 0.2s, background 0.2s;
            z-index: 1000;
        }

        .theme-toggle:hover {
            transform: scale(1.1);
            background: var(--primary-hover);
        }

        /* Comments Section */
        .comments-section {
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-color);
        }

        .comments-section h3 {
            font-size: 1.25rem;
            margin-bottom: 1.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        /* Footer */
        .post-footer {
            margin-top: 3rem;
            padding-top: 1.5rem;
            border-top: 1px solid var(--border-color);
            text-align: center;
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        /* Responsive */
        @media (max-width: 1024px) {
            .toc-sidebar {
                display: none;
            }
            
            .page-wrapper {
                padding: 1.5rem;
            }
        }

        @media (max-width: 768px) {
            .post-header h1 {
                font-size: 1.75rem;
            }
            
            .post-meta {
                flex-direction: column;
                gap: 0.5rem;
            }
            
            .theme-toggle {
                bottom: 1rem;
                right: 1rem;
                width: 44px;
                height: 44px;
            }
        }

        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-secondary);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--border-color);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--text-muted);
        }
    </style>
</head>
<body>
    <div class="page-wrapper">
        <!-- TOC Sidebar -->
        <aside class="toc-sidebar">
            <div class="toc-container">
                <h3><i class="fas fa-list"></i> 目录</h3>
                <div class="toc">
<ul>
<li><a href="#pvit-prior-augmented-vision-transformer-for-out-of-distribution-detection">PViT: Prior-augmented Vision Transformer for Out-of-distribution Detection</a></li>
<li><a href="#0-摘要">0. 摘要</a></li>
<li><a href="#1-引言">1. 引言</a></li>
<li><a href="#2-相关工作">2. 相关工作</a><ul>
<li><a href="#21-分布外ood检测">2.1. 分布外（OOD）检测</a></li>
<li><a href="#22-视觉-transformer">2.2. 视觉 Transformer</a></li>
</ul>
</li>
<li><a href="#3-方法论">3. 方法论</a><ul>
<li><a href="#31-预备知识">3.1. 预备知识</a></li>
<li><a href="#32-先验增强视觉-transformerpvit">3.2. 先验增强视觉 Transformer（PViT）</a></li>
<li><a href="#33-先验引导能量用于-ood-检测">3.3. 先验引导能量用于 OOD 检测</a></li>
</ul>
</li>
<li><a href="#4-实验">4. 实验</a><ul>
<li><a href="#41-ood-检测评估">4.1. OOD 检测评估</a></li>
<li><a href="#42-消融研究">4.2. 消融研究</a></li>
</ul>
</li>
<li><a href="#5-讨论与未来工作">5. 讨论与未来工作</a><ul>
<li><a href="#51-贝叶斯视角的讨论">5.1. 贝叶斯视角的讨论</a></li>
<li><a href="#52-归纳偏差的讨论">5.2. 归纳偏差的讨论</a></li>
<li><a href="#53-讨论与未来工作">5.3. 讨论与未来工作</a></li>
</ul>
</li>
<li><a href="#6-结论">6. 结论</a></li>
<li><a href="#致谢">致谢</a></li>
</ul>
</div>

            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <article>
                <header class="post-header">
                    <a href="../../../index.html" class="back-link">
                        <i class="fas fa-arrow-left"></i> 返回博客列表
                    </a>
                    <h1>Untitled</h1>
                    <div class="post-meta">
                        <span><i class="fas fa-calendar-alt"></i> 2026-01-28</span>
                        <span><i class="fas fa-folder"></i> AINotes/51.分布外检测</span>
                        <span><i class="fas fa-user"></i> Hongwei Zhao</span>
                    </div>
                    <div class="tags">
                        
                    </div>
                </header>

                <div class="post-content">
                    <h2 id="pvit-prior-augmented-vision-transformer-for-out-of-distribution-detection"><a href="http://arxiv.org/abs/2410.20631">PViT: Prior-augmented Vision Transformer for Out-of-distribution Detection</a><a class="anchor-link" href="#pvit-prior-augmented-vision-transformer-for-out-of-distribution-detection" title="Permanent link">&para;</a></h2>
<h2 id="0-摘要">0. 摘要<a class="anchor-link" href="#0-摘要" title="Permanent link">&para;</a></h2>
<p>视觉 Transformer（ViTs）在各种视觉任务中取得了显著的成功，然而它们在数据分布变化和固有归纳偏差方面的鲁棒性仍然未被充分探索。为了增强 ViT 模型在图像分布外（OOD）检测中的鲁棒性，我们提出了一种新颖且通用的框架，称为先验增强视觉 Transformer（PViT）。PViT 以来自预训练模型的先验类别 logits 作为输入，训练 PViT 以预测类别 logits。在推理过程中，PViT 通过量化预测的类别 logits 与从预训练模型获得的先验 logits 之间的差异来识别 OOD 样本。与现有的最先进（SOTA）OOD 检测方法不同，PViT 通过利用提出的先验引导置信度来塑造 ID 和 OOD 之间的决策边界，而不需要额外的数据建模、生成方法或结构修改。在大规模 IMAGENET 基准上进行的广泛实验，针对七个 OOD 数据集的评估表明，PViT 在 FPR95 和 AUROC 方面显著优于现有的 SOTA OOD 检测方法。代码库公开在 https://github.com/RanchoGoose/PViT。</p>
<p>关键词：图像分类，视觉 Transformer，分布外检测，深度学习，计算机视觉</p>
<h2 id="1-引言">1. 引言<a class="anchor-link" href="#1-引言" title="Permanent link">&para;</a></h2>
<p>近年来，Transformer 凭借其创新的注意力机制在各个领域取得了显著的成功，将其成功从自然语言处理扩展到各种视觉任务。视觉 Transformer（ViT）的出现标志着 Transformer 架构在视觉应用中的关键转折点，为后续模型奠定了舞台，这些模型通过增加深度和规模展示了显著的性能提升，尽管以计算需求的增加为代价。然而，与基于卷积神经网络（CNNs）的模型相比，这些架构在增强 OOD 检测方面的探索滞后了。</p>
<p>OOD 检测是一种关键的机器学习技术，旨在识别与训练数据分布不同的测试样本。该技术对于区分属于训练分布的输入和不属于训练分布的输入至关重要。在安全关键的实际部署中，遇到新类别是不可避免的，因此熟练的 OOD 检测的重要性不言而喻。解决 ViTs 的泛化和 OOD 检测能力变得至关重要。</p>
<p>我们探索了在视觉模型中战略性地融入先验知识以增强其安全相关能力。假设与人类一样，AI 模型可以从上下文线索中受益，提高其准确识别和分类数据的能力。这表明先验知识可能在帮助模型辨别细微的数据分布方面起到关键作用，从而引出了我们的核心研究问题：</p>
<p><strong>如果提供先验知识，模型是否会增强 OOD 检测的性能？</strong></p>
<p>在这项工作中，我们的目标是利用 ViTs 的优势设计一个可扩展的解决方案，以提高其在 OOD 检测中的鲁棒性和性能。与传统的 CNNs 相比，ViTs 直接操作于图像块的序列，通过注意力机制产生结果。ViTs 在捕捉图像块之间的长程依赖关系方面非常有效，但往往忽略了局部特征提取，因为 2D 图像块通过简单的线性层投影为向量。最近的一些研究开始关注增强局部信息的建模能力。使 ViTs 能够融入额外的上下文数据，扩展了其分析范围，超越了直接的视觉输入，创造了一个战略机会，将来自高性能预训练视觉模型的先验知识融入学习过程。</p>
<p>基于将先验知识融入 ViT 的想法，我们提出了新颖的先验增强视觉 Transformer（Prior-augmented Vision Transformer, PViT）用于 OOD 检测。如图 1 所示，PViT 旨在生成与先验 logits 紧密对齐的预测，同时对 OOD 数据表现出显著差异。先验知识来自在 ID 数据集上预训练的模型，本文中称为先验模型。PViT 使用先验模型生成的先验预测进行训练。在推理过程中，PViT 采用提出的先验引导能量（PGE）评分，通过量化先验 logits 与预测 logits 之间的差异来有效区分 OOD 实例。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20250317170129.png" style="zoom: 80%;" /></div>

<p>我们证明了我们提出的框架 PViT 在 OOD 检测中非常有效，特别是在像 IMAGENET 这样的大规模数据集上。与现有的 SOTA OOD 检测方法相比，PViT 实现了显著的性能提升，与最佳基线相比，FPR95 降低了高达 20%，AUROC 提高了高达 7%。此外，PViT 无需生成合成的异常数据，同时在 ID 数据集上保持高准确性。</p>
<p>本文的主要贡献总结如下：</p>
<ol>
<li>我们提出了 PViT，一个新颖且通用的框架，将先验知识融入 ViT，从而增强模型的鲁棒性和 OOD 检测能力。</li>
<li>我们提出了先验引导能量作为一种有效的 OOD 检测评分方法，通过测量先验类别 logits 与预测类别之间的相似性。</li>
<li>我们在各种基准上进行了全面的实验，提供了 PViT 的定性分析，并就将先验知识融入 ViT 模型的影响进行了深入的讨论。</li>
</ol>
<p>本文的其余部分结构如下：第 2 节回顾了相关工作，并为我们研究提供了必要的背景。第 3 节详细描述了我们提出的 PViT 框架，包括其架构和新的先验引导 OOD 评分机制。在第 4 节中，我们通过在大规模 OOD 基准（包括 CIFAR 和 IMAGENET-1K）上的广泛实验评估了 PViT 的性能。第 4.2 节提供了全面的消融研究，包括定量和定性分析。第 5 节讨论了我们方法的影响，强调了其当前的局限性，并概述了未来研究的潜在方向。最后，第 6 节总结了主要结果和贡献。附录提供了有关本文使用的数据集和模型的更多细节，以确保可重复性。</p>
<h2 id="2-相关工作">2. 相关工作<a class="anchor-link" href="#2-相关工作" title="Permanent link">&para;</a></h2>
<h3 id="21-分布外ood检测">2.1. 分布外（OOD）检测<a class="anchor-link" href="#21-分布外ood检测" title="Permanent link">&para;</a></h3>
<p>深度学习模型在分类来自不同语义分布的样本时通常过于自信，导致在图像分类和文本分类等任务中出现不适当的预测。这一问题促使了 OOD 检测领域的出现，该领域要求模型拒绝与训练分布语义不同的输入，这些输入不应由模型预测。OOD 检测是一个关键的研究领域，旨在确保 AI 系统的安全部署。多年来，各种 OOD 检测方法层出不穷，大致分为专注于网络修改的技术和基于评分的方法，以在嵌入或潜在特征空间中区分 ID 和 OOD 样本。</p>
<p>修改网络行为的方法通常采用截断等技术。例如，ODIN 使用梯度向量扰动输入以放大检测评分，而 ReAct 应用阈值来剪裁隐藏层激活。这些方法增强了网络分离 ID 和 OOD 样本的能力。</p>
<p>基于评分的方法涉及开发标量指标以量化样本为 OOD 的可能性。基于分类器的方法，通常称为置信度评分，利用神经网络的分类层。该领域的开创性工作是最大软最大概率（MSP）方法，它是 OOD 检测的基线。随后的进展包括能量函数，它提供了类别条件概率的无偏估计，以及最大 logit 技术，它结合了类别似然和特征幅度以提高性能。此外，Kullback–Leibler（KL）散度已被用于比较预测与均匀分布，增强了类别依赖信息。</p>
<p>基于距离的方法形成了另一类关键的 OOD 检测，基于样本在特征空间中与 ID 数据的空间关系来识别样本。Mahalanobis 检测器计算到类别均值的距离，具有共享的特征协方差，而 SSD 假设 ID 样本为单一高斯分布。非参数方法如 k 近邻（k-NN）提供了精确的边界划分，并通过 NNGuide 增强了在遥远数据集中的区分能力。</p>
<p>其他流行的 OOD 检测方法包括通过创建异常值来增强模型鲁棒性，也称为异常值暴露方法。这些方法对 OOD 训练数据的可用性提出了强烈的假设，这在实践中可能不可行。当没有 OOD 样本可用时，一些方法尝试合成 OOD 样本以实现 ID/OOD 可分离性。现有工作利用 GAN 生成 OOD 训练样本，并强制模型预测为均匀分布，生成低密度区域的边界样本，或生成高置信度的 OOD 样本。然而，在高维像素空间中合成图像可能难以优化。最近的工作 VOS 提出了从特征空间中的低似然区域合成虚拟异常值，这在较低维度下更易于处理。在目标检测中，有研究提出从野外视频中合成未知对象，使用时空未知蒸馏。最近的进展集中在复杂视觉环境（如城市驾驶场景）中定位 OOD 区域。这些异常值暴露方法通常需要额外的训练和生成合成数据，这降低了可扩展性和适应性。与现有的 SOTA 方法相比，我们提出的 PViT 通过不依赖合成数据或外部异常值进行训练，从而增强了其跨不同框架的可扩展性和适应性。</p>
<h3 id="22-视觉-transformer">2.2. 视觉 Transformer<a class="anchor-link" href="#22-视觉-transformer" title="Permanent link">&para;</a></h3>
<p>最初为机器翻译提出的 Transformer 在许多自然语言处理（NLP）任务中已跻身于最先进技术。vanilla ViT 代表了首个纯基于 Transformer 的图像分类模型，展示了与 SOTA CNN 相媲美的性能。与它们的 NLP 对应物一样，ViTs 缺乏 CNNs 的局部感受野和权重共享特性。相反，它们使用位置编码和自注意力来捕捉位置关系。这种灵活性允许 ViTs 学习任何数据关系，但它们可能需要更多的数据来学习 CNNs 由于其空间层次和局部偏差而自然捕获的模式。</p>
<p>随着 ViT 在显著性能上展示出优异结果，一系列 ViT 变体被提出，以提高在图像分类、图像分割和目标检测等各种视觉任务上的性能。DeiT，也称为数据高效图像 Transformer，后来被提出，通过在 ImageNet 数据库上训练作为一种有竞争力的无卷积 Transformer。Swin Transformers 在窗口内执行局部注意力，并引入了一种移位窗口分区方法以实现跨窗口连接。</p>
<p>除了纯视觉任务外，Transformer 还被用于贝叶斯推理。最近的一项研究探索了使用 Transformer 进行贝叶斯推理，拓宽了其适用性。这项研究表明，当在先验样本上训练时，Transformer 能够有效地近似后验预测分布（PPD），即使在涉及小型表格数据集的情况下。相比之下，我们的方法设计用于大规模图像数据，展示了利用先验信息处理不同数据规模和类型的多功能性。</p>
<p>虽然 ViTs 中专用 token 的引入是一个相对未探索的领域，但我们的工作开创了将先验 token 用于 OOD 检测的先河。先验 token 的概念并非我们首次引入，如在图像抠图中的应用 MatteFormer 中，通过先验注意力 Swin Transformer 块集成 trimap 信息。然而，我们的方法显著不同，因为它重新利用了这一概念来增强 ViTs 中的 OOD 检测。</p>
<h2 id="3-方法论">3. 方法论<a class="anchor-link" href="#3-方法论" title="Permanent link">&para;</a></h2>
<p>本节为我们方法奠定了基础，从问题设置（第 3.1 节）开始，以建立必要的背景。然后，我们提供了我们提出的 PViT 的全面概述（第 3.2 节），详细描述了其架构和功能，并附有图 2。最后，我们探讨了我们的 OOD 评分机制，特别强调了 PGE 评分在区分 OOD 实例中的作用（第 3.3 节）。</p>
<h3 id="31-预备知识">3.1. 预备知识<a class="anchor-link" href="#31-预备知识" title="Permanent link">&para;</a></h3>
<p>在图像分类的背景下，令 <span class="math-inline">X=\mathbb{R}^d</span> 表示输入空间，<span class="math-inline">Y={1,\dots,K}</span> 表示 <span class="math-inline">K</span> 个类别的有限标签集。训练数据集 <span class="math-inline">\mathcal{D}<em>{\text{train}}^{\text{in}}={(x_i,y_i)}</em>{i=1}^n</span> 由对 <span class="math-inline">(x_i,y_i)</span> 组成，其中分类函数 <span class="math-inline">f_\theta:X\rightarrow \mathbb{R}^K</span> 预测类别分数。预测标签 <span class="math-inline">\hat{y}</span> 通过 <span class="math-inline">\hat{y}=\arg\max_k f_\theta^{(k)}(x)</span> 获得，对应于得分最高的类别。</p>
<p>为了测试未见过的数据，目标是训练一个能够区分 OOD 输入 <span class="math-inline">\mathcal{D}<em>{\text{test}}^{\text{out}}={(x_j,y_j)}</em>{j=1}^m</span> 的模型，其中标签 <span class="math-inline">y_j</span> 不属于 <span class="math-inline">Y</span>。为了实现这一点，采用二分类方法，也称为 OOD 检测的决策规则：<br />
<div class="math-display"><br />
    x=\begin{cases} \text{ID} &amp; \text{if } S(x;\theta)\geq \gamma, \ \text{OOD} &amp; \text{if } S(x;\theta)&lt; \gamma, \end{cases} \tag{1}<br />
</div><br />
其中阈值 <span class="math-inline">\gamma</span> 被选择以确保 ID 数据的高分类准确性，通常设置为 95%。评分 <span class="math-inline">S(x;\theta)</span>，也称为“置信度”，表示基于分类器的检测评分。</p>
<h3 id="32-先验增强视觉-transformerpvit">3.2. 先验增强视觉 Transformer（PViT）<a class="anchor-link" href="#32-先验增强视觉-transformerpvit" title="Permanent link">&para;</a></h3>
<p>先验增强视觉 Transformer（PViT）的架构如图 2 所示。PViT 的实现遵循 vanilla Vision Transformer（ViT）的基础结构。在传统的 ViT 中，输入图像 <span class="math-inline">x\in \mathbb{R}^{H\times W\times C}</span> 被转换为一系列扁平化的 2D 图像块 <span class="math-inline">x_p\in \mathbb{R}^{N\times (P^2\cdot C)}</span>。这里，<span class="math-inline">(H,W)</span> 表示原始图像的分辨率，<span class="math-inline">C</span> 表示通道数，<span class="math-inline">(P,P)</span> 定义每个图像块的分辨率，<span class="math-inline">N=\frac{HW}{P^2}</span> 表示图像块的总数，这实际上决定了 Transformer 的序列长度。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20250317170147.png" style="zoom: 80%;" /></div>

<p>与 vanilla ViT 类似，可学习的嵌入 <span class="math-inline">z_0^0</span>，初始设置为 <span class="math-inline">x_{\text{class}}</span>，作为类别嵌入。该嵌入旨在捕捉全局图像表示，并在 Transformer 层中迭代更新。经过 <span class="math-inline">L</span> 层 Transformer 编码器后，其最终状态 <span class="math-inline">z_L^0\in \mathbb{R}^{1\times D}</span> 作为聚合的图像表示，记为 <span class="math-inline">y\in \mathbb{R}^D</span>，其中 <span class="math-inline">D</span> 是嵌入空间的维度。给定输入图像，模型首先计算图像块嵌入 <span class="math-inline">E_{\text{patches}}\in \mathbb{R}^{N\times D}</span>，其中 <span class="math-inline">N</span> 表示图像块的数量。然后，将类别 token <span class="math-inline">t_{\text{cls}}\in \mathbb{R}^{1\times D}</span> 添加到嵌入序列的前面。位置编码 <span class="math-inline">P_{\text{pos}}\in \mathbb{R}^{(N+1)\times D}</span> 被添加以提供序列的空间信息，从而形成最终的嵌入序列 <span class="math-inline">E_{\text{pos}}=[t_{\text{cls}};E_{\text{patches}}]+P_{\text{pos}}</span>。</p>
<p><strong>先验 token 的集成</strong>。给定一个参数化为 <span class="math-inline">\theta_{\text{prior}}</span> 的预训练先验模型，先验 logits 向量 <span class="math-inline">p\in \mathbb{R}^K</span> 表示分类输出 logits，作为 PViT 的先验知识。为了将这一先验知识融入 ViT 架构，我们引入了一个特殊 token，称为先验 token。该 token <span class="math-inline">t_{\text{prior}}\in \mathbb{R}^{1\times D}</span> 封装了先验知识，并与图像块 token 和类别 token 一起输入到先验增强编码器中，在注意力机制中进行处理。</p>
<p>为了创建先验 token，首先使用 softmax 函数对预训练分类器的 logits 向量 <span class="math-inline">p\in \mathbb{R}^K</span> 进行归一化。然后将这些归一化的 logits 投影到嵌入维度 <span class="math-inline">D</span>，形成先验 token <span class="math-inline">t_{\text{prior}}</span>：<br />
<div class="math-display"><br />
    t_{\text{prior}}=W_{\text{proj}}\cdot \text{softmax}(p), \tag{2}<br />
</div><br />
其中 <span class="math-inline">W_{\text{proj}}\in \mathbb{R}^{D\times C}</span> 是一个可学习的投影矩阵，旨在将类别先验转换为嵌入空间，使其与图像块嵌入在维度上对齐。</p>
<p>然后，先验 token 通过因子 <span class="math-inline">\alpha\in \mathbb{R}</span> 进行缩放，这是一个超参数，用于调节先验知识的影响。这种缩放在先验 token 和图像派生的嵌入之间平衡了模型的注意力，从而优化整体性能。缩放后的先验 token <span class="math-inline">t_{\text{prior}}</span> 在批次中进行复制，结果如下：<br />
<div class="math-display"><br />
    T_{\text{prior}}=\alpha\cdot t_{\text{prior}}\otimes \mathbf{1}<em>{B\times 1\times D}, \tag{3}<br />
</div><br />
其中 <span class="math-inline">B</span> 表示批次大小，<span class="math-inline">\otimes</span> 表示与全 1 向量的外积，有效地将 <span class="math-inline">t</em>{\text{prior}}</span> 广播到整个批次。批次级先验 token <span class="math-inline">T_{\text{prior}}</span> 被附加到位置编码的序列中，形成编码器的完整输入 <span class="math-inline">T=[E_{\text{pos}};T_{\text{prior}}]</span>。</p>
<p>连接后的序列 <span class="math-inline">T</span> 通过 Transformer 编码器层处理，得到最终表示。我们的模型遵循 vanilla ViT 的架构，采用多头自注意力（MSA）和多层感知器（MLP）块。在每个块之前应用层归一化（LN），如下式所述：<br />
<div class="math-display"><br />
    Z_0=T, \tag{4}<br />
</div></p>
<p><div class="math-display"><br />
    Z'<em>\ell=\text{MSA}(\text{LN}(Z</em>{\ell-1}))+Z_{\ell-1}, \quad \ell=1\dots L, \tag{5}<br />
</div></p>
<p><div class="math-display"><br />
    Z_\ell=\text{MLP}(\text{LN}(Z'<em>\ell))+Z'</em>\ell, \quad \ell=1\dots L, \tag{6}<br />
</div></p>
<p><div class="math-display"><br />
    Y=\text{LN}(Z_L[0]), \tag{7}<br />
</div><br />
其中 <span class="math-inline">Z_L[0]</span> 表示最后一层的类别 token 表示。输出 <span class="math-inline">Y\in \mathbb{R}^D</span> 作为分类器头的输入，用于当前任务。</p>
<p>在图像分类的背景下，PViT 的主要训练目标是最小化模型的预测分布与真实标签分布之间的差异。整体训练目标通过最小化交叉熵损失函数 <span class="math-inline">L_{\text{CE}}</span> 来实现，其公式如下：<br />
<div class="math-display"><br />
    L_{\text{CE}}=-\sum_{i=1}^K \log P_{\text{PViT}}(y_i|x_i,\mathcal{D},\pi;\theta), \tag{8}<br />
</div><br />
其中 <span class="math-inline">\theta</span> 表示 PViT 的参数，<span class="math-inline">y_i</span> 是真实标签，<span class="math-inline">x_i</span> 是输入数据，<span class="math-inline">\mathcal{D}</span> 是数据集，<span class="math-inline">\pi</span> 是先验信息。</p>
<h3 id="33-先验引导能量用于-ood-检测">3.3. 先验引导能量用于 OOD 检测<a class="anchor-link" href="#33-先验引导能量用于-ood-检测" title="Permanent link">&para;</a></h3>
<p>给定一个基础置信度评分函数 <span class="math-inline">S_{\text{base}}(x;\theta)</span>，我们提出了一种先验引导能量（PGE）方法，通过融入先验知识来有效区分 ID 和 OOD 数据：<br />
<div class="math-display"><br />
    S_{\text{PGE}}(x;\theta)=S_{\text{base}}(x;\theta)\cdot G(x;\theta,\theta_{\text{prior}}), \tag{9}<br />
</div><br />
其中 <span class="math-inline">G(x;\theta,\theta_{\text{prior}})</span> 是引导项，旨在测量先验嵌入与 PViT 输出之间的相似性。</p>
<p><strong>能量作为基础置信度评分</strong>。基础置信度评分 <span class="math-inline">S_{\text{base}}(x;\theta)</span> 源自能量评分，定义为：<br />
<div class="math-display"><br />
    E(x;\theta)=-\log \sum_{i=1}^K e^{f_i(x;\theta)}, \tag{10}<br />
</div><br />
其中 <span class="math-inline">f_i(x;\theta)</span> 表示模型输出的类别 <span class="math-inline">i</span> 的 logit。能量评分在 OOD 检测中的有效性源于其与模型学习到的表示的关系。能量评分在 OOD 检测中特别有效，原因如下：</p>
<ul>
<li><strong>推拉动态</strong>：在使用负对数似然（NLL）损失进行训练时，正确标签的能量被最小化，而错误标签的能量被增加，从而在 ID 和 OOD 样本之间创建了一个尖锐的置信度边界。</li>
<li><strong>自由能解释</strong>：能量评分隐含地融入了系统的自由能（对数配分函数），使其能够建模所有类别预测的整体不确定性。</li>
<li><strong>非概率效率</strong>：该评分通过 logsumexp 操作符高效计算，相比于概率密度估计在计算上更具优势。</li>
</ul>
<p>对于 ID 数据，logits <span class="math-inline">f_i(x;\theta)</span> 形成一个集中的分布，导致低能量评分。相反，对于 OOD 数据，logits 倾向于在所有类别上更均匀地分布，从而导致更高的能量评分。这种分离是 NLL 训练目标的自然结果，它明确地降低了 ID 数据的能量，同时增加了不相关类别的能量。为了与传统的 OOD 检测定义一致，我们使用负能量评分 <span class="math-inline">-E(x;\theta)</span> 作为 <span class="math-inline">S_{\text{base}}(x;\theta)</span>。</p>
<p><strong>先验引导项</strong>。虽然能量评分是一个强大的独立置信度度量，但它可以通过额外的先验信息进一步增强其判别能力。为此，我们引入了引导项<span class="math-inline">G(x;\theta,\theta_{\text{prior}})</span>，它评估先验知识与当前模型预测之间的相似性。这里我们介绍了一种可能的选项：交叉熵（Cross Entropy, CE）。CE 是一种广泛用于量化多类预测匹配成本的度量，被认为是分类任务中定义训练目标的有效方法。正如第 4 节的结果所示，CE 通过利用先验 logits 和预测类别作为输入，成为最优的引导项：<br />
<div class="math-display"><br />
    G(x;\theta,\theta_{\text{prior}})=-\sum_{i=1}^K y_i \log\left(q_i(x;\theta_{\text{prior}})\right), \tag{11}<br />
</div><br />
其中<span class="math-inline">q_i(x;\theta_{\text{prior}})</span> 基于先验 logits 的类别<span class="math-inline">i</span> 概率，<span class="math-inline">y_i</span>  PViT 预测的类别。该引导项测量了模型预测与先验分布之间的差异。较高的交叉熵得分表明与先验分布更一致，表明数据可能是 ID，而较低的得分则表明可能是 OOD 数据。其他可选的引导项（如欧氏距离和 KL 散度）在第 4.2 节中讨论。 </p>
<p><strong>整体 PGE 评分</strong>。通过结合基础置信度评分和先验引导项，PGE 评分定义为：<br />
<div class="math-display"><br />
    S_{\text{PGE}}(x;\theta)=\underbrace{S_{\text{energy}}(x;\theta)}<em>{\text{↑对于 ID 数据}}\cdot \underbrace{G(x;\theta,\theta</em>{\text{prior}})}<em>{\text{↑对于 ID 数据}}. \tag{12}<br />
</div><br />
引导项放大了基础置信度评分，从而对 ID 数据产生了更高的整体 PGE 评分。相反，OOD 数据的特点是 PGE 评分较低。通过设置适当的阈值<span class="math-inline">\gamma</span>，PGE 评分可以有效地区分 ID 和 OOD 数据：<br />
<div class="math-display"><br />
    \begin{cases} S</em>{\text{PGE}}(x;\theta) &gt; \gamma, &amp; \text{if } x\in \mathcal{D}<em>{\text{in}}, \ S</em>{\text{PGE}}(x;\theta) \leq \gamma, &amp; \text{if } x\in \mathcal{D}_{\text{out}}. \end{cases} \tag{13}<br />
</div><br />
下一节将详细介绍数据集、评估指标以及所提出的 OOD 检测方法的整体评估。 </p>
<h2 id="4-实验">4. 实验<a class="anchor-link" href="#4-实验" title="Permanent link">&para;</a></h2>
<p><strong>数据集</strong>。为了评估模型性能，我们使用小规模的 CIFAR 和大规模的 IMAGENET-1K 数据集作为 ID 训练数据集。CIFAR-10 和 CIFAR-100 因其相似但不同的特性被交替用作 ID 和 OOD 数据集。我们使用标准的训练/验证/测试集进行训练和测试。在表 1 和表 2 中报告的主要结果中，IMAGENET-1K 被用作 ID 数据，我们采用了一系列自然图像数据集作为 OOD 基准，包括 INATURALIST、SUN、TEXTURES、PLACES、NINCO、OPENIMAGE-O 和 SSB-HARD。在表 3 中，CIFAR-100 被用作 ID 数据集，以下 OOD 测试数据集用于测试 PViT 的 OOD 性能：CIFAR-10、TEXTURES、PLACES、LSUN、ISUN 和 SVHN。 </p>
<p><strong>训练细节</strong>。PViT 的配置包括隐藏维度为 384，深度为 12 层，6 个 MSA 头，MLP 维度为 768。使用 Adam 优化器，超参数<span class="math-inline">\beta_1=0.9</span> <span class="math-inline">\beta_2=0.999</span>，训练 20 个 epoch。训练开始时的初始学习率为 0.1，批次大小为 256，动量为 0.9，权重衰减为<span class="math-inline">1\times 10^{-3}</span>。在 5 个预热 epoch 后应用线性学习率衰减计划。对于不同的 ID 数据集，我们使用各种先验模型，包括 ResNet 和 ViT 模型及其变体。对于 IMAGENET-1K 作为 ID 数据集，ViT 模型在 IMAGENET-21K 上预训练，随后在 IMAGENET-1K 上微调。所有用作先验模型的预训练模型均公开可用。 </p>
<p><strong>评估指标</strong>。为了评估我们提出的模型在 OOD 检测中的性能，我们采用了两个评估指标：(1) FPR95，表示当 ID 样本的真实阳性率为 95% 时，OOD 样本的假阳性率；(2) AUROC，计算接收者操作特征曲线下的面积。 </p>
<h3 id="41-ood-检测评估">4.1. OOD 检测评估<a class="anchor-link" href="#41-ood-检测评估" title="Permanent link">&para;</a></h3>
<p>我们评估了 PViT 在 OOD 检测中的性能，与包括 MSP、MaxLogit 评分、Mahalanobis 评分、能量评分、SSD、ViM、KNN 和 NNGuide 在内的竞争基线进行比较。为了确保公平比较，我们没有包括任何基于合成的 OOD 方法，如 VOS 或 Dream-OOD，因为我们的 PViT 也可以通过包含合成数据训练进一步增强。对于表 1 和表 2 中的所有 PViT 实例，先验 token 的缩放因子<span class="math-inline">\alpha</span> 置为 0.1。 如表 1 和表 2 所示，PViT 在七个 OOD 数据集上进行了广泛评估，展示了其卓越的性能。我们的实验表明，PViT 在大规模 ID 数据集 IMAGENET-1K 上表现出色，特别是在使用基于 ViT 的先验模型时。值得注意的是，即使与 Mahalanobis 和 ViM 检测器相比，PViT 在 vanilla ViT 变体的先验模型中也显著优于这些基准。 为了全面评估 PViT，我们还在小规模 CIFAR-100 数据集上作为 ID 数据进行了实验，结果如表 3 所示。尽管 PViT 在 CIFAR-100 上未达到最佳性能，这可能是由于小规模 CIFAR-100 数据集的数据样本不足以及将图像像素从<span class="math-inline">32\times 32</span> 放到<span class="math-inline">224\times 224</span> 保持 PViT 配置的一致性，但 PViT 仍然位居前列。表中显示的结果是六个 OOD 数据集上的平均值。 </p>
<h3 id="42-消融研究">4.2. 消融研究<a class="anchor-link" href="#42-消融研究" title="Permanent link">&para;</a></h3>
<p><strong>OOD 先验引导的消融研究</strong>。虽然我们在公式 (11) 中引入了使用 CE 作为引导项来检测 OOD 实例，但我们也考虑了其他度量来测量先验与预测 logits 之间的差异：(1) 欧氏距离（Euclidean Distance, ED）。欧氏距离是一种几何度量，计算预测 logits 和先验概率向量之间的“直线”距离，公式为<span class="math-inline">\sqrt{\sum(p_i-q_i)^2}</span>，其中<span class="math-inline">p_i</span> <span class="math-inline">q_i</span> 别是先验和预测 logits 向量中的对应元素。(2) Kullback–Leibler 散度（KL 散度），用于测量两个概率分布之间的距离，定义为<span class="math-inline">\text{KL}(P|Q)=\sum P(x)\log \frac{P(x)}{Q(x)}</span>，其中<span class="math-inline">P</span> 示数据（在我们的上下文中为先验）的真实分布，<span class="math-inline">Q</span> 示模型推断的分布（预测 logits）。 如图 4 所示，评分分布显示，我们的先验引导能量评分比原始能量评分更好地区分了 ID 和 OOD 数据。CE 和 ED 引导项产生了相似的结果，尽管评分值不同。图 3 中的性能比较进一步表明，ED 作为引导项的性能与 CE 相当。 </p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20250317170321.png" style="zoom: 80%;" /></div>

<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20250317170334.png" style="zoom: 80%;" /></div>

<p><strong>先验效果的消融研究</strong>。为了证明我们集成的先验 token 在引导 PViT 有效区分 ID 和 OOD 数据方面的功效，我们进行了一项消融研究，将我们的方法与 vanilla ViT 模型进行了比较。具体来说，我们通过直接计算 vanilla ViT 模型（由 Google 提供，详细信息见附录）与先验模型之间的差异来评估 OOD 检测性能。结果如表 4 所示。 如表 4 所示，vanilla ViT 的性能在所有三种先验引导下均显著较差。在没有集成先验 token 的情况下，使用 CE 作为先验引导的 OOD 检测在七个 OOD 数据集的平均值上达到了近 100% 的 FPR95。同样，使用 KL 和 ED 作为先验引导的结果也不理想，FPR95 和 AUROC 指标与表 1 和表 2 中的结果相比表现较差。这些发现表明，vanilla 形式的 ViT 架构缺乏有效区分 OOD 和 ID 数据的内在能力。这表明，集成先验 token 对于 PViT 实现强大的 OOD 检测至关重要。</p>
<p><strong>先验缩放的消融研究</strong>。图 5 展示了缩放先验权重<span class="math-inline">\alpha</span>  PViT 输出的影响。我们的分析旨在减轻先验的过度影响。观察到，较高的<span class="math-inline">\alpha</span> 导致更多的注意力集中在先验 token 上，可能会减少对图像块的关注。相反，较低的<span class="math-inline">\alpha</span> 能会增强对图像块的注意力。当 PViT 在先验 token 和图像 token 之间平衡其注意力时，性能达到最佳。这种平衡导致了先验和预测之间的清晰区分，支撑了 PViT 在 OOD 检测中的有效性。 为了进一步研究图像块 token 之间的注意力权重权衡，我们在图 5 中可视化了注意力图，使用具有相应先验和预测标签的图像示例。这些图中的颜色条从最高到最低的注意力范围，凸显了缩放因子<span class="math-inline">\alpha</span> 模型注意力机制的影响。值得注意的是，先验 token 的注意力权重（表明 PViT 对先验 token 的关注）在颜色条上用红线标记。作为 OOD 检测的结果，我们可以看到，在图 5 的第三行中，PViT 的预测通常与先验模型的预测显著不同。</p>
<div align=center><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/20250317170402.png" style="zoom: 80%;" /></div>

<h2 id="5-讨论与未来工作">5. 讨论与未来工作<a class="anchor-link" href="#5-讨论与未来工作" title="Permanent link">&para;</a></h2>
<h3 id="51-贝叶斯视角的讨论">5.1. 贝叶斯视角的讨论<a class="anchor-link" href="#51-贝叶斯视角的讨论" title="Permanent link">&para;</a></h3>
<p>贝叶斯神经网络（BNNs）已在多项研究中被探索用于 OOD 检测。BNNs 将贝叶斯方法融入神经网络，利用模型参数上的概率分布来表示预测中的不确定性。在 OOD 检测的背景下，BNNs 可以通过比较模型在给定输入和已知 ID 数据上的不确定性来使用。然而，BNNs 在 OOD 检测中的适用性在最近的工作中引起了争议。 从贝叶斯角度来看，我们的方法可以解释为利用 ID 数据集中的先验知识在贝叶斯框架内建立预测后验分布（PPD）。这与 Transformer 促进贝叶斯推理的概念一致。在我们的模型中，先验可以视为从贝叶斯模型中采样的先验的均值，使 PViT 能够近似后验分布<span class="math-inline">P(y|x,\mathcal{D},\pi)</span>。这种近似遵循公式<span class="math-inline">p(y|x,\mathcal{D})=\int_t p(y|x,t)p(t|\mathcal{D})dt</span>，其中<span class="math-inline">P(x|y,\mathcal{D},\pi)</span> 示在给定标签和先验的情况下观察到数据的似然，<span class="math-inline">P(y|\mathcal{D},\pi)</span> 示由数据集和先验模型提供信息的先验概率，<span class="math-inline">P(x|\mathcal{D},\pi)</span> 表示证据，通常通过对标签空间进行边缘化计算。</p>
<p>然而，[46] 提出的方法专门设计用于单序列数据，旨在在单次前向传递中提供超快速的贝叶斯推理。这种方法虽然不直接适用于图像数据（由于 ViTs 的处理限制），但为我们 PViT 在 OOD 检测中的有效性提供了启示。通过捕捉贝叶斯推理中的不确定性，它为 PViT 在识别 OOD 样本中的强大性能提供了令人信服的解释。</p>
<h3 id="52-归纳偏差的讨论">5.2. 归纳偏差的讨论<a class="anchor-link" href="#52-归纳偏差的讨论" title="Permanent link">&para;</a></h3>
<p>一般而言，ViTs 和 CNNs 被认为具有根本不同的归纳偏差。ViTs 通过将图像块类比为 token，本质上关注全局图像模式。这种全局视角与 CNNs 的局部特征强调形成鲜明对比，CNNs 固有地编码了局部空间层次和邻近性的偏差。虽然这使得 ViTs 在需要整体图像理解的任务中表现出色，但其缺乏内置的局部偏差可能会限制其在需要详细局部特征分析的任务中的有效性。</p>
<p>ViTs 可以通过数据增强或混合架构采用归纳偏差，改善其局部特征处理能力，这传统上是 CNNs 的优势。我们的研究通过将额外的先验信息嵌入 ViTs 中，增强了其鲁棒性，并作为一种引入归纳偏差的方法。这一策略为将手动设计的归纳偏差嵌入 ViTs 开辟了一条新途径，可能会提高其鲁棒性和可解释性。通过引入额外的先验信息，我们希望我们的 PViT 能够弥补 ViTs 中较弱的传统归纳偏差，特别是在归纳偏差起关键作用的具有挑战性的场景中。</p>
<h3 id="53-讨论与未来工作">5.3. 讨论与未来工作<a class="anchor-link" href="#53-讨论与未来工作" title="Permanent link">&para;</a></h3>
<p>所提出方法的准确性与先验模型的准确性和结构密切相关，特别是在 ID 先验准确性上。PViT 在使用 ViT 架构作为先验时表现出更好的 OOD 检测性能，这可能是由于结构上的相似性。此外，PViT 需要在 ID 数据上进行训练，尽管受益于先验知识并实现了快速收敛，但这也带来了额外的复杂性。在推理过程中，需要对先验模型和 PViT 进行推理，这也增加了计算成本。</p>
<p>此外，我们的探索表明，PViT 为大型视觉模型引入了一种有益的归纳偏差，这些模型日益普及并不断发展。与大型语言模型（LLMs）的进步类似，计算机视觉领域也见证了大型视觉模型的显著增长。这些大型模型面临着“规划”的挑战——这是指导这些模型能力以实现特定、受控结果的关键方面。展望未来，扩展 PViT 以融入不同级别的先验知识，可以将大型视觉模型引导向特定目标，类似于为不同场景定制 OOD 检测。这是一个有前景的研究方向，可能对不同视觉 Transformer 架构产生广泛影响。</p>
<h2 id="6-结论">6. 结论<a class="anchor-link" href="#6-结论" title="Permanent link">&para;</a></h2>
<p>在本工作中，我们提出了先验增强视觉 Transformer（PViT），一个新颖且通用的 OOD 检测框架。PViT 独特地将先验知识作为先验 token 融入，训练其近似真实标签，通过检查模型预测与先验 logits 之间的相对距离来有效区分 OOD 数据。我们的实证结果表明，PViT 在 OOD 检测基准中表现出色。此外，PViT 创新性地融入先验知识，不仅增强了 OOD 检测能力，还为针对特定实际应用的大型视觉模型的战略规划和控制提供了一种多功能方法。</p>
<h2 id="致谢">致谢<a class="anchor-link" href="#致谢" title="Permanent link">&para;</a></h2>
<p>我们感谢英国 EPSRC 通过项目 NSF-EPSRC: ShiRAS. Towards Safe and Reliable Autonomy in Sensor Driven Systems（资助号 EP/T013265/1）以及美国国家科学基金会（资助号 NSF ECCS 1903466）的支持。本工作还得到了英国 RI 可信自主系统节点（REASON）项目 EP/V026747/1 的支持。</p>
                </div>

                <!-- Comments Section (Giscus) -->
                <section class="comments-section">
                    <h3><i class="fas fa-comments"></i> 评论</h3>
                    <script src="https://giscus.app/client.js"
                        data-repo="Geeks-Z/Geeks-Z.github.io"
                        data-repo-id=""
                        data-category="Announcements"
                        data-category-id=""
                        data-mapping="pathname"
                        data-strict="0"
                        data-reactions-enabled="1"
                        data-emit-metadata="0"
                        data-input-position="bottom"
                        data-theme="preferred_color_scheme"
                        data-lang="zh-CN"
                        crossorigin="anonymous"
                        async>
                    </script>
                </section>
            </article>

            <footer class="post-footer">
                <p>© 2025 Hongwei Zhao. Built with ❤️</p>
            </footer>
        </main>
    </div>

    <!-- Theme Toggle Button -->
    <button class="theme-toggle" id="themeToggle" aria-label="切换主题">
        <i class="fas fa-moon"></i>
    </button>

    <script>
        // Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        const html = document.documentElement;
        const icon = themeToggle.querySelector('i');
        const hljsDark = document.getElementById('hljs-theme-dark');
        const hljsLight = document.getElementById('hljs-theme-light');
        
        // Check saved theme or system preference
        const savedTheme = localStorage.getItem('theme');
        const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
        
        if (savedTheme === 'dark' || (!savedTheme && prefersDark)) {
            html.setAttribute('data-theme', 'dark');
            icon.className = 'fas fa-sun';
            hljsDark.disabled = false;
            hljsLight.disabled = true;
        }
        
        themeToggle.addEventListener('click', () => {
            const isDark = html.getAttribute('data-theme') === 'dark';
            if (isDark) {
                html.removeAttribute('data-theme');
                icon.className = 'fas fa-moon';
                localStorage.setItem('theme', 'light');
                hljsDark.disabled = true;
                hljsLight.disabled = false;
            } else {
                html.setAttribute('data-theme', 'dark');
                icon.className = 'fas fa-sun';
                localStorage.setItem('theme', 'dark');
                hljsDark.disabled = false;
                hljsLight.disabled = true;
            }
            
            // Update Giscus theme
            const giscusFrame = document.querySelector('iframe.giscus-frame');
            if (giscusFrame) {
                giscusFrame.contentWindow.postMessage({
                    giscus: {
                        setConfig: {
                            theme: isDark ? 'light' : 'dark'
                        }
                    }
                }, 'https://giscus.app');
            }
        });
        
        // Highlight.js
        document.addEventListener('DOMContentLoaded', () => {
            hljs.highlightAll();
        });
        
        // KaTeX auto-render
        document.addEventListener('DOMContentLoaded', () => {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
        
        // TOC active state
        const tocLinks = document.querySelectorAll('.toc-container a');
        const headings = document.querySelectorAll('.post-content h2, .post-content h3, .post-content h4');
        
        function updateTocActive() {
            let current = '';
            headings.forEach(heading => {
                const rect = heading.getBoundingClientRect();
                if (rect.top <= 100) {
                    current = heading.getAttribute('id');
                }
            });
            
            tocLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === '#' + current) {
                    link.classList.add('active');
                }
            });
        }
        
        window.addEventListener('scroll', updateTocActive);
        updateTocActive();
    </script>
</body>
</html>
