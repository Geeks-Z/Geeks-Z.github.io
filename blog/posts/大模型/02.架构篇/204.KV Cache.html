<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Untitled</title>
    <meta name="description" content="Untitled - Hongwei Zhao's Blog">
    <meta name="author" content="Hongwei Zhao">
    
    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <!-- Code Highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css" id="hljs-theme-dark">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css" id="hljs-theme-light" disabled>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    
    <style>
        :root {
            /* Light Theme */
            --primary-color: #2980b9;
            --primary-hover: #1a5276;
            --link-color: #c0392b;
            --text-color: #333;
            --text-light: #666;
            --text-muted: #999;
            --bg-color: #fff;
            --bg-secondary: #f8f9fa;
            --bg-code: #f5f5f5;
            --border-color: #e5e7eb;
            --shadow: 0 1px 3px rgba(0,0,0,0.1);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.1);
        }

        [data-theme="dark"] {
            --primary-color: #5dade2;
            --primary-hover: #85c1e9;
            --link-color: #e74c3c;
            --text-color: #e5e7eb;
            --text-light: #9ca3af;
            --text-muted: #6b7280;
            --bg-color: #1a1a2e;
            --bg-secondary: #16213e;
            --bg-code: #0f0f23;
            --border-color: #374151;
            --shadow: 0 1px 3px rgba(0,0,0,0.3);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.4);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 16px;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            transition: background-color 0.3s, color 0.3s;
        }

        a {
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--primary-hover);
            text-decoration: underline;
        }

        /* Layout */
        .page-wrapper {
            display: flex;
            max-width: 1400px;
            margin: 0 auto;
            padding: 2rem;
            gap: 3rem;
        }

        /* Sidebar TOC */
        .toc-sidebar {
            width: 260px;
            flex-shrink: 0;
            position: sticky;
            top: 2rem;
            height: fit-content;
            max-height: calc(100vh - 4rem);
            overflow-y: auto;
        }

        .toc-container {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 1.5rem;
            border: 1px solid var(--border-color);
        }

        .toc-container h3 {
            font-size: 0.85rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-muted);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .toc-container ul {
            list-style: none;
        }

        .toc-container li {
            margin-bottom: 0.5rem;
        }

        .toc-container a {
            font-size: 0.9rem;
            color: var(--text-light);
            display: block;
            padding: 0.25rem 0;
            border-left: 2px solid transparent;
            padding-left: 0.75rem;
            transition: all 0.2s;
        }

        .toc-container a:hover,
        .toc-container a.active {
            color: var(--primary-color);
            border-left-color: var(--primary-color);
            text-decoration: none;
        }

        .toc-container ul ul {
            margin-left: 1rem;
        }

        /* Main Content */
        .main-content {
            flex: 1;
            min-width: 0;
            max-width: 800px;
        }

        /* Header */
        .post-header {
            margin-bottom: 2rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
            margin-bottom: 1.5rem;
        }

        .back-link:hover {
            color: var(--primary-color);
            text-decoration: none;
        }

        .post-header h1 {
            font-size: 2.25rem;
            font-weight: 700;
            line-height: 1.3;
            margin-bottom: 1rem;
            color: var(--text-color);
        }

        .post-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
        }

        .post-meta span {
            display: flex;
            align-items: center;
            gap: 0.4rem;
        }

        .post-meta i {
            color: var(--text-muted);
        }

        .tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 1rem;
        }

        .tag {
            display: inline-block;
            font-size: 0.8rem;
            background: var(--bg-secondary);
            color: var(--text-light);
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            border: 1px solid var(--border-color);
            transition: all 0.2s;
        }

        .tag:hover {
            background: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }

        /* Article Content */
        .post-content {
            font-size: 1rem;
            line-height: 1.9;
        }

        .post-content h1,
        .post-content h2,
        .post-content h3,
        .post-content h4,
        .post-content h5,
        .post-content h6 {
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-weight: 600;
            line-height: 1.4;
            color: var(--text-color);
        }

        .post-content h1 { font-size: 1.875rem; }
        .post-content h2 { 
            font-size: 1.5rem; 
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }
        .post-content h3 { font-size: 1.25rem; }
        .post-content h4 { font-size: 1.125rem; }

        .post-content p {
            margin-bottom: 1.25rem;
        }

        .post-content ul,
        .post-content ol {
            margin: 1.25rem 0;
            padding-left: 1.5rem;
        }

        .post-content li {
            margin-bottom: 0.5rem;
        }

        .post-content blockquote {
            margin: 1.5rem 0;
            padding: 1rem 1.5rem;
            background: var(--bg-secondary);
            border-left: 4px solid var(--primary-color);
            border-radius: 0 8px 8px 0;
            color: var(--text-light);
            font-style: italic;
        }

        .post-content blockquote p:last-child {
            margin-bottom: 0;
        }

        /* Code */
        .post-content code {
            font-family: 'JetBrains Mono', 'Fira Code', Consolas, monospace;
            font-size: 0.9em;
            background: var(--bg-code);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            color: var(--link-color);
        }

        .post-content pre {
            margin: 1.5rem 0;
            padding: 1.25rem;
            background: var(--bg-code);
            border-radius: 10px;
            overflow-x: auto;
            border: 1px solid var(--border-color);
        }

        .post-content pre code {
            background: none;
            padding: 0;
            color: inherit;
            font-size: 0.875rem;
            line-height: 1.6;
        }

        /* Images */
        .post-content img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            margin: 1.5rem 0;
            box-shadow: var(--shadow-lg);
        }

        /* Tables */
        .post-content table {
            width: 100%;
            margin: 1.5rem 0;
            border-collapse: collapse;
            font-size: 0.95rem;
        }

        .post-content th,
        .post-content td {
            padding: 0.75rem 1rem;
            border: 1px solid var(--border-color);
            text-align: left;
        }

        .post-content th {
            background: var(--bg-secondary);
            font-weight: 600;
        }

        .post-content tr:nth-child(even) {
            background: var(--bg-secondary);
        }

        /* Math */
        .math-display {
            margin: 1.5rem 0;
            overflow-x: auto;
            padding: 1rem;
            background: var(--bg-secondary);
            border-radius: 8px;
        }

        /* Anchor links */
        .anchor-link {
            opacity: 0;
            margin-left: 0.5rem;
            color: var(--text-muted);
            font-weight: 400;
            transition: opacity 0.2s;
        }

        .post-content h2:hover .anchor-link,
        .post-content h3:hover .anchor-link,
        .post-content h4:hover .anchor-link {
            opacity: 1;
        }

        /* Theme Toggle */
        .theme-toggle {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            background: var(--primary-color);
            color: white;
            border: none;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.25rem;
            box-shadow: var(--shadow-lg);
            transition: transform 0.2s, background 0.2s;
            z-index: 1000;
        }

        .theme-toggle:hover {
            transform: scale(1.1);
            background: var(--primary-hover);
        }

        /* Comments Section */
        .comments-section {
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-color);
        }

        .comments-section h3 {
            font-size: 1.25rem;
            margin-bottom: 1.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        /* Footer */
        .post-footer {
            margin-top: 3rem;
            padding-top: 1.5rem;
            border-top: 1px solid var(--border-color);
            text-align: center;
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        /* Responsive */
        @media (max-width: 1024px) {
            .toc-sidebar {
                display: none;
            }
            
            .page-wrapper {
                padding: 1.5rem;
            }
        }

        @media (max-width: 768px) {
            .post-header h1 {
                font-size: 1.75rem;
            }
            
            .post-meta {
                flex-direction: column;
                gap: 0.5rem;
            }
            
            .theme-toggle {
                bottom: 1rem;
                right: 1rem;
                width: 44px;
                height: 44px;
            }
        }

        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-secondary);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--border-color);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--text-muted);
        }
    </style>
</head>
<body>
    <div class="page-wrapper">
        <!-- TOC Sidebar -->
        <aside class="toc-sidebar">
            <div class="toc-container">
                <h3><i class="fas fa-list"></i> 目录</h3>
                <div class="toc">
<ul>
<li><a href="#mha">MHA</a></li>
<li><a href="#瓶颈">瓶颈</a></li>
<li><a href="#mqa">MQA</a></li>
<li><a href="#gqa">GQA</a></li>
<li><a href="#mla">MLA</a><ul>
<li><a href="#part-1">Part 1</a></li>
<li><a href="#part-2">Part 2</a></li>
<li><a href="#part-3">Part 3</a></li>
</ul>
</li>
<li><a href="#小结">小结</a></li>
</ul>
</div>

            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <article>
                <header class="post-header">
                    <a href="../../../index.html" class="back-link">
                        <i class="fas fa-arrow-left"></i> 返回博客列表
                    </a>
                    <h1>Untitled</h1>
                    <div class="post-meta">
                        <span><i class="fas fa-calendar-alt"></i> 2026-01-28</span>
                        <span><i class="fas fa-folder"></i> 大模型/02.架构篇</span>
                        <span><i class="fas fa-user"></i> Hongwei Zhao</span>
                    </div>
                    <div class="tags">
                        
                    </div>
                </header>

                <div class="post-content">
                    <blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/662498827">大模型推理加速：看图学KV Cache</a></p>
</blockquote>
<p>KV Cache是Transformer标配的推理加速功能，transformer官方use_cache这个参数默认是True，但是它<strong>只能用于Decoder架构的模型</strong>，这是因为Decoder有Causal Mask，在推理的时候前面已经生成的字符不需要与后面的字符产生attention，从而使得前面已经计算的K和V可以缓存起来。</p>
<p>我们先看一下不使用KV Cache的推理过程。假设模型最终生成了“遥遥领先”4个字。</p>
<p>当模型生成第一个“遥”字时，input="\&lt;s&gt;", "\&lt;s&gt;"是起始字符。Attention的计算如下：</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202412231543831.jpg" />  </p>
<p>为了看上去方便，我们<strong>暂时忽略scale项</strong> <span class="math-inline">\sqrt{d}</span>， 但是要注意这个scale面试时经常考。</p>
<p>如上图所示，最终Attention的计算公式如下，（softmaxed 表示已经按行进行了softmax）:</p>
<p><div class="math-display">{\color{orange}{Att_1}}(Q, K, V) = \text{softmax}({\color{orange}{Q_1}} K_1^T) \vec{V_1} = \text{softmaxed}({\color{orange}{Q_1}} K_1^T) \vec{V_1} \</div></p>
<p>当模型生成第二个“遥”字时，input="\&lt;s&gt;遥", Attention的计算如下：</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202412231543832.jpg" />  </p>
<p>当 <span class="math-inline">QK^T</span> 变为矩阵时，softmax 会针对 <strong>行</strong> 进行计算。写详细一点如下，softmaxed 表示已经按行进行了softmax。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202412231543833.jpg" />  </p>
<p>假设 <span class="math-inline">{\color{orange}{Att_1}}(Q, K, V)</span> 表示 Attention 的第一行， <span class="math-inline">{\color{orange}{Att_2}}(Q, K, V)</span> 表示 Attention 的第二行，则根据上面推导，</p>
<p>其计算公式为：</p>
<p><div class="math-display"> \begin{aligned} {\color{orange}{Att_1}}(Q, K, V) &amp;= \text{softmaxed}({\color{orange}{Q_1}} K_1^T) \vec{V_1} \ {\color{red}{Att_2}}(Q, K, V) &amp;= \text{softmaxed}({\color{red}{Q_2}} K_1^T) \vec{V_1} + \text{softmaxed}({\color{red}{Q_2}} K_2^T) \vec{V_2 } \end{aligned}  \</div></p>
<p>你会发现，由于 <span class="math-inline">Q_1 K_2^T</span> 这个值会mask掉，</p>
<ul>
<li><span class="math-inline">Q_1</span> <strong>在第二步参与的计算与第一步是一样的，而且第二步生成的</strong> <span class="math-inline">V_1</span> <strong>也仅仅依赖于</strong> <span class="math-inline">Q_1</span> <strong>，与</strong> <span class="math-inline">Q_2</span> <strong>毫无关系。</strong></li>
<li><span class="math-inline">V_2</span> <strong>的计算也仅仅依赖于</strong> <span class="math-inline">Q_2</span> <strong>，与</strong> <span class="math-inline">Q_1</span> <strong>毫无关系。</strong></li>
</ul>
<p>当模型生成第三个“领”字时，input="\&lt;s&gt;遥遥"Attention的计算如下：</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202412231543834.jpg" />  </p>
<p>详细的推导参考第二步，其计算公式为：</p>
<p><div class="math-display"> \begin{aligned} {\color{orange}{Att_1}}(Q, K, V) &amp;= \text{softmaxed}({\color{orange}{Q_1}} K_1^T) \vec{V_1} \ {\color{red}{Att_2}}(Q, K, V) &amp;= \text{softmaxed}({\color{red}{Q_2}} K_1^T) \vec{V_1} + \text{softmaxed}({\color{red}{Q_2}} K_2^T) \vec{V_2 } \ {\color{purple}{Att_3}}(Q, K, V) &amp;= \text{softmaxed}({\color{purple}{Q_3}} K_1^T) \vec{V_1} + \text{softmaxed}({\color{purple}{Q_3}} K_2^T) \vec{V_2 } + \text{softmaxed}({\color{purple}{Q_3}} K_3^T) \vec{V_3 } \end{aligned}  \</div></p>
<p>同样的， <span class="math-inline">Att_k</span> 只与 <span class="math-inline">Q_k</span> 有关。</p>
<p>当模型生成第四个“先”字时，input="\&lt;s&gt;遥遥领"Attention的计算如下：</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202412231543835.jpg" />  </p>
<p><div class="math-display"> \begin{aligned} {\color{orange}{Att_1}}(Q, K, V) &amp;= \text{softmaxed}({\color{orange}{Q_1}} K_1^T) \vec{V_1} \ {\color{red}{Att_2}}(Q, K, V) &amp;= \text{softmaxed}({\color{red}{Q_2}} K_1^T) \vec{V_1} + \text{softmaxed}({\color{red}{Q_2}} K_2^T) \vec{V_2 } \ {\color{purple}{Att_3}}(Q, K, V) &amp;= \text{softmaxed}({\color{purple}{Q_3}} K_1^T) \vec{V_1} + \text{softmaxed}({\color{purple}{Q_3}} K_2^T) \vec{V_2 } + \text{softmaxed}({\color{purple}{Q_3}} K_3^T) \vec{V_3 } \ {\color{brown}{Att_4}}(Q, K, V) &amp;= \text{softmaxed}({\color{brown}{Q_4}} K_1^T) \vec{V_1} + \text{softmaxed}({\color{brown}{Q_4}} K_2^T) \vec{V_2 } + \text{softmaxed}({\color{brown}{Q_4}} K_3^T) \vec{V_3 } + \text{softmaxed}({\color{brown}{Q_4}} K_4^T) \vec{V_4 }  \end{aligned}  \</div></p>
<p>和之前类似，不再赘述。</p>
<p>看上面图和公式，我们可以得出结论：</p>
<ol>
<li><strong>当前计算方式存在大量冗余计算。</strong></li>
<li><span class="math-inline">Att_k</span> <strong>只与</strong> <span class="math-inline">Q_k</span> <strong>有关。</strong></li>
<li><strong>推理第</strong> <span class="math-inline">x_k</span> <strong>个字符的时候只需要输入字符</strong> <span class="math-inline">x_{k-1}</span><strong>即可。</strong></li>
</ol>
<p>我们每一步其实之需要根据 <span class="math-inline">Q_k</span> 计算 <span class="math-inline">Att_k</span> 就可以，之前已经计算的Attention完全不需要重新计算。但是 <span class="math-inline">K</span> 和 <span class="math-inline">V</span> 是全程参与计算的，所以这里我们需要把每一步的 <span class="math-inline">K,V</span> 缓存起来。所以说叫KV Cache好像有点不太对，因为KV本来就需要全程计算，可能叫增量KV计算会更好理解。</p>
<p>下面4张图展示了使用KV Cache和不使用的对比。</p>
<p><img alt="" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/202412231543836.jpg" />  </p>
<p>下面是gpt里面KV Cache的实现。其实明白了原理后代码实现简单的不得了,就是concat操作而已。</p>
<p><a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L318C1-L331C97"><span class="invisible">https://</span></a></p>
<pre><code class="language-python">if layer_past is not None:
        past_key, past_value = layer_past
        key = torch.cat((past_key, key), dim=-2)
        value = torch.cat((past_value, value), dim=-2)

    if use_cache is True:
        present = (key, value)
    else:
        present = None

    if self.reorder_and_upcast_attn:
        attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)
    else:
        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
</code></pre>
<p>最后需要注意当<strong>sequence特别长的时候，KV Cache其实还是个Memory刺客</strong>。</p>
<p>比如batch_size=32, head=32, layer=32, dim_size=4096, seq_length=2048, float32类型，则需要占用的显存为（感谢网友指正） 2 * 32 * 4096 * 2048 * 32 * 4 / 1024/1024/1024 /1024 = 64G。</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/700588653">缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA</a></p>
</blockquote>
<p>前几天，幻方发布的<a href="https://arxiv.org/abs/2405.04434">DeepSeek-V2</a>引起了大家的热烈讨论。首先，最让人哗然的是1块钱100万token的价格，普遍比现有的各种竞品API便宜了两个数量级，以至于有人调侃“这个价格哪怕它输出乱码，我也会认为这个乱码是一种艺术”；其次，从模型的技术报告看，如此便宜的价格背后的关键技术之一是它新提出的MLA（<strong>M</strong>ulti-head <strong>L</strong>atent <strong>A</strong>ttention），这是对GQA的改进，据说能比GQA更省更好，也引起了读者的广泛关注。</p>
<h2 id="mha">MHA<a class="anchor-link" href="#mha" title="Permanent link">&para;</a></h2>
<p>MHA（<strong>M</strong>ulti-<strong>H</strong>ead <strong>A</strong>ttention），也就是多头注意力，是开山之作<a href="https://kexue.fm/archives/4765">《Attention is all you need》</a>所提出的一种Attention形式，可以说它是当前主流LLM的基础工作。在数学上，多头注意力MHA等价于多个独立的单头注意力的拼接，假设输入的（行）向量序列为<span class="math-inline">\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_l</span>，其中<span class="math-inline">\boldsymbol{x}_i\in\mathbb{R}^d</span>，那么MHA可以形式地记为</p>
<p><div class="math-display">\begin{gathered} \boldsymbol{o}<em>t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\right] \[10pt] \boldsymbol{o}_t^{(s)} = Attention\left(\boldsymbol{q}_t^{(s)}, \boldsymbol{k}</em>{\leq t}^{(s)} ,\boldsymbol{v}<em>{\leq t}^{(s)}\right)\triangleq\frac{\sum</em>{i\leq t}\exp\left(\boldsymbol{q}<em>t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)\boldsymbol{v}_i^{(s)}}{\sum</em>{i\leq t}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)} \[15pt] \boldsymbol{q}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d\times d_k}\  \boldsymbol{k}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_k^{(s)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_k^{(s)}\in\mathbb{R}^{d\times d_k} \ \boldsymbol{v}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d\times d_v} \end{gathered} \tag1</div></p>
<p>简单起见，这里省略了Attention矩阵的缩放因子。实践上，常见的设置是<span class="math-inline">d_k = d_v = d / h</span>，对于LLAMA2-7b有<span class="math-inline">d=4096, h=32, d_k = d_v = 128</span>，LLAMA2-70b则是<span class="math-inline">d=8192,h=64, d_k = d_v = 128</span></p>
<p>由于这里只考虑了主流的自回归LLM所用的Causal Attention，因此在token by token递归生成时，新预测出来的第<span class="math-inline">t+1</span> token，并不会影响到已经算好的<span class="math-inline">\boldsymbol{k}<em>{\leq t}^{(s)} ,\boldsymbol{v}</em>{\leq t}^{(s)}</span>，因此这部分结果我们可以缓存下来供后续生成调用，避免不必要的重复计算，这就是所谓的KV Cache。</p>
<p>而后面的MQA、GQA、MLA，都是围绕“如何减少KV Cache同时尽可能地保证效果”这个主题发展而来的产物。</p>
<h2 id="瓶颈">瓶颈<a class="anchor-link" href="#瓶颈" title="Permanent link">&para;</a></h2>
<p>一个自然的问题是：为什么降低KV Cache的大小如此重要？</p>
<p>众所周知，一般情况下LLM的推理都是在GPU上进行，单张GPU的显存是有限的，一部分我们要用来存放模型的参数和前向计算的激活值，这部分依赖于模型的体量，选定模型后它就是个常数；另外一部分我们要用来存放模型的KV Cache，这部分不仅依赖于模型的体量，还依赖于模型的输入长度，也就是在推理过程中是动态增长的，当Context长度足够长时，它的大小就会占主导地位，可能超出一张卡甚至一台机（8张卡）的总显存量。</p>
<p>在GPU上部署模型的原则是：能一张卡部署的，就不要跨多张卡；能一台机部署的，就不要跨多台机。这是因为“卡内通信带宽 &gt; 卡间通信带宽 &gt; 机间通信带宽”，由于“木桶效应”，模型部署时跨的设备越多，受设备间通信带宽的的“拖累”就越大，事实上即便是单卡H100内SRAM与HBM的带宽已经达到了3TB/s，但对于Short Context来说这个速度依然还是推理的瓶颈，更不用说更慢的卡间、机间通信了。</p>
<p>所以，减少KV Cache的目的就是要实现在更少的设备上推理更长的Context，或者在相同的Context长度下让推理的batch size更大，从而实现更快的推理速度或者更大的吞吐总量。当然，最终目的都是为了实现更低的推理成本。</p>
<p>要想更详细地了解这个问题，读者可以进一步阅读<a href="https://arxiv.org/abs/2205.14135">《FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness》</a>、<a href="https://www.baseten.co/blog/llm-transformer-inference-guide/">《A guide to LLM inference and performance》</a>、<a href="https://zeux.io/2024/03/15/llm-inference-sol/">《LLM inference speed of light》</a>等文章。</p>
<h2 id="mqa">MQA<a class="anchor-link" href="#mqa" title="Permanent link">&para;</a></h2>
<p>MQA，即“<strong>M</strong>ulti-<strong>Q</strong>uery <strong>A</strong>ttention”，是减少KV Cache的一次非常朴素的尝试，首次提出自<a href="https://arxiv.org/abs/1911.02150">《Fast Transformer Decoding: One Write-Head is All You Need》</a>，这已经是2019年的论文了，这也意味着早在LLM火热之前，减少KV Cache就已经是研究人员非常关注的一个课题了。</p>
<p>MQA的思路很简单，直接让所有Attention Head共享同一个K、V，用公式来说，就是取消MHA所有的<span class="math-inline">\boldsymbol{k},\boldsymbol{v}</span> 上标<span class="math-inline">{}^{(s)}</span>：</p>
<p><div class="math-display"> \begin{gathered} \boldsymbol{o}<em>t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\right] \[10pt] \boldsymbol{o}_t^{(s)} = Attention\left(\boldsymbol{q}_t^{(s)}, \boldsymbol{k}</em>{\leq t}^{\color{#ccc}{\smash{\bcancel{(s)}}}} ,\boldsymbol{v}<em>{\leq t}^{\color{#ccc}{\smash{\bcancel{(s)}}}}\right)\triangleq\frac{\sum</em>{i\leq t}\exp\left(\boldsymbol{q}<em>t^{(s)} \boldsymbol{k}_i^{\color{#ccc}{\smash{\bcancel{(s)}}}}{}^{\top}\right)\boldsymbol{v}_i^{\color{#ccc}{\smash{\bcancel{(s)}}}}}{\sum</em>{i\leq t}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{\color{#ccc}{\smash{\bcancel{(s)}}}}{}^{\top}\right)} \[15pt] \boldsymbol{q}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d\times d_k}\  \boldsymbol{k}_i^{\color{#ccc}{\smash{\bcancel{(s)}}}} = \boldsymbol{x}_i\boldsymbol{W}_k^{\color{#ccc}{\smash{\bcancel{(s)}}}}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_k^{\color{#ccc}{\smash{\bcancel{(s)}}}}\in\mathbb{R}^{d\times d_k} \ \boldsymbol{v}_i^{\color{#ccc}{\smash{\bcancel{(s)}}}} = \boldsymbol{x}_i\boldsymbol{W}_v^{\color{#ccc}{\smash{\bcancel{(s)}}}}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{\color{#ccc}{\smash{\bcancel{(s)}}}}\in\mathbb{R}^{d\times d_v} \end{gathered} \tag2</div></p>
<p>使用MQA的模型包括<a href="https://arxiv.org/pdf/2204.02311">PaLM</a>、<a href="https://arxiv.org/abs/2305.06161">StarCoder</a>、<a href="https://arxiv.org/abs/2312.11805">Gemini</a>等。很明显，MQA直接将KV Cache减少到了原来的<span class="math-inline">1/h</span>，这是非常可观的，单从节省显存角度看已经是天花板了。</p>
<p>效果方面，目前看来大部分任务的损失都比较有限，且MQA的支持者相信这部分损失可以通过进一步训练来弥补回。此外，注意到MQA由于共享了K、V，将会导致Attention的参数量减少了将近一半，而为了模型总参数量的不变，通常会相应地增大FFN/GLU的规模，这也能弥补一部分效果损失。</p>
<h2 id="gqa">GQA<a class="anchor-link" href="#gqa" title="Permanent link">&para;</a></h2>
<p>然而，也有人担心MQA对KV Cache的压缩太严重，以至于会影响模型的学习效率以及最终效果。为此，一个MHA与MQA之间的过渡版本GQA（<strong>G</strong>rouped-<strong>Q</strong>uery <strong>A</strong>ttention）应运而生，出自论文<a href="https://arxiv.org/abs/2305.13245">《GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints》</a>，是去年的工作。</p>
<p>事后看来，GQA的思想也很朴素，它就是将所有Head分为<span class="math-inline">g</span> 组（<span class="math-inline">g</span> 以整除<span class="math-inline">h</span>），每组共享同一对K、V，用数学公式表示为</p>
<p><div class="math-display">\begin{gathered} \boldsymbol{o}<em>t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\right] \[10pt] \boldsymbol{o}_t^{(s)} = Attention\left(\boldsymbol{q}_t^{(s)}, \boldsymbol{k}</em>{\leq t}^{\color{red}{(\lceil sg/h\rceil)}} ,\boldsymbol{v}<em>{\leq t}^{\color{red}{(\lceil sg/h\rceil)}}\right)\triangleq\frac{\sum</em>{i\leq t}\exp\left(\boldsymbol{q}<em>t^{(s)} \boldsymbol{k}_i^{\color{red}{(\lceil sg/h\rceil)}}{}^{\top}\right)\boldsymbol{v}_i^{\color{red}{(\lceil sg/h\rceil)}}}{\sum</em>{i\leq t}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{\color{red}{(\lceil sg/h\rceil)}}{}^{\top}\right)} \[15pt] \boldsymbol{q}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d\times d_k}\  \boldsymbol{k}_i^{\color{red}{(\lceil sg/h\rceil)}} = \boldsymbol{x}_i\boldsymbol{W}_k^{\color{red}{(\lceil sg/h\rceil)}}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_k^{\color{red}{(\lceil sg/h\rceil)}}\in\mathbb{R}^{d\times d_k} \ \boldsymbol{v}_i^{\color{red}{(\lceil sg/h\rceil)}} = \boldsymbol{x}_i\boldsymbol{W}_v^{\color{red}{(\lceil sg/h\rceil)}}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{\color{red}{(\lceil sg/h\rceil)}}\in\mathbb{R}^{d\times d_v} \end{gathered} \</div></p>
<p>这里的<span class="math-inline">\lceil\cdot\rceil</span> 上取整符号。GQA提供了MHA到MQA的自然过渡，当<span class="math-inline">g=h</span> 就是MHA，<span class="math-inline">g=1</span> 就是MQA，当<span class="math-inline">1 &lt; g &lt; h</span> ，它只将KV Cache压缩到<span class="math-inline">g/h</span>，压缩率不如MQA，但同时也提供了更大的自由度，效果上更有保证。GQA最知名的使用者，大概是Meta开源的<a href="https://llama.meta.com/llama2/">LLAMA2-70B</a>，以及<a href="https://llama.meta.com/llama3/">LLAMA3</a>全系列，此外使用GQA的模型还有<a href="https://arxiv.org/abs/2312.08688">TigerBot</a>、<a href="https://arxiv.org/abs/2401.02954">DeepSeek-V1</a>、<a href="https://arxiv.org/abs/2402.19173">StarCoder2</a>、<a href="https://arxiv.org/abs/2403.04652">Yi</a>、<a href="https://github.com/THUDM/ChatGLM2-6B">ChatGLM2</a>、<a href="https://github.com/THUDM/ChatGLM3">ChatGLM3</a>等，相比使用MQA的模型更多（ChatGLM虽然在它的介绍中说自己是MQA，但实际是<span class="math-inline">g=2</span> GQA）。</p>
<p>在llama2/3-70B中，GQA的<span class="math-inline">g=8</span>，其他用了GQA的同体量模型基本上也保持了这个设置，这并非偶然，而是同样出于推理效率的考虑。我们知道，70B这个体量的模型，如果不进行极端的量化，那么不可能部署到单卡（A100/H100 80G）上。单卡不行，那么就能单机了，一般情况下一台机可以装8张卡，刚才我们说了，Attention的每个Head实际上是独立运算然后拼接起来的，当<span class="math-inline">g=8</span> ，正好可以每张卡负责计算一组K、V对应的Attention Head，这样可以在尽可能保证K、V多样性的同时最大程度上减少卡间通信。</p>
<h2 id="mla">MLA<a class="anchor-link" href="#mla" title="Permanent link">&para;</a></h2>
<p>有了MHA、MQA、GQA的铺垫，我们理解MLA（<strong>M</strong>ulti-head <strong>L</strong>atent <strong>A</strong>ttention）就相对容易一些了。DeepSeek-V2的技术报告里是从低秩投影的角度引入MLA的，以至于有部分读者提出“为什么LoRA提出这么久了，直到MLA才提出对KV Cache低秩分解的做法”之类的疑问。</p>
<p>然而，笔者认为低秩投影这个角度并不贴近本质，因为要说低秩投影的话，事实上只要我们将GQA的所有K、V叠在一起，就会发现GQA也相当于在做低秩投影：</p>
<p><div class="math-display">\underbrace{\left[\boldsymbol{k}<em>i^{(1)},\cdots,\boldsymbol{k}_i^{(g)},\boldsymbol{v}_i^{(1)},\cdots,\boldsymbol{v}_i^{(g)}\right]}</em>{\boldsymbol{c}<em>i\in\mathbb{R}^{g(d_k+d_v)}} = \boldsymbol{x}_i \underbrace{\left[\boldsymbol{W}_k^{(1)},\cdots,\boldsymbol{W}_k^{(g)},\boldsymbol{W}_v^{(1)},\cdots,\boldsymbol{W}_v^{(g)}\right]}</em>{\boldsymbol{W}_c\in\mathbb{R}^{d\times g(d_k+d_v)}} \</div></p>
<p>这里我们将所有<span class="math-inline">\boldsymbol{k}_i^{(s)},\boldsymbol{v}_i^{(s)}</span> 在一起记为<span class="math-inline">\boldsymbol{c}_i</span>，相应的投影矩阵也拼在一起记为<span class="math-inline">\boldsymbol{W}_c</span>，注意到一般都有<span class="math-inline">d_c = g(d_k+d_v) &lt; d</span>，所以<span class="math-inline">\boldsymbol{x}_i</span> <span class="math-inline">\boldsymbol{c}_i</span> 变换就是一个低秩投影。所以，MLA的本质改进不是低秩投影，而是低秩投影之后的工作。</p>
<h3 id="part-1">Part 1<a class="anchor-link" href="#part-1" title="Permanent link">&para;</a></h3>
<p>GQA在投影之后做了什么呢？首先它将向量对半分为两份分别作为K、V，然后每一份又均分为<span class="math-inline">g</span> ，每一份复制<span class="math-inline">h/g</span> ，以此来“凑”够<span class="math-inline">h</span> Attention Head所需要的K、V。我们知道分割、复制都是简单的线性变换，所以MLA的第一个想法是将这些简单的线性变换换成一般的线性变换，以增强模型的能力：</p>
<p><div class="math-display">\begin{gathered} \boldsymbol{o}<em>t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\right] \[10pt] \boldsymbol{o}_t^{(s)} = Attention\left(\boldsymbol{q}_t^{(s)}, \boldsymbol{k}</em>{\leq t}^{(s)} ,\boldsymbol{v}<em>{\leq t}^{(s)}\right)\triangleq\frac{\sum</em>{i\leq t}\exp\left(\boldsymbol{q}<em>t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)\boldsymbol{v}_i^{(s)}}{\sum</em>{i\leq t}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)} \[15pt] \boldsymbol{q}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d\times d_k}\  \boldsymbol{k}_i^{(s)} = \boldsymbol{c}_i\boldsymbol{W}_k^{(s)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_k^{(s)}\in\mathbb{R}^{d_c\times d_k} \ \boldsymbol{v}_i^{(s)} = \boldsymbol{c}_i\boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_c\times d_v} \[10pt] \boldsymbol{c}_i = \boldsymbol{x}_i \boldsymbol{W}_c\in\mathbb{R}^{d_c},\quad \boldsymbol{W}_c\in\mathbb{R}^{d\times d_c} \end{gathered} \</div></p>
<p>然而，理论上这样是能增加模型能力，但别忘了GQA的主要目的是减少KV Cache，出于节省计算和通信成本的考虑，我们一般会缓存的是投影后的<span class="math-inline">\boldsymbol{k}_i, \boldsymbol{v}_i</span> 不是投影前的<span class="math-inline">\boldsymbol{c}_i</span> <span class="math-inline">\boldsymbol{x}_i</span>，而MLA的这个做法，通过不同的投影矩阵再次让所有的K、V Head都变得各不相同，那么KV Cache的大小就恢复成跟MHA一样大了，违背了GQA的初衷。</p>
<p>对此，MLA发现，我们可以结合Dot-Attention的具体形式，通过一个简单但不失巧妙的恒等变换来规避这个问题。首先，在训练阶段还是照常进行，此时优化空间不大；然后，在推理阶段，我们利用</p>
<p><div class="math-display">\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top} = \left(\boldsymbol{x}_t\boldsymbol{W}_q^{(s)}\right) \left(\boldsymbol{c}_i\boldsymbol{W}_k^{(s)}\right){}^{\top} = \boldsymbol{x}_t\left(\boldsymbol{W}_q^{(s)}\boldsymbol{W}_k^{(s)}{}^{\top}\right)\boldsymbol{c}_i^{\top}  \</div></p>
<p>这意味着推理阶段，我们可以将<span class="math-inline">\boldsymbol{W}_q^{(s)}\boldsymbol{W}_k^{(s)}{}^{\top}</span> 并起来作为Q的投影矩阵，那么<span class="math-inline">\boldsymbol{c}_i</span> 取代了原本的<span class="math-inline">\boldsymbol{k}_i</span>，同理，在<span class="math-inline">\boldsymbol{o}_t</span> 面我们还有一个投影矩阵，于是<span class="math-inline">\boldsymbol{v}_i^{(s)} = \boldsymbol{c}_i\boldsymbol{W}_v^{(s)}</span> <span class="math-inline">\boldsymbol{W}_v^{(s)}</span> 可以吸收到后面的投影矩阵中去，于是等效地<span class="math-inline">\boldsymbol{v}_i</span> 可以用<span class="math-inline">\boldsymbol{c}_i</span> 替，也就是说此时KV Cache只需要存下所有的<span class="math-inline">\boldsymbol{c}_i</span> 行，而不至于存下所有的<span class="math-inline">\boldsymbol{k}_i^{(s)}</span>、<span class="math-inline">\boldsymbol{v}_i^{(s)}</span>。注意到<span class="math-inline">\boldsymbol{c}_i</span> <span class="math-inline">{}^{(s)}</span> 关，也就是说是所有头共享的，即MLA在推理阶段它可以恒等变换为一个MQA。</p>
<p>再次强调，本文的主题是一直都是减少KV Cache，那到目前为止，MLA做到了什么呢？答案是通过不同的投影矩阵来增强了GQA的能力，并且推理时可以保持同样大小的KV Cache。那么反过来，如果我们只需要跟GQA相近的能力，那么是不是就可以再次减少KV Cache了？换言之，<span class="math-inline">d_c</span> 必要取<span class="math-inline">g(d_k+d_v)</span>，而是取更小的值（DeepSeek-V2取了512），从而进一步压缩KV Cache，这就是MLA的核心思想。</p>
<p>（注：这里有一个细节，就是<span class="math-inline">\boldsymbol{W}_q^{(s)}\boldsymbol{W}_k^{(s)}{}^{\top}</span> 并成一个矩阵的恒等变换，理论上只有在无限精度下才成立，实际上如果我们使用单精度尤其是BF16的话，经过变换后的精度损失往往还是挺明显的，经过多层累积后可能放大到比较可观的程度，这里可能要根据实际误差看要不要做一些后处理。）</p>
<h3 id="part-2">Part 2<a class="anchor-link" href="#part-2" title="Permanent link">&para;</a></h3>
<p>一切似乎都很完美，看上去一个又好又省的理想设计就要出炉了。不过别急，当我们再深入思考一下就会发现，到目前为止的MLA有一个难以绕开的缺陷——不兼容<a href="https://kexue.fm/archives/8265">RoPE（旋转位置编码）</a>。</p>
<p>刚才我们说了，MLA之所以能保持跟GQA一样大小的KV Cache，其关键一步是“将<span class="math-inline">\boldsymbol{W}<em>q^{(s)}\boldsymbol{W}_k^{(s)}{}^{\top}</span> 并成一个（跟位置无关的）矩阵作为Q的投影矩阵”，但如果加了RoPE的话，这一步就无法实现了。这是因为RoPE是一个跟位置相关的、<span class="math-inline">d_k\times d_k</span> 分块对角矩阵<span class="math-inline">\boldsymbol{\mathcal{R}}_m</span>，满足<span class="math-inline">\boldsymbol{\mathcal{R}}_m\boldsymbol{\mathcal{R}}_n^{\top}=\boldsymbol{\mathcal{R}}</em>{m-n}</span>，MLA加入RoPE之后会让<span class="math-inline">\boldsymbol{W}<em>q^{(s)}\boldsymbol{W}_k^{(s)}{}^{\top}</span> 间多插入了一项<span class="math-inline">\boldsymbol{\mathcal{R}}</em>{t-i}</span>：</p>
<p><div class="math-display">\boldsymbol{q}<em>i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i}\quad,\quad\boldsymbol{k}_i^{(s)} = \boldsymbol{c}_i\boldsymbol{W}_k^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i} \ \boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top} = \left(\boldsymbol{x}_t\boldsymbol{W}_q^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_t}\right) \left(\boldsymbol{c}_i\boldsymbol{W}_k^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i}\right){}^{\top} = \boldsymbol{x}_t\left(\boldsymbol{W}_q^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}</em>{t-i}}\boldsymbol{W}_k^{(s)}{}^{\top}\right)\boldsymbol{c}_i^{\top} \</div></p>
<p>这里的<span class="math-inline">\boldsymbol{W}<em>q^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}</em>{t-i}}\boldsymbol{W}_k^{(s)}{}^{\top}</span> 无法合并为一个固定的投影矩阵了（跟位置差<span class="math-inline">t-i</span> 关），从而MLA的想法无法结合RoPE实现。</p>
<p>前段时间，笔者也很荣幸跟DeepSeek团队讨论过这个问题，但这个问题可以说非常本质，所以当时笔者实际上也没能提出什么有效的建议。最简单的方式是放弃RoPE，换用其他基于Attention Bias的位置编码，如<a href="https://kexue.fm/archives/9431#ALIBI">ALIBI</a>，但DeepSeek的实验显示它明显不如RoPE（注意，MLA不是不能加RoPE，而是加了RoPE之后无法用恒等变换技巧来减少KV Cache），笔者也提议过换<a href="https://kexue.fm/archives/9431#Sandwich">Sandwich</a>，它不像ALIBI单调衰减到负无穷，估计效果会好些，但感觉是治标不治本。还有一个折中的办法是将<span class="math-inline">\boldsymbol{q}_i</span> 输入也改为<span class="math-inline">\boldsymbol{c}_i</span>，然后RoPE加在<span class="math-inline">\boldsymbol{c}_i</span> 后，即</p>
<p><div class="math-display">\boldsymbol{q}_i^{(s)} = \boldsymbol{c}_i\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i}\boldsymbol{W}_q^{(s)},\quad\boldsymbol{k}_i^{(s)} = \boldsymbol{c}_i\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i}\boldsymbol{W}_k^{(s)} \</div></p>
<p>这样<span class="math-inline">\boldsymbol{\mathcal{R}}<em>i</span> 可以吸收到<span class="math-inline">\boldsymbol{c}_i</span> 去，但这样就没有<span class="math-inline">\boldsymbol{\mathcal{R}}_m\boldsymbol{\mathcal{R}}_n^{\top}=\boldsymbol{\mathcal{R}}</em>{m-n}</span> 运算了，此时的RoPE不再是通过绝对位置实现相对位置，而单纯是在Q、K上加绝对位置，让模型自己想办法提炼相对位置信息。</p>
<p>最后发布的MLA，采取了一种混合的方法——每个Attention Head的Q、K新增<span class="math-inline">d_r</span> 维度用来添加RoPE，其中K新增的维度每个Head共享：</p>
<p><div class="math-display">\begin{gathered} \boldsymbol{o}<em>t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\right] \[10pt] \boldsymbol{o}_t^{(s)} = Attention\left(\boldsymbol{q}_t^{(s)}, \boldsymbol{k}</em>{\leq t}^{(s)} ,\boldsymbol{v}<em>{\leq t}^{(s)}\right)\triangleq\frac{\sum</em>{i\leq t}\exp\left(\boldsymbol{q}<em>t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)\boldsymbol{v}_i^{(s)}}{\sum</em>{i\leq t}\exp\left(\boldsymbol{q}<em>t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)} \[15pt] \boldsymbol{q}_i^{(s)} = \left[\boldsymbol{x}_i\boldsymbol{W}</em>{qc}^{(s)}, \boldsymbol{x}<em>i\boldsymbol{W}</em>{qr}^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}<em>i}\right]\in\mathbb{R}^{d_k + d_r},\quad \boldsymbol{W}</em>{qc}^{(s)}\in\mathbb{R}^{d\times d_k},\boldsymbol{W}<em>{qr}^{(s)}\in\mathbb{R}^{d\times d_r}\  \boldsymbol{k}_i^{(s)} = \left[\boldsymbol{c}_i\boldsymbol{W}</em>{kc}^{(s)}, \boldsymbol{x}<em>i\boldsymbol{W}</em>{kr}^{\color{#ccc}{\smash{\bcancel{(s)}}}}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}<em>i}\right]\in\mathbb{R}^{d_k+d_r},\quad \boldsymbol{W}</em>{kc}^{(s)}\in\mathbb{R}^{d_c\times d_k}, \boldsymbol{W}_{kr}^{\color{#ccc}{\smash{\bcancel{(s)}}}}\in\mathbb{R}^{d\times d_r} \ \boldsymbol{v}_i^{(s)} = \boldsymbol{c}_i\boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_c\times d_v} \[10pt] \boldsymbol{c}_i = \boldsymbol{x}_i \boldsymbol{W}_c\in\mathbb{R}^{d_c},\quad \boldsymbol{W}_c\in\mathbb{R}^{d\times d_c} \end{gathered} \</div></p>
<p>这样一来，没有RoPE的维度就可以重复“Part 1”的操作，在推理时KV Cache只需要存<span class="math-inline">\boldsymbol{c}_i</span>，新增的带RoPE的维度就可以用来补充位置信息，并且由于所有Head共享，所以也就只有在K Cache这里增加了<span class="math-inline">d_r</span> 维度，原论文取了<span class="math-inline">d_r = d_k / 2 = 64</span>，相比原本的<span class="math-inline">d_c=512</span>，增加的幅度不大。</p>
<h3 id="part-3">Part 3<a class="anchor-link" href="#part-3" title="Permanent link">&para;</a></h3>
<p>最后有一个细节，就是MLA的最终版本，还将Q的输入也改为了低秩投影形式，这与减少KV Cache无关，主要是为了减少训练期间参数量和相应的梯度（原论文说的是激活值，个人表示不大理解）所占的显存：</p>
<p><div class="math-display">\begin{gathered} \boldsymbol{o}<em>t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\right] \[10pt] \boldsymbol{o}_t^{(s)} = Attention\left(\boldsymbol{q}_t^{(s)}, \boldsymbol{k}</em>{\leq t}^{(s)} ,\boldsymbol{v}<em>{\leq t}^{(s)}\right)\triangleq\frac{\sum</em>{i\leq t}\exp\left(\boldsymbol{q}<em>t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)\boldsymbol{v}_i^{(s)}}{\sum</em>{i\leq t}\exp\left(\boldsymbol{q}<em>t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)} \[15pt] \boldsymbol{q}_i^{(s)} = \left[\boldsymbol{c}_i'\boldsymbol{W}</em>{qc}^{(s)}, \boldsymbol{c}<em>i'\boldsymbol{W}</em>{qr}^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}<em>i}\right]\in\mathbb{R}^{d_k + d_r},\quad \boldsymbol{W}</em>{qc}^{(s)}\in\mathbb{R}^{d_c'\times d_k},\boldsymbol{W}<em>{qr}^{(s)}\in\mathbb{R}^{d_c'\times d_r}\  \boldsymbol{k}_i^{(s)} = \left[\boldsymbol{c}_i\boldsymbol{W}</em>{kc}^{(s)}, \boldsymbol{x}<em>i\boldsymbol{W}</em>{kr}^{\color{#ccc}{\smash{\bcancel{(s)}}}}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}<em>i}\right]\in\mathbb{R}^{d_k+d_r},\quad \boldsymbol{W}</em>{kc}^{(s)}\in\mathbb{R}^{d_c\times d_k}, \boldsymbol{W}_{kr}^{\color{#ccc}{\smash{\bcancel{(s)}}}}\in\mathbb{R}^{d\times d_r} \ \boldsymbol{v}_i^{(s)} = \boldsymbol{c}_i\boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_c\times d_v} \[10pt] \boldsymbol{c}_i' = \boldsymbol{x}_i \boldsymbol{W}_c'\in\mathbb{R}^{d_c'},\quad \boldsymbol{W}_c'\in\mathbb{R}^{d\times d_c'} \ \boldsymbol{c}_i = \boldsymbol{x}_i \boldsymbol{W}_c\in\mathbb{R}^{d_c},\quad \boldsymbol{W}_c\in\mathbb{R}^{d\times d_c} \ \end{gathered} \</div></p>
<p>注意<span class="math-inline">\boldsymbol{k}_i^{(s)}</span> 的第二项，带RoPE的部分，其输入还是<span class="math-inline">\boldsymbol{x}_i</span> 不是<span class="math-inline">\boldsymbol{c}_i</span>，这里保持了原论文的设置，不是笔误，<span class="math-inline">d_c'</span> 论文的取值是1536，跟<span class="math-inline">d_c=512</span> 同。同时，我们把带RoPE的MHA放在下面，方便大家对比：</p>
<p><div class="math-display">\begin{gathered} \boldsymbol{o}<em>t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\right] \[10pt] \boldsymbol{o}_t^{(s)} = Attention\left(\boldsymbol{q}_t^{(s)}, \boldsymbol{k}</em>{\leq t}^{(s)} ,\boldsymbol{v}<em>{\leq t}^{(s)}\right)\triangleq\frac{\sum</em>{i\leq t}\exp\left(\boldsymbol{q}<em>t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)\boldsymbol{v}_i^{(s)}}{\sum</em>{i\leq t}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)} \[15pt] \boldsymbol{q}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d\times d_k}\  \boldsymbol{k}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_k^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_k^{(s)}\in\mathbb{R}^{d\times d_k} \ \boldsymbol{v}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d\times d_v} \end{gathered} \</div></p>
<p>可以发现，其实在训练阶段，除了多了一步低秩投影以及只在部分维度加RoPE外，MLA与Q、K的Head Size由<span class="math-inline">d_k</span> 成<span class="math-inline">d_k + d_r</span> MHA基本无异。</p>
<p>推理阶段的MLA则改为</p>
<p><div class="math-display">\begin{gathered} \boldsymbol{o}<em>t = \left[\boldsymbol{o}_t^{(1)}\boldsymbol{W}_v^{(1)}, \boldsymbol{o}_t^{(2)}\boldsymbol{W}_v^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\boldsymbol{W}_v^{(h)}\right] \[10pt] \boldsymbol{o}_t^{(s)} = Attention\left(\boldsymbol{q}_t^{(s)}, \boldsymbol{k}</em>{\leq t}^{(s)} ,\boldsymbol{c}<em>{\leq t}\right)\triangleq\frac{\sum</em>{i\leq t}\exp\left(\boldsymbol{q}<em>t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)\boldsymbol{c}_i}{\sum</em>{i\leq t}\exp\left(\boldsymbol{q}<em>t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)} \[15pt] \boldsymbol{q}_i^{(s)} = \left[\boldsymbol{c}_i'\boldsymbol{W}</em>{qc}^{(s)}\boldsymbol{W}<em>{kc}^{(s)}{}^{\top}, \boldsymbol{c}_i'\boldsymbol{W}</em>{qr}^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}<em>i}\right]\in\mathbb{R}^{d_c + d_r}\  \boldsymbol{k}_i^{(s)} = \left[\boldsymbol{c}_i, \boldsymbol{x}_i\boldsymbol{W}</em>{kr}^{\color{#ccc}{\smash{\bcancel{(s)}}}}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}<em>i}\right]\in\mathbb{R}^{d_c+d_r}\ \boldsymbol{W}</em>{qc}^{(s)}\in\mathbb{R}^{d_c'\times d_k},\boldsymbol{W}<em>{kc}^{(s)}\in\mathbb{R}^{d_c\times d_k},\boldsymbol{W}</em>{qr}^{(s)}\in\mathbb{R}^{d_c'\times d_r},\boldsymbol{W}_{kr}^{\color{#ccc}{\smash{\bcancel{(s)}}}}\in\mathbb{R}^{d\times d_r} \[10pt] \boldsymbol{c}_i' = \boldsymbol{x}_i \boldsymbol{W}_c'\in\mathbb{R}^{d_c'},\quad \boldsymbol{W}_c'\in\mathbb{R}^{d\times d_c'} \ \boldsymbol{c}_i = \boldsymbol{x}_i \boldsymbol{W}_c\in\mathbb{R}^{d_c},\quad \boldsymbol{W}_c\in\mathbb{R}^{d\times d_c} \ \end{gathered} \</div></p>
<p>此时Q、K的Head Size变成了<span class="math-inline">d_c + d_r</span>，V的Head Size 则变成了<span class="math-inline">d_c</span>，按照原论文的设置，这是<span class="math-inline">d_k</span>、<span class="math-inline">d_v</span> 4倍。所以实际上MLA在推理阶段做的这个转换，虽然能有效减少KV Cache，但其推理的计算量是增加的。</p>
<p>那为什么还能提高推理效率呢？这又回到“瓶颈”一节所讨论的问题了，我们可以将LLM的推理分两部分：第一个Token的生成（Prefill）和后续每个Token的生成（Generation），Prefill阶段涉及到对输入所有Token的并行计算，然后把对应的KV Cache存下来，这部分对于计算、带宽和显存都是瓶颈，MLA虽然增大了计算量，但KV Cache的减少也降低了显存和带宽的压力，大家半斤八两；但是Generation阶段由于每步只计算一个Token，实际上它更多的是带宽瓶颈和显存瓶颈，因此MLA的引入理论上能明显提高Generation的速度。</p>
<p>还有一个细节充分体现了这个特性。一般的LLM架构参数满足<span class="math-inline">h \times d_k = d</span>，即num_heads * head_size = hidden_size，但DeepSeek-V2不一样，它<span class="math-inline">d_k=128,d=5120</span>，但<span class="math-inline">h=128</span>，是一般设置的3倍！这是因为MLA的KV Cache大小跟<span class="math-inline">h</span> 关，增大<span class="math-inline">h</span> 会增加计算量和提升模型能力，但不会增加KV Cache，所以不会带来速度瓶颈。</p>
<h2 id="小结">小结<a class="anchor-link" href="#小结" title="Permanent link">&para;</a></h2>
<p>本文简单概述了多头注意力的演变历程，特别是从MHA向MQA、GQA，最终到MLA的变化理念，最后详细展开了对MLA的介绍。在本文中，MLA被视为GQA的一般化，它用投影矩阵的方式替代了GQA的分割、重复，并引入了一个恒等变换技巧来可以进一步压缩KV Cache，同时采用了一种混合方法来兼容RoPE。总的来说，MLA称得上是一种非常实用的注意力变体。</p>
                </div>

                <!-- Comments Section (Giscus) -->
                <section class="comments-section">
                    <h3><i class="fas fa-comments"></i> 评论</h3>
                    <script src="https://giscus.app/client.js"
                        data-repo="Geeks-Z/Geeks-Z.github.io"
                        data-repo-id=""
                        data-category="Announcements"
                        data-category-id=""
                        data-mapping="pathname"
                        data-strict="0"
                        data-reactions-enabled="1"
                        data-emit-metadata="0"
                        data-input-position="bottom"
                        data-theme="preferred_color_scheme"
                        data-lang="zh-CN"
                        crossorigin="anonymous"
                        async>
                    </script>
                </section>
            </article>

            <footer class="post-footer">
                <p>© 2025 Hongwei Zhao. Built with ❤️</p>
            </footer>
        </main>
    </div>

    <!-- Theme Toggle Button -->
    <button class="theme-toggle" id="themeToggle" aria-label="切换主题">
        <i class="fas fa-moon"></i>
    </button>

    <script>
        // Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        const html = document.documentElement;
        const icon = themeToggle.querySelector('i');
        const hljsDark = document.getElementById('hljs-theme-dark');
        const hljsLight = document.getElementById('hljs-theme-light');
        
        // Check saved theme or system preference
        const savedTheme = localStorage.getItem('theme');
        const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
        
        if (savedTheme === 'dark' || (!savedTheme && prefersDark)) {
            html.setAttribute('data-theme', 'dark');
            icon.className = 'fas fa-sun';
            hljsDark.disabled = false;
            hljsLight.disabled = true;
        }
        
        themeToggle.addEventListener('click', () => {
            const isDark = html.getAttribute('data-theme') === 'dark';
            if (isDark) {
                html.removeAttribute('data-theme');
                icon.className = 'fas fa-moon';
                localStorage.setItem('theme', 'light');
                hljsDark.disabled = true;
                hljsLight.disabled = false;
            } else {
                html.setAttribute('data-theme', 'dark');
                icon.className = 'fas fa-sun';
                localStorage.setItem('theme', 'dark');
                hljsDark.disabled = false;
                hljsLight.disabled = true;
            }
            
            // Update Giscus theme
            const giscusFrame = document.querySelector('iframe.giscus-frame');
            if (giscusFrame) {
                giscusFrame.contentWindow.postMessage({
                    giscus: {
                        setConfig: {
                            theme: isDark ? 'light' : 'dark'
                        }
                    }
                }, 'https://giscus.app');
            }
        });
        
        // Highlight.js
        document.addEventListener('DOMContentLoaded', () => {
            hljs.highlightAll();
        });
        
        // KaTeX auto-render
        document.addEventListener('DOMContentLoaded', () => {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
        
        // TOC active state
        const tocLinks = document.querySelectorAll('.toc-container a');
        const headings = document.querySelectorAll('.post-content h2, .post-content h3, .post-content h4');
        
        function updateTocActive() {
            let current = '';
            headings.forEach(heading => {
                const rect = heading.getBoundingClientRect();
                if (rect.top <= 100) {
                    current = heading.getAttribute('id');
                }
            });
            
            tocLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === '#' + current) {
                    link.classList.add('active');
                }
            });
        }
        
        window.addEventListener('scroll', updateTocActive);
        updateTocActive();
    </script>
</body>
</html>
