<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Untitled</title>
    <meta name="description" content="Untitled - Hongwei Zhao's Blog">
    <meta name="author" content="Hongwei Zhao">
    
    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <!-- Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <!-- Code Highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css" id="hljs-theme-dark">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css" id="hljs-theme-light" disabled>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    
    <style>
        :root {
            /* Light Theme */
            --primary-color: #2980b9;
            --primary-hover: #1a5276;
            --link-color: #c0392b;
            --text-color: #333;
            --text-light: #666;
            --text-muted: #999;
            --bg-color: #fff;
            --bg-secondary: #f8f9fa;
            --bg-code: #f5f5f5;
            --border-color: #e5e7eb;
            --shadow: 0 1px 3px rgba(0,0,0,0.1);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.1);
        }

        [data-theme="dark"] {
            --primary-color: #5dade2;
            --primary-hover: #85c1e9;
            --link-color: #e74c3c;
            --text-color: #e5e7eb;
            --text-light: #9ca3af;
            --text-muted: #6b7280;
            --bg-color: #1a1a2e;
            --bg-secondary: #16213e;
            --bg-code: #0f0f23;
            --border-color: #374151;
            --shadow: 0 1px 3px rgba(0,0,0,0.3);
            --shadow-lg: 0 4px 15px rgba(0,0,0,0.4);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 16px;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            transition: background-color 0.3s, color 0.3s;
        }

        a {
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--primary-hover);
            text-decoration: underline;
        }

        /* Layout */
        .page-wrapper {
            display: flex;
            max-width: 1400px;
            margin: 0 auto;
            padding: 2rem;
            gap: 3rem;
        }

        /* Sidebar TOC */
        .toc-sidebar {
            width: 260px;
            flex-shrink: 0;
            position: sticky;
            top: 2rem;
            height: fit-content;
            max-height: calc(100vh - 4rem);
            overflow-y: auto;
        }

        .toc-container {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 1.5rem;
            border: 1px solid var(--border-color);
        }

        .toc-container h3 {
            font-size: 0.85rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-muted);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .toc-container ul {
            list-style: none;
        }

        .toc-container li {
            margin-bottom: 0.5rem;
        }

        .toc-container a {
            font-size: 0.9rem;
            color: var(--text-light);
            display: block;
            padding: 0.25rem 0;
            border-left: 2px solid transparent;
            padding-left: 0.75rem;
            transition: all 0.2s;
        }

        .toc-container a:hover,
        .toc-container a.active {
            color: var(--primary-color);
            border-left-color: var(--primary-color);
            text-decoration: none;
        }

        .toc-container ul ul {
            margin-left: 1rem;
        }

        /* Main Content */
        .main-content {
            flex: 1;
            min-width: 0;
            max-width: 800px;
        }

        /* Header */
        .post-header {
            margin-bottom: 2rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
            margin-bottom: 1.5rem;
        }

        .back-link:hover {
            color: var(--primary-color);
            text-decoration: none;
        }

        .post-header h1 {
            font-size: 2.25rem;
            font-weight: 700;
            line-height: 1.3;
            margin-bottom: 1rem;
            color: var(--text-color);
        }

        .post-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
        }

        .post-meta span {
            display: flex;
            align-items: center;
            gap: 0.4rem;
        }

        .post-meta i {
            color: var(--text-muted);
        }

        .tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 1rem;
        }

        .tag {
            display: inline-block;
            font-size: 0.8rem;
            background: var(--bg-secondary);
            color: var(--text-light);
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            border: 1px solid var(--border-color);
            transition: all 0.2s;
        }

        .tag:hover {
            background: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }

        /* Article Content */
        .post-content {
            font-size: 1rem;
            line-height: 1.9;
        }

        .post-content h1,
        .post-content h2,
        .post-content h3,
        .post-content h4,
        .post-content h5,
        .post-content h6 {
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-weight: 600;
            line-height: 1.4;
            color: var(--text-color);
        }

        .post-content h1 { font-size: 1.875rem; }
        .post-content h2 { 
            font-size: 1.5rem; 
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }
        .post-content h3 { font-size: 1.25rem; }
        .post-content h4 { font-size: 1.125rem; }

        .post-content p {
            margin-bottom: 1.25rem;
        }

        .post-content ul,
        .post-content ol {
            margin: 1.25rem 0;
            padding-left: 1.5rem;
        }

        .post-content li {
            margin-bottom: 0.5rem;
        }

        .post-content blockquote {
            margin: 1.5rem 0;
            padding: 1rem 1.5rem;
            background: var(--bg-secondary);
            border-left: 4px solid var(--primary-color);
            border-radius: 0 8px 8px 0;
            color: var(--text-light);
            font-style: italic;
        }

        .post-content blockquote p:last-child {
            margin-bottom: 0;
        }

        /* Code */
        .post-content code {
            font-family: 'JetBrains Mono', 'Fira Code', Consolas, monospace;
            font-size: 0.9em;
            background: var(--bg-code);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            color: var(--link-color);
        }

        .post-content pre {
            margin: 1.5rem 0;
            padding: 1.25rem;
            background: var(--bg-code);
            border-radius: 10px;
            overflow-x: auto;
            border: 1px solid var(--border-color);
        }

        .post-content pre code {
            background: none;
            padding: 0;
            color: inherit;
            font-size: 0.875rem;
            line-height: 1.6;
        }

        /* Images */
        .post-content img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            margin: 1.5rem 0;
            box-shadow: var(--shadow-lg);
        }

        /* Tables */
        .post-content table {
            width: 100%;
            margin: 1.5rem 0;
            border-collapse: collapse;
            font-size: 0.95rem;
        }

        .post-content th,
        .post-content td {
            padding: 0.75rem 1rem;
            border: 1px solid var(--border-color);
            text-align: left;
        }

        .post-content th {
            background: var(--bg-secondary);
            font-weight: 600;
        }

        .post-content tr:nth-child(even) {
            background: var(--bg-secondary);
        }

        /* Math */
        .math-display {
            margin: 1.5rem 0;
            overflow-x: auto;
            padding: 1rem;
            background: var(--bg-secondary);
            border-radius: 8px;
        }

        /* Anchor links */
        .anchor-link {
            opacity: 0;
            margin-left: 0.5rem;
            color: var(--text-muted);
            font-weight: 400;
            transition: opacity 0.2s;
        }

        .post-content h2:hover .anchor-link,
        .post-content h3:hover .anchor-link,
        .post-content h4:hover .anchor-link {
            opacity: 1;
        }

        /* Theme Toggle */
        .theme-toggle {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            background: var(--primary-color);
            color: white;
            border: none;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.25rem;
            box-shadow: var(--shadow-lg);
            transition: transform 0.2s, background 0.2s;
            z-index: 1000;
        }

        .theme-toggle:hover {
            transform: scale(1.1);
            background: var(--primary-hover);
        }

        /* Comments Section */
        .comments-section {
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-color);
        }

        .comments-section h3 {
            font-size: 1.25rem;
            margin-bottom: 1.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        /* Footer */
        .post-footer {
            margin-top: 3rem;
            padding-top: 1.5rem;
            border-top: 1px solid var(--border-color);
            text-align: center;
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        /* Responsive */
        @media (max-width: 1024px) {
            .toc-sidebar {
                display: none;
            }
            
            .page-wrapper {
                padding: 1.5rem;
            }
        }

        @media (max-width: 768px) {
            .post-header h1 {
                font-size: 1.75rem;
            }
            
            .post-meta {
                flex-direction: column;
                gap: 0.5rem;
            }
            
            .theme-toggle {
                bottom: 1rem;
                right: 1rem;
                width: 44px;
                height: 44px;
            }
        }

        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-secondary);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--border-color);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--text-muted);
        }
    </style>
</head>
<body>
    <div class="page-wrapper">
        <!-- TOC Sidebar -->
        <aside class="toc-sidebar">
            <div class="toc-container">
                <h3><i class="fas fa-list"></i> 目录</h3>
                <div class="toc">
<ul>
<li><a href="#基本概念">基本概念</a></li>
<li><a href="#点估计">点估计</a><ul>
<li><a href="#点估计的概念">点估计的概念</a></li>
<li><a href="#点估计的方法">点估计的方法</a></li>
<li><a href="#点估计的优良性准则">点估计的优良性准则</a></li>
</ul>
</li>
<li><a href="#区间估计">区间估计</a><ul>
<li><a href="#区间估计的概念">区间估计的概念</a></li>
<li><a href="#区间估计的方法">区间估计的方法</a></li>
<li><a href="#一些情况下的区间估计">一些情况下的区间估计</a></li>
</ul>
</li>
<li><a href="#最大似然估计">最大似然估计</a><ul>
<li><a href="#基本假设">基本假设</a></li>
<li><a href="#似然函数">似然函数</a></li>
<li><a href="#最大似然估计_1">最大似然估计</a></li>
<li><a href="#离散型随机变量的最大似然估计">离散型随机变量的最大似然估计</a></li>
<li><a href="#连续型随机变量的最大似然估计">连续型随机变量的最大似然估计</a></li>
</ul>
</li>
<li><a href="#贝叶斯估计">贝叶斯估计</a></li>
</ul>
</div>

            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <article>
                <header class="post-header">
                    <a href="../../../index.html" class="back-link">
                        <i class="fas fa-arrow-left"></i> 返回博客列表
                    </a>
                    <h1>Untitled</h1>
                    <div class="post-meta">
                        <span><i class="fas fa-calendar-alt"></i> 2026-01-28</span>
                        <span><i class="fas fa-folder"></i> 数学笔记/05.分布</span>
                        <span><i class="fas fa-user"></i> Hongwei Zhao</span>
                    </div>
                    <div class="tags">
                        
                    </div>
                </header>

                <div class="post-content">
                    <p>统计学与概率论的区别就是归纳和演绎，前者通过样本推测总体的分布，而后者已知总体分布去研究样本。因此参数估计则是归纳的过程，参数估计有两种形式：<strong>点估计</strong>和<strong>区间估计</strong><u>（点估计和区间估计都是对于未知参数的估计，而<strong>点估计给出的是一个参数可能的值</strong>，<strong>区间估计给出的是参数可能在的范围</strong>）</u>。</p>
<h2 id="基本概念">基本概念<a class="anchor-link" href="#基本概念" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>统计量</strong>: 样本中包含着总体的信息， 针对不同要求构造出样本的某种函数， 这种函数在统计学中称统计量。</li>
<li><strong>参数空间</strong>: 假设总体概率密度函数形式已知，未知分布中的参数<span class="math-inline">\theta</span>, <span class="math-inline">\theta</span> 全部可容许值组成的集合称为参数空间， 记为<span class="math-inline">\Theta</span></li>
</ul>
<h2 id="点估计">点估计<a class="anchor-link" href="#点估计" title="Permanent link">&para;</a></h2>
<h3 id="点估计的概念">点估计的概念<a class="anchor-link" href="#点估计的概念" title="Permanent link">&para;</a></h3>
<p><strong>点估计（<em>Point estimation</em>）</strong> ：设<span class="math-inline">x_1, ..., x_n</span> 来自总体的一个样本，用于估计未知参数<span class="math-inline">\theta</span> 统计量<span class="math-inline">\hat{\theta}=\hat{\theta}(x_1,...,x_n)</span> 为<span class="math-inline">\theta</span> 估计量，或称为<span class="math-inline">\theta</span> 点估计。</p>
<h3 id="点估计的方法">点估计的方法<a class="anchor-link" href="#点估计的方法" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>矩估计</strong></li>
</ol>
<p><strong>定义</strong> ：设总体概率函数已知，为<span class="math-inline">p(x;\theta_1,...,\theta_k)</span>，<span class="math-inline">(\theta_1,...,\theta_k)\in\Theta</span> 未知参数或参数向量，<span class="math-inline">x_1,...,x_n</span> 样本，假定总体的<span class="math-inline">k</span> 原点矩<span class="math-inline">\mu_k</span> 在，则对所有的<span class="math-inline">j</span>，<span class="math-inline">o&lt;j&lt;k</span>，<span class="math-inline">\mu_j</span> 存在，若假设<span class="math-inline">\theta_1,...,\theta_k</span> 够表示成<span class="math-inline">\mu_1,...,\mu_k</span> 函数<span class="math-inline">\theta_j=\theta_j(\mu_1,...,\mu_k)</span>，则可给出诸<span class="math-inline">\theta_j</span> 矩估计：</p>
<p><div class="math-display"><br />
   \hat{\theta_j}=\theta_j(a_1,...,a_k),\quad j=1,...,k<br />
</div></p>
<p>其中<span class="math-inline">a_1,...,a_k</span> 前<span class="math-inline">k</span> 样本原点矩<span class="math-inline">a_j=\frac{1}{n}\sum_{i=1}^{n}x_i^j</span>.</p>
<p>矩估计基于大数定律（格里纹科定理），实质是用经验分布函数去替换总体分布，矩估计可以概括为：</p>
<ul>
<li>
<p>用样本矩代替总体矩（可以是原点矩也可以是中心矩）；</p>
</li>
<li>
<p>用样本矩的函数去替换相应的总体矩的函数。</p>
</li>
</ul>
<blockquote>
<p><strong>注</strong> ：矩估计可能是不唯一的，尽量使用低阶矩给出未知参数的估计 。</p>
</blockquote>
<ol start="2">
<li><strong>最大似然估计</strong></li>
</ol>
<p><strong>定义</strong> ：设总体的概率函数为<span class="math-inline">p(x;\theta),\ \theta\in\Theta</span>，其中<span class="math-inline">\theta</span> 一个未知参数或几个未知参数组成的参数向量，<span class="math-inline">\Theta</span> 参数空间，<span class="math-inline">x_1,...,x_n</span> 来自该总体的样本，将样本的联合概率函数看成<span class="math-inline">\theta</span> 函数，用<span class="math-inline">L(\theta;x_1,...,x_n)</span> 示，简记为<span class="math-inline">L(\theta)</span>，<br />
   <div class="math-display"><br />
   L(\theta)=L(\theta;x_1,...,x_n)=p(x_1;\theta)p(x_2;\theta)...p(x_n;\theta)<br />
   </div><br />
   <span class="math-inline">L(\theta)</span> 为样本的<strong>似然函数</strong>。若统计量<span class="math-inline">\hat{\theta}=\hat{\theta}(x_1,...,x_n)</span> 足<br />
   <div class="math-display"><br />
   L(\hat{\theta})=\max_{\theta\in\Theta}L(\theta)<br />
   </div><br />
   则称<span class="math-inline">\hat{\theta}</span> <span class="math-inline">\theta</span> <strong>最大似然估计</strong>，简称<strong>MLE（maximum likelihood estimate）.</strong></p>
<p><strong>注</strong> ：最大似然估计基于样本观测数据，根据概率论思想进行参数估计，首先抽取一定样本，默认这些样本的出现概率是符合原始分布的，即恰好抽到这些样本是因为这些样本出现的概率极大，然后根据概率密度计算联合概率，形成似然函数，似然函数极值位置即为参数的估计值。<strong><u>最大似然估计的前提是已知数据的分布。</u></strong></p>
<p><strong>最大似然估计步骤</strong> ：</p>
<ul>
<li>写出似然函数；</li>
<li>对似然函数取对数，并整理；</li>
<li>求参数向量的偏导，令其为0，得到似然方程；</li>
<li>求解似然方程，其解为参数值。</li>
</ul>
<ol start="3">
<li><strong>最小均方误差估计</strong></li>
</ol>
<p>在样本量一定时，评价一个点估计好坏的度量指标可使用估计值<span class="math-inline">\hat{\theta}</span> 参数真值<span class="math-inline">\theta</span> 距离函数，最常用的是距离平方，由于<span class="math-inline">\hat{\theta}</span> 有随机性，对该函数求期望即得<strong>均方误差</strong>：<br />
   <div class="math-display"><br />
   \begin{align}<br />
   MSE(\hat{\theta})&amp;=E(\hat{\theta}-\theta)^2\<br />
   &amp;=E[(\hat{\theta}-E\hat{\theta})+(E\hat{\theta}-\theta)]^2\<br />
   &amp;=E(\hat{\theta}-E\hat{\theta})^2+(E\hat{\theta}-\theta)^2+\underbrace{2E[(\hat{\theta}-E\hat{\theta})(E\hat{\theta}-\theta)]}<em>{E(\hat{\theta}-E\hat{\theta})=0}\<br />
   &amp;=\underbrace{Var(\hat{\theta})}</em>{点估计的方差}+\underbrace{(E\hat{\theta}-\theta)^2}_{偏差的平方}<br />
   \end{align}<br />
   </div><br />
   其中，<strong>如果<span class="math-inline">\hat{\theta}</span> <span class="math-inline">\theta</span> 无偏估计，则<span class="math-inline">MSE(\hat{\theta})=Var(\hat{\theta})</span>，此时用均方误差评价点估计与用方差是完全一样的</strong>。如果如果<span class="math-inline">\hat{\theta}</span> 是<span class="math-inline">\theta</span> 无偏估计，就要看其均方误差<span class="math-inline">MSE(\hat{\theta})</span>，即不仅要看其方差大小，还要看其偏差大小。</p>
<p><strong>定义</strong> ：设有样本<span class="math-inline">x_1,...,x_n</span>，对待估参数<span class="math-inline">\theta</span>，设有一个估计类，如果对该估计类中另外任意一个<span class="math-inline">\theta</span> 估计<span class="math-inline">\widetilde{\theta}</span>，在参数空间<span class="math-inline">\Theta</span> 都有<span class="math-inline">MSE_\theta(\hat{\theta})\leq MSE_\theta(\widetilde{\theta})</span>，称<span class="math-inline">\hat{\theta}(x_1,...,x_n)</span> 该估计类中<span class="math-inline">\theta</span> 一致最小均方误差估计。</p>
<ol start="4">
<li><strong>最小方差无偏估计</strong></li>
</ol>
<p><strong>定义</strong> ：设<span class="math-inline">\hat{\theta}</span> <span class="math-inline">\theta</span> 一个无偏估计，如果对另外任意一个<span class="math-inline">\theta</span> 无偏估计<span class="math-inline">\widetilde{\theta}</span>，在参数空间<span class="math-inline">\Theta={\theta}</span> 都有<span class="math-inline">Var_{\theta}(\hat{\theta})\leq Var_{\theta}(\widetilde{\theta})</span>，则称<span class="math-inline">\hat{\theta}</span> <span class="math-inline">\theta</span> 一致最小方差无偏估计，简记为<strong>UMVUE</strong>。</p>
<p><strong>判断准则</strong> ：设<span class="math-inline">\hat{\theta}=\hat{\theta}(x_1,...,x_n)</span> <span class="math-inline">\theta</span> 一个无偏估计，<span class="math-inline">Var(\hat{\theta})&lt;+\infty</span>.如果对任意一个满足<span class="math-inline">E(\varphi(x_1,...,x_n))=0</span> <span class="math-inline">\varphi</span>，都有<br />
   <div class="math-display"><br />
   Cov_\theta(\hat{\theta},\varphi)=0,\quad\forall\theta\in\Theta,<br />
   </div><br />
   则<span class="math-inline">\hat{\theta}</span> <span class="math-inline">\theta</span> UMVUE.</p>
<ol start="5">
<li><strong>贝叶斯估计</strong></li>
</ol>
<p>区别于频率学派，在统计推断中贝叶斯用到了三种信息<strong> ：总体信息、样本信息和先验信息</strong>（频率学派只用了前两种），其中：</p>
<ul>
<li>总体信息：总体信息即总体分布或总体所属分布族提供的信息，如，若已知总体是正态分布，则可以知道很多信息；</li>
<li>样本信息：样本信息即抽取样本所得观测值提供的信息，如，在有了样本观测值后，可以根据它知道总体的一些特征数；</li>
<li>先验信息：若把抽取样本看作做一次试验，则样本信息就是试验中得到的信息，如，在一次抽样后，这第一次的抽样就是先验信息。先验信息来源于经验和历史资料。</li>
</ul>
<p><strong>回顾贝叶斯公式</strong>：设<span class="math-inline">{B_1, B_2, ...B_n}</span> 样本空间的一个分割，<span class="math-inline">A</span> <span class="math-inline">\Omega</span> 的一个事件，<span class="math-inline">P(B_i)&gt;0</span>，<span class="math-inline">i=1,2,...,n</span>，<span class="math-inline">P(A)&gt;0</span>，则<br />
   <div class="math-display"><br />
   P(B_i|A)=\frac{P(A|B_i)P(B_i)}{\sum_{j=1}^{n}P(A|B_j)P(B_j)}<br />
   </div><br />
<strong>贝叶斯密度函数形式</strong> ：</p>
<ul>
<li>
<p>在参数<span class="math-inline">\theta</span> 布已知（已假设）的情况下，<span class="math-inline">p(x|\theta)</span> 示随机变量<span class="math-inline">\theta</span> 某个给定值时总体的<strong>条件概率函数</strong>，（参考<span class="math-inline">P(A|B)</span>）；</p>
</li>
<li>
<p>任一未知量<span class="math-inline">\theta</span> 可以看作随机变量，可用一个概率分布去描述，这个分布成为<strong>先验分布</strong>，该先验分布<span class="math-inline">\pi(\theta)</span>，（参考<span class="math-inline">P(B)</span>）；</p>
</li>
<li>
<p>贝叶斯的观点，样本<span class="math-inline">X=(x_1,...,x_n)</span> 产生需分两步：</p>
<ul>
<li>从先验分布<span class="math-inline">\pi(\theta)</span> 生一个样本<span class="math-inline">\theta_0</span>；</li>
<li>从<span class="math-inline">p(X|\theta_0)</span> 产生一组样本。</li>
</ul>
<p>此时，样本<span class="math-inline">X=(x_1,...,x_n)</span> <strong>联合条件概率函数</strong>（参考<span class="math-inline">\sum_{j=1}^{n}P(A|B_j)</span>）为<br />
 <div class="math-display"><br />
 p(X|\theta_0)=p(x_1,...,x_n|\theta_0)=\prod^{n}_{i=1}p(x_i|\theta_0)<br />
 </div></p>
</li>
<li>
<p>因为<span class="math-inline">\theta_0</span> 知，是从先验分布<span class="math-inline">\pi(\theta)</span> 产生的，所以需要考虑它的发生概率，样本<span class="math-inline">X</span> 参数<span class="math-inline">\theta</span> <strong>联合分布</strong>（参考<span class="math-inline">\sum_{j=1}^{n}P(A|B_j)P(B_j)</span>）为<br />
     <div class="math-display"><br />
     h(X,\theta)=p(X|\theta)\pi(\theta)<br />
     </div></p>
</li>
<li>
<p>因为目的是对<span class="math-inline">\theta</span> 行推断，所以在有样本观测值<span class="math-inline">X=(x_1,...,x_n)</span> 后，可依据<span class="math-inline">h(X,\theta)</span> <span class="math-inline">\theta</span> 出推断，按照乘法公式（参考1.5.2节），<span class="math-inline">h(X,\theta)</span> 分解为<br />
     <div class="math-display"><br />
     h(X,\theta)=\pi(\theta|X)m(X)<br />
     </div><br />
     其中，<span class="math-inline">m(X)</span> <span class="math-inline">X</span> 边际概率函数，类比<span class="math-inline">\pi(\theta)</span>，<br />
     <div class="math-display"><br />
     m(X)=\int_\Theta h(X,\theta)d\theta=\int_\Theta p(X|\theta)\pi(\theta)d\theta<br />
     </div><br />
     所以可通过条件概率<span class="math-inline">\pi(\theta|X)</span> 断<span class="math-inline">\theta</span> 分布<br />
     <div class="math-display"><br />
     \pi(\theta|X)=\frac{h(X,\theta)}{m(X)}=\frac{p(X|\theta)\pi(\theta)}{\int_{\Theta}p(X|\theta)\pi(\theta)d\theta}<br />
     </div><br />
     该分布成为<span class="math-inline">\theta</span> <strong>后验分布</strong>。<strong>它其实是利用总体和样本对先验分布<span class="math-inline">\pi(\theta)</span> 整的结果，比<span class="math-inline">\pi(\theta)</span> 接近<span class="math-inline">\theta</span> 实际情况（机器学习里的贝叶斯模型就是基于这样的原理）</strong>。</p>
<blockquote>
<p><strong>Flag</strong> ：感觉贝叶斯定理很有意思，今后也会学习相关的贝叶斯分析数据，敬请期待～</p>
</blockquote>
</li>
</ul>
<h3 id="点估计的优良性准则">点估计的优良性准则<a class="anchor-link" href="#点估计的优良性准则" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>无偏性</strong> ：设<span class="math-inline">\hat{\theta}=\hat{\theta}(x_1,...,x_n)</span> <span class="math-inline">\theta</span> 一个估计，<span class="math-inline">\theta</span> 参数空间为<span class="math-inline">\Theta</span>，若对任意的<span class="math-inline">\theta \in \Theta</span>，有<br />
   <div class="math-display"><br />
   E_{\theta}(\hat{\theta})=\theta<br />
   </div><br />
   则称<span class="math-inline">\hat{\theta}</span> <span class="math-inline">\theta</span> <strong>无偏估计</strong>，否则称为<strong>有偏估计</strong>。无偏性的要求也可以改写为<span class="math-inline">E_{\theta}(\hat{\theta-\theta})=0</span>，无偏性表示表示估计参数与真实参数没有系统偏差。</li>
</ol>
<blockquote>
<p><strong>一个重要的结论</strong> ：样本均值<span class="math-inline">\bar{x}=\frac{1}{n}\sum_{i=1}^nx_i</span> 总体均值<span class="math-inline">\mu</span> 无偏估计。样本方差<span class="math-inline">s_n^2=\frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})^2</span> 是总体方差<span class="math-inline">\sigma^2</span> 无偏估计（而是渐进无偏估计），因此需要对样本方差进行修正，<span class="math-inline">s^2=\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})^2</span>.</p>
<ul>
<li>样本均值的无偏性推导</li>
</ul>
<p><div class="math-display"><br />
\begin{align}<br />
E(\bar{x})=&amp;E(\frac{1}{n}\sum_{i=1}^nx_i)\<br />
=&amp;\frac{1}{n}\sum_{i=1}^nE(x_i),\ x_i为iid\<br />
=&amp;\frac{1}{n}\sum_{i=1}^n\mu\<br />
=&amp;\mu<br />
\end{align}<br />
</div></p>
<ul>
<li>样本方差的有偏性推导<br />
  <div class="math-display"><br />
  \begin{align}<br />
  E(s_n^2)=&amp;E[\frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})^2]\<br />
  =&amp;E[\frac{1}{n}\sum_{i=1}^n((x_i-\mu)-\frac{1}{n}(\bar{x}-\mu))^2]\<br />
  =&amp;E[\frac{1}{n}\sum_{i=1}^n((x_i-\mu)^2-\frac{2}{n}(x_i-\mu)(\bar{x}-\mu)+\frac{1}{n}(\bar{x}-\mu)^2)]\<br />
  =&amp;E[\frac{1}{n}\sum_{i=1}^n(x_i-\mu)^2-\frac{2}{n}\sum_{i=1}^n(x_i-\mu)(\bar{x}-\mu)+(\bar{x}-\mu)^2],\ 其中,\bar{X}-\mu=\frac{1}{n}\sum_{i=1}^n(x_i-\mu)\<br />
  =&amp;E[\frac{1}{n}\sum_{i=1}^n(x_i-\mu)^2-2(\bar{x}-\mu)^2+(\bar{x}-\mu)^2]\<br />
  =&amp;E[\frac{1}{n}\sum_{i=1}^n(x_i-\mu)^2-(\bar{x}-\mu)^2]\<br />
  =&amp;E[\frac{1}{n}\sum_{i=1}^n(x_i-\mu)^2]-E[(\bar{x}-\mu)^2]\<br />
  =&amp;\sigma^2-E[(\bar{x}-\mu)^2]\ ...(1)\<br />
  =&amp;\sigma^2-\frac{\sigma^2}{n}\<br />
  =&amp;\frac{n-1}{n}\sigma^2,\quad 当n\rightarrow \infty时, E(s_n^2)\rightarrow \sigma^2\<br />
  \<br />
  E[(\bar{x}-\mu)^2]=&amp;E(\bar{x}^2)-2\mu E(\bar{x})+\mu^2\<br />
  =&amp;E(\bar{x}^2)-\mu^2\<br />
  =&amp;Var(\bar{x})+E^2(\bar{x})-\mu^2\<br />
  =&amp;Var(x)\<br />
  =&amp;\frac{\sigma^2}{n}\ ...代入(1)式<br />
  \end{align}<br />
  </div></li>
</ul>
</blockquote>
<ol start="2">
<li><strong>有效性</strong></li>
</ol>
<p>无偏估计往往有很多种，以总体均值为例，<span class="math-inline">x_1,...,x_n</span> 取自某总体的样本，样本均值<span class="math-inline">\mu</span> 样本<span class="math-inline">x_i</span> 是总体均值的无偏估计，对于两个估计参数的选取需要基于一个度量无偏估计优劣的准则。有效性作为这样的准则，反映了<strong>参数估计值和参数真值的波动</strong>，波动大小可用方差来衡量，波动越小表示参数的估计越有效。</p>
<p>设<span class="math-inline">\hat{\theta_1}</span>，<span class="math-inline">\hat{\theta_2}</span> <span class="math-inline">\theta</span> 两个无偏估计，如果对任意的<span class="math-inline">\theta\in\Theta</span> <br />
   <div class="math-display"><br />
   Var(\hat{\theta}_1)\leq Var(\hat{\theta}_2)<br />
   </div><br />
   且至少有一个<span class="math-inline">\theta\in\Theta</span> 得上述不等号严格成立，则称<span class="math-inline">\hat{\theta}_1</span> <span class="math-inline">\hat{\theta}_2</span> 效。</p>
<ol start="3">
<li><strong>相合性</strong></li>
</ol>
<p>根据格里纹科定理，随着样本量不断增大，经验分布函数逼近真实分布函数，即设<span class="math-inline">\theta\in\Theta</span> 未知参数，<span class="math-inline">\hat{\theta}<em>n=\hat{\theta}_n(x_1,...,x_n)</span> <span class="math-inline">\theta</span> 一个估计量，<span class="math-inline">n</span> 样本容量，若对任何一个<span class="math-inline">\epsilon&gt;0</span>，有<br />
   <div class="math-display"><br />
   \lim</em>{n\rightarrow\infty}P(|\hat{\theta}_n-\theta|\geq\epsilon)=0<br />
   </div><br />
   则称<span class="math-inline">\hat{\theta}_n</span> 参数<span class="math-inline">\theta</span> 相合估计。</p>
<p><strong>定理1</strong> ：设<span class="math-inline">\hat{\theta}<em>n=\hat{\theta}_n(x_1,...,x_n)</span> <span class="math-inline">\theta</span> 一个估计量，若<br />
   <div class="math-display"><br />
   \lim</em>{n\rightarrow\infty}E(\hat{\theta}<em>n)=\theta,\quad\lim</em>{n\rightarrow\infty}Var(\hat{\theta}_n)=0<br />
   </div><br />
   则<span class="math-inline">\hat{\theta}_n</span> <span class="math-inline">\theta</span> 相合估计。</p>
<p><strong>定理2</strong> ：若<span class="math-inline">\hat{\theta}<em>{n1},...,\hat{\theta}</em>{nk}</span> 别是<span class="math-inline">\theta_1,...,\theta_k</span> 相合估计，<span class="math-inline">\eta=g(\theta_1,...,\theta_k)</span> <span class="math-inline">\theta_1,...,\theta_k</span> 连续函数，则<span class="math-inline">\hat{\eta}<em>n=g(\hat{\theta}</em>{n1},...,\hat{\theta}_{nk})</span> <span class="math-inline">\eta</span> 相合估计。</p>
<blockquote>
<p>矩估计一般都具有相合性：</p>
<ul>
<li>样本均值是总体均值的相合估计；</li>
<li>样本标准差是总体标准差的相合估计；</li>
<li>样本变异系数<span class="math-inline">s/\bar{x}</span> 总体变异系数的相合估计。</li>
</ul>
</blockquote>
<ol start="4">
<li><strong>渐进正态性</strong>（MLE）</li>
</ol>
<p>在很一般条件下，总体分布<span class="math-inline">p(x;\theta)</span> 的<span class="math-inline">\theta</span> MLE<span class="math-inline">\hat{\theta}<em>n</span> 有相合性和渐进正态性，即<span class="math-inline">\hat{\theta}_n\sim AN(\theta,\frac{1}{nI(\theta)})</span>，其中<span class="math-inline">n</span> 样本容量，<span class="math-inline">I(\theta)=\int</em>{-\infty}^{\infty}(\frac{\part{lnp}}{\part\theta})^2p(x;\theta)dx</span> 费希尔信息量。</p>
<ol start="5">
<li><strong>充分性</strong>（UMVUE）</li>
</ol>
<ul>
<li>任一参数<span class="math-inline">\theta</span> UMVUE不一定存在，若存在，则它一定是充分统计量的函数；</li>
<li>若<span class="math-inline">\theta</span> 某个无偏估计<span class="math-inline">\hat{\theta}</span> 是充分统计量<span class="math-inline">T=T(x_1,...,x_n)</span> 函数，则通过条件期望可以获得一个新的无偏估计<span class="math-inline">\widetilde{\theta}=E(\hat{\theta|T})</span>，且方差比原估计的方差要小；</li>
<li>考虑<span class="math-inline">\theta</span> 估计时，只需要在其充分统计量的函数中寻找即可，该说法对所有统计推断都是正确的，这便是充分性原则。</li>
</ul>
<h2 id="区间估计">区间估计<a class="anchor-link" href="#区间估计" title="Permanent link">&para;</a></h2>
<h3 id="区间估计的概念">区间估计的概念<a class="anchor-link" href="#区间估计的概念" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>双侧区间</strong></li>
</ol>
<p>设<span class="math-inline">\theta</span> 总体的一个参数，其参数空间为<span class="math-inline">\Theta</span>，<span class="math-inline">x_1,...,x_n</span> 来自该总体的样本，对给定的一个<span class="math-inline">\alpha\quad(0&lt;\alpha&lt;1)</span>，假设有两个统计量<span class="math-inline">\hat{\theta}<em>L=\hat{\theta}_L(x_1,...,x_n)</span> <span class="math-inline">\hat{\theta}_U=\hat{\theta}_U(x_1,...,x_n)</span>，若对任意的<span class="math-inline">\theta\in\Theta</span>，有<br />
   <div class="math-display"><br />
   P</em>\theta(\hat{\theta}_L\leq\theta\leq\hat{\theta}_U)\geq(=)1-\alpha<br />
   </div><br />
   其中，总体为连续分布时取等号，表示用足了置信水平。称随机区间<span class="math-inline">[\hat{\theta}_L,\hat{\theta}_U]</span> <span class="math-inline">\theta</span> <strong>置信水平为<span class="math-inline">1-\alpha</span> 置信区间</strong>，或简称<span class="math-inline">[\hat{\theta}_L,\hat{\theta}_U]</span> <span class="math-inline">\theta</span> <strong><span class="math-inline">1-\alpha</span> 信区间</strong>，<span class="math-inline">\hat{\theta}_L</span> <span class="math-inline">\hat{\theta}_U</span> 别称为<span class="math-inline">\theta</span> <strong>置信下限</strong>和<strong>置信上限</strong>。</p>
<blockquote>
<p>置信水平<span class="math-inline">1-\alpha</span> 频率解释：在大量的区间估计观测值中，至少有<span class="math-inline">100(1-\alpha)\%</span> 含<span class="math-inline">\theta</span>，如下图所示，其置信度为0.95.</p>
<p><img src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202209222015291.gif" style="zoom:100%;" /></p>
</blockquote>
<ol start="2">
<li><strong>单侧区间</strong></li>
</ol>
<p>设<span class="math-inline">\hat{\theta}<em>L=\hat{\theta}_L(x_1,...,x_n)</span> 统计量，对给定的<span class="math-inline">\alpha\in(0,1)</span> 任意的<span class="math-inline">\theta\in\Theta</span>，有<br />
   <div class="math-display"><br />
   P</em>\theta(\hat{\theta}<em>L\leq\theta)\geq1-\alpha,\quad\forall\theta\in\Theta<br />
   </div><br />
   则称<span class="math-inline">\hat{\theta}_L</span> <span class="math-inline">\theta</span> 置信水平为<span class="math-inline">1-\alpha</span> <strong>置信下限</strong>。同理，设<span class="math-inline">\hat{\theta}_U=\hat{\theta}_U(x_1,...,x_n)</span> 统计量，对给定的<span class="math-inline">\alpha\in(0,1)</span> 任意的<span class="math-inline">\theta\in\Theta</span>，有<br />
   <div class="math-display"><br />
   P</em>\theta(\hat{\theta}_L\geq\theta)\geq1-\alpha,\quad\forall\theta\in\Theta<br />
   </div><br />
   则称<span class="math-inline">\hat{\theta}_L</span> <span class="math-inline">\theta</span> 置信水平为<span class="math-inline">1-\alpha</span> <strong>置信上限</strong>。</p>
<h3 id="区间估计的方法">区间估计的方法<a class="anchor-link" href="#区间估计的方法" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>枢轴量法</strong></li>
</ol>
<p><strong><em>Step 1</em></strong>：设法构造一个样本和<span class="math-inline">\theta</span> 函数<span class="math-inline">G=G(x_1,...,x_n,\theta)</span> 得<strong><span class="math-inline">G</span> 分布不依赖于未知参数</strong>，称具有这种性质的<span class="math-inline">G</span> 枢轴量。</p>
<p><strong><em>Step 2</em></strong>：适当地选择两个常数c，d，使对给定的<span class="math-inline">\alpha\quad(0&lt;\alpha&lt;1)</span>，有<br />
   <div class="math-display"><br />
   P(c\leq G\leq d)=1-\alpha<br />
   </div><br />
   （在离散场合，将上式等号改为<span class="math-inline">\geq</span>）</p>
<p><strong><em>Step 3</em></strong>：假如能将<span class="math-inline">c\leq G\leq d</span> 行不等式等价变形化为<span class="math-inline">\hat{\theta}<em>L\leq\theta\leq\hat{\theta}_U</span>，则有<br />
   <div class="math-display"><br />
   P</em>\theta(\hat{\theta}_L\leq\theta\leq\hat{\theta}_U)=1-\alpha<br />
   </div><br />
   表明<span class="math-inline">[\hat{\theta}_L,\hat{\theta}_U]</span> <span class="math-inline">\theta</span> <span class="math-inline">1-\alpha</span> 等置信区间。</p>
<blockquote>
<p><strong>注</strong> ：满足条件的c和d有很多，最终选择的目的是希望平均长度<span class="math-inline">E_\theta(\hat{\theta}<em>U)-\hat{\theta}_L</span> 可能短，但在一些场合中很难做到这一点，因此可以选择c和d，使得两个尾部概率各为<span class="math-inline">\alpha/2</span>，即<br />
<div class="math-display"><br />
P</em>\theta(G<c)=P_\theta(G>d)=\alpha/2<br />
</div><br />
得到<strong>等尾置信区间</strong>。</p>
<p>例：设<span class="math-inline">x_1,...,x_n</span> 来自均匀总体<span class="math-inline">U(0,\theta)</span> 一个样本，试对设定的<span class="math-inline">\alpha\ (0&lt;\alpha&lt;1)</span> 出<span class="math-inline">\theta</span> <span class="math-inline">1-\alpha</span> 等置信区间。</p>
<p>解：三步法：</p>
<ul>
<li>已知<span class="math-inline">\theta</span> 最大似然估计为样本的最大次序统计量<span class="math-inline">x_{(n)}</span>，而<span class="math-inline">x_{(n)}/\theta</span> 密度函数为</li>
</ul>
<p><div class="math-display"><br />
p(y;\theta)=ny^{n-1},\quad 0&lt;y&lt;1<br />
</div></p>
<p>它与参数<span class="math-inline">\theta</span> 关，故可取<span class="math-inline">x_{(n)}/\theta</span> 为枢轴量<span class="math-inline">G</span>。</p>
<ul>
<li>由于<span class="math-inline">x_{(n)}/\theta</span> 分布函数为<span class="math-inline">F(y)=y^n</span>，<span class="math-inline">0&lt;y&lt;1</span>，故<span class="math-inline">P(c\leq x_{(n)}/\theta\leq d=d^n-c^n)</span>，因此可以选择适当的c和d满足</li>
</ul>
<p><div class="math-display"><br />
d^n-c^n=1-\alpha<br />
</div></p>
<ul>
<li>在<span class="math-inline">0\leq c&lt;d\leq 1</span> <span class="math-inline">d^n-c^n=1-\alpha</span> 条件下，当<span class="math-inline">d=1, c=\sqrt[n]{\alpha}</span> ，<span class="math-inline">E_\theta(\hat{\theta}<em>U)-\hat{\theta}_L</span> 最小值，所以<span class="math-inline">[x</em>{(n)},x_{(n)}/\sqrt[n]{\alpha}]</span> <span class="math-inline">1-\alpha</span> 信区间</li>
</ul>
</blockquote>
<h3 id="一些情况下的区间估计">一些情况下的区间估计<a class="anchor-link" href="#一些情况下的区间估计" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>单个正态总体参数的置信区间</strong><br />
   * <strong><span class="math-inline">\sigma</span> 知时<span class="math-inline">\mu</span> 置信区间</strong>：<span class="math-inline">[\bar{x}-u_{1-\alpha/2}\sigma/\sqrt{n},\quad\bar{x}+u_{1-\alpha/2}\sigma/\sqrt{n}]</span><br />
   * <strong><span class="math-inline">\sigma</span> 知时<span class="math-inline">\mu</span> 置信区间</strong>：<span class="math-inline">[\bar{x}-t_{1-\alpha/2}(n-1)s/\sqrt{n},\quad\bar{x}+t_{1-\alpha/2}(n-1)s/\sqrt{n}]</span><br />
   * <strong><span class="math-inline">\sigma^2</span> 置信区间（<span class="math-inline">\mu</span> 知）</strong>：<span class="math-inline">[(n-1)s^2/\chi^2_{1-\alpha/2}(n-1),\quad(n-1)s^2/\chi^2_{\alpha/2}(n-1)]</span></p>
</li>
<li>
<p><strong>大样本置信区间</strong>：<span class="math-inline">[\bar{x}-u_{1-\alpha/2}\sqrt{\frac{\bar{x}(1-\bar{x})}{n}},\quad \bar{x}+u_{1-\alpha/2}\sqrt{\frac{\bar{x}(1-\bar{x})}{n}}]</span></p>
</li>
<li>
<p><strong>两个正态总体下的置信区间</strong></p>
</li>
</ol>
<ul>
<li>
<p><strong><span class="math-inline">\mu_1-\mu_2</span> 置信区间</strong></p>
<ul>
<li><strong><span class="math-inline">\sigma^2_1</span> <span class="math-inline">\sigma^2_2</span> 知时</strong>：<span class="math-inline">[\bar{x}-\bar{y}-u_{1-\alpha/2}\sqrt{\frac{\sigma^2_1}{m}+\frac{\sigma^2_2}{n}},\quad \bar{x}-\bar{y}+u_{1-\alpha/2}\sqrt{\frac{\sigma^2_1}{m}+\frac{\sigma^2_2}{n}}]</span></li>
<li><strong><span class="math-inline">\sigma^2_1=\sigma^2_2=\sigma^2</span> 知时</strong>：<span class="math-inline">[\bar{x}-\bar{y}-\sqrt{\frac{m+n}{mn}}s_wt_{1-\alpha/2}(m+n-2),\quad \bar{x}-\bar{y}+\sqrt{\frac{m+n}{mn}}s_wt_{1-\alpha/2}(m+n-2)]</span></li>
<li><strong><span class="math-inline">\sigma^2_2/\sigma^2_1=c</span> 知时</strong>：<span class="math-inline">[\bar{x}-\bar{y}-\sqrt{\frac{mc+n}{mn}}s_wt_{1-\alpha/2}(m+n-2),\quad \bar{x}-\bar{y}+\sqrt{\frac{mc+n}{mn}}s_wt_{1-\alpha/2}(m+n-2)]</span></li>
<li><strong>当m和n都很大时的近似置信区间</strong>：<span class="math-inline">[\bar{x}-\bar{y}-u_{1-\alpha/2}\sqrt{\frac{s^2_x}{m}+\frac{s^2_y}{n}},\quad \bar{x}-\bar{y}+u_{1-\alpha/2}\sqrt{\frac{s^2_x}{m}+\frac{s^2_y}{n}}]</span></li>
<li><strong>一般情况下的近似置信区间</strong>：<span class="math-inline">[\bar{x}-\bar{y}-s_0t_{1-\alpha/2}(l),\quad \bar{x}-\bar{y}+s_0t_{1-\alpha/2}(l)]</span></li>
</ul>
</li>
<li>
<p><strong><span class="math-inline">\sigma^2_1/\sigma^2_2</span> 置信区间</strong>：<span class="math-inline">[\frac{s_x^2}{s_y^2}\cdot\frac{1}{F_{1-\alpha/2(m-1,n-1)}},\quad \frac{s_x^2}{s_y^2}\cdot\frac{1}{F_{\alpha/2(m-1,n-1)}}]</span></p>
</li>
</ul>
<h2 id="最大似然估计">最大似然估计<a class="anchor-link" href="#最大似然估计" title="Permanent link">&para;</a></h2>
<blockquote>
<p>Maximum Likelihood Estimation</p>
</blockquote>
<p>最大似然估计原理：利用已知的样本，找出最有可能生成该样本的参数。</p>
<h3 id="基本假设">基本假设<a class="anchor-link" href="#基本假设" title="Permanent link">&para;</a></h3>
<ul>
<li>参数<span class="math-inline">\theta</span> 确定（非随机） 的而未知的量</li>
<li>按类别把样本集分开， <span class="math-inline">R_j</span> 中的每个样本都是独立地从概率密度为<span class="math-inline">p(x|w_j)</span> 总体中独立地抽取出来的 – <strong>独立同分布</strong></li>
<li>类条件概率密度<span class="math-inline">p(x|w_j)</span> 已知分布， 参数向量未知</li>
<li>假设<span class="math-inline">R_j</span> 不包含关于<span class="math-inline">\theta_j(j\neq i)</span> 信息， 即不同类别的参数在函数上是独立的</li>
</ul>
<h3 id="似然函数">似然函数<a class="anchor-link" href="#似然函数" title="Permanent link">&para;</a></h3>
<p>似然性（likelihood）与概率（possibility）同样可以表示事件发生的可能性大小，但是二者有着很大的区别：</p>
<ul>
<li>概率<span class="math-inline">p(x\mid\theta)</span> 是在已知参数<span class="math-inline">\theta</span>  的情况下，发生观测结果<span class="math-inline">x</span> 可能性大小；</li>
<li>似然性<span class="math-inline">L(\theta\mid x) </span>  则是从观测结果<span class="math-inline">x</span> 出发，分布函数的参数为<span class="math-inline">\theta</span> 的可能性大小；</li>
</ul>
<p>可能听着不是那么好理解。我们再详细说明下，似然函数如下：<br />
<div class="math-display"><br />
L(\theta\mid x)=p(x\mid\theta)<br />
</div><br />
其中<span class="math-inline">x</span> 知，<span class="math-inline">\theta</span> 未知。若对于两个参数 <span class="math-inline">\theta_1</span>  <span class="math-inline">\theta_2</span> ，有<br />
<div class="math-display"><br />
L(\theta_1\mid x)=p(x\mid\theta_1) &gt; p(x\mid\theta_2)=L(\theta_2\mid x)<br />
</div><br />
那么意味着 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=\theta%3D\theta_1" /> 时，随机变量 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=X" /> 生成 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=x" /> 的概率大于当参数 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=\theta%3D\theta_2" /> 时。这也正是似然的意义所在，若观测数据为 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=x" /> ，那么 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=\theta_1" /> 是比 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=\theta_2" /> 更有可能为分布函数的参数。</p>
<p>在不同的时候， <img alt="[公式]" src="https://www.zhihu.com/equation?tex=p(x|\theta)" /> 可以表示概率也可以用于计算似然，这里给出个人的理解，整理如下：</p>
<ul>
<li>在 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=\theta" /> 已知，<img alt="[公式]" src="https://www.zhihu.com/equation?tex=x" /> 为变量的情况下，<img alt="[公式]" src="https://www.zhihu.com/equation?tex=p(x|\theta)" /> 为概率，表示通过已知的分布函数与参数，随机生成出 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=x" /> 的概率；</li>
<li>在 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=\theta" /> 为变量，<img alt="[公式]" src="https://www.zhihu.com/equation?tex=x" /> 已知的情况下，<img alt="[公式]" src="https://www.zhihu.com/equation?tex=p(x|\theta)" /> 为似然函数，它表示对于不同的 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=%5Ctheta" /> ，出现 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=x" /> 的概率是多少。此时可写成 <span class="math-inline">L(\theta\mid x)=p(x\mid\theta)</span>，更严格地，我们也可写成 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=L(\theta|x)%3Dp(x%3B\theta)" /> 。</li>
</ul>
<h3 id="最大似然估计_1">最大似然估计<a class="anchor-link" href="#最大似然估计_1" title="Permanent link">&para;</a></h3>
<p>搞清楚了似然函数，就可以进阶到最大似然估计了。</p>
<p>最大似然估计的思想在于，对于给定的观测数据 <span class="math-inline">x</span>，我们希望能从所有的参数 <span class="math-inline">\theta_1, \theta_2, \cdots, \theta_n </span> 中找出能最大概率生成观测数据的参数 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=\theta^*" /> 作为估计结果。</p>
<p>回到前面所说的似然函数，被估计出的参数 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=\theta^*" /> 应该满足：</p>
<p><img alt="[公式]" src="https://www.zhihu.com/equation?tex=L(\theta^*|x)%3Dp(x|\theta^*)\ge+p(x|\theta)%3DL(\theta|x)%2C\theta%3D\theta_1\cdots%2C\theta_n\" /></p>
<p>那么在实际运算中，我们将待估计的参数 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=%5Ctheta" /> 看成是变量，计算得到生成观测数据 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=x" /> 的概率函数 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=p(x|\theta)" /> ，并找到能最大化概率函数的参数即可：</p>
<p><img alt="[公式]" src="https://www.zhihu.com/equation?tex=\theta^*%3Darg\max_{\theta}p(x|\theta)\" /></p>
<p>而这最大化的步骤通过求导等于0来解得。</p>
<p>给出维基百科的例子来加深理解：</p>
<p><img alt="img" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202209222015537.jpg" /></p>
<h3 id="离散型随机变量的最大似然估计">离散型随机变量的最大似然估计<a class="anchor-link" href="#离散型随机变量的最大似然估计" title="Permanent link">&para;</a></h3>
<p>离散型随机变量 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=X" /> 的分布律为 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=P{X%3Dx}%3Dp(x%3B\theta)" /> ，设 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=X_1%2C⋯%2CX_n" /> 为来自 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=X" /> 的样本，<img alt="[公式]" src="https://www.zhihu.com/equation?tex=x_1%2C⋯%2Cx_n" /> 为相应的观察值，<img alt="[公式]" src="https://www.zhihu.com/equation?tex=%5Ctheta" /> 为待估参数。</p>
<p>在参数 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=%5Ctheta" /> 下，分布函数随机取到<img alt="[公式]" src="https://www.zhihu.com/equation?tex=x_1%2C⋯%2Cx_n" /> 的概率为</p>
<p><img alt="[公式]" src="https://www.zhihu.com/equation?tex=p(x|\theta)%3D\prod_{i%3D1}^{n}p(x_{i}%3B\theta)\" /></p>
<p>构造似然函数：</p>
<p><img alt="image-20211218160315830" src="/Users/geeks_z/Documents/OneDrive/Library/Application%252520Support/typora-user-images/image-20211218160315830.png" /></p>
<p>可知似然函数是一个关于 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=%5Ctheta" /> 的函数，要找到最大概率生成 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=x" /> 的参数，即找到当 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=L(\theta|x)" /> 取最大值时的 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=%5Ctheta" /> 。</p>
<p>求解出最大值，通常的方法就是求导=0：</p>
<p><img alt="[公式]" src="https://www.zhihu.com/equation?tex=\frac{d}{d\theta}L(\theta|x)%3D0\" /></p>
<p>由于式子通常是累乘的形式，我们借助对数函数来简化问题：</p>
<p><img alt="[公式]" src="https://www.zhihu.com/equation?tex=\frac{d}{d\theta}lnL(\theta|x)%3D0\" /></p>
<p>上式也通常被称作<strong>对数似然方程</strong>。如果 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=θ" /> 包含多个参数 <span class="math-inline">\theta_1, \theta_2, \cdots, \theta_k</span>，可对多个参数分别求偏导来连立方程组。下面举一个例子：</p>
<p><img alt="img" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202209222015588.jpg" /></p>
<h3 id="连续型随机变量的最大似然估计">连续型随机变量的最大似然估计<a class="anchor-link" href="#连续型随机变量的最大似然估计" title="Permanent link">&para;</a></h3>
<p>连续型随机变量 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=X" /> 的概率密度为 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=f(x%3B\theta)" /> ，设 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=X_1%2C⋯%2CX_n" /> 为来自 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=X" /> 的样本，<img alt="[公式]" src="https://www.zhihu.com/equation?tex=x_1%2C⋯%2Cx_n" />为相应的观察值，同样地，<img alt="[公式]" src="https://www.zhihu.com/equation?tex=%5Ctheta" /> 为待估参数。</p>
<p>概率密度的图像与横轴所围成的面积大小代表了概率的大小，当随机变量<span class="math-inline">X</span> 到了某一个值 <span class="math-inline">x_1</span>，可看做是选取到了 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=f(x_1%3B\theta)" /> 与 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=dx" /> 所围成的小矩形。如图所示：</p>
<p><img alt="img" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202209222015604.jpg" /></p>
<p>接着与离散型随机变量类似，随机取到观察值 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=x" /> 的概率为：</p>
<p><img alt="[公式]" src="https://www.zhihu.com/equation?tex=p(x%3B\theta)%3D\prod_{i%3D1}^{n}f(x_{i}%3B\theta)dx\" /></p>
<p>构造似然函数：</p>
<p><img alt="[公式]" src="https://www.zhihu.com/equation?tex=L(\theta|x)%3D\prod_{i%3D1}^{n}f(x_{i}%3B\theta)dx\" /></p>
<p>由于<img alt="image-20211218160651232" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202209222015626.png" />不随参数变化，故我们选择忽略，似然函数变为：</p>
<p><img alt="[公式]" src="https://www.zhihu.com/equation?tex=L(\theta|x)%3D\prod_{i%3D1}^{n}f(x_{i}%3B\theta)\" /></p>
<p>接着计算步骤和离散型类似，取对数求导等于0。例如：</p>
<p><img alt="img" src="https://markdownimg-hw.oss-cn-beijing.aliyuncs.com/image202209222015294.jpg" /></p>
<p>Reference </p>
<ul>
<li>https://zhuanlan.zhihu.com/p/55791843</li>
<li>《概率论与数理统计》第四版</li>
<li><a href="https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0">维基百科-似然性</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//www.cnblogs.com/shixisheng/p/7136890.html">最大似然估计和最大后验估计（转） - 段子手实习生 - 博客园</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/u011508640/article/details/72815981">详解最大似然估计（MLE）、最大后验概率估计（MAP），以及贝叶斯公式的理解 - nebulaf91的博客 - CSDN博客</a></li>
<li><a href="https://www.zhihu.com/question/54082000/answer/145495695">如何理解似然函数?</a></li>
</ul>
<h2 id="贝叶斯估计">贝叶斯估计<a class="anchor-link" href="#贝叶斯估计" title="Permanent link">&para;</a></h2>
<blockquote>
<p>Bayesian Estimation</p>
</blockquote>
<p>贝叶斯估计实质： 贝叶斯决策来决策参数的取值。与最大似然估计同为概率密度估计中的主要参数估计方法，结果多数情况下与最大似然估计相同</p>
<p>区别：</p>
<ul>
<li>最大似然估计把待估计的参数当作未知但固定的量</li>
<li>贝叶斯估计把待估计的参数也看为随机变量</li>
</ul>
<p><span class="math-inline">R(\hat{\theta}|x )</span> 给定<span class="math-inline">x</span> 件下估计量<span class="math-inline">\hat{\theta}</span> 期望损失，称为条件风险。 如果<span class="math-inline">\theta</span> 估计量<span class="math-inline">\hat{\theta}</span> 得条件风险最小，则称<span class="math-inline">\hat{\theta}</span> <span class="math-inline">{\theta}</span> 贝叶斯估计量</p>
                </div>

                <!-- Comments Section (Giscus) -->
                <section class="comments-section">
                    <h3><i class="fas fa-comments"></i> 评论</h3>
                    <script src="https://giscus.app/client.js"
                        data-repo="Geeks-Z/Geeks-Z.github.io"
                        data-repo-id=""
                        data-category="Announcements"
                        data-category-id=""
                        data-mapping="pathname"
                        data-strict="0"
                        data-reactions-enabled="1"
                        data-emit-metadata="0"
                        data-input-position="bottom"
                        data-theme="preferred_color_scheme"
                        data-lang="zh-CN"
                        crossorigin="anonymous"
                        async>
                    </script>
                </section>
            </article>

            <footer class="post-footer">
                <p>© 2025 Hongwei Zhao. Built with ❤️</p>
            </footer>
        </main>
    </div>

    <!-- Theme Toggle Button -->
    <button class="theme-toggle" id="themeToggle" aria-label="切换主题">
        <i class="fas fa-moon"></i>
    </button>

    <script>
        // Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        const html = document.documentElement;
        const icon = themeToggle.querySelector('i');
        const hljsDark = document.getElementById('hljs-theme-dark');
        const hljsLight = document.getElementById('hljs-theme-light');
        
        // Check saved theme or system preference
        const savedTheme = localStorage.getItem('theme');
        const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
        
        if (savedTheme === 'dark' || (!savedTheme && prefersDark)) {
            html.setAttribute('data-theme', 'dark');
            icon.className = 'fas fa-sun';
            hljsDark.disabled = false;
            hljsLight.disabled = true;
        }
        
        themeToggle.addEventListener('click', () => {
            const isDark = html.getAttribute('data-theme') === 'dark';
            if (isDark) {
                html.removeAttribute('data-theme');
                icon.className = 'fas fa-moon';
                localStorage.setItem('theme', 'light');
                hljsDark.disabled = true;
                hljsLight.disabled = false;
            } else {
                html.setAttribute('data-theme', 'dark');
                icon.className = 'fas fa-sun';
                localStorage.setItem('theme', 'dark');
                hljsDark.disabled = false;
                hljsLight.disabled = true;
            }
            
            // Update Giscus theme
            const giscusFrame = document.querySelector('iframe.giscus-frame');
            if (giscusFrame) {
                giscusFrame.contentWindow.postMessage({
                    giscus: {
                        setConfig: {
                            theme: isDark ? 'light' : 'dark'
                        }
                    }
                }, 'https://giscus.app');
            }
        });
        
        // Highlight.js
        document.addEventListener('DOMContentLoaded', () => {
            hljs.highlightAll();
        });
        
        // KaTeX auto-render
        document.addEventListener('DOMContentLoaded', () => {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
        
        // TOC active state
        const tocLinks = document.querySelectorAll('.toc-container a');
        const headings = document.querySelectorAll('.post-content h2, .post-content h3, .post-content h4');
        
        function updateTocActive() {
            let current = '';
            headings.forEach(heading => {
                const rect = heading.getBoundingClientRect();
                if (rect.top <= 100) {
                    current = heading.getAttribute('id');
                }
            });
            
            tocLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === '#' + current) {
                    link.classList.add('active');
                }
            });
        }
        
        window.addEventListener('scroll', updateTocActive);
        updateTocActive();
    </script>
</body>
</html>
